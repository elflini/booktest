<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Binary and Categorical data Analysis | Bio-Health Data Analysis</title>
  <meta name="description" content="This is lecture for the bio-health data analysis" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Binary and Categorical data Analysis | Bio-Health Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is lecture for the bio-health data analysis" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Binary and Categorical data Analysis | Bio-Health Data Analysis" />
  
  <meta name="twitter:description" content="This is lecture for the bio-health data analysis" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2022-08-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Chapter3.html"/>
<link rel="next" href="Chapter5.html"/>
<script src="libs/header-attrs-2.15/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bio-Health Data Analysis</a></li>

<li class="divider"></li>
<li><a href="index.html#머리말" id="toc-머리말">머리말<span></span></a></li>
<li><a href="chapter1.html#chapter1" id="toc-chapter1"><span class="toc-section-number">1</span> Introduction to Claims Data<span></span></a>
<ul>
<li><a href="chapter1.html#국내-보건의료-빅데이터" id="toc-국내-보건의료-빅데이터"><span class="toc-section-number">1.1</span> 국내 보건의료 빅데이터<span></span></a></li>
<li><a href="chapter1.html#건강보험-청구자료" id="toc-건강보험-청구자료"><span class="toc-section-number">1.2</span> 건강보험 청구자료<span></span></a>
<ul>
<li><a href="chapter1.html#건강보험심사평가원" id="toc-건강보험심사평가원">건강보험심사평가원<span></span></a></li>
<li><a href="chapter1.html#건강보험공단" id="toc-건강보험공단">건강보험공단<span></span></a></li>
</ul></li>
<li><a href="chapter1.html#청구자료-구성-요소" id="toc-청구자료-구성-요소"><span class="toc-section-number">1.3</span> 청구자료 구성 요소<span></span></a></li>
<li><a href="chapter1.html#건강보험청구자료-활용" id="toc-건강보험청구자료-활용"><span class="toc-section-number">1.4</span> 건강보험청구자료 활용<span></span></a>
<ul>
<li><a href="chapter1.html#활용-예시---명세서-기반" id="toc-활용-예시---명세서-기반"><span class="toc-section-number">1.4.1</span> 활용 예시 - 명세서 기반<span></span></a></li>
<li><a href="chapter1.html#활용-예시---환자-기반" id="toc-활용-예시---환자-기반"><span class="toc-section-number">1.4.2</span> 활용 예시 - 환자 기반<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="Chapter2.html#Chapter2" id="toc-Chapter2"><span class="toc-section-number">2</span> Observational Study<span></span></a>
<ul>
<li><a href="Chapter2.html#case-control-study" id="toc-case-control-study"><span class="toc-section-number">2.1</span> Case-Control study<span></span></a>
<ul>
<li><a href="Chapter2.html#환자군-선정" id="toc-환자군-선정"><span class="toc-section-number">2.1.1</span> 환자군 선정<span></span></a></li>
<li><a href="Chapter2.html#대조군-선정" id="toc-대조군-선정"><span class="toc-section-number">2.1.2</span> 대조군 선정<span></span></a></li>
<li><a href="Chapter2.html#비교성-확보를-위한-짝짓기-수행" id="toc-비교성-확보를-위한-짝짓기-수행"><span class="toc-section-number">2.1.3</span> 비교성 확보를 위한 짝짓기 수행<span></span></a></li>
<li><a href="Chapter2.html#과거-정보-수집" id="toc-과거-정보-수집"><span class="toc-section-number">2.1.4</span> 과거 정보 수집<span></span></a></li>
<li><a href="Chapter2.html#기본-특징-분석" id="toc-기본-특징-분석"><span class="toc-section-number">2.1.5</span> 기본 특징 분석<span></span></a></li>
<li><a href="Chapter2.html#결과변수와-중재-노출간의-관련성-파악" id="toc-결과변수와-중재-노출간의-관련성-파악"><span class="toc-section-number">2.1.6</span> 결과변수와 중재 노출간의 관련성 파악<span></span></a></li>
<li><a href="Chapter2.html#환자-대조군-연구의-장점과-단점" id="toc-환자-대조군-연구의-장점과-단점"><span class="toc-section-number">2.1.7</span> 환자-대조군 연구의 장점과 단점<span></span></a></li>
</ul></li>
<li><a href="Chapter2.html#cohort-study" id="toc-cohort-study"><span class="toc-section-number">2.2</span> Cohort study<span></span></a>
<ul>
<li><a href="Chapter2.html#코호트-선정" id="toc-코호트-선정"><span class="toc-section-number">2.2.1</span> 코호트 선정<span></span></a></li>
<li><a href="Chapter2.html#중재-노출-정의" id="toc-중재-노출-정의"><span class="toc-section-number">2.2.2</span> 중재 노출 정의<span></span></a></li>
<li><a href="Chapter2.html#결과변수-정의" id="toc-결과변수-정의"><span class="toc-section-number">2.2.3</span> 결과변수 정의<span></span></a></li>
<li><a href="Chapter2.html#교란요인-정의" id="toc-교란요인-정의"><span class="toc-section-number">2.2.4</span> 교란요인 정의<span></span></a></li>
<li><a href="Chapter2.html#기본-특징-분석-1" id="toc-기본-특징-분석-1"><span class="toc-section-number">2.2.5</span> 기본 특징 분석<span></span></a></li>
<li><a href="Chapter2.html#인과성-평가" id="toc-인과성-평가"><span class="toc-section-number">2.2.6</span> 인과성 평가<span></span></a></li>
<li><a href="Chapter2.html#코호트-연구의-장점과-단점" id="toc-코호트-연구의-장점과-단점"><span class="toc-section-number">2.2.7</span> 코호트 연구의 장점과 단점<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="Chapter3.html#Chapter3" id="toc-Chapter3"><span class="toc-section-number">3</span> Medical Cost Data Analysis<span></span></a>
<ul>
<li><a href="Chapter3.html#health-care-cost" id="toc-health-care-cost"><span class="toc-section-number">3.1</span> health care cost<span></span></a></li>
<li><a href="Chapter3.html#regression-analysis-with-log-transformation" id="toc-regression-analysis-with-log-transformation"><span class="toc-section-number">3.2</span> Regression analysis with log transformation<span></span></a>
<ul>
<li><a href="Chapter3.html#general-linear-model" id="toc-general-linear-model"><span class="toc-section-number">3.2.1</span> General linear model<span></span></a></li>
<li><a href="Chapter3.html#log-cost-regression-model" id="toc-log-cost-regression-model"><span class="toc-section-number">3.2.2</span> Log cost regression model<span></span></a></li>
</ul></li>
<li><a href="Chapter3.html#gamma-generalized-linear-models" id="toc-gamma-generalized-linear-models"><span class="toc-section-number">3.3</span> Gamma generalized Linear Models<span></span></a>
<ul>
<li><a href="Chapter3.html#generalized-linear-models-glms" id="toc-generalized-linear-models-glms"><span class="toc-section-number">3.3.1</span> Generalized linear models (GLMs)<span></span></a></li>
<li><a href="Chapter3.html#gamma-generalized-linear-models-1" id="toc-gamma-generalized-linear-models-1"><span class="toc-section-number">3.3.2</span> Gamma generalized linear models<span></span></a></li>
</ul></li>
<li><a href="Chapter3.html#transformation-vs.-glms" id="toc-transformation-vs.-glms"><span class="toc-section-number">3.4</span> Transformation vs. GLMs<span></span></a></li>
<li><a href="Chapter3.html#appendix" id="toc-appendix"><span class="toc-section-number">3.5</span> Appendix<span></span></a>
<ul>
<li><a href="Chapter3.html#exponential-family" id="toc-exponential-family"><span class="toc-section-number">3.5.1</span> Exponential Family<span></span></a></li>
<li><a href="Chapter3.html#canonical-links" id="toc-canonical-links"><span class="toc-section-number">3.5.2</span> Canonical Links<span></span></a></li>
<li><a href="Chapter3.html#estimation-of-the-model-parameters" id="toc-estimation-of-the-model-parameters"><span class="toc-section-number">3.5.3</span> Estimation of the Model Parameters<span></span></a></li>
<li><a href="Chapter3.html#distribution-and-link-function" id="toc-distribution-and-link-function"><span class="toc-section-number">3.5.4</span> Distribution and Link Function<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="Chapter4.html#Chapter4" id="toc-Chapter4"><span class="toc-section-number">4</span> Binary and Categorical data Analysis<span></span></a>
<ul>
<li><a href="Chapter4.html#introduction" id="toc-introduction"><span class="toc-section-number">4.1</span> Introduction<span></span></a></li>
<li><a href="Chapter4.html#binary-outcomes" id="toc-binary-outcomes"><span class="toc-section-number">4.2</span> Binary Outcomes<span></span></a>
<ul>
<li><a href="Chapter4.html#two-way-tables" id="toc-two-way-tables"><span class="toc-section-number">4.2.1</span> Two-Way Tables<span></span></a></li>
</ul></li>
<li><a href="Chapter4.html#linear-regression-with-a-binary-outcome" id="toc-linear-regression-with-a-binary-outcome"><span class="toc-section-number">4.3</span> Linear Regression with a Binary Outcome<span></span></a></li>
<li><a href="Chapter4.html#logistic-regression" id="toc-logistic-regression"><span class="toc-section-number">4.4</span> Logistic Regression<span></span></a></li>
<li><a href="Chapter4.html#interpretation-of-a-logistic-regression" id="toc-interpretation-of-a-logistic-regression"><span class="toc-section-number">4.5</span> Interpretation of a Logistic Regression<span></span></a>
<ul>
<li><a href="Chapter4.html#a-single-binary-covariate" id="toc-a-single-binary-covariate"><span class="toc-section-number">4.5.1</span> A Single Binary Covariate<span></span></a></li>
<li><a href="Chapter4.html#the-general-case" id="toc-the-general-case"><span class="toc-section-number">4.5.2</span> The General Case<span></span></a></li>
</ul></li>
<li><a href="Chapter4.html#interpretation-on-the-probability-scale" id="toc-interpretation-on-the-probability-scale"><span class="toc-section-number">4.6</span> Interpretation on the Probability Scale<span></span></a>
<ul>
<li><a href="Chapter4.html#estimating-probabilities" id="toc-estimating-probabilities"><span class="toc-section-number">4.6.1</span> Estimating Probabilities<span></span></a></li>
<li><a href="Chapter4.html#marginal-effects" id="toc-marginal-effects"><span class="toc-section-number">4.6.2</span> Marginal Effects<span></span></a></li>
</ul></li>
<li><a href="Chapter4.html#model-building-and-assessment" id="toc-model-building-and-assessment"><span class="toc-section-number">4.7</span> Model Building and Assessment<span></span></a>
<ul>
<li><a href="Chapter4.html#model-comparison-aic-and-bic" id="toc-model-comparison-aic-and-bic"><span class="toc-section-number">4.7.1</span> Model Comparison: AIC and BIC<span></span></a></li>
<li><a href="Chapter4.html#model-calibration-hosmerlemeshow-test" id="toc-model-calibration-hosmerlemeshow-test"><span class="toc-section-number">4.7.2</span> Model Calibration: Hosmer–Lemeshow Test<span></span></a></li>
<li><a href="Chapter4.html#model-prediction-roc-and-auc" id="toc-model-prediction-roc-and-auc"><span class="toc-section-number">4.7.3</span> Model Prediction: ROC and AUC<span></span></a></li>
</ul></li>
<li><a href="Chapter4.html#multinomial-regression" id="toc-multinomial-regression"><span class="toc-section-number">4.8</span> Multinomial Regression<span></span></a>
<ul>
<li><a href="Chapter4.html#an-extension-of-logistic-regression" id="toc-an-extension-of-logistic-regression"><span class="toc-section-number">4.8.1</span> An Extension of Logistic Regression<span></span></a></li>
<li><a href="Chapter4.html#marginal-effects-1" id="toc-marginal-effects-1"><span class="toc-section-number">4.8.2</span> Marginal Effects<span></span></a></li>
<li><a href="Chapter4.html#ordered-multinomial-regression" id="toc-ordered-multinomial-regression"><span class="toc-section-number">4.8.3</span> Ordered Multinomial Regression<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="Chapter5.html#Chapter5" id="toc-Chapter5"><span class="toc-section-number">5</span> Count Data Analysis<span></span></a>
<ul>
<li><a href="Chapter5.html#poisson-regression" id="toc-poisson-regression"><span class="toc-section-number">5.1</span> Poisson Regression<span></span></a></li>
<li><a href="Chapter5.html#example-danish-lung-cancer-counts" id="toc-example-danish-lung-cancer-counts"><span class="toc-section-number">5.2</span> Example: Danish Lung Cancer Counts<span></span></a></li>
<li><a href="Chapter5.html#what-about-accounting-for-population-size" id="toc-what-about-accounting-for-population-size"><span class="toc-section-number">5.3</span> What About Accounting for Population Size?<span></span></a></li>
</ul></li>
<li><a href="Chapter6.html#Chapter6" id="toc-Chapter6"><span class="toc-section-number">6</span> Survival Analysis<span></span></a>
<ul>
<li><a href="Chapter6.html#생존분석" id="toc-생존분석"><span class="toc-section-number">6.1</span> 생존분석<span></span></a></li>
<li><a href="Chapter6.html#비모수적-생존함수-추정" id="toc-비모수적-생존함수-추정"><span class="toc-section-number">6.2</span> 비모수적 생존함수 추정<span></span></a>
<ul>
<li><a href="Chapter6.html#생명표-방법" id="toc-생명표-방법"><span class="toc-section-number">6.2.1</span> 생명표 방법<span></span></a></li>
<li><a href="Chapter6.html#누적한계추정법" id="toc-누적한계추정법"><span class="toc-section-number">6.2.2</span> 누적한계추정법<span></span></a></li>
<li><a href="Chapter6.html#생존분포-비교" id="toc-생존분포-비교"><span class="toc-section-number">6.2.3</span> 생존분포 비교<span></span></a></li>
</ul></li>
<li><a href="Chapter6.html#모수적-생존함수-추정과-회귀모형" id="toc-모수적-생존함수-추정과-회귀모형"><span class="toc-section-number">6.3</span> 모수적 생존함수 추정과 회귀모형<span></span></a>
<ul>
<li><a href="Chapter6.html#지수분포" id="toc-지수분포"><span class="toc-section-number">6.3.1</span> 지수분포<span></span></a></li>
<li><a href="Chapter6.html#와이블분포" id="toc-와이블분포"><span class="toc-section-number">6.3.2</span> 와이블분포<span></span></a></li>
<li><a href="Chapter6.html#감마분포" id="toc-감마분포"><span class="toc-section-number">6.3.3</span> 감마분포<span></span></a></li>
<li><a href="Chapter6.html#로그정규분포" id="toc-로그정규분포"><span class="toc-section-number">6.3.4</span> 로그정규분포<span></span></a></li>
<li><a href="Chapter6.html#로그로지스틱분포" id="toc-로그로지스틱분포"><span class="toc-section-number">6.3.5</span> 로그로지스틱분포<span></span></a></li>
<li><a href="Chapter6.html#곰페르츠분포" id="toc-곰페르츠분포"><span class="toc-section-number">6.3.6</span> 곰페르츠분포<span></span></a></li>
<li><a href="Chapter6.html#회귀모형aft" id="toc-회귀모형aft"><span class="toc-section-number">6.3.7</span> 회귀모형(AFT)<span></span></a></li>
</ul></li>
<li><a href="Chapter6.html#cox-비례위험모형" id="toc-cox-비례위험모형"><span class="toc-section-number">6.4</span> Cox 비례위험모형<span></span></a>
<ul>
<li><a href="Chapter6.html#시간에-따라-변화하는-공변량이-있을-경우" id="toc-시간에-따라-변화하는-공변량이-있을-경우"><span class="toc-section-number">6.4.1</span> 시간에 따라 변화하는 공변량이 있을 경우<span></span></a></li>
</ul></li>
<li><a href="Chapter6.html#경쟁위험모형" id="toc-경쟁위험모형"><span class="toc-section-number">6.5</span> 경쟁위험모형<span></span></a>
<ul>
<li><a href="Chapter6.html#원인별-누적발생함수" id="toc-원인별-누적발생함수"><span class="toc-section-number">6.5.1</span> 원인별 누적발생함수<span></span></a></li>
<li><a href="Chapter6.html#원인별-위험함수" id="toc-원인별-위험함수"><span class="toc-section-number">6.5.2</span> 원인별 위험함수<span></span></a></li>
<li><a href="Chapter6.html#누적발생함수에-대한-회귀분석" id="toc-누적발생함수에-대한-회귀분석"><span class="toc-section-number">6.5.3</span> 누적발생함수에 대한 회귀분석<span></span></a></li>
</ul></li>
<li><a href="Chapter6.html#sas-실습" id="toc-sas-실습"><span class="toc-section-number">6.6</span> SAS 실습<span></span></a>
<ul>
<li><a href="Chapter6.html#생명표-방법-1" id="toc-생명표-방법-1"><span class="toc-section-number">6.6.1</span> 생명표 방법<span></span></a></li>
<li><a href="Chapter6.html#누적한계추정법-1" id="toc-누적한계추정법-1"><span class="toc-section-number">6.6.2</span> 누적한계추정법<span></span></a></li>
<li><a href="Chapter6.html#생존분포-비교-1" id="toc-생존분포-비교-1"><span class="toc-section-number">6.6.3</span> 생존분포 비교<span></span></a></li>
<li><a href="Chapter6.html#parametric-model" id="toc-parametric-model"><span class="toc-section-number">6.6.4</span> Parametric Model<span></span></a></li>
<li><a href="Chapter6.html#aft-model" id="toc-aft-model"><span class="toc-section-number">6.6.5</span> AFT model<span></span></a></li>
<li><a href="Chapter6.html#cox-비례위험모형-1" id="toc-cox-비례위험모형-1"><span class="toc-section-number">6.6.6</span> Cox 비례위험모형<span></span></a></li>
<li><a href="Chapter6.html#cox-비례위험모형---time-dependent-covariate" id="toc-cox-비례위험모형---time-dependent-covariate"><span class="toc-section-number">6.6.7</span> Cox 비례위험모형 - time dependent covariate<span></span></a></li>
<li><a href="Chapter6.html#competing-risks" id="toc-competing-risks"><span class="toc-section-number">6.6.8</span> Competing Risks<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="Chapter7.html#Chapter7" id="toc-Chapter7"><span class="toc-section-number">7</span> propensity score Methods<span></span></a>
<ul>
<li><a href="Chapter7.html#observational-study" id="toc-observational-study"><span class="toc-section-number">7.1</span> Observational Study?<span></span></a></li>
<li><a href="Chapter7.html#propensity-score" id="toc-propensity-score"><span class="toc-section-number">7.2</span> Propensity Score?<span></span></a></li>
<li><a href="Chapter7.html#propensity-score-methods" id="toc-propensity-score-methods"><span class="toc-section-number">7.3</span> Propensity Score Methods<span></span></a>
<ul>
<li><a href="Chapter7.html#propensity-score-matching" id="toc-propensity-score-matching"><span class="toc-section-number">7.3.1</span> Propensity Score Matching<span></span></a></li>
<li><a href="Chapter7.html#stratification-using-propensity-score" id="toc-stratification-using-propensity-score"><span class="toc-section-number">7.3.2</span> Stratification using Propensity Score<span></span></a></li>
<li><a href="Chapter7.html#inverse-probability-of-treatment-weighting" id="toc-inverse-probability-of-treatment-weighting"><span class="toc-section-number">7.3.3</span> Inverse Probability of Treatment Weighting<span></span></a></li>
<li><a href="Chapter7.html#propensity-score-adjustment-as-covariate" id="toc-propensity-score-adjustment-as-covariate"><span class="toc-section-number">7.3.4</span> Propensity Score Adjustment as Covariate<span></span></a></li>
<li><a href="Chapter7.html#property-of-propensity-score" id="toc-property-of-propensity-score"><span class="toc-section-number">7.3.5</span> Property of Propensity Score<span></span></a></li>
</ul></li>
<li><a href="Chapter7.html#practice---sas" id="toc-practice---sas"><span class="toc-section-number">7.4</span> Practice - SAS<span></span></a>
<ul>
<li><a href="Chapter7.html#greedy-nearest-neighbor-matching" id="toc-greedy-nearest-neighbor-matching"><span class="toc-section-number">7.4.1</span> Greedy Nearest Neighbor Matching<span></span></a></li>
<li><a href="Chapter7.html#propensity-score-stratification" id="toc-propensity-score-stratification"><span class="toc-section-number">7.4.2</span> Propensity Score Stratification<span></span></a></li>
<li><a href="Chapter7.html#inverse-probability-treatment-weighting" id="toc-inverse-probability-treatment-weighting"><span class="toc-section-number">7.4.3</span> Inverse Probability Treatment Weighting<span></span></a></li>
<li><a href="Chapter7.html#matching-with-existing-propensity-scores" id="toc-matching-with-existing-propensity-scores"><span class="toc-section-number">7.4.4</span> Matching with Existing propensity scores<span></span></a></li>
<li><a href="Chapter7.html#matching-with-existing-propensity-scores-1" id="toc-matching-with-existing-propensity-scores-1"><span class="toc-section-number">7.4.5</span> Matching with Existing propensity scores<span></span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bio-Health Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Chapter4" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Binary and Categorical data Analysis<a href="Chapter4.html#Chapter4" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!---------- Chapter 4 Binary and Categorical Outcomes-------------------->
<ul>
<li><p>This chapter builds a regression framework for binary and categorical outcomes, where the goal is to determine how the distribution of the outcome depends on covariates.</p></li>
<li><p>The most commonly used model for binary outcomes is logistic regression.</p>
<ul>
<li><p>Logistic regression coefficients are directly interpretable as relative odds of a positive outcome corresponding to changes in covariate values.</p></li>
<li><p>For categorical outcomes, we introduce multinomial regression, which produces coefficients that are interpretable as a multi-category version of relative odds.</p></li>
<li><p>For both outcomes, we introduce marginal effects as a way to estimate effects of covariates on outcome probabilities rather than odds.</p></li>
</ul></li>
</ul>
<div id="introduction" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction<a href="Chapter4.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Binary and categorical outcomes arise in many health services and health outcome research studies.</p></li>
<li><p>Examples of binary outcomes</p>
<ul>
<li><p>presence or absence of certain behaviors or conditions, such as smoking, wearing a seatbelt, or having diabetes.</p></li>
<li><p>Occurrence or nonoccurrence of disease events, such as hospital re-admission after surgery, in-hospital mortality, or cancer progression after primary treatment.</p></li>
</ul></li>
<li><p>Categorical or multinomial outcomes have more than two choices.</p></li>
<li><p>Example</p>
<ul>
<li>body mass index (BMI) can be categorized as underweight, normal weight, overweight, or obese.</li>
<li>While categorical outcomes like BMI have a logical order, outcomes like race/ethnicity have no natural ordering.</li>
</ul></li>
<li><p>Unlike continuous outcomes, categorical outcomes do not have a default numerical scale.</p></li>
<li><p>Consequently, standard statistical summaries such as the mean, median, or variance are not meaningful.</p></li>
<li><p>Even if the outcome is coded using numbers, these should not be interpreted quantitatively.</p>
<ul>
<li>Example, a survey may ask respondents to report their health status by choosing 1 (excellent), 2 (very good), 3 (good), 4 (fair), or 5 (poor).</li>
</ul></li>
</ul>
</div>
<div id="binary-outcomes" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Binary Outcomes<a href="Chapter4.html#binary-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>A binary outcome, <span class="math inline">\(Y\)</span> , can take one of two possible values that we can generically label positive or negative and denote as 1 or 0, respectively.</p></li>
<li><p>Example, in a study of 10-day re-admissions after hospitalization</p>
<ul>
<li>Readmitted (<span class="math inline">\(Y=1\)</span>) or not (<span class="math inline">\(Y=0\)</span>)</li>
</ul></li>
<li><p>Example, in a study of chronic disease</p>
<ul>
<li>Have diabetes (<span class="math inline">\(Y=1\)</span>) or not (<span class="math inline">\(Y=0\)</span>);</li>
</ul></li>
<li><p>Example, in a study of medical costs in a given year</p>
<ul>
<li>Have positive (<span class="math inline">\(Y=1\)</span>) or zero (<span class="math inline">\(Y=0\)</span>) medical expenditures.</li>
</ul></li>
<li><p>We can think of a binary outcome as a random variable with distribution comprised of the probabilities that <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span>. (This is called a Bernoulli distribution)</p></li>
<li><p>Because these two probabilities must sum to one, the distribution is completely determined by one of the two probabilities, which is conventionally taken to be the probability of a positive outcome, <span class="math inline">\(P(Y=1)\)</span>.</p></li>
<li><p>When considering the association between a covariate and a binary outcome, therefore, we need simply to understand how this probability is associated with the covariate.</p></li>
<li><p>Regardless of the application setting, we can observe that the mean of the binary outcome <span class="math inline">\(Y\)</span> is the same as the proportion of positive outcomes, or <span class="math inline">\(E(Y)=P(Y=1)\)</span>.</p></li>
<li><p>As a practical example, we will examine how the proportion of obese persons changes over time and by age.</p>
<ul>
<li>We will use data from the National Health and Nutrition Examination Survey (NHANES), which is the most authoritative source of data on the body mass index (BMI) in the United States.</li>
</ul></li>
</ul>
<div id="two-way-tables" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Two-Way Tables<a href="Chapter4.html#two-way-tables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The National Health and Nutrituion Examination Survey (NHANES) dataset include 6864 individuals age 20–59 years: 3046 from the 1999–2000 survey and 3818 from the 2015–2016 survey.</p></li>
<li><p>Mean BMI increased from 28.4 kg/m2 in 1999–2000 to 29.6 kg/m2 in 2015–2016, an increase of 1.2 units. This might not be a public health concern if it were due to changes in BMI within the underweight or normal weight groups.</p>
<ul>
<li>However, it would be concerning if it indicated an increase in overweight and obese persons as this would affect the incidence of chronic diseases, such as heart failure and diabetes.</li>
</ul></li>
</ul>
<p><img src="fig4/table4.1.png" />
*Statistics for Health Data Science, Ruth et al., Springer</p>
<ul>
<li><p>The two-way table shown in Table presents the frequency of obese and non-obese persons for the two calendar years. The table shows an increase in the proportion obese from 32.5% to 40.4% in 15 years.</p></li>
<li><p>The absolute 8% increase has the same meaning regardless of the baseline proportion in 1999–2000. Sometimes the relative increase is reported, in which case the baseline proportion matters.</p></li>
<li><p>For example, when the baseline proportion is 5%, a 10% relative increase translates into an absolute 0.5% increase; when the baseline proportion is 50%, a 10% relative increase translates into an absolute 5% increase.</p></li>
<li><p>The relative increase is often expressed using a risk ratio or relative risk (RR). In the obesity example, the RR for obesity in 2015–2016 relative to 1999–2000 is calculated as:</p></li>
</ul>
<p><span class="math display">\[
RR = \frac{Proportion \,\,\, obese \,\,\, in \,\,\, 2015-2016}{Proportion \,\,\, obese \,\,\, in \,\,\, 1999-2000} = \frac{0.404}{0.325} = 1.24.
\]</span></p>
<ul>
<li><p>An RR close to 1 means there is little relative change; an RR that is substantially smaller or larger than 1 means there is a substantial relative change.</p></li>
<li><p>In this example, the RR of 1.24 means the proportion obese in 2015–2016 represented a 24% increase relative to 1999–2000.</p></li>
<li><p>The change in RR across values of a covariate (calendar year in this example) measures the strength of association between the outcome and that covariate.</p></li>
<li><p>An alternative measure of association is the odds ratio or relative odds (OR).</p></li>
<li><p>The “odds” is a well-known quantity in gambling. It is itself a ratio, namely, the probability of the positive outcome divided by the probability of the negative outcome, i.e., <span class="math inline">\(P(Y=1)/P(Y=0)\)</span>.</p></li>
<li><p>When the covariate is binary, the OR is simply the ratio of the odds of the outcome in each group. In our obesity example, calendar year is a binary covariate, and the OR is defined as:</p></li>
</ul>
<p><span class="math display">\[
OR=\frac{(Proportion\,\,\, obese \,\,\,in \,\,\,2015–2016)/(Proportion\,\,\, not \,\,\, obese \,\,\,in \,\,\,2015–2016)} {(Proportion\,\,\, obese\,\,\, in\,\,\, 1999–2000)/(Proportion\,\,\, not\,\,\, obese\,\,\, in\,\,\, 1999–2000)}.
\]</span></p>
<ul>
<li>Calculation of the OR from the two-way table is simple; it is just the product of the diagonal elements divided by the product of the off-diagonal elements:</li>
</ul>
<p><span class="math display">\[
OR=\frac{(Number\,\,\, not\,\,\, obese\,\,\, in\,\,\, 1999–2000) \times (Number\,\,\, obese\,\,\, in\,\,\, 2015–2016)} {(Number\,\,\, obese\,\,\,\ in\,\,\, 1999–2000) \times (Number\,\,\, not\,\,\, obese\,\,\, in\,\,\, 2015–2016)}
\]</span></p>
<p><span class="math display">\[
=\frac{1899 \times 1469}{914 \times 2165}=1.41
\]</span></p>
<ul>
<li><p>The OR is symmetric, with the roles of the outcome and the covariate interchangeable.</p></li>
<li><p>While the RR is intuitive and readily interpretable, the OR is neither.</p></li>
<li><p>However, as we will see, the OR is what is typically modeled. Consequently, it is helpful to observe a few properties of the OR:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>The OR indicates the same direction of association as the RR. If there is no association between the outcome and covariate, both the OR and RR equal 1. Also, the OR &gt; 1 whenever the RR &gt; 1, and the OR &lt; 1 whenever the RR &lt; 1.</p></li>
<li><p>If the outcome is associated with the covariate, the OR is always farther from 1 than the RR. For example, if OR 1.41, we know that 1 &lt; RR &lt; 1.41.</p></li>
<li><p>For rare positive outcomes, the OR and the RR are similar. Since odds <span class="math inline">\(P(Y=1)/P(Y=0)\)</span> is close to <span class="math inline">\(P(Y= 1)\)</span> when <span class="math inline">\(P(Y=0)\)</span> is close to 1, the ratio of odds is close to the ratio of probabilities.</p></li>
</ol>
<ul>
<li><p>The third property is handy because it means we can interpret the OR as being approximately equal to the RR when positive outcomes are uncommon. Unfortunately, this is not true when positive outcomes are common.</p></li>
<li><p>When <span class="math inline">\(P(Y=1)\)</span> is non-trivial, <span class="math inline">\(P(Y=0)\)</span> is not close to 1, and this can make the OR and RR dissimilar.</p></li>
</ul>
<p><img src="fig4/table4.2.png" /></p>
<p>Table 4.2 shows the RR and the OR for different instances of a setting with a binary covariate <span class="math inline">\(X\)</span>.</p>
<ul>
<li><p>When the probabilities of a positive outcome are very small, the OR is close to the RR.</p></li>
<li><p>But when the probabilities of positive outcomes are not very small, the OR is not close to the RR.</p></li>
</ul>
</div>
</div>
<div id="linear-regression-with-a-binary-outcome" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Linear Regression with a Binary Outcome<a href="Chapter4.html#linear-regression-with-a-binary-outcome" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Can we use a linear regression to examine the association between a binary outcome and a covariate?</li>
</ul>
<p><img src="fig4/fig4.1.png" /></p>
<ul>
<li><p>Figure 4.1 shows a scatterplot of obesity status versus age in the NHANES sample data.</p>
<ul>
<li>To avoid “overplotting” points with the same age and obesity status, the size of each point is proportional to the number of observations.</li>
</ul></li>
<li><p>While a linear regression line can technically be calculated, it is not immediately clear what it represents.</p></li>
<li><p>Thinking of each age as defining a sub-population, we can try to model the proportion of obese persons across sub-populations.</p></li>
<li><p>Noting that the proportion of obese persons at each age is just the mean of the obesity outcome at that age, this framing of the problem exactly mirrors the idea behind linear regression.</p></li>
</ul>
<p><img src="fig4/fig4.2.png" /></p>
<p>The dots in Fig. 4.2 show the proportion of obese persons at each age. The fitted linear regression is shown as the blue line.</p>
<ul>
<li><p>While the linear model reasonably captures the proportion of obese persons across the age range considered in this example, the linear regression formulation does not constrain the predicted probabilities to be between zero and one.</p></li>
<li><p>When outcomes are rare, the predictions may even be outside these bounds. We would not trust a model that could predict the percent of obese persons to be 12% or 131%!</p></li>
<li><p>Furthermore, we should be concerned about making any inferences when a linear regression that assumes a continuous, normally distributed outcome variable is applied to a discrete, non-normal outcome.</p></li>
<li><p>Binary regression constrains the regression equation to be between zero and one, and it represents a coherent way of addressing the discrete, non-normal nature of the data.</p></li>
<li><p>The orange line in Fig. 4.2 shows the prediction from a logistic regression, the most popular type of binary regression. In our obesity example, the logistic regression is very close to the linear model for the range of data considered, but this is not always the case.</p></li>
</ul>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Logistic Regression<a href="Chapter4.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Logistic regression is a way to model the association between the probability of a positive outcome <span class="math inline">\(P(Y= 1)\)</span> and one or more covariates <span class="math inline">\(X\)</span> so that the regression line stays within the interval [0, 1].</p></li>
<li><p>How is this done?</p></li>
<li><p>To understand the reasoning behind logistic regression, recall that linear regression of a continuous outcome <span class="math inline">\(Y\)</span> on a set of covariates <span class="math inline">\(X_1,\ldots, X_k\)</span> has two parts:</p></li>
</ul>
<p><span class="math display">\[
Y = \beta_0 + \beta_1X_1 +\cdots    + \beta_kX_k + \epsilon,
\]</span></p>
<p>where <span class="math inline">\(\beta_0 + \beta_1X_1 +\cdots + \beta_kX_k\)</span> is the systematic part, determining the mean of <span class="math inline">\(Y\)</span> in the sub-populations defined by the <span class="math inline">\(X\)</span>s, and <span class="math inline">\(\epsilon\)</span> the error term that reflects random deviation of a subject from the mean in that sub-population.</p>
<ul>
<li>However, with a binary outcome, this approach fails because</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>The systematic part for the probability <span class="math inline">\(P (Y= 1)\)</span> can be smaller than 0 or larger than 1, an undesirable possibility</p></li>
<li><p>The error term has a very limited range of possible values and depends on the systematic part since the sum of the two parts must be either 0 or 1.</p></li>
</ol>
<ul>
<li><p>As the formulation “outcome = systematic part + error part” fails, we must adopt a different formulation to model binary outcomes.</p></li>
<li><p>A natural way is to think about the outcome of each person as being a result of a binary experiment, such as tossing a coin. The systematic part defines the probability of “success” of the experiment, corresponding to <span class="math inline">\(P(Y=1)\)</span>, and the random part of the model is replaced by a random result of the experiment.</p></li>
<li><p>Since the linear formulation for the systematic part does not guarantee that the model will conform to the 0, 1 range, a mathematical “trick” is required. This mathematical trick is simply a transformation of the systematic part from the linear formulation.</p></li>
<li><p>The most commonly used transformation is the logistic function, which yields:</p></li>
</ul>
<p><span class="math display">\[
P(Y_i=1)=\frac{exp(\beta_0 + \beta_1X_{1i} + \cdots + \beta_kX_{ki})} {1 + exp(\beta_0 + \beta_1X_{1i} + \cdots + \beta_kX_{ki})},
\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> is the outcome of subject <span class="math inline">\(i\)</span> and <span class="math inline">\(X_{1i},\ldots, X_{ki}\)</span> are the covariates.</p>
<ul>
<li><p>Let’s think about why this model works.</p></li>
<li><p>Suppose that we have only one continuous covariate, <span class="math inline">\(X_1\)</span>, and suppose also that <span class="math inline">\(\beta_1\)</span> is positive.</p></li>
<li><p>Then when <span class="math inline">\(X_1\)</span> is positive and increases in magnitude, both the numerator and the denominator in Equation increase, and their ratio approaches 1.</p></li>
<li><p>Conversely, when <span class="math inline">\(X_1\)</span> is negative and increases in magnitude, the numerator approaches 0, and the denominator approaches 1, so the ratio in Equation approaches 0.</p></li>
<li><p>More generally, the numerator is always less than the denominator by construction, so the ratio is always between 0 and 1.</p></li>
<li><p>Thus, this function is a perfect candidate to satisfy the requirements of a regression model for probabilities.</p></li>
<li><p>In Equation, the effect of the <span class="math inline">\(X\)</span>s is additive and linear before the transformation, but the transformation generates a complicated, non-linear function for <span class="math inline">\(P(Y=1)\)</span>.</p></li>
<li><p>An equivalent formulation of this model preserves the linear specification for the <span class="math inline">\(X\)</span>s but applies the inverse transformation to <span class="math inline">\(Y\)</span>.</p></li>
<li><p>This transformation is called the logit, and the formulation is as follows:</p></li>
</ul>
<p><span class="math display">\[
logit[P(Y_i=1)]= log[\frac{P(Y_i = 1)}{P(Y_i=0)}]=\beta_0+\beta_1X_{1i}+\cdots +\beta_kX_{ki}.
\]</span></p>
<ul>
<li><p>Note that the logit is simply the natural logarithm of the odds.</p></li>
<li><p>Thus, the logistic regression model assumes that the log of the odds depends linearly on the covariates.</p></li>
<li><p>The transformation that produces logistic regression is only one of several available for binary outcomes.</p></li>
<li><p>In general, to convert the range of the systematic part to be between 0 and 1, any function that converts a number in the range <span class="math inline">\((-\infty ,\infty )\)</span> to a number in 0, 1 in a sensible way will do the trick.</p></li>
</ul>
</div>
<div id="interpretation-of-a-logistic-regression" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Interpretation of a Logistic Regression<a href="Chapter4.html#interpretation-of-a-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="a-single-binary-covariate" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> A Single Binary Covariate<a href="Chapter4.html#a-single-binary-covariate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Due to the form of the transformed response variable, logistic regression tells us how the log odds of a positive outcome (<span class="math inline">\(Y=1\)</span>) changes when one of the <span class="math inline">\(X\)</span>s change.</li>
</ul>
<p><img src="fig4/table4.3.png" /></p>
<ul>
<li><p>Table 4.3 is similar to that obtained in linear regression, where the standard error (SE) is a measure of uncertainty of the coefficient estimate and a p-value is given for the hypothesis test that each coefficient is equal to 0.</p></li>
<li><p>As in the linear model, the p-value is calculated by dividing the coefficient by its standard error and comparing the result to a standard normal distribution.</p></li>
<li><p>How can we interpret the estimate of 0.343 for the association with year?</p></li>
<li><p>This is not interpreted as the expected increase in Y on the original scale; it must be interpreted on the log odds scale.</p></li>
<li><p>Symbolically we have:</p></li>
</ul>
<p><span class="math display">\[
logit[P (Y = 1 | X = 1)]− logit[P (Y = 1 | X = 0)]= 0.343.
\]</span></p>
<ul>
<li>If we exponentiate both sides, we get:</li>
</ul>
<p><span class="math display">\[
odds[P (Y = 1 | X = 1)]/odds[P (Y = 1 | X = 0)]= exp(0.343) = 1.41.
\]</span></p>
<ul>
<li><p>The exponentiated log odds ratio is simply the odds ratio.</p></li>
<li><p>Because the response variable was log transformed, the logistic model induces a multiplicative effect on the odds of <span class="math inline">\(Y=1\)</span>.</p></li>
<li><p>Thus, while linear regression is an additive model for the mean of the outcome, logistic regression is a multiplicative model for the odds.</p></li>
<li><p>Also notice that the result in this example (<span class="math inline">\(OR=1.41\)</span>) is exactly the result we obtained in Sect. 4.2 using the two-way table.</p></li>
</ul>
<p>Now look at the corresponding probabilities; this can be done with the help of above Equation:</p>
<p><span class="math display">\[
P (Y=   1|   X= 0)= \frac{exp(−0.731)}{1 + exp(−0.731)}=0.325\\
P (Y=   1|   X= 1)= \frac{exp(−0.731 + 0.343)}{1 + exp(−0.731 + 0.343)}=0.404
\]</span></p>
<ul>
<li><p>Again, these are exactly the numbers obtained using the two-way table in Sect. 4.2.</p></li>
<li><p>A logistic regression with a single binary covariate is quite simple: it just replicates the calculations of the proportions and their odds in the comparison of two groups.</p></li>
</ul>
</div>
<div id="the-general-case" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> The General Case<a href="Chapter4.html#the-general-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>In this subsection, we consider models with multiple covariates and work through issues with interpreting the results of a logistic regression.</p></li>
<li><p>We fit a logistic regression to the indicator of obesity (OBESE) with binary covariates SEX (with reference level male) and YEAR (with reference level 1999–2000), a five-level categorical covariate RACE (with reference level non-Hispanic white) coded as a set of four dummy variables and continuous covariate AGE (centered at 20 years, the youngest age in our analysis).</p></li>
<li><p>For this setup, the reference group is the subpopulation of non-Hispanic white men age 20 in 1999–2000.</p></li>
</ul>
<p><img src="fig4/table4.4.png" /></p>
<ul>
<li><p>Table 4.4 provides the fraction obese corresponding to each level of each covariate.</p></li>
<li><p>For non-reference levels, the odds of being obese relative to the reference level is also shown.</p></li>
<li><p>The OR for SEX is greater than 1, suggesting the risk of being obese is higher for women than for men.</p></li>
<li><p>This can also be seen by comparing the percentages: about 32% of men are obese compared to 41% of women.</p></li>
<li><p>For RACE, we see that the OR of the Other/mixed group is smaller than 1, indicating that persons in this group tend to be less at risk of obesity than the reference group (non-Hispanic white persons).</p></li>
<li><p>The ORs for the other RACE groups are greater than 1, and their order matches the ordering of the percentages of obese persons.</p></li>
</ul>
<p><img src="fig4/table4.5.png" /></p>
<ul>
<li><p>Table 4.5 provides the results of a logistic regression with multiple covariates.</p></li>
<li><p>The baseline category for YEAR is 1999–2000; the coefficient estimate for YEAR 2015–2016 is 0.44.</p></li>
<li><p>This tells us that, given AGE, SEX, and RACE, persons in 2015–2016 have a 0.44 increased log odds of being obese compared with persons in 1999–2000.</p></li>
<li><p>The corresponding odds ratio is 1.55; thus, in 2015–2016, the odds of being obese are 1.55 times higher than in 1999–2000. The ORs for the multivariate regression have a conditional interpretation in that they are interpretable as the change in the odds of being obese for the sub-population defined by the values of all the other covariates.</p></li>
<li><p>The interpretation of the coefficient in a logistic regression is similar in principle to a linear regression setting: we can think of it as the estimated effect of a one-unit change in the covariate holding the other covariates fixed. (Note: we use the term “effect” here for conciseness and not to imply causality [1].)</p></li>
<li><p>However, in a logistic regression, we are not estimating the effect on E(Y)—we’re estimating the effect on the log odds of <span class="math inline">\(Y=1\)</span>.</p></li>
<li><p>Thus, the coefficient of a categorical covariate is the estimated difference in log odds compared to the reference level.</p></li>
<li><p>For example, being a woman is associated with a 0.40 difference in the log odds of being obese compared to being a man.</p></li>
<li><p>Alternatively, being a woman is associated with 1.48 times the odds of being obese compared to being a man. The estimated association between covariate and log odd or odds of the outcome is the same for any race, persons of any age, and in either survey calendar year.</p></li>
<li><p>In the case of a continuous covariate, interpretation is similar.</p></li>
<li><p>For example, being 1 year older is associated with an increase in the log odds of being obese of 0.02 or, alternatively, an OR of 1.02.</p></li>
<li><p>This estimated association does not depend on the specific value of age, and it does not depend on the specific values of the other covariates.</p></li>
<li><p>To estimate the effect of a 10-year increase in age, we simply multiply the coefficient by 10, obtaining a difference in log odds of 0.2 and a corresponding OR of <span class="math inline">\(exp(0.2)=1.22\)</span>.</p></li>
<li><p>So far we have discussed how changes in covariates are associated with the log odds or odds of a positive response.</p></li>
<li><p>But we often want to know how changes in covariates are associated with the probability of a positive response.</p></li>
<li><p>This is generally easier to understand. As noted above (Sect. 4.2.1), the OR is sometimes interpretable as an approximate relative risk—i.e., a ratio of probabilities rather than a ratio of odds—but this interpretation is only valid when the outcome is rare.</p></li>
<li><p>When positive outcomes are not rare, the OR can be quite different from the relative risk. For example, a 1999 article in The New England Journal of Medicine that compared rates of referral to cardiac catheterization in African American and white patients found an OR of 0.6.</p></li>
<li><p>This result was considered to be evidence of a dramatic racial disparity in the receipt of cardiac catheterization, but the corresponding risk ratio was actually 0.93 [3]—much closer to 1!</p></li>
<li><p>The two measures were so different owing to a relatively high fraction of each group receiving a referral (92% of white patients and 89% of black patients).</p></li>
<li><p>This left rather small denominators for the odds in each group, which led to an odds ratio that completely misrepresented the relatively minor difference between the probabilities.</p></li>
<li><p>It is not infrequent to see results of logistic regression analyses interpreted in this way— the natural language of risk is probability, not odds.</p></li>
<li><p>So, what if we really want to understand our logistic regression results in terms of relative probabilities or differences in probabilities instead of relative odds and differences in log odds?</p></li>
</ul>
</div>
</div>
<div id="interpretation-on-the-probability-scale" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Interpretation on the Probability Scale<a href="Chapter4.html#interpretation-on-the-probability-scale" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="estimating-probabilities" class="section level3 hasAnchor" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Estimating Probabilities<a href="Chapter4.html#estimating-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Equation tells us how to calculate the probability of a positive outcome for any sub-population. For example, the probability of a positive outcome in the reference group of non-Hispanic white (NHW) men age 20 years in 1999–2000 is:</li>
</ul>
<p><span class="math display">\[
P (Y    =1 |  NHW, Male, 20)=   \frac{exp(−1.46)}{1 + exp(−1.46)}=0.188.
\]</span></p>
<ul>
<li>If we want to estimate the probability for an otherwise similar woman, we simply add the coefficient of SEX:</li>
</ul>
<p><span class="math display">\[
P (Y=   1|   NHW, Female, 20)=  \frac{exp(−1.46 + 0.40)}{1 + exp(−1.46 + 0.40)}=0.257,
\]</span></p>
<ul>
<li>and if we want to estimate the probability for an otherwise similar woman age 52 years, we simply add the coefficient of AGE times 32 (= 52 − 20):</li>
</ul>
<p><span class="math display">\[
P (Y=   1 |  NHW, Female, 52)=  \frac{exp(−1.46 + 0.40 + 0.02 × 32)}{1 + exp(−1.46 + 0.40 + 0.02 × 32)}=0.397.
\]</span></p>
<ul>
<li><p>Because interpreting ORs is not straightforward, we would like to define the effect of a covariate on the probability of a positive outcome, <span class="math inline">\(P(Y=1)\)</span>.</p></li>
<li><p>There are two types of effects for a covariate X:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>An additive effect (AE), equal to the absolute change in <span class="math inline">\(P(Y= 1| X)\)</span> for a one-unit change in <span class="math inline">\(X\)</span>:</li>
</ol>
<p><span class="math display">\[
AE = P (Y = 1 | X = 1) − P (Y = 1 | X = 0).
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>A multiplicative effect (ME) equal to the relative change in <span class="math inline">\(P (Y= 1| X)\)</span> for a one-unit change in <span class="math inline">\(X\)</span>:</li>
</ol>
<p><span class="math display">\[
ME = P (Y = 1 | X = 1)/P (Y = 1 | X = 0).
\]</span></p>
<ul>
<li><p>The additive effect is sometimes referred to as a risk difference.</p></li>
<li><p>The multiplicative effect is the previously defined risk ratio. For either effect, we would ideally like an estimate that does not depend on the specific values of the other covariates, just as in linear regression.</p></li>
</ul>
<p><img src="fig4/fig4.3.png" /></p>
<ul>
<li><p>Figure 4.3 shows why this is a tall order.</p></li>
<li><p>The figure shows predicted probabilities of being obese implied by the fitted logistic regression in Table 4.5 for persons in race category Other/mixed and different values of AGE, SEX, and YEAR.</p></li>
<li><p>One can observe that the predicted probabilities are not quite linear in the covariates.</p></li>
<li><p>This is a consequence of the logit transformation that converted the model from a linear to a non-linear model.</p></li>
<li><p>The model is only linear on the log odds scale—not on the original probability scale.</p></li>
<li><p>In fact, not only do the predicted probabilities increase non-linearly with age, but the effect of year differs depending on age.</p></li>
<li><p>This is reminiscent of the normal linear model with an interaction between age and year, which was a non-linear model.</p></li>
<li><p>The regression equation in this case does not explicitly include an interaction, but when converted to the probability scale, it is also non-linear.</p></li>
</ul>
<p><img src="fig4/fig4.4.png" /></p>
<ul>
<li><p>The top panel of Fig. 4.4 shows the difference between the predicted probabilities in years 2015–2016 and 1999–2000 across values of SEX and AGE for RACE Other/mixed.</p></li>
<li><p>If the model were additive and linear on the probability scale, the differences would be similar for men and women and constant across ages.</p></li>
<li><p>However, this is not the case.</p></li>
<li><p>In fact, the additive effect of YEAR depends on AGE and SEX; the absolute change in BMI over time is greater for older people and for women. We refer to this effect as a conditional additive effect—the additive effect depends on the exact values of the other covariates: AGE, SEX, and RACE.</p></li>
<li><p>The conditional multiplicative effect is defined similarly as the conditional ratio of the probabilities of being obese in the later and earlier years.</p></li>
<li><p>The middle panel of Fig. 4.4 shows this ratio across values of SEX and AGE for RACE Other/mixed.</p></li>
<li><p>In contrast to the absolute change, the relative change over time in BMI is lower for older persons and for women.</p></li>
<li><p>In Fig. 4.4, only the odds ratio (bottom panel) is the same across ages and for men and women.</p></li>
<li><p>In logistic regression, the odds ratio is the single measure of association that does not depend on the specific values of other covariates.</p></li>
<li><p>The question that remains, then, is whether we can estimate an additive or multiplicative effect on the probability scale (i.e., “risk differences”).</p></li>
</ul>
</div>
<div id="marginal-effects" class="section level3 hasAnchor" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> Marginal Effects<a href="Chapter4.html#marginal-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>One way to estimate the effect of a covariate on the probability scale is to convert the conditional additive or multiplicative effect to a marginal effect.</p></li>
<li><p>Here, we are using the term “marginal” in its technical sense, meaning as an average.</p></li>
<li><p>In the logistic regression setting, the marginal additive effect of YEAR is an average of the conditional additive effects, defined above, over the distribution of all the other covariates: AGE, SEX, and RACE. The result is meaningful as a difference in the probability of being obese in 2015–2016 versus 1999–2000; it is adjusted for all covariates, and it does not depend on their specific values since it averages over them.</p></li>
<li><p>Estimation of a marginal effect consists of a two-step process:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Estimate the conditional effect of interest (e.g., of YEAR) based on the observed data.</p></li>
<li><p>Average the estimates in (1) over the joint distribution of all other covariates.</p></li>
</ol>
<ul>
<li><p>To estimate the marginal effect of YEAR in the obesity example, step (2) averages the conditional effect estimates over the distributions of AGE, SEX, and RACE.</p></li>
<li><p>This may make sense theoretically, but how can we implement it practically in our sample?</p></li>
<li><p>The recycled prediction method implements step (2) by averaging over the empirical distribution of the other covariates (i.e., the set of values actually observed in the data), yielding a sample average version of step (2).</p></li>
<li><p>We explain how the method works using the association between BMI and YEAR as an illustrative example.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>For each person in the data, calculate a person-specific conditional additive effect as the difference between their predicted probabilities:</li>
</ol>
<p><span class="math display">\[
P (Y = 1 | YEAR = 2015–2016) − P (Y = 1 | YEAR = 1999–2000).
\]</span></p>
<ul>
<li><p>The first predicted probability is calculated from the fitted logistic regression by setting YEAR to 1 (2015–2016), and the second is calculated by setting YEAR to 0 (1999–2000) fixing the other covariates at the values observed for that person.</p></li>
<li><p>Thus, two predicted probabilities are calculated for each person.</p></li>
<li><p>The differences are individual-level conditional effects and are presented in the top panel of Fig. 4.4 for persons in the race category Other/mixed.</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Calculate the sample average of the individual-level conditional effects.</li>
</ol>
<ul>
<li><p>This is the same thing as calculating a weighted average of the conditional effects over the empirical (sample) distribution of the other covariates.</p></li>
<li><p>This produces a sample average version of step (2) above.</p></li>
<li><p>Variance estimation for the marginal additive effect is generally performed using a technique called the delta method or by numerical methods such as bootstrapping; the package margins in R can do all the necessary calculations. The variance can be used to construct a confidence interval for the marginal additive effect.</p></li>
<li><p>The marginal additive effect for YEAR is estimated as 0.102 with 95% confidence interval (0.077, 0.128).</p></li>
<li><p>Thus, the fraction of adults that are obese in 2015–2016 is about 10% higher than in 1999–2000.</p></li>
<li><p>Even though this estimate averages over the other covariates and therefore does not explicitly depend on them, it does depend on their distribution in the sample.</p></li>
<li><p>If the sample distribution of the other covariates is not representative of the population distribution, the result may not generalize to the population.</p></li>
<li><p>Finally, although recycled predictions may be reported using terms like “effects” and so may seem to suggest a causal framework, this is simply convenient nomenclature, and results should not be interpreted causally unless causal inference methods are employed.</p></li>
<li><p>If interest instead focuses on multiplicative effects, we can calculate the ratio:</p></li>
</ul>
<p><span class="math display">\[
\frac{P (Y = 1 | YEAR = 2015–2016)}{P (Y = 1 | YEAR = 1999–2000)}
\]</span></p>
<p>rather than the difference in step (1) and average over all the estimates.</p>
<ul>
<li>This will yield an average risk ratio over the sample.</li>
</ul>
</div>
</div>
<div id="model-building-and-assessment" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Model Building and Assessment<a href="Chapter4.html#model-building-and-assessment" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>The principles of building a logistic regression model and assessing its fit are similar to those used for linear regression models (Chap. 3), but certain additional considerations apply due to the binary nature of the response variable.</p></li>
<li><p>If the goal of the analysis is to conduct inference to test a pre-specified hypothesis, then model adequacy will hinge on whether the model accurately reflects the data-generating mechanism.</p></li>
<li><p>If the goal of the analysis is to deliver a conclusion that points to a causal relationship, then model adequacy will rest on whether the model properly accounts for other potential confounding variables.</p></li>
<li><p>And if the goal is to predict the occurrence of an event or of a condition encoded using a binary response, then predictive accuracy will be key.</p></li>
</ul>
<div id="model-comparison-aic-and-bic" class="section level3 hasAnchor" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Model Comparison: AIC and BIC<a href="Chapter4.html#model-comparison-aic-and-bic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The parameters of logistic regression are estimated by maximum likelihood, where the parameters <span class="math inline">\(\beta_0, \beta_1,\ldots, \beta_k\)</span> are chosen to maximize the likelihood of obtaining the data observed.</p></li>
<li><p>We can test for the adequacy of a model with fewer covariates, say a model that assumes there is no effect of the race variable in the obesity example, by omitting those variables, re-estimating the regression model, and comparing the likelihood of the observed data with and without those variables.</p></li>
<li><p>This is the likelihood ratio test for nested models, described in Chap. 3.</p></li>
<li><p>In Chap. 3, we also introduced the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for comparison of nested and non-nested models.</p></li>
<li><p>These are versions of the maximized log-likelihood <span class="math inline">\(L\)</span> of a model M that include an added penalty for model complexity.</p></li>
<li><p>The goal is to find the model that minimizes the AIC (or BIC). The two statistics are similar, but the BIC invokes a more severe penalty for model complexity that also depends on the sample size n, so it may select simpler, more parsimonious models than the AIC in certain cases.</p></li>
</ul>
</div>
<div id="model-calibration-hosmerlemeshow-test" class="section level3 hasAnchor" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> Model Calibration: Hosmer–Lemeshow Test<a href="Chapter4.html#model-calibration-hosmerlemeshow-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The maximum likelihood estimates of the parameters have several appealing properties.</p></li>
<li><p>For example, the sum of predicted probabilities of being obese (i.e., the sum of predicted <span class="math inline">\(P(Y=1|X)\)</span> over the sample) exactly equals the observed number of obese persons.</p></li>
<li><p>Moreover, this is true not only for the total number of obese persons in the sample but also within each discrete sub-population defined by SEX, RACE, or YEAR.</p></li>
<li><p>Thus, the predicted number of obese persons in 1999–2000 equals the observed number of obese persons in that survey year.</p></li>
<li><p>But what about sub-populations defined by continuous variables or by more than one discrete variable? We say that a model is well calibrated if the number of obese persons it predicts in each possible sub-population is close to the observed number in that group.</p></li>
<li><p>For example, we would like the predicted number of obese non-Hispanic white men in the age group 30–35 in 1999–2000 to closely match the observed number in that group. Since there are many possible sub-populations of this kind to look at, a systematic way to assess model calibration is needed.</p></li>
<li><p>A now common approach was suggested by Hosmer and Lemeshow.</p></li>
<li><p>The Hosmer-Lemeshow approach compares observed and predicted quantities as follows:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Order the observations according to their predicted <span class="math inline">\(P(Y=1|X)\)</span>, so the person with the lowest predicted probability of being obese is first, and the person with the highest predicted probability of being obese is last.</p></li>
<li><p>Partition the ordered observations into m equal-sized groups. For example, if m 10, partition the ordered observations into ten groups, with each group comprising approximately 10% of the sample.</p></li>
<li><p>Create a two-way contingency table by cross-tabulating counts of obese and non-obese persons across groups.</p></li>
<li><p>Create the corresponding table for the predicted number of persons in each group, where the predicted number is given by the sum of the predicted probabilities in that group.</p></li>
<li><p>Compare the observed and predicted numbers in corresponding cells in the two tables.</p></li>
</ol>
<ul>
<li>For contingency tables, this is known as Pearson’s residual:</li>
</ul>
<p><span class="math display">\[
Res = \frac{Observed - Predicted}{\sqrt{Predicted}}
\]</span></p>
<ul>
<li><p>Pearson’s residual tells us about over-prediction and under-prediction within each cell of the table.</p></li>
<li><p>To summarize across groups, Hosmer and Lemeshow suggest summing the square of the Pearson’s residuals (<span class="math inline">\(Res^2\)</span>) over the table and comparing it to a <span class="math inline">\(\chi^2\)</span> (“chi-squared”) distribution with <span class="math inline">\(m=2\)</span> degrees of freedom.</p></li>
<li><p>The Hosmer–Lemeshow <span class="math inline">\(\chi^2\)</span> test can be problematic for large sample sizes because it can reject models that approximate the data reasonably well but not perfectly.</p></li>
<li><p>A simulation study suggested not using the test when sample size is larger than 10,000 and to use more than <span class="math inline">\(m=100\)</span> groups for sample sizes around 5000.</p></li>
<li><p>Still, partitioning the data into few groups (say <span class="math inline">\(m=10\)</span> or <span class="math inline">\(20\)</span>) and looking for cells with relatively extreme residuals can be a useful informal tool for model checking.</p></li>
<li><p>Any systematic pattern of residuals across the cells can be informative about the validity of linearity assumptions or may suggest a need to include additional covariates.</p></li>
</ul>
<p><img src="fig4/table4.6.png" /></p>
<ul>
<li><p>Table 4.6 shows observed and predicted numbers of obese and non-obese persons in <span class="math inline">\(m=10\)</span> groups.</p></li>
<li><p>The Pearson’s residuals are distributed across groups without patterns or extreme values.</p></li>
<li><p>However, the Hosmer–Lemeshow statistic is calculated to be 21.1, corresponding to a p-value of 0.007 according to a <span class="math inline">\(\chi^2\)</span> with 8 degrees of freedom.</p></li>
<li><p>If we partition the data into m 100 groups, the p-value is 0.02. In both cases, we would conclude that the model was not well calibrated, but, given our sample size of over 6800 individuals, would take this result with a grain of salt.</p></li>
<li><p>What is the Hosmer–Lemeshow test actually evaluating? It is sometimes billed as a check on the linearity assumption of the logistic regression model.</p>
<ul>
<li>But really it is an omnibus test of the ability of the model to replicate the pattern of <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span> across groups of observations given the covariates included and the structure of the model.</li>
</ul></li>
</ul>
</div>
<div id="model-prediction-roc-and-auc" class="section level3 hasAnchor" number="4.7.3">
<h3><span class="header-section-number">4.7.3</span> Model Prediction: ROC and AUC<a href="Chapter4.html#model-prediction-roc-and-auc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>If the objective of the analysis is prediction, measures of predictive accuracy are most relevant.</p></li>
<li><p>By prediction, we mean an educated guess about a future outcome based on the values of relevant covariates.</p></li>
<li><p>In the case of binary outcomes, there are several standard measures of predictive accuracy.</p></li>
<li><p>A simple way to think about the predictive accuracy of a binary regression model is to look at the overlap between the distributions of the predicted probabilities for positive and negative observed outcomes.</p></li>
</ul>
<p><img src="fig4/fig4.5.png" /></p>
<ul>
<li><p>Figure 4.5 shows histograms of predicted probabilities for obese and non-obese persons.</p></li>
<li><p>While the predicted probabilities tend to be lower for non-obese persons than for obese persons, there is considerable overlap between the two histograms, illustrating relatively weak discrimination between these two groups.</p></li>
<li><p>Formal measures of predictive accuracy effectively quantify the agreement between the observed binary outcome and the model prediction.</p></li>
<li><p>The model prediction is not binary; it is a probability between zero and one.</p></li>
<li><p>But if one is willing to specify a threshold <span class="math inline">\(T\)</span> above which a prediction is positive (one) and below which a prediction is negative (zero), then one could assess agreement between the observed binary outcome and the binarized predicted outcome.</p></li>
<li><p>A natural threshold is <span class="math inline">\(T=0.5\)</span>, where a person is predicted to be positive (<span class="math inline">\(Y=1\)</span>) if the predicted probability is greater than 0.5 and he/she is predicted to be negative (<span class="math inline">\(Y=0\)</span>) otherwise.</p></li>
</ul>
<p><img src="fig4/table4.7.png" /></p>
<ul>
<li><p>Table 4.7 presents the results of this approach for the multivariate logistic regression in Table 4.5.</p></li>
<li><p>The rows of this two-way table show the observed obesity status, and the columns show predicted obesity status based on the threshold <span class="math inline">\(T=0.5\)</span>.</p></li>
<li><p>We see that 92% of non-obese persons are correctly predicted to be non-obese by this model and this criterion, but only 18% of obese persons are correctly predicted to be obese.</p></li>
<li><p>Correctly classifying obese persons may be more important than correctly classifying non-obese persons, and this can be controlled by changing the threshold.</p></li>
<li><p>In the setting of binary outcomes, two basic measures of predictive accuracy play a central role:</p>
<ul>
<li><p>Sensitivity: Probability that the model prediction is positive when the outcome is positive.</p></li>
<li><p>Sensitivity measures the ability of the model to correctly predict a positive outcome.</p></li>
<li><p>In the obesity example, for the threshold <span class="math inline">\(T=0.5\)</span>, the sensitivity is 18%.</p></li>
<li><p>Specificity: The probability that the model prediction is negative when the outcome is negative.</p></li>
<li><p>Specificity measures the ability of the model to correctly predict a negative outcome.</p></li>
<li><p>In the obesity example, for the threshold <span class="math inline">\(T=0.5\)</span>, the specificity is 92%.</p></li>
</ul></li>
<li><p>The sensitivity is also referred to as the true-positive rate (TPR), while one minus the specificity is the false-positive rate (FPR).</p></li>
<li><p>Good predictive performance is characterized by high TPR and low FPR, but in many settings, the importance of one outweighs that of the other.</p></li>
<li><p>For example, in developing tests to diagnose COVID-19, high sensitivity is critical because infected individuals who test negative (false negative) may continue to spread the infection unwittingly.</p></li>
<li><p>On the other hand, in developing tests to detect COVID-19 antibodies, high specificity is critical because individuals who have not had the virus but who think that they have had it (false positive) may behave as if they are protected when they are not.</p></li>
<li><p>It should be apparent that sensitivity and specificity will vary depending on the threshold <span class="math inline">\(T\)</span> . As the threshold increases, the sensitivity will decrease but specificity will increase.</p></li>
<li><p>Thus, sensitivity and specificity move in opposite directions as the threshold <span class="math inline">\(T\)</span> varies.</p></li>
<li><p>When choosing a specific threshold <span class="math inline">\(T\)</span> to dichotomize predictions from a binary regression model, it is important to consider the consequences of both false negatives and false positives and to weight them appropriately in the selection of <span class="math inline">\(T\)</span>.</p></li>
</ul>
<p>However, generally speaking, we would like to judge predictive accuracy without specifying a threshold. The receiver operating characteristic (ROC) curve does this.</p>
<ul>
<li>The ROC curve plots the TPR (sensitivity) versus the FPR (1-specificity) as <span class="math inline">\(T\)</span> varies.</li>
</ul>
<p><img src="fig4/fig4.6.png" /></p>
<ul>
<li><p>Figure 4.6 considers the problem of predicting obesity based on the model in Table 4.5.</p></li>
<li><p>Naturally, if we really wanted to predict whether a person was obese or would become obese, we would bring many more covariates to bear. But for illustration, we plot the ROC curve for this model.</p></li>
<li><p><span class="math inline">\(T\)</span> increases from zero to one; both the TPR (sensitivity) and FPR (1-specificity) increase from zero to one.</p></li>
<li><p>The ideal ROC curve hugs the upper left corner of the unit square, where the TPR is highest and the FPR is lowest.</p></li>
<li><p>A quantitative measure of the predictive performance of an ROC curve is the area under the curve (AUC), sometimes also called the concordance or C-statistic. The ideal ROC curve has an AUC of 1.</p></li>
<li><p>An ROC curve that sits on the 45-degree line that bisects the unit square reflects predictive accuracy that is no better than flipping a coin; it has an AUC of 0.5.</p></li>
<li><p>For the multivariate regression in Table 4.5, the ROC is only somewhat above the 45 degree line, and the AUC is a relatively unimpressive 0.63.</p></li>
<li><p>Incorporating other variables in the model, such as chronic conditions (e.g., diabetes, hypertension, and cardiovascular disease), behaviors like exercise, and socioeconomic factors like income and education, might improve the predictive performance.</p></li>
</ul>
</div>
</div>
<div id="multinomial-regression" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Multinomial Regression<a href="Chapter4.html#multinomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="an-extension-of-logistic-regression" class="section level3 hasAnchor" number="4.8.1">
<h3><span class="header-section-number">4.8.1</span> An Extension of Logistic Regression<a href="Chapter4.html#an-extension-of-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Multinomial regression extends logistic regression to settings where the outcome variable has more than two categories.</p></li>
<li><p>To illustrate, we examine how SEX, RACE, AGE, and YEAR are associated with the probability of four BMI categories: underweight (<span class="math inline">\(BMI &lt; 18.5\)</span>), normal (<span class="math inline">\(18.5\le BMI &lt; 25\)</span>), overweight (<span class="math inline">\(25\le BMI &lt; 30\)</span>), and obese (<span class="math inline">\(BMI \ge 30\)</span>).</p></li>
<li><p>Multinomial regression translates this question into how the covariates are associated with observed proportions in these categories.</p></li>
<li><p>In principle, we could repeat the logistic regression analysis in Table 4.5 for each BMI category separately.</p></li>
<li><p>However, such an analysis ignores important aspects of the multi-category setting.</p></li>
<li><p>We will compare the two approaches and discuss the differences between them.</p></li>
<li><p>We start this section with a single binary covariate, YEAR.</p></li>
<li><p>Table 4.8 shows frequencies within the different BMI categories in 1999–2000 and 2015–2016.</p></li>
</ul>
<p><img src="fig4/table4.8.png" /></p>
<ul>
<li><p>The absolute 8% increase in the obese category coincided with an absolute decrease of 5% in the normal category and an absolute decrease of 3% in the overweight category.</p></li>
<li><p>The proportions of underweight persons were small and similar in absolute magnitude in the 2 survey years.</p></li>
<li><p>The risk ratios (RRs) and odds ratios (ORs) summarize relative change between years, but these measures do not take the multi-category structure into account.</p></li>
<li><p>First we define a multi-category OR of category <span class="math inline">\(k\)</span> relative to a reference category
as:</p></li>
</ul>
<p><span class="math display">\[
OR=\frac{P(Y = k | 2015–2016)/P (Y = reference | 2015–2016)}{P (Y = k | 1999–2000)/P (Y = reference | 1999–2000)}
\]</span></p>
<ul>
<li><p>This definition of OR extends the one for binary outcomes for which the relative category was always <span class="math inline">\(Y=0\)</span>.</p></li>
<li><p>The bottom row of Table 4.8 shows the multi-category ORs taking normal weight as the reference category.</p></li>
<li><p>These multi-category ORs could be obtained by calculating two-category ORs from two-way tables of each category with the reference category.</p></li>
<li><p>For example, the multi-category OR of 1.07 for overweight persons is the two-category OR based on the normal and overweight columns in Table 4.8, which can be calculated as <span class="math inline">\((920\times 1095)/(930\times 1013)\)</span>.</p></li>
<li><p>Based on the multi-category OR, we estimate a 6% increase in the odds of being underweight, a 7% increase in the odds of being overweight, and a 46% increase in the odds of being obese relative to the change in normal weight persons in 2015–2016 compared to 1999–2000.</p></li>
<li><p>Thus, the multi-category ORs characterize the relative change within each category relative to the change in the reference category.</p></li>
</ul>
<p><img src="fig4/fig4.7.png" /></p>
<ul>
<li><p>For further comparison with the binary case, Fig. 4.7 shows the proportions of persons in each BMI category by age. The points show the observed proportions, and the lines are separate linear regressions for each category.</p></li>
<li><p>We introduced binary regression out of concerns about predictions that are outside the sensible 0, 1 range and the non-normal distribution of binary outcomes.</p></li>
<li><p>For multinomial outcomes, an additional consideration is that predictions across categories should sum to 1 at each age—and, more generally, at each value of the covariates. Multinomial regression is precisely designed for this purpose.</p></li>
<li><p>For each subject, multinomial regression predicts the probability that they belong to each category in a way that respects the multi-category nature of the outcome.</p></li>
<li><p>In both binary and multinomial regressions, the central question is whether and how the proportions of the outcome variable change with the covariates. Whether the outcome variable has two, three, or more categories, addressing this question involves assessing the shape of the outcome distribution given the covariates.</p></li>
<li><p>In the case of a binary outcome, the overall proportions are <span class="math inline">\(p_0 = P (Y = 0)\)</span> and <span class="math inline">\(p_1 = P (Y = 1)\)</span>.</p></li>
<li><p>In the case of a multinomial outcome with three categories, the overall proportions are <span class="math inline">\(p_0 = P (Y = 0)\)</span>, <span class="math inline">\(p_1 = P (Y = 1)\)</span>, and <span class="math inline">\(p_2 = P (Y = 2)\)</span>.</p></li>
<li><p>In general, for <span class="math inline">\(K\)</span> categories, the overall proportions are <span class="math inline">\(p_0 = P (Y = 0)\)</span>, <span class="math inline">\(p_1 = P (Y = 1)\)</span>, <span class="math inline">\(p_2 = P (Y = 2), \ldots, p_{K−1} = P (Y = K − 1)\)</span>.</p></li>
<li><p>Binary regression addresses the central question by examining how the ratio <span class="math inline">\(p1/p0\)</span>, the odds of a positive outcome, depends on the covariates.</p></li>
<li><p>Testing whether the odds of <span class="math inline">\(Y\)</span> changes as <span class="math inline">\(X\)</span> changes from 0 to 1 is equivalent to testing whether the outcome distribution changes.</p></li>
<li><p>In the case of a binary covariate, the OR is the ratio of the odds (of a positive outcome) when <span class="math inline">\(X = 1\)</span> compared to when <span class="math inline">\(X = 0\)</span>.</p></li>
<li><p>In the case of a <span class="math inline">\(K\)</span>-category outcome, multinomial regression examines <span class="math inline">\(K − 1\)</span> ratios, <span class="math inline">\(p_1/p_0, p_2/p_0, \ldots, p_{K−1}/p_0\)</span>.</p></li>
<li><p>As in binary regression, testing whether any of these ratios changes as X changes from 0 to 1 is equivalent to testing whether the outcome distribution changes.</p></li>
<li><p>Multinomial regression analysis therefore consists of multiple regressions, each one corresponding to one of these ratios.</p></li>
<li><p>In our example of <span class="math inline">\(K=4\)</span> BMI categories, <span class="math inline">\(p_0=P (Y= normal)\)</span>, <span class="math inline">\(p_1=P (Y= underweight)\)</span>, <span class="math inline">\(p_2= P (Y= overweight)\)</span>, and <span class="math inline">\(p_3= P(Y=obese)\)</span>, and there are three regression models to fit, each of which is similar to the logistic regression in Eq. (4.2):</p></li>
</ul>
<p><span class="math display">\[
log(p_1/p_0) = \beta_0 + \beta_1X \\log(p_2/p_0) = \gamma_0 + \gamma_1X \\log(p_3/p_0) = \delta_0 + \delta_1X.
\]</span></p>
<ul>
<li><p>Note that each model has its own set of parameters and, moreover, the covariates can vary across models.</p></li>
<li><p>However, the odds in the three models share the same reference category (<span class="math inline">\(Y=0\)</span>).</p></li>
<li><p>In each model, the coefficient of X can be interpreted as the log of a quantity analogous to an OR.</p></li>
<li><p>In the first model, this quantity is the ratio <span class="math inline">\(p_1/p_0\)</span> when <span class="math inline">\(X=1\)</span> divided by the ratio <span class="math inline">\(p_1/p_0\)</span> when <span class="math inline">\(X =0\)</span>.</p></li>
<li><p>This is sometimes referred to as a relative risk ratio, but for consistency with the binary case, we will continue to refer to it as an OR.</p></li>
<li><p>If the OR is greater than 1, corresponding to <span class="math inline">\(\beta_1 &gt; 0\)</span>, this means that an increase in <span class="math inline">\(X\)</span> from 0 to 1 corresponds to a shift in the proportion of the outcome in category <span class="math inline">\(Y=0\)</span> into category <span class="math inline">\(Y=1\)</span>.</p></li>
<li><p>Similarly, if the OR in the second regression is greater than 1, corresponding to <span class="math inline">\(\gamma_1 &gt; 0\)</span>, this means that an increase in <span class="math inline">\(X\)</span> from 0 to 1 corresponds to a shift in the proportion of the outcome in category <span class="math inline">\(Y=0\)</span> into category <span class="math inline">\(Y=2\)</span>.</p></li>
<li><p>We fit only three regressions for our four-category variable because the fourth category is completely determined by the first three.</p></li>
</ul>
<p><img src="fig4/table4.9.png" /></p>
<ul>
<li><p>Table 4.9 compares separate logistic regressions to multinomial regression.</p></li>
<li><p>For example, the logistic regression for obese persons was fit using only data for obese and normal weight persons after removing underweight and overweight persons.</p></li>
<li><p>It is therefore different from the logistic regression in Table 4.5, which was fit using underweight, normal and overweight as “non-obese” persons.</p></li>
<li><p>The coefficients in the separate logistic regression and the joint multinomial regression are almost identical; the two sets of results give similar insights into the data.</p></li>
<li><p>For example, the coefficients for YEAR 2015–2016 are all positive; equivalently, the ORs are greater than 1, indicating that the odds of each “non-normal” BMI category relative to the normal category were higher in 2015–2016.</p></li>
<li><p>For a more specific interpretation of these results, consider the multi-category ORs from the multinomial regression, which (like the ORs in the logistic regres- sions) are just the exponentiated coefficients. The ORs for <span class="math inline">\(YEAR=2015–2016\)</span> for the underweight and overweight categories are 1.16 and 1.18, which means that from 1999–2000 to 2015–2016, the estimated probability of each of these categories relative to the normal BMI category increased by 16% and 18%.</p></li>
<li><p>The OR for the obese category is much larger (1.69), showing a larger relative shift into that category between the years.</p></li>
<li><p>In summary, interpretation of multinomial regression results is similar to the interpretation of logistic regression results when data are restricted to only two categories. The effects of the covariates may differ from category to category and may be positive for one category and negative for another.</p></li>
<li><p>For example, age has a significant negative effect for underweight persons, meaning that older people have lower odds of being underweight than younger people (in the age range considered).</p></li>
<li><p>However, age has a significant positive effect for overweight and obese persons, meaning that older people have higher odds of being in these groups than younger people (in the age range considered).</p></li>
</ul>
</div>
<div id="marginal-effects-1" class="section level3 hasAnchor" number="4.8.2">
<h3><span class="header-section-number">4.8.2</span> Marginal Effects<a href="Chapter4.html#marginal-effects-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>If there are <span class="math inline">\(K\)</span> categories in the outcome, multinomial regression runs <span class="math inline">\(K-1\)</span> regressions and yields <span class="math inline">\(K-1\)</span> sets of coefficients to interpret.</p></li>
<li><p>This can get cumbersome quite quickly; moreover, it can be difficult to make inferences about how the overall shape of the outcome distribution changes based on the estimated relative risk ratios.</p></li>
<li><p>A recycled predictions approach can be used to obtain inferences on the original probability scale of the outcome variable.</p></li>
<li><p>For the YEAR variable, this works by calculating the predicted probabilities of each of the four categories of BMI for each person, first setting their YEAR to 1999–2000 and then to 2015–2016.</p></li>
<li><p>This produces two sets of four numbers with predicted probabilities in either set that sum to 1.</p></li>
<li><p>The four numbers are then averaged across individuals in the sample, and then the difference in averages between sets is calculated to produce a marginal additive effect on the probability scale.</p></li>
<li><p>This exactly replicates the method used for logistic regression.</p></li>
<li><p>The marginal additive effect is the difference in average predicted values for each BMI category and reflects the estimated change in the probability of being in that BMI category between years.</p></li>
</ul>
<p><img src="fig4/table4.10.png" /></p>
<ul>
<li><p>Table 4.10 shows the recycled prediction estimates of the marginal additive effect of YEAR on the probability of being in each BMI category.</p></li>
<li><p>The columns show the average predicted probability of being in each BMI category under the estimated model if all persons were in the indicated survey year.</p></li>
<li><p>The last column shows the difference in 2015–2016 relative to 1999–2000.</p></li>
<li><p>The results indicate that the more recent survey year is associated with a nearly 10% increase in the probability of being in the obese category. This increase in the obese category corresponds to decreases of 7% in the normal and of 3% in the overweight categories.</p></li>
<li><p>There is virtually no change in the underweight category.</p></li>
</ul>
</div>
<div id="ordered-multinomial-regression" class="section level3 hasAnchor" number="4.8.3">
<h3><span class="header-section-number">4.8.3</span> Ordered Multinomial Regression<a href="Chapter4.html#ordered-multinomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Multinomial regression is sometimes referred to as an “unordered” multinomial logit model, because it is agnostic to any implied ordering of the categories. When categories have an ordering, such as BMI categories, or self-reported health status, another multi-category regression model is also available.</p></li>
<li><p>It makes fairly strong assumptions, so we do not generally recommend its use; however, it is widely available, so we briefly summarize its structure and limitations.</p></li>
<li><p>The proportional odds model is derived assuming that the categories reflect intervals partitioning an underlying continuous scale.</p></li>
<li><p>Like the multinomial model, the proportional odds model includes a set of logit-type regression equations.</p></li>
<li><p>For <span class="math inline">\(Y = BMI\)</span> and <span class="math inline">\(X = YEAR\)</span>, the equations can be written:</p></li>
</ul>
<p><span class="math display">\[
logit[P (Y &gt; k)]= \gamma k + \beta X,
\]</span></p>
<p>for <span class="math inline">\(k=1\)</span> to <span class="math inline">\(K-1\)</span>, where <span class="math inline">\(K=4\)</span> is the number of BMI categories.</p>
<ul>
<li><p>Thus, this model describes how the log odds of being in a higher category relative to being in a lower one depend on covariates.</p></li>
<li><p>When <span class="math inline">\(X=0\)</span>, the coefficients <span class="math inline">\(\gamma_k\)</span> describe the multinomial probability distribution of BMI categories in 1999–2000.</p></li>
<li><p>When <span class="math inline">\(X=1\)</span>, the distribution of BMI is allowed to change but only in a highly restrictive, monotone way because there is only one coefficient, <span class="math inline">\(\beta\)</span>, describing this change.</p></li>
<li><p>If <span class="math inline">\(\beta\)</span> is positive, then the log odds of being in a higher BMI category increases in the later year, and this increase is the same regardless of the specific category.</p></li>
<li><p>Thus, the log odds of being overweight or higher and the log odds of being obese both increase by the same amount from 1999–2000 to 2015–2016.</p></li>
<li><p>On the probability scale, we have probability shifting from lower categories to higher categories in a manner prescribed by <span class="math inline">\(\beta\)</span>.</p></li>
<li><p>But this monotone constraint does not accommodate a variety of association patterns.</p></li>
<li><p>For example, if <span class="math inline">\(X\)</span> is a binary version of an extended age variable that includes all ages, we might find that both overweight and underweight increase for elderly individuals.</p></li>
<li><p>This kind of pattern, where multinomial probabilities become weighted toward extreme categories as a covariate changes values, would not be possible to model or detect with a proportional odds model, but it would be identified with an unordered multinomial logit model.</p></li>
<li><p>Therefore, we generally recommend the unordered model unless a specific hypothesis that conforms with the assumptions of proportional odds is of particular interest.</p></li>
</ul>
<p><br></p>
<p><br></p>
<!-------------------------------------->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Chapter3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Chapter5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Bio-Health Data Analysis.pdf", "Bio-Health Data Analysis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
