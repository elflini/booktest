<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Regression Analysis | Statistics for Health Data Science</title>
  <meta name="description" content="Lecture notes for Statistics for Health Data Science" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Regression Analysis | Statistics for Health Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes for Statistics for Health Data Science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Regression Analysis | Statistics for Health Data Science" />
  
  <meta name="twitter:description" content="Lecture notes for Statistics for Health Data Science" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2021-12-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter2.html"/>
<link rel="next" href="chapter4.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistics for Health Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Statistics and Health Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#statistics-and-organic-statistics"><i class="fa fa-check"></i><b>1.2</b> Statistics and Organic Statistics</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#statistical-methods-and-models"><i class="fa fa-check"></i><b>1.3</b> Statistical Methods and Models</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#health-care-data"><i class="fa fa-check"></i><b>1.4</b> Health Care Data</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="chapter1.html"><a href="chapter1.html#medical-claims"><i class="fa fa-check"></i><b>1.4.1</b> Medical Claims</a></li>
<li class="chapter" data-level="1.4.2" data-path="chapter1.html"><a href="chapter1.html#medical-records"><i class="fa fa-check"></i><b>1.4.2</b> Medical Records</a></li>
<li class="chapter" data-level="1.4.3" data-path="chapter1.html"><a href="chapter1.html#health-surveys"><i class="fa fa-check"></i><b>1.4.3</b> Health Surveys</a></li>
<li class="chapter" data-level="1.4.4" data-path="chapter1.html"><a href="chapter1.html#disease-registries"><i class="fa fa-check"></i><b>1.4.4</b> Disease Registries</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#outline-of-the-text"><i class="fa fa-check"></i><b>1.5</b> Outline of the Text</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Key Statistical Concepts</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#samples-and-populations"><i class="fa fa-check"></i><b>2.1</b> Samples and Populations</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#statistics-basics"><i class="fa fa-check"></i><b>2.2</b> Statistics Basics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter2.html"><a href="chapter2.html#random-variables"><i class="fa fa-check"></i><b>2.2.1</b> Random Variables</a></li>
<li class="chapter" data-level="2.2.2" data-path="chapter2.html"><a href="chapter2.html#dependent-and-independent-variables"><i class="fa fa-check"></i><b>2.2.2</b> Dependent and Independent Variables</a></li>
<li class="chapter" data-level="2.2.3" data-path="chapter2.html"><a href="chapter2.html#statistical-distributions-and-their-summaries"><i class="fa fa-check"></i><b>2.2.3</b> Statistical Distributions and Their Summaries</a></li>
<li class="chapter" data-level="2.2.4" data-path="chapter2.html"><a href="chapter2.html#parameters-and-models"><i class="fa fa-check"></i><b>2.2.4</b> Parameters and Models</a></li>
<li class="chapter" data-level="2.2.5" data-path="chapter2.html"><a href="chapter2.html#estimation-and-inference"><i class="fa fa-check"></i><b>2.2.5</b> Estimation and Inference</a></li>
<li class="chapter" data-level="2.2.6" data-path="chapter2.html"><a href="chapter2.html#variation-and-standard-error"><i class="fa fa-check"></i><b>2.2.6</b> Variation and Standard Error</a></li>
<li class="chapter" data-level="2.2.7" data-path="chapter2.html"><a href="chapter2.html#conditional-and-marginal-means"><i class="fa fa-check"></i><b>2.2.7</b> Conditional and Marginal Means</a></li>
<li class="chapter" data-level="2.2.8" data-path="chapter2.html"><a href="chapter2.html#joint-and-mixture-distributions"><i class="fa fa-check"></i><b>2.2.8</b> Joint and Mixture Distributions</a></li>
<li class="chapter" data-level="2.2.9" data-path="chapter2.html"><a href="chapter2.html#pmf-plot"><i class="fa fa-check"></i><b>2.2.9</b> pmf plot</a></li>
<li class="chapter" data-level="2.2.10" data-path="chapter2.html"><a href="chapter2.html#comparison-with-simulated-value"><i class="fa fa-check"></i><b>2.2.10</b> comparison with simulated value</a></li>
<li class="chapter" data-level="2.2.11" data-path="chapter2.html"><a href="chapter2.html#the-poisson-and-negative-binomial-distributions-for-counts"><i class="fa fa-check"></i><b>2.2.11</b> The Poisson and Negative Binomial Distributions for Counts</a></li>
<li class="chapter" data-level="2.2.12" data-path="chapter2.html"><a href="chapter2.html#pmf-plot-1"><i class="fa fa-check"></i><b>2.2.12</b> pmf plot</a></li>
<li class="chapter" data-level="2.2.13" data-path="chapter2.html"><a href="chapter2.html#comparison-with-simulated-value-1"><i class="fa fa-check"></i><b>2.2.13</b> comparison with simulated value</a></li>
<li class="chapter" data-level="2.2.14" data-path="chapter2.html"><a href="chapter2.html#pdf-plot"><i class="fa fa-check"></i><b>2.2.14</b> pdf plot</a></li>
<li class="chapter" data-level="2.2.15" data-path="chapter2.html"><a href="chapter2.html#comparison-with-simulated-value-2"><i class="fa fa-check"></i><b>2.2.15</b> comparison with simulated value</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Regression Analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#trends-in-body-mass-index-in-the-united-states"><i class="fa fa-check"></i><b>3.2</b> Trends in Body Mass Index in the United States</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#regression-overview"><i class="fa fa-check"></i><b>3.3</b> Regression Overview</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="chapter3.html"><a href="chapter3.html#regression-to-quantify-association"><i class="fa fa-check"></i><b>3.3.1</b> Regression to Quantify Association</a></li>
<li class="chapter" data-level="3.3.2" data-path="chapter3.html"><a href="chapter3.html#regression-to-explain-variability"><i class="fa fa-check"></i><b>3.3.2</b> Regression to Explain Variability</a></li>
<li class="chapter" data-level="3.3.3" data-path="chapter3.html"><a href="chapter3.html#regression-to-estimate-the-effect-of-an-intervention"><i class="fa fa-check"></i><b>3.3.3</b> Regression to Estimate the Effect of an Intervention</a></li>
<li class="chapter" data-level="3.3.4" data-path="chapter3.html"><a href="chapter3.html#regression-to-predict-outcomes"><i class="fa fa-check"></i><b>3.3.4</b> Regression to Predict Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#an-organic-view-of-regression"><i class="fa fa-check"></i><b>3.4</b> An Organic View of Regression</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#the-linear-regression-equation-and-its-assumptions"><i class="fa fa-check"></i><b>3.5</b> The Linear Regression Equation and Its Assumptions</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#linear-regression-estimation-and-interpretation"><i class="fa fa-check"></i><b>3.6</b> Linear Regression Estimation and Interpretation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="chapter3.html"><a href="chapter3.html#estimation-of-the-regression-coefficients"><i class="fa fa-check"></i><b>3.6.1</b> Estimation of the Regression Coefficients</a></li>
<li class="chapter" data-level="3.6.2" data-path="chapter3.html"><a href="chapter3.html#interpretation-of-the-regression-coefficients"><i class="fa fa-check"></i><b>3.6.2</b> Interpretation of the Regression Coefficients</a></li>
<li class="chapter" data-level="3.6.3" data-path="chapter3.html"><a href="chapter3.html#confounding"><i class="fa fa-check"></i><b>3.6.3</b> Confounding</a></li>
<li class="chapter" data-level="3.6.4" data-path="chapter3.html"><a href="chapter3.html#moderation-or-interaction"><i class="fa fa-check"></i><b>3.6.4</b> Moderation or Interaction</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#model-selection-and-hypothesis-testing"><i class="fa fa-check"></i><b>3.7</b> Model Selection and Hypothesis Testing</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#checking-assumptions-about-the-random-part"><i class="fa fa-check"></i><b>3.8</b> Checking Assumptions About the Random Part</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#do-i-have-a-good-model-goodness-of-fit-and-model-adequacy"><i class="fa fa-check"></i><b>3.9</b> Do I Have a Good Model? Goodness of Fit and Model Adequacy</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#quantile-regression"><i class="fa fa-check"></i><b>3.10</b> Quantile Regression</a></li>
<li class="chapter" data-level="3.11" data-path="chapter3.html"><a href="chapter3.html#non-parametric-regression"><i class="fa fa-check"></i><b>3.11</b> Non-parametric Regression</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Binary and Categorical Outcomes</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#binary-outcomes"><i class="fa fa-check"></i><b>4.1</b> Binary Outcomes</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics for Health Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter3" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Regression Analysis</h1>
<!--------------- Chapter 3. Regression Analysis ---------------->
<ul>
<li><p>Regression analysis is the quantitative framework that is most commonly used to establish whether outcomes are associated with individual, community, or environmental characteristics.</p>
<p><code>-</code> Strength of relationships in conceptual models of health care utilization and costs.</p>
<p><code>-</code> Estimates effects of health interventions.</p>
<p><code>-</code> Enables prediction of future costs and outcomes.</p></li>
</ul>
<div id="introduction-1" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<ul>
<li><p>Question: How variables relate to one another in health services and health outcome research.</p>
<p><code>-</code> Regression analysis is a body of statistical tools and techniques.</p>
<p><code>-</code> Traditional regression analysis (OLS) focuses on modeling the relationship between the mean of an outcome and subject characteristics.</p>
<p><code>-</code> Different regression techniques are appropriate depending on the specific question of interest and the data available.</p></li>
<li><p>Example: Study trends in overweight and obese persons</p>
<p><code>-</code> Linear regression analysis</p>
<p><code>-</code> Outcome: Body mass index (BMI)</p>
<p><code>-</code> Goal: Summarize the association between mean BMI and calendar year.</p></li>
<li><p>Certain outcomes and types of questions call for specialized regression techniques.</p></li>
<li><p>Example: How the third quartile (75th percentile) or the highest decile (90th percentile) of BMI changes over time?</p>
<p><code>-</code> Quantile regression would be appropriate instead of OLS.</p></li>
<li><p>Example: proportion of the population that is overweight or obese,
<code>-</code> Logistic regression analysis</p></li>
<li><p>Example: Studying health care utilization outcomes (e.g., numbers of hospitalizations and outpatient visits)</p>
<p><code>-</code> Regression analysis for count outcomes such as Poisson or negative binomial regression.</p></li>
<li><p>A prototype for regression analysis of medical expenditures:</p>
<p><code>-</code> Research question: Association between chronic pain as assessed via survey questions and annual medical expenditures</p>
<p><code>-</code> Data source: Administrative records of the survey participants.</p>
<p><code>-</code> Fraction of the population does not use any medical care, the distribution of annual medical expenditures includes a portion of values that are zero.</p>
<p><code>-</code> Specialized regression technique is needed to handle this type of mixture.</p></li>
<li><p>There are many other factors that may drive medical expenditures in addition to, and potentially in lockstep with, chronic pain.</p></li>
<li><p>In the chronic pain study, the stated goal is to capture the “incremental costs of medical care due to pain by comparing the costs of health care of persons with chronic pain to those who do not report chronic pain, controlling for health needs, demographic characteristics, and socioeconomic status” (emphasis added).</p></li>
<li><p>We discuss what is meant by <strong>“controlling for”</strong> variables in regression analysis.</p></li>
</ul>
</div>
<div id="trends-in-body-mass-index-in-the-united-states" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Trends in Body Mass Index in the United States</h2>
<ul>
<li><p>The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess nutrition and health status among children and adults in the United States.</p>
<p><code>-</code> Began in the early 1960s</p>
<p><code>-</code> Examines a nationally representative sample of about 5000 individuals every 2 years.</p>
<p><code>-</code> Collection of self-reported information about nutrition and health</p>
<p><code>-</code> Broad range of objective health measurements, including anthropometric measures and markers associated with chronic conditions like hypertension and diabetes.</p>
<p><code>-</code> The NHANES is the most authoritative source of data on overweight and obese persons in the United States.</p></li>
<li><p>BMI is a measure of body fat based on height and weight</p>
<p><code>-</code> body mass in kilograms divided by the square of the body height in meters.</p></li>
<li><p>Standard adult BMI by the World Health Organization (WHO)</p>
<p><code>-</code> 30.0 ~ higher: Obese</p>
<p><code>-</code> 25.0 ~ 29.9: Overweight</p>
<p><code>-</code> 18.5 ~ 24.9: Normal or healthy weight</p>
<p><code>-</code> ~ 18.5: Underweight</p></li>
<li><p>Many studies have been conducted of BMI trends in the United States, sounding an alarm about the obesity epidemic and its implications for chronic disease morbidity and mortality.</p></li>
<li><p>In this chapter, we use regression analysis to study data from NHANES surveys conducted in 1999–2000 and 2015–2016 to quantify the change in BMI over this interval and how it relates to age, sex, and race/ethnicity.</p></li>
<li><p>We focus on BMI in adults age 20–59 years since patterns of body weight and their drivers can differ markedly in younger and older persons.</p></li>
</ul>
</div>
<div id="regression-overview" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Regression Overview</h2>
<ul>
<li><p>There are many ways to think about regression.</p></li>
<li><p>In essence, regression is a way of learning about mechanisms, or drivers, of an outcome.</p></li>
<li><p>The variable <span class="math inline">\(X\)</span>: Called the independent variable, covariate, risk factor, predictor, or feature.</p></li>
<li><p>The variable <span class="math inline">\(Y\)</span>: Called the dependent variable, response, or outcome.</p></li>
<li><p>Basic assumption: It is possible to clearly identify which variables are covariates and which are outcomes.</p></li>
<li><p>Key method: Univariate regression models which have a single outcome.</p></li>
<li><p>Key example: <span class="math inline">\(Y\)</span> is BMI, and <span class="math inline">\(X\)</span> represents a list of variables: age, sex, race/ethnicity, and calendar year.</p></li>
<li><p>Four distinct but connected ways of thinking about linear regression:</p>
<p><code>-</code> To quantify association</p>
<p><code>-</code> To explain variability</p>
<p><code>-</code> To estimate the effect of an intervention</p>
<p><code>-</code> To predict future outcomes</p></li>
</ul>
<div id="regression-to-quantify-association" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Regression to Quantify Association</h3>
<ul>
<li><p>A way to quantify the extent to which a covariate (<span class="math inline">\(X\)</span>) is associated with an outcome (<span class="math inline">\(Y\)</span>).</p></li>
<li><p>The term “association”: generic relationship observed between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> that may or may not be causal.</p></li>
<li><p>This perspective zeros in on whether <span class="math inline">\(X\)</span> is a driver of <span class="math inline">\(Y\)</span>, even though, in observational settings, <strong>establishing an association is not the same as establishing causality</strong>.</p></li>
<li><p>Linear regression provides a one-number summary of how the mean of <span class="math inline">\(Y (E(Y))\)</span> changes as <span class="math inline">\(X\)</span> changes.</p></li>
<li><p>A non-linear association can be studied by including covariates (e.g., <span class="math inline">\(log(X)\)</span> or <span class="math inline">\(X^2\)</span>) or by transforming the response <span class="math inline">\(Y\)</span> (e.g., <span class="math inline">\(log(Y)\)</span>) in the regression analysis.</p></li>
</ul>
</div>
<div id="regression-to-explain-variability" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Regression to Explain Variability</h3>
<ul>
<li><p>Understanding the variability of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Question: How much of the variability can be attributed to variation in values of <span class="math inline">\(X\)</span>?</p>
<ul>
<li>In the case of linear regression, all of the variability in <span class="math inline">\(Y\)</span> is driven by <span class="math inline">\(X\)</span>, then we expect the values of <span class="math inline">\(Y\)</span> to fall on a line in the <span class="math inline">\(X–Y\)</span> plane.</li>
</ul></li>
</ul>
<p><img src="fig/fig3.1.png" /></p>
<p><code>-</code> The left panel also shows the variability of <span class="math inline">\(Y\)</span> as deviations of the <span class="math inline">\(Y\)</span> values from the horizontal line at the average <span class="math inline">\(Y\)</span> value; this panel ignores any dependence of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and amounts to assuming that there is no systematic component.</p>
<p><code>-</code> The right panel gives more insight into how much of the variability in <span class="math inline">\(Y\)</span> can be explained by a linear relationship with <span class="math inline">\(X\)</span>.</p>
<p><code>-</code> We would infer from Fig. 3.1 that much of the variance of <span class="math inline">\(Y\)</span> is explained by variation in <span class="math inline">\(X\)</span> and there is little unexplained variance.</p>
<ul>
<li><p>The modest unexplained variance may be attributable to other <span class="math inline">\(X\)</span> variables and/or to random variation in the population.</p></li>
<li><p>However, in the setting of health care utilization and costs, set of variables <span class="math inline">\(X\)</span> to be associated with <span class="math inline">\(Y\)</span> but not to explain much of the variability in <span class="math inline">\(Y\)</span>.</p></li>
</ul>
</div>
<div id="regression-to-estimate-the-effect-of-an-intervention" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Regression to Estimate the Effect of an Intervention</h3>
<ul>
<li><p>Association between a driver <span class="math inline">\(X\)</span> and an outcome <span class="math inline">\(Y\)</span>, but this perspective specifically considers drivers that are interventions.</p></li>
<li><p><strong>Intervention</strong>: Activity or process designed to change outcomes</p>
<p><code>-</code> New treatment to improve cancer survival</p>
<p><code>-</code> Practice of wearing seatbelts to reduce automobile fatalities</p></li>
<li><p><strong>Regression analyses of interventions typically aim to quantify causal effects</strong>.</p></li>
<li><p>However, an association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> based on observational data generally does not permit a causal interpretation.</p>
<p><code>-</code> Do not control the value of the intervention (<span class="math inline">\(X\)</span>).</p>
<p><code>-</code> Do not know why a specific subject selects their observed <span class="math inline">\(X\)</span> value.</p></li>
<li><p>Nevertheless, there are methods designed for causal inference that build on a regression framework.</p></li>
</ul>
</div>
<div id="regression-to-predict-outcomes" class="section level3" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Regression to Predict Outcomes</h3>
<ul>
<li><p>Predicting values of the response <span class="math inline">\(Y\)</span> using information on the predictor(<span class="math inline">\(s\)</span>) X.</p></li>
<li><p>Example: an insurance company trying to determine client premiums might want to use all available data on a new client to predict their expected annual medical expenditures.</p></li>
<li><p>If a linear regression model is predictive of <span class="math inline">\(Y\)</span>, then this is generally synonymous with <span class="math inline">\(X\)</span> explaining a large portion of the observed variability in <span class="math inline">\(Y\)</span>.</p>
<p><code>-</code> Association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> exists.</p>
<p><code>-</code> However, the opposite is not necessarily true.</p>
<p><code>-</code> <span class="math inline">\(X\)</span> may be strongly associated with <span class="math inline">\(Y\)</span>, but it may only account for a minority of the observed variability in <span class="math inline">\(Y\)</span>.</p></li>
<li><p>In general, it is important to be clear about the objective of an analysis because this determines the type of regression analysis that may be appropriate.</p></li>
</ul>
<p><img src="fig/table3.1.png" /></p>
<ul>
<li><p>Same regression model and the same tools can often be used for several objectives.</p></li>
<li><p>Similarly, a model built for prediction may not be a good model for understanding the effect of an intervention.</p></li>
</ul>
</div>
</div>
<div id="an-organic-view-of-regression" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> An Organic View of Regression</h2>
<ul>
<li><p>At a very basic level, a regression model is just an organized collection of averages of outcomes within sub-populations defined by their covariate values.</p></li>
<li><p>For a single binary or categorical covariate, regression reduces to calculating the average of the outcome at each level of the covariate.</p></li>
<li><p>For a continuous covariate or when there are multiple covariates, calculating the average for each sub-population separately becomes tedious.</p></li>
<li><p>More importantly, the sample size for each sub-population may become very small, making the individual averages unreliable.</p></li>
<li><p>Regression modeling formulates the dependence of the outcome on the covariates in a way that addresses this problem.</p></li>
<li><p>We start with the simple case of a binary covariate and the perspective of explained variation.</p></li>
</ul>
<p><img src="fig/fig3.2.png" /></p>
<ul>
<li><p>Figure 3.2 shows superimposed histograms of BMI levels in 1999–2000 and 2015–2016.</p></li>
<li><p>There is a shift toward higher BMI in 2015–2016 compared to 1999– 2000.</p></li>
<li><p>When ignoring year, average BMI is 29.1.</p>
<p><code>-</code> The average BMI was 28.4 in 1999–2000 and 29.6 in 2015–2016, an increase of 1.2 units.</p></li>
<li><p>What about the variability in BMI? When ignoring year, the variance is 49.8; this is based on subtracting the overall mean from each BMI value.</p></li>
<li><p>How much of this variability is due to differences between years?</p></li>
<li><p>To answer this question, we calculate the variance after subtracting the year-specific mean from each observed BMI and obtain 49.5.</p></li>
<li><p>Thus, by considering the sub-populations defined by year, the variance is reduced from 49.8 to 49.5, a very modest reduction of (49.8-49.5)/49.8=0.7%.</p></li>
<li><p>This result suggests that less than 1% of the variability in BMI levels is attributable to calendar year.</p></li>
<li><p>Indeed, many factors are involved in explaining population variability in BMI.</p></li>
<li><p>The fact that year explains only a small minority of the variability in BMI does not, however, imply that BMI and calendar year are not associated.</p></li>
<li><p>In fact, we will show that there is a statistically significant increase in BMI over time.</p></li>
<li><p>The distribution of BMI levels can also be calculated for sub-populations defined by variables with more than two categories, such as age.</p></li>
</ul>
<p><img src="fig/fig3.3.png" /></p>
<p>Figure 3.3 shows a scatterplot of age versus BMI, with line segments indicating average BMI levels for each single-age sub-population.</p>
<ul>
<li><p>Several important properties of the data are apparent.</p></li>
<li><p>First, average BMI levels in the sub-populations show small fluctuations from one age to the next, probably due to the small sample size within each sub- population (compared to the total sample size).</p></li>
<li><p>Second, the average BMI tends to increase with age.</p></li>
<li><p>Third, a straight line reasonably approximates the increasing average BMI with age.</p></li>
<li><p>This third property motivates using the line to characterize how the average BMI changes across sub-populations defined by age.</p></li>
<li><p>This smooths out the fluctuations due to small sample sizes and more simply represents the relationship between age and BMI.</p></li>
<li><p>Both smoothing and simplicity are important goals when the covariate X takes many values or there are several covariates.</p></li>
</ul>
<p><img src="fig/table3.2.png" /></p>
<ul>
<li>Table 3.2 compares average BMI calculated for each age directly to those obtained by the regression line (REG) in Fig. 3.3:</li>
</ul>
<p><span class="math display">\[
Mean BMI = 26.37 + 0.07 \times AGE.
\]</span></p>
<ul>
<li><p>As the table shows, the differences are relatively small, and it is much simpler to cite the regression equation to describe the relationship between age and BMI than to provide a detailed listing of averages.</p></li>
<li><p>The decision to replace the list of averages with a straight line is often not justified, however.</p></li>
<li><p>Whether and when to use linear regression bring us to a closer examination of the model and its assumptions.</p></li>
</ul>
</div>
<div id="the-linear-regression-equation-and-its-assumptions" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> The Linear Regression Equation and Its Assumptions</h2>
<ul>
<li>The well-known formula for how Y depends on a set of <span class="math inline">\(k\)</span> covariates <span class="math inline">\(X_1,\ldots , X_k\)</span>: the regression model is</li>
</ul>
<p><span class="math display">\[
E(Y| X_1,\ldots, X_k) = \beta_0 + \beta_1X_1 +\cdots    + \beta_kX_k.
\]</span></p>
<ul>
<li><p>The expression on the left, <span class="math inline">\(E(Y|X1,\ldots, X_k)\)</span>, indicates the mean of the outcome <span class="math inline">\(Y\)</span> (e.g., BMI) in the sub-population defined by specific values for the covariates <span class="math inline">\(X_1,\ldots, X_k\)</span> (e.g., age, sex, race/ethnicity, and calendar year).</p></li>
<li><p>The assumption of linearity is not trivial.</p>
<p><code>-</code> Rarely holds in observational datasets.</p>
<p><code>-</code> Approximation for how the actual mean varies across sub-populations.</p></li>
<li><p>The linearity assumption specifies that a one-unit increase in the covariate <span class="math inline">\(X_1\)</span> will result in an increase of <span class="math inline">\(\beta_1\)</span> units of the mean of <span class="math inline">\(Y\)</span> <em>regardless of the initial value of <span class="math inline">\(X_1\)</span> and given any set of values for the other covariates <span class="math inline">\(X_2,\ldots ,X_k\)</span></em>.</p></li>
<li><p>A similar interpretation applies for all the other covariates including categorical covariate.</p></li>
<li><p>Even if transformations of <span class="math inline">\(X\)</span>, such as polynomials (e.g., X2) or other non-linear functions (e.g., <span class="math inline">\(log(X)\)</span>), are added to the list of covariates, the model is still considered a linear model so long as these terms enter additively into the regression equation.</p></li>
<li><p>The objective: to estimate the coefficients <span class="math inline">\(\beta_1,\ldots ,\beta_k\)</span> that quantify the relationships between the <span class="math inline">\(X\)</span>s and <span class="math inline">\(Y\)</span>.</p></li>
<li><p>We isolate the relationship between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y\)</span>, and we say that we are <strong>adjusting</strong> or <strong>controlling</strong> for the other <span class="math inline">\(X\)</span>s.</p></li>
<li><p>In practice, individuals in a sub-population rarely have identical values for the response variable.</p></li>
<li><p>The model for individual <span class="math inline">\(i\)</span> is then:</p></li>
</ul>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1X_{i1} + \cdots + \beta_kX_{ik} + \epsilon_i.
\]</span></p>
<ul>
<li><p>The <span class="math inline">\(\epsilon\)</span> on the left-hand side of the previous equation that was used to indicate the mean (or expected value) of a sub-population is omitted, and a new term <span class="math inline">\(\epsilon_i\)</span> is added to the right-hand side to reflect a subject-specific deviation from the sub-population mean <span class="math inline">\(\beta_0 +\beta_1X_{i1}+\cdots +\beta_kX_{ik}\)</span>.</p></li>
<li><p>Three key assumptions.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>The random terms follow a normal distribution with mean zero.</p></li>
<li><p>This normal distribution has the same variance in all sub-populations.</p></li>
<li><p>The random parts of all observations are independent.</p></li>
</ol>
<ul>
<li><p>These three assumptions are not actually needed to validly fit the regression model.</p></li>
<li><p>So long as the systematic part is correctly specified, the estimates of the <span class="math inline">\(\beta\)</span>s are valid and unbiased, regardless of whether assumptions 1–3 are satisfied.</p></li>
<li><p>But standard errors for the <span class="math inline">\(\beta\)</span> estimates are based on assumptions about the random part</p>
<p><code>-</code> If these don’t hold, the standard errors may not be valid.</p>
<p><code>-</code> May ultimately generate incorrect confidence intervals, p-values, and inferences.</p></li>
</ul>
</div>
<div id="linear-regression-estimation-and-interpretation" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Linear Regression Estimation and Interpretation</h2>
<div id="estimation-of-the-regression-coefficients" class="section level3" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Estimation of the Regression Coefficients</h3>
<ul>
<li>Two methods for estimating the <span class="math inline">\(\beta\)</span>s:</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Least squares</p></li>
<li><p>Maximum likelihood</p></li>
</ol>
<ul>
<li><p>Each method amounts to solving an optimization problem.</p></li>
<li><p>Least squares</p>
<p><code>-</code> The problem is to identify the <span class="math inline">\(\beta\)</span>s that minimize the sum of the squared differences between the responses <span class="math inline">\(Y\)</span> and the systematic part of the regression equation, <span class="math inline">\(\beta_0+\beta_1X_1+\cdots+ \beta_kX_k\)</span>, which is a straight line.</p>
<p><code>-</code> Figure 3.1 demonstrates the least squares approach.</p>
<p><code>-</code> Note: the method does not use any of the assumptions regarding the random part of the regression equation; instead, it fits the regression line using only the systematic part.</p></li>
<li><p>Maximum likelihood</p>
<p><code>-</code> Exploit the random part of the model.</p>
<p><code>-</code> Maximum likelihood tries to determine the line <span class="math inline">\(\beta_0+\beta_1X_1+\cdots+ \beta_kX_k\)</span> that is most plausible or most likely to have generated the observed <span class="math inline">\(Y\)</span> values.</p>
<p><code>-</code> When the random part is normally distributed, the least squares estimates are identical to the maximum likelihood estimates.</p></li>
</ul>
</div>
<div id="interpretation-of-the-regression-coefficients" class="section level3" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Interpretation of the Regression Coefficients</h3>
<ul>
<li>Table 3.3 provides the results from fitting a linear regression model to NHANES data for persons age 20–59 years in 1999–2000 and 2015–2016.</li>
</ul>
<p><img src="fig/table3.3.png" /></p>
<ul>
<li><p>The outcome is BMI, and the main question is how this outcome has changed over time.</p></li>
<li><p>We have two time points, and the regression equation includes a binary variable SEX (with reference level male), a categorical variable RACE (with reference level non- Hispanic white) coded as a set of dummy variables, a binary variable AGE (with reference level 50 years), and a binary variable YEAR (with reference level 1999–2000).</p></li>
<li><p>Although we will show that assumptions of normality and constant variance are likely not satisfied for this regression model, we use it here to illustrate interpretation of the regression coefficients that comprise the systematic part of the model.</p></li>
<li><p>The intercept represents mean BMI at the reference level for all variables.</p></li>
<li><p>Thus, the regression analysis estimates that the mean BMI for non-Hispanic white men age 50 years in 1999–2000 was 26.97.</p></li>
<li><p>The coefficient for YEAR is 1.54, meaning that the average BMI increased by 1.54 units from the 1999–2000 survey to the 2015–2016 survey.</p></li>
<li><p>The 95% confidence interval for the increase in BMI during the 15 years examined is <span class="math inline">\(1.54 \pm 1.96 \times 0.18\)</span>, or 1.19 to 1.89.</p></li>
</ul>
<p><img src="fig/table3.4.png" /></p>
<ul>
<li><p>Table 3.4 shows the fitted regression model if age is coded as a continuous variable instead.</p></li>
<li><p>The model estimates a slightly smaller increase in mean BMI over time, about 1.47 units.</p></li>
<li><p>The regression intercept still represents mean BMI at the reference level for all categorical variables and, now that a continuous variable is included, when that variable is equal to zero.</p></li>
<li><p>For categorical or factor covariates, consideration should be given to the choice of reference level.</p></li>
<li><p>The reference level should be meaningful for the analysis and adequately represented in the sample. For race/ethnicity in this example, we used non-Hispanic white as the reference level.</p></li>
<li><p>Ordered categorical variables can sometimes be specified as continuous variables (e.g., by replacing household income brackets by their midpoints), but this will impose strict constraints that may not match the data (linear dependence on household brackets).</p></li>
</ul>
<p><img src="fig/table3.5.png" /></p>
<ul>
<li>Table 3.5 summarizes some important analytic considerations and principles when incorporating different types of variables in the regression equation.</li>
</ul>
</div>
<div id="confounding" class="section level3" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> Confounding</h3>
<ul>
<li><p>It is tempting to interpret the coefficient of a covariate as its causal effect on the outcome, meaning that changing the value of that covariate by one unit will result in a change of the outcome by the corresponding <span class="math inline">\(beta\)</span> coefficient.</p></li>
<li><p>However, in observational data the covariate of interest is often associated with other covariates that vary together to affect the outcome.</p>
<p><code>-</code> This phenomenon, called <strong>confounding</strong>, is an unavoidable problem whenever observational data are analyzed.</p>
<p><code>-</code> Formally, we define a covariate <span class="math inline">\(Z\)</span> as being a <strong>confounder</strong> of the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> if <span class="math inline">\(Z\)</span> affects both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Confounding is a primary reason for why we must be very cautious when making causal inferences from observational data and why we always have to question whether there may be other explanations for any associations identified between a covariate and the outcome.</p></li>
<li><p>For clarity, suppose we are interested in the association between an outcome <span class="math inline">\(Y\)</span> and a covariate <span class="math inline">\(X\)</span>.</p>
<p><code>-</code> For example, <span class="math inline">\(Y\)</span> may be BMI and <span class="math inline">\(X\)</span> may be a binary indicator of regular physical exercise (PE), with <span class="math inline">\(X=1\)</span> if yes and <span class="math inline">\(X=0\)</span> if no.</p>
<p><code>-</code> In the NHANES sample data from 2015–2016, a question about weekly recreational exercise asked: “In a typical week do you do any vigorous-intensity sports, fitness, or recreational activities that cause large increases in breathing or heart rate like running or basketball for at least 10 min continuously?”</p>
<p><code>-</code> We use the response to this question as our PE variable.</p>
<p><code>-</code> It is well known that, as we age, we become less active and BMI tends to increase; thus AGE could be reasonably considered to be a confounder in our analysis because it affects both BMI and PE.</p>
<p><code>-</code> In the NHANES sample data, again restricted to ages 20–59 years, there is a significant negative correlation between AGE and PE and, as previously shown, a significant positive correlation between AGE and BMI.</p></li>
<li><p>Running a simple linear regression of BMI on PE, we unsurprisingly find that average BMI is significantly lower for individuals who exercise, with the estimated coefficient of PE equal to 2.15.</p></li>
<li><p>If we intervene and convince all inactive individuals to engage in regular physical exercise, is it reasonable to expect a reduction of 2.15 units in average BMI?</p>
<p><code>-</code> The problem with interpreting this association as a causal relationship is that AGE is lurking in the shadows.</p>
<p><code>-</code> As the PE indicator changes from 0 to 1, the corresponding sub-population becomes younger; the PE variable and the AGE variable change together.</p>
<p><code>-</code> Thus, in the simple linear regression with just PE as the lone covariate, the estimated association between BMI and PE is boosted by the (lurking) association between AGE and PE.</p></li>
<li><p>When we include both PE and AGE in the model, the coefficient of PE is still negative and significant, but it is reduced in magnitude to -1.88.</p></li>
<li><p>The boost from the implicit effect of the younger sub- population is gone because the effect of PE is estimated keeping age constant.</p></li>
<li><p>When we interpret the coefficient of PE, we are now assessing only the effect of PE on BMI, adjusting or controlling for the effect of AGE.</p></li>
<li><p>This explanation assumes that the linear model is correct, and there are likely other confounders of the BMI-PE association, making causal interpretation on the basis of these few covariates a bit of a stretch.</p></li>
<li><p>Still, this example serves to illustrate the mechanics of confounder adjustment in multiple regression.</p></li>
<li><p>In observational studies, associations of interest are almost always subject to confounding, sometimes by multiple variables.</p></li>
</ul>
</div>
<div id="moderation-or-interaction" class="section level3" number="3.6.4">
<h3><span class="header-section-number">3.6.4</span> Moderation or Interaction</h3>
<ul>
<li><p>In the previous sections, an increase in mean BMI over time was assumed to be the same in all sub-populations.</p></li>
<li><p>But is the change in mean BMI over time indeed the same for younger as for older persons? Is it the same for all racial/ethnic groups?</p></li>
<li><p>This question brings us to the concept of effect modification, also called <strong>moderation</strong> or <strong>interaction</strong>.</p></li>
<li><p><strong>Effect modification</strong> occurs when the association between a covariate <span class="math inline">\(X\)</span> and an outcome <span class="math inline">\(Y\)</span> differs according to the levels of another covariate (<span class="math inline">\(Z\)</span>).</p>
<p><code>-</code> We say that covariate <span class="math inline">\(Z\)</span> modifies or moderates the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p><code>-</code> In the case where <span class="math inline">\(X\)</span> is YEAR and <span class="math inline">\(Z\)</span> is (categorical) AGE, we say that AGE modifies the association between YEAR and BMI if the change in BMI over time differs for younger versus older persons.</p></li>
<li><p>There are at least two ways to learn about whether the association between YEAR and BMI is modified by AGE:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Fit two separate regression models corresponding to the two levels of AGE. In our example, this would yield one analysis for persons age 50 years and another analysis for persons age &gt;50 years.</p></li>
<li><p>Fit a single regression model, but include an interaction term (i.e., the product of YEAR and AGE) in the linear model that already includes these covariates.</p></li>
</ol>
<ul>
<li><p>The first approach provides separate estimates of the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> within each subgroup defined by the levels of <span class="math inline">\(Z\)</span>.</p>
<p><code>-</code> However, it also provides separate estimates of the regression coefficients for all other covariates in the model.</p>
<p><code>-</code> Not only is this approach potentially inefficient, it also does not provide a formal mechanism for testing whether the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> differs across the levels of <span class="math inline">\(Z\)</span>.</p></li>
<li><p>Following Equation shows the regression equation for the second approach:</p></li>
</ul>
<p><span class="math display">\[
E(Y) = \beta_0 + \beta_1YEAR + \beta_2AGE + \beta_3AGE \times YEAR. 
\]</span></p>
<ul>
<li>The interaction term is non-zero when both AGE and YEAR are equal to 1.</li>
</ul>
<p><code>-</code> It therefore contributes to <span class="math inline">\(E(Y)\)</span> only for the older age group in 2015–2016.</p>
<p><code>-</code> This means that, when we estimate the change in mean BMI from 1999–2000 to 2015– 2016, we end up with a different estimate for older than for younger persons.</p>
<p><img src="fig/table3.6.png" /></p>
<ul>
<li><p>Table 3.6 Change in body mass index between 1999–2000 and 2015–2016
for persons age 20–59 years in NHANES sample data controlling for categorical age, sex, race/ethnicity, and age-year interaction</p>
<p><code>-</code> Specifically, the change for younger persons is given by <span class="math inline">\(\beta_1\)</span>, and the change for older persons is given by <span class="math inline">\(\beta_1+\beta_3\)</span>.</p>
<p><code>-</code> Symmetrically, when we estimate the difference in mean BMI for older versus younger persons, we end up with a different estimate for 1999–2000 (<span class="math inline">\(\beta_2\)</span>) than for 2015–2016 (<span class="math inline">\(\beta_2+\beta_3\)</span>).</p>
<p><code>-</code> The same idea also applies when YEAR is continuous, in which case the interaction term implies a different slope of the corresponding regression line for older versus younger persons.</p></li>
<li><p>Table 3.6 repeats the analysis in Table 3.3 but adds an interaction between AGE and YEAR.</p></li>
<li><p>The interaction term is negative; the estimated change in BMI from 1999–2000 to 2015–2016 is less for older than for younger individuals.</p></li>
<li><p>The global change over the 15-year period of 1.54 units presented in</p></li>
<li><p>Table 3.3 that does not include an interaction term is replaced here with a change of 1.64 units for younger and 1.64 0.49 1.15 units for older individuals.</p></li>
<li><p>Because the model implies that the change in mean BMI depends on AGE, we cannot interpret the coefficient of the YEAR variable as a stand-alone estimate of the change in mean BMI over time.</p></li>
<li><p>In general, when there is an interaction term in the model, the main effects (i.e., the coefficients of the covariates that go into the interaction) cannot be interpreted by themselves without qualification.</p></li>
<li><p>To understand why, recall that the coefficient of a single variable <span class="math inline">\(X\)</span> is interpreted as the expected change in <span class="math inline">\(Y\)</span> corresponding to a unit change in <span class="math inline">\(X\)</span> holding values for all the other covariates in the model constant.</p></li>
<li><p>In the model of BMI with an AGE-YEAR interaction, we cannot change the values of AGE or YEAR by one unit while holding the interaction term (product of AGE and YEAR) fixed.</p></li>
<li><p>Therefore, we cannot interpret the coefficients of AGE and YEAR in the standard manner.</p></li>
<li><p>This is an important point to remember in general in regression analysis; interpretation of coefficients is conditional, and if we cannot conceive of changing one covariate in isolation while holding the others constant, then we may have to re-think our model or its interpretation.</p></li>
</ul>
<p><img src="fig/fig3.4.png" /></p>
<ul>
<li><p>Figure 3.4 shows distributions of BMI levels using centered, stacked dots representing binned observations with lines connecting means for the two survey years within each age, sex, and race/ethnicity stratum.</p></li>
<li><p>A simple linear model without any interaction term assumes that all the slopes are equal, so that the effect of year is the same in all sub-populations.</p></li>
<li><p>The figure shows some deviation from that assumption, suggesting that changes in BMI levels over time may not be the same for all sub-populations.</p></li>
</ul>
</div>
</div>
<div id="model-selection-and-hypothesis-testing" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Model Selection and Hypothesis Testing</h2>
<ul>
<li><p>Several models for mean BMI were fit in the previous section, and many other models could be considered.</p></li>
<li><p>How should we choose the final model? Then, once a final model is selected, how do we evaluate it and draw valid inferences?</p></li>
<li><p>There is a direct line from the objective of the analysis to the final model selected and the evaluation of its goodness.</p></li>
<li><p>If the objective is to test a pre-specified scientific hypothesis, decisions about which variables to include are appropriately guided by a conceptual model and should be made before examining the data.</p></li>
<li><p>If this is not possible, or if the analyst’s intent is to use the data to identify variables that are predictive, classical inference and hypothesis testing based on the fitted model are not warranted and can be seriously biased.</p></li>
<li><p>When we are addressing pre-specified hypotheses, the significance of a single covariate may be evaluated using the Wald test of the null hypothesis that a given coefficient (<span class="math inline">\(beta\)</span>) is different from zero.</p>
<p><code>-</code> In the BMI example, the coefficient of SEX is highly significant (p-value &lt;0.001) in all regressions (Tables 3.3, 3.4, 3.5, and 3.6), but the coefficient of the interaction between AGE and YEAR is not significant at the conventional 5% level (Table 3.6).</p></li>
<li><p>To evaluate the significance of two or more coefficients, we can employ the likelihood ratio test, a powerful approach for many statistical testing problems.</p></li>
<li><p>The likelihood ratio test is a way of determining whether adding RACE to the model adds more explanatory power than would be expected by chance.</p>
<p><code>-</code> Note that the test is appropriate only when comparing two nested models, that is, when the restricted model can be obtained by setting one or more of the coefficients in the full model to zero.</p>
<p><code>-</code> The likelihood ratio test for linear regression with a normally distributed error term is equivalent to an F-test that compares the explained variation of the full and restricted models.</p>
<p><code>-</code> Specifically, it compares the residual or unexplained variance in the full model, <span class="math inline">\(Var_{res}(M_{full})\)</span>, to that in the restricted model, <span class="math inline">\(Var_{res}(M_{restricted})\)</span>.</p></li>
<li><p>In some settings, we may be interested in comparing models that are not nested, e.g., when selecting the final model.</p>
<p><code>-</code> For example, we may want to compare models that include log(AGE) instead of AGE or to compare a categorical coding of AGE versus a continuous coding of AGE. Of course, if we compare a model with many covariates to a model with only few, the former may perform better simply because it is more flexible and carries more explanatory power.</p></li>
<li><p>To even the playing field when comparing less versus more flexible models, a common approach is to penalize the measured performance by the number of parameters used; as the model becomes more flexible, the penalty goes up.</p></li>
<li><p>Two likelihood-based statistics that take this approach are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC).</p></li>
<li><p>These are scaled versions of the negative (maximized) likelihood for a given model (M), but each imposes a penalty for including more variables in the model. Specifically, these statistics are defined as:</p></li>
</ul>
<p><span class="math display">\[
AIC = −2 log[L(M)]+ 2k 
\]</span>
<span class="math display">\[
BIC = −2 log[L(M)]+ k log(n).
\]</span></p>
<ul>
<li><p>Here, <span class="math inline">\(k\)</span> denotes the number of parameters in the model (e.g., in a linear model, <span class="math inline">\(k\)</span> is equal to the number of <span class="math inline">\(\beta\)</span>s plus 1 for <span class="math inline">\(\sigma^2\)</span>, the variance of the residuals, which is also estimated).</p></li>
<li><p>As the model becomes more complex and the likelihood increases, the first term decreases, but the second term increases.</p></li>
<li><p>The model that minimizes the sum of the two terms is preferred. Thus, the goal is to find the model that minimizes the AIC (or BIC).</p></li>
<li><p>The two statistics are similar, but the BIC imposes a more severe penalty for model complexity, so it may select simpler, more parsimonious models than the AIC in certain cases.</p></li>
</ul>
<p><img src="fig/table3.7.png" /></p>
<ul>
<li><p>Table 3.7 shows the number of parameters, log likelihood, AIC, and BIC for the regression models with age coded as a categorical or as a continuous variable with and without the AGE-YEAR interaction term.</p>
<p><code>-</code> Both the AIC and BIC are smallest for the model with age coded as a continuous variable without the AGE–YEAR interaction term.</p>
<p><code>-</code> Consequently, this model should be preferred among the four.</p></li>
<li><p>One important thing to remember is that although the likelihood depends on the random part of the model, the model selection methods compare only the systematic part of the model, and not other aspects like the distribution of the response.</p>
<p><code>-</code> In particular, all models that are compared must use the same response variable, so a model with BMI coded as continuous cannot be compared to a model with BMI coded as a binary response or a model that uses a transformation of BMI, such as log(BMI). Transformations are allowed only for the covariates (<span class="math inline">\(X\)</span>s).</p>
<p><code>-</code> The models must also be based on the same set of observations.</p></li>
<li><p>This is a point that is sometimes overlooked, particularly as some estimation software quietly drops observations with missing values for covariates, which may lead to different numbers of observations in models with different sets of covariates.</p></li>
</ul>
</div>
<div id="checking-assumptions-about-the-random-part" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Checking Assumptions About the Random Part</h2>
<ul>
<li><p>Checking whether these assumptions are satisfied is important because their violation can lead to invalid inference, including incorrect confidence intervals and biased p-values.</p></li>
<li><p>Recall the three basic assumptions regarding the residuals:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Normal distribution</p></li>
<li><p>Variance is similar in all sub-populations</p></li>
<li><p>Independent</p></li>
</ol>
<ul>
<li>The third assumption is usually verified on a case-by-case basis by scrutinizing whether the sample collection imposed dependencies between individuals, such as repeated measurements or observations that were clustered together (e.g., patients treated in the same hospital).</li>
</ul>
<p><img src="fig/fig3.5.png" /></p>
<ul>
<li><p>Figure 3.5 shows diagnostic plots to assess the normality and constant variance assumptions in the BMI example.</p>
<p><code>-</code> The left panel shows a quantile-quantile or Q-Q normal plot, where the empirical quantiles of the model residuals are plotted against the theoretical quantiles expected under a normal model.</p>
<p><code>-</code> When the normality assumption holds, the QQ-normal plot shows a linear trend. Because the quantiles of the model residuals are not linearly related to the theoretical quantiles from a normal distribution, the assumption of normality is suspect for this model.</p>
<p><code>-</code> The right panel shows a scatterplot of residuals versus fitted values. Because the variance of the residuals increases for larger fitted values, the assumption of constant variance also appears to be violated in this model.</p></li>
<li><p>Issues with non-normality diminish in large samples, but nonconstant variance may still impact the validity of hypothesis tests and p-values.</p></li>
<li><p>Standard errors for the <span class="math inline">\(\beta\)</span>s also assume that the observations are independent.</p></li>
<li><p>This independence may not hold for clustered observations (e.g., body mass measurements taken from multiple members of a household) or observations that are taken close together in space or time (e.g., income measurements from a shared census tract).</p></li>
<li><p>When observations are not independent, the regression modeling approach can be extended to accommodate dependence even when the structure of the dependence is not known.</p></li>
<li><p>In general, misspecification of the model (e.g., incorrectly assuming an association is linear or that observations are independent) and omission of key covariates constitute the greatest threats to valid inference and prediction from a regression model.</p></li>
</ul>
</div>
<div id="do-i-have-a-good-model-goodness-of-fit-and-model-adequacy" class="section level2" number="3.9">
<h2><span class="header-section-number">3.9</span> Do I Have a Good Model? Goodness of Fit and Model Adequacy</h2>
<ul>
<li><p>The question of whether a model is “good” is tightly bound to the objective of the analysis.</p></li>
<li><p>To be sure, each of the objectives identified in this chapter requires that the model be correctly specified.</p></li>
<li><p>Yet different criteria emerge for evaluating model adequacy depending on how the model will be used.</p></li>
<li><p>In a hypothesis-driven analysis, there is a well-formed question that translates into a statistical hypothesis test, such as whether there is an increase in BMI over time.</p></li>
<li><p>So long as the assumptions for validity of the test are satisfied, we can feel comfortable that we have a “good” model. If, for example, we use the Wald test to address whether average BMI is higher in 2015–2016 than in 1999–2000, then the relevant coefficient estimate should be normally distributed with true standard error equal to that corresponding to a standard linear regression model.</p></li>
<li><p>Even if the data are not normal, the coefficient estimate may be normally distributed if the sample size is sufficiently large.</p></li>
<li><p>A popular measure that is often cited as a measure of the goodness of a regression model is the coefficient of determination (<span class="math inline">\(R^2\)</span>), which quantifies the variance attributable to the model.</p>
<p><code>-</code> For example, <span class="math inline">\(R^2\)</span> 0.70 means that 70% of the variance in <span class="math inline">\(Y\)</span> is due to (its dependence on) <span class="math inline">\(X\)</span> in the data.</p>
<p><code>-</code> <span class="math inline">\(R^2\)</span> measures the proportion of explained variation in the data, not in the population, and always increases when new covariates are added to the model.</p>
<p><code>-</code> An adjusted <span class="math inline">\(R^2\)</span> (<span class="math inline">\(R^2\)</span>) is used to correct for this and is generally preferred.</p>
<p><code>-</code> If explaining variability is the primary objective, then ideally <span class="math inline">\(R^2\)</span> or its adjusted version should be high.</p>
<p><code>-</code> This can occur even when the regression assumptions are not all met.</p></li>
<li><p>Conversely, a linear regression model may be perfectly satisfied, but <span class="math inline">\(R^2\)</span> may be quite low. For the model in Table 3.4, <span class="math inline">\(R^2\)</span> and <span class="math inline">\(R_{adj}^2\)</span> are 4.8% and 4.7%, respectively.</p>
<p><code>-</code> Thus, AGE, SEX, RACE, and YEAR explain approximately 5% of the variability in BMI values; such low <span class="math inline">\(R^2\)</span> values are typical in observational studies.</p></li>
<li><p>In this hypothesis-driven analysis, however, the objective is to evaluate an association (between AGE and YEAR), so, as noted above, the validity of the assumptions underlying the hypothesis test is paramount.</p></li>
<li><p>And if we want to ascribe a causal interpretation to our inferences, then the assumptions necessary for causal inference must be satisfied.</p></li>
<li><p>While the absolute level of <span class="math inline">\(R^2\)</span> is meaningful, the likelihood-based statistics including the AIC and BIC can only be used to compare models; their absolute values are not indicative of model adequacy or quality.</p></li>
<li><p>The likelihood ratio test and the F-test can only compare nested models, and the F-test is similarly limited in that it is a comparative statistic.</p></li>
<li><p>In fact, none of the standard tests or established summaries used in regression analysis are able to assess a model’s fitness for purpose.</p></li>
<li><p>When prediction is the goal, accuracy of the predictions is the ultimate criterion for evaluating model adequacy.</p>
<p><code>-</code> Predictive accuracy is typically summarized by the average squared prediction error (observed minus predicted values), but other summaries may be used in practice, particularly in the case of binary responses.</p>
<p><code>-</code> Some considerations should be kept in mind when assessing adequacy of a prediction model. First, it may be difficult to identify a specific threshold for the predictive accuracy that reflects an adequately “good” model.</p>
<p><code>-</code> Indeed, in many settings, comparisons of measures of predictive accuracy are used to judge relative rather than absolute performance of models.</p>
<p><code>-</code> Second, models chosen on the basis of predictive accuracy may not be informative about the mechanistic process that generated the data.</p>
<p><code>-</code> Such models are designed to predict, rather than to explain, the data.</p>
<p><code>-</code> Finally, if a model is chosen on the basis of its predictive performance, then hypothesis testing of specific model estimates or coefficients is not advised; making well-founded statistical inferences on the basis of data-adaptive predictive models is still an evolving field.</p></li>
</ul>
</div>
<div id="quantile-regression" class="section level2" number="3.10">
<h2><span class="header-section-number">3.10</span> Quantile Regression</h2>
<ul>
<li><p>In Sect. 3.4, we conceptualized a regression model as an organized collection of average outcomes across sub-populations defined by their covariate values.</p></li>
<li><p>In some settings, other summaries are more informative about the outcome distribution than the mean.</p>
<p><code>-</code> For example, we might prefer the median when the outcome is right skewed or multimodal.</p>
<p><code>-</code> Or we might focus on a high percentile if we want to understand what drives high values of the outcome—for example, if we seek to investigate factors associated with people being in the highest decile of medical expenditures.</p></li>
<li><p>We might even be interested in modeling summaries that tell us about the variability of the outcome rather than its mean.</p></li>
</ul>
<p><code>-</code> As an example, suppose we are interested in the first and third quartiles (i.e., the 25th and 75th percentiles) of BMI for different ages.</p>
<ul>
<li>This might be motivated by questions about whether the variability in BMI grows or shrinks with age; the difference between these two quartiles (inter-quartile range) is a common measure of variability.</li>
</ul>
<p><img src="fig/fig3.6.png" /></p>
<ul>
<li><p>Figure 3.6 shows the fitted values from a quantile regression fit to the first and third quartiles of BMI with age as a covariate.</p></li>
<li><p>The fitted model suggests that if linearity holds, then the inter-quartile range is approximately constant across ages.</p></li>
</ul>
<p><img src="fig/table3.8.png" /></p>
<ul>
<li><p>Multiple covariates can be included in a quantile regression model. Table 3.8 shows the results of quantile regression for the first and third quartiles of BMI with (continuous) age, sex, race/ethnicity, and calendar year as covariates.</p></li>
<li><p>The interpretation is similar to standard regression analysis. For example, the table suggests that the first quartile of BMI increases by 0.08 for each year of age and the third quartile by 0.05.</p></li>
<li><p>The change with calendar year is noticeably larger for the third quartile (1.85) compared to the first quartile (0.89), which is consistent with BMI skewing higher over time.</p></li>
</ul>
</div>
<div id="non-parametric-regression" class="section level2" number="3.11">
<h2><span class="header-section-number">3.11</span> Non-parametric Regression</h2>
<ul>
<li><p>Section 3.10 considered summaries of the distribution of the outcome besides the mean.</p></li>
<li><p>Another way that linear regression can be extended beyond the classical setting is to accommodate more flexible forms of the association between covariates and outcomes.</p></li>
<li><p>One way to do this while still preserving the linear model is to include polynomial and interaction terms.</p></li>
<li><p>A different way is to use smoothing techniques, also known as non-parametric regression.</p></li>
<li><p>A linear regression model implies that a one-unit change in any of the covariates is associated with a constant change in the mean of the outcome, regardless of the actual value of the covariate.</p></li>
<li><p>When there is a more or less constant trend in the outcome as the covariate increases or decreases, this model provides a simple summary of the underlying relationship.</p></li>
<li><p>For a continuous covariate spanning a wide range of values, however, this assumption can be highly restrictive; it may be more reasonable to assume that the mean of the outcome <span class="math inline">\(Y\)</span> changes slowly and smoothly, but not necessarily linearly, with the covariate <span class="math inline">\(X\)</span>.</p></li>
<li><p>An alternative to imposing linearity or indeed any mathematical formula for how <span class="math inline">\(E(Y)\)</span> changes with <span class="math inline">\(X\)</span> would be to model the outcome at each value of <span class="math inline">\(X\)</span> (i.e., for each sub-population defined by X) following the reasoning that close sub-populations (in <span class="math inline">\(X\)</span>) share similar distributions and means of the outcome <span class="math inline">\(Y\)</span>.</p></li>
<li><p>This is the idea motivating smoothing methods and is a version of a non-parametric regression model.</p></li>
<li><p>The simplest way to model this type of relationship between the outcome and a continuous covariate is to divide the covariate into pre-specified groups and to calculate the average within each group.</p></li>
<li><p>Figure 3.3 showed average BMI levels in groups defined by single ages, but other groupings can be used.</p></li>
</ul>
<p><img src="fig/fig3.7.png" /></p>
<ul>
<li><p>Figure 3.7 illustrates this approach using a random sample of 1000 observations from the NHANES sample for all persons age 85 years.</p>
<p><code>-</code> The left panel presents the regression line from a dichotomization into two age groups; the right panel presents a more granular breakdown.</p>
<p><code>-</code> As more groups are added, the line better reflects the pattern in the data, particularly the non-linearity among the youngest and oldest persons.</p>
<p><code>-</code> However, the more granular groupings also result in a smaller sample size in each group and hence a less reliable estimate.</p>
<p><code>-</code> The need to pre-specify the groups and the resulting disconnected lines are obvious drawbacks of the grouping approach.</p></li>
<li><p>Non-parametric regression predicts the mean of <span class="math inline">\(Y\)</span> for a group with covariate <span class="math inline">\(X\)</span> by borrowing information from neighboring groups.</p>
<p><code>-</code> There are many approaches to do this; here we present the simple kernel method that requires minimal mathematical machinery.</p>
<p><code>-</code> The uniform kernel method constructs a window of fixed width around each observed <span class="math inline">\(X\)</span> value and predicts the mean of <span class="math inline">\(Y\)</span> at that <span class="math inline">\(X\)</span> value using all the observations in the window.</p>
<p><code>-</code> As an example, consider estimating the expected BMI of a person aged 42 years using a uniform kernel with window width 10.</p>
<p><code>-</code> This is done by averaging the BMI value of individuals with ages <span class="math inline">\(42\pm 5\)</span>.</p>
<p><code>-</code> Similarly simple averaging is done at each age using the corresponding 10-year window around it.</p></li>
</ul>
<p><img src="fig/fig3.8.png" /></p>
<ul>
<li><p>The resulting regression line is shown in Fig. 3.8 along with a semi-transparent rectangle around age 42 and the resulting average (in red).</p>
<p><code>-</code> In uniform kernel regression, all observations in the window contribute equally to the average.</p></li>
<li><p>Alternatively, we can weight the contribution of each observation according to its distance from the point of interest, decreasing the contribution of further observations.</p>
<p><code>-</code> The right panel of Figure 3.8 shows a normal kernel with bandwidth <span class="math inline">\(\sigma=3\)</span>, where the weight of each observation is calculated using a normal density with this standard deviation.</p>
<p><code>-</code> For example, the average at age 42 used a weight 0.133 for persons age 42, 0.126 for persons age 41 or 43, 0.106 for persons age 40 or 44, and so on.</p></li>
<li><p>The semi-transparent rectangles show the diminishing weight applied to observations that are further away from the point of interest (again, age 42 years).</p></li>
<li><p>The kernel method provides a graphical estimate of the relationship between the covariate and the outcome.</p>
<p><code>-</code> In the current example, BMI increases until adulthood and then flattens.</p>
<p><code>-</code> This is an interesting observation, but the contribution of other covariates, such as sex, race/ethnicity, and calendar year, may change the picture.</p></li>
<li><p>A major limitation of smoothing methods like the kernel approach is that they become difficult to apply when the number of covariates increases.</p></li>
<li><p>This is sometime referred to as the curse of dimensionality, and it occurs because when the covariate space becomes high dimensional, the number of observations within any neighborhood becomes small.</p></li>
<li><p>Smoothing methods for individual covariates are often used in exploratory analysis, but they should not be used to actually select the covariates to include in regression models or their functional form.</p></li>
<li><p>Kernel smoothing is just one approach used for non-parametric regression; another approach uses splines, which are polynomial functions that are combined to fit the observed data.</p></li>
<li><p>A spline-based approach that assumes additivity but relaxes the linearity assumption is known as a generalized additive model.</p></li>
<li><p>Non-parametric regression predicts the mean of Y for a group with covariate X by borrowing information from neighboring groups : <strong>simple kernel method</strong></p></li>
<li><p>The kernel method provides a graphical estimate of the relationship between the covariate and the outcome.</p></li>
<li><p><em>curse of dimensionality</em>: A major limitation of smoothing methods like the kernel approach is that they become difficult to apply when the number of covariates increases.</p>
<ul>
<li>when the covariate space becomes high dimensional, the number of observations within any neighborhood becomes small</li>
</ul></li>
<li><p>Other non parametric method: <em>generalized additive model</em></p></li>
</ul>
<!-------------------- End --------------->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["hdsbook.pdf", "hdsbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
