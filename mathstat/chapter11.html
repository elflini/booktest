<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Tests as Decision Rules | Mathematical Statistics</title>
  <meta name="description" content="This is a Mathematical Statistics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Tests as Decision Rules | Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Mathematical Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Tests as Decision Rules | Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is a Mathematical Statistics" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2024-08-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter10.html"/>
<link rel="next" href="chapter12.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#algebra-of-events"><i class="fa fa-check"></i><b>1.2</b> Algebra of Events</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#experiments-with-symmetries"><i class="fa fa-check"></i><b>1.3</b> Experiments with Symmetries</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#composition-of-experiments-counting-rules"><i class="fa fa-check"></i><b>1.4</b> Composition of Experiments: Counting Rules</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#sampling-at-random"><i class="fa fa-check"></i><b>1.5</b> Sampling at Random</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#binomial-multinomial-coefficients"><i class="fa fa-check"></i><b>1.6</b> Binomial &amp; Multinomial Coefficients</a></li>
<li class="chapter" data-level="1.7" data-path="chapter1.html"><a href="chapter1.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="1.8" data-path="chapter1.html"><a href="chapter1.html#subjective-probability"><i class="fa fa-check"></i><b>1.8</b> Subjective Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#probability-functions"><i class="fa fa-check"></i><b>2.1</b> Probability Functions</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Joint Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#conditional-probability"><i class="fa fa-check"></i><b>2.3</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#bayes-theorem-law-of-inverse-probability"><i class="fa fa-check"></i><b>2.4</b> Bayes Theorem (Law of Inverse Probability)</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#statistical-independence-of-random-variables"><i class="fa fa-check"></i><b>2.5</b> Statistical Independence of Random Variables</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#exchangeability"><i class="fa fa-check"></i><b>2.6</b> Exchangeability</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#application-probability-of-winning-in-craps"><i class="fa fa-check"></i><b>2.7</b> Application: Probability of Winning in Craps</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Expectations of Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#the-mean"><i class="fa fa-check"></i><b>3.1</b> The Mean</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#expectation-of-a-function"><i class="fa fa-check"></i><b>3.2</b> Expectation of a Function</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#variability"><i class="fa fa-check"></i><b>3.3</b> Variability</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#sums-of-random-variables"><i class="fa fa-check"></i><b>3.5</b> Sums of Random Variables</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#probability-generating-functions"><i class="fa fa-check"></i><b>3.6</b> Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Bernoulli and Related Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#sampling-bernoulli-populations"><i class="fa fa-check"></i><b>4.1</b> Sampling Bernoulli Populations</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#binomial-distribution"><i class="fa fa-check"></i><b>4.2</b> Binomial Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>4.3</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#negative-hypergeometric-distribution"><i class="fa fa-check"></i><b>4.6</b> Negative Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#approximating-binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Approximating Binomial Probabilities</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="chapter4.html"><a href="chapter4.html#normal-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.1</b> Normal approximation to the Binomial</a></li>
<li class="chapter" data-level="4.7.2" data-path="chapter4.html"><a href="chapter4.html#poisson-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.2</b> Poisson Approximation to the Binomial</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#poisson-distribution"><i class="fa fa-check"></i><b>4.8</b> Poisson Distribution</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#law-of-large-numbers"><i class="fa fa-check"></i><b>4.9</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="4.10" data-path="chapter4.html"><a href="chapter4.html#multinomial-distributions"><i class="fa fa-check"></i><b>4.10</b> Multinomial Distributions</a></li>
<li class="chapter" data-level="4.11" data-path="chapter4.html"><a href="chapter4.html#using-probability-generating-functions"><i class="fa fa-check"></i><b>4.11</b> Using Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.1</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#density-and-the-probability-element"><i class="fa fa-check"></i><b>5.2</b> Density and the Probability Element</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#the-median-and-other-percentiles"><i class="fa fa-check"></i><b>5.3</b> The Median and Other Percentiles</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#expected-value"><i class="fa fa-check"></i><b>5.4</b> Expected Value</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#expected-value-of-a-function"><i class="fa fa-check"></i><b>5.5</b> Expected Value of a Function</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#average-deviations"><i class="fa fa-check"></i><b>5.6</b> Average Deviations</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#bivariate-distributions"><i class="fa fa-check"></i><b>5.7</b> Bivariate Distributions</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#several-variables"><i class="fa fa-check"></i><b>5.8</b> Several Variables</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#covariance-and-correlation-1"><i class="fa fa-check"></i><b>5.9</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#independence"><i class="fa fa-check"></i><b>5.10</b> Independence</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#conditional-distributions"><i class="fa fa-check"></i><b>5.11</b> Conditional Distributions</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#moment-generating-functions"><i class="fa fa-check"></i><b>5.12</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Families of Continuous Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#normal-distributions"><i class="fa fa-check"></i><b>6.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#exponential-distributions"><i class="fa fa-check"></i><b>6.2</b> Exponential Distributions</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#gamma-distributions"><i class="fa fa-check"></i><b>6.3</b> Gamma Distributions</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#chi-squared-distributions"><i class="fa fa-check"></i><b>6.4</b> Chi Squared Distributions</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#distributions-for-reliability"><i class="fa fa-check"></i><b>6.5</b> Distributions for Reliability</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#t-f-and-beta-distributions"><i class="fa fa-check"></i><b>6.6</b> <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and Beta Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Organizing &amp; Describing Data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#frequency-distributions"><i class="fa fa-check"></i><b>7.1</b> Frequency Distributions</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#data-on-continuous-variables"><i class="fa fa-check"></i><b>7.2</b> Data on Continuous Variables</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#order-statistics"><i class="fa fa-check"></i><b>7.3</b> Order Statistics</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#data-analysis"><i class="fa fa-check"></i><b>7.4</b> Data Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#the-sample-mean"><i class="fa fa-check"></i><b>7.5</b> The Sample Mean</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#measures-of-dispersion"><i class="fa fa-check"></i><b>7.6</b> Measures of Dispersion</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#correlation"><i class="fa fa-check"></i><b>7.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Samples, Statistics, &amp; Sampling Distributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#random-sampling"><i class="fa fa-check"></i><b>8.1</b> Random Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#likelihood"><i class="fa fa-check"></i><b>8.2</b> Likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#sufficient-statistics"><i class="fa fa-check"></i><b>8.3</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#sampling-distributions"><i class="fa fa-check"></i><b>8.4</b> Sampling Distributions</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>8.5</b> Simulating Sampling Distributions</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#order-statistics-1"><i class="fa fa-check"></i><b>8.6</b> Order Statistics</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#moments-of-sample-means-and-proportionssp"><i class="fa fa-check"></i><b>8.7</b> Moments of Sample Means and Proportionssp</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>8.8</b> The Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#using-the-moment-generating-function"><i class="fa fa-check"></i><b>8.9</b> Using the Moment Generating Function</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#normal-populations"><i class="fa fa-check"></i><b>8.10</b> Normal Populations</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#updating-prior-probabilities-via-likelihood"><i class="fa fa-check"></i><b>8.11</b> Updating Prior Probabilities Via Likelihood</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#some-conjudate-families"><i class="fa fa-check"></i><b>8.12</b> Some conjudate Families</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#predictive-distributions"><i class="fa fa-check"></i><b>8.13</b> Predictive Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Estimation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#point-estimation"><i class="fa fa-check"></i><b>9.1</b> Point Estimation</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#errors-in-estimation"><i class="fa fa-check"></i><b>9.2</b> Errors in Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#consistency"><i class="fa fa-check"></i><b>9.3</b> Consistency</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#large-sample-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Large Sample Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#determining-sample-size"><i class="fa fa-check"></i><b>9.5</b> Determining Sample Size</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#small-sample-confidence-intervals-for-mu_x"><i class="fa fa-check"></i><b>9.6</b> Small Sample Confidence Intervals for <span class="math inline">\(\mu_X\)</span></a></li>
<li class="chapter" data-level="9.7" data-path="chapter9.html"><a href="chapter9.html#the-distribution-of-t"><i class="fa fa-check"></i><b>9.7</b> The Distribution of <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="9.8" data-path="chapter9.html"><a href="chapter9.html#pivotal-quantities"><i class="fa fa-check"></i><b>9.8</b> Pivotal Quantities</a></li>
<li class="chapter" data-level="9.9" data-path="chapter9.html"><a href="chapter9.html#estimating-a-mean-difference"><i class="fa fa-check"></i><b>9.9</b> Estimating a Mean Difference</a></li>
<li class="chapter" data-level="9.10" data-path="chapter9.html"><a href="chapter9.html#umvue"><i class="fa fa-check"></i><b>9.10</b> UMVUE</a></li>
<li class="chapter" data-level="9.11" data-path="chapter9.html"><a href="chapter9.html#bayes-estimators"><i class="fa fa-check"></i><b>9.11</b> Bayes Estimators</a></li>
<li class="chapter" data-level="9.12" data-path="chapter9.html"><a href="chapter9.html#efficiency"><i class="fa fa-check"></i><b>9.12</b> Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Significance Testing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chapter10.html"><a href="chapter10.html#hypotheses"><i class="fa fa-check"></i><b>10.1</b> Hypotheses</a></li>
<li class="chapter" data-level="10.2" data-path="chapter10.html"><a href="chapter10.html#assessing-the-evidence"><i class="fa fa-check"></i><b>10.2</b> Assessing the Evidence</a></li>
<li class="chapter" data-level="10.3" data-path="chapter10.html"><a href="chapter10.html#one-sample-z-tests"><i class="fa fa-check"></i><b>10.3</b> One Sample <span class="math inline">\(Z\)</span> Tests</a></li>
<li class="chapter" data-level="10.4" data-path="chapter10.html"><a href="chapter10.html#one-sample-t-tests"><i class="fa fa-check"></i><b>10.4</b> One Sample <span class="math inline">\(t\)</span> Tests</a></li>
<li class="chapter" data-level="10.5" data-path="chapter10.html"><a href="chapter10.html#some-nonparametric-tests"><i class="fa fa-check"></i><b>10.5</b> Some Nonparametric Tests</a></li>
<li class="chapter" data-level="10.6" data-path="chapter10.html"><a href="chapter10.html#probability-of-the-null-hypothesis"><i class="fa fa-check"></i><b>10.6</b> Probability of the Null Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Tests as Decision Rules</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#rejection-regions-and-errors"><i class="fa fa-check"></i><b>11.1</b> Rejection Regions and Errors</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#the-power-function"><i class="fa fa-check"></i><b>11.2</b> The Power function</a></li>
<li class="chapter" data-level="11.3" data-path="chapter11.html"><a href="chapter11.html#choosing-a-sample-size"><i class="fa fa-check"></i><b>11.3</b> Choosing a Sample Size</a></li>
<li class="chapter" data-level="11.4" data-path="chapter11.html"><a href="chapter11.html#most-powerful-tests"><i class="fa fa-check"></i><b>11.4</b> Most Powerful Tests</a></li>
<li class="chapter" data-level="11.5" data-path="chapter11.html"><a href="chapter11.html#uniformly-most-powerful-tests"><i class="fa fa-check"></i><b>11.5</b> Uniformly Most Powerful Tests</a></li>
<li class="chapter" data-level="11.6" data-path="chapter11.html"><a href="chapter11.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>11.6</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="11.7" data-path="chapter11.html"><a href="chapter11.html#bayesian-testing"><i class="fa fa-check"></i><b>11.7</b> Bayesian Testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chapter12.html"><a href="chapter12.html"><i class="fa fa-check"></i><b>12</b> Appendix</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chapter12.html"><a href="chapter12.html#greek-alphabet"><i class="fa fa-check"></i><b>12.1</b> Greek Alphabet</a></li>
<li class="chapter" data-level="12.2" data-path="chapter12.html"><a href="chapter12.html#abbreviations"><i class="fa fa-check"></i><b>12.2</b> Abbreviations</a></li>
<li class="chapter" data-level="12.3" data-path="chapter12.html"><a href="chapter12.html#practice-exams"><i class="fa fa-check"></i><b>12.3</b> PRACTICE EXAMS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter11" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">Chapter 11</span> Tests as Decision Rules<a href="chapter11.html#chapter11" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="rejection-regions-and-errors" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Rejection Regions and Errors<a href="chapter11.html#rejection-regions-and-errors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Suppose that the data consist of <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span>.</p></li>
<li><p>The joint sample space of <span class="math inline">\(X\)</span> is partitioned into two pieces, the rejection region and the acceptance region.</p></li>
<li><p>In practice, the partitioning is accomplished by using a test statistic.</p>
<ul>
<li>Rejection region: the set of values of the test statistic that call for rejecting <span class="math inline">\(H_0\)</span>.</li>
<li>Acceptance region: the set of values of the test statistic that call for accepting <span class="math inline">\(H_0\)</span>.</li>
<li>The acceptance region is the complement of the rejection region.</li>
</ul></li>
<li><p>Errors</p>
<ul>
<li>Type I: rejecting a true <span class="math inline">\(H_0\)</span>.</li>
<li>Type II: accepting a false <span class="math inline">\(H_0\)</span>.</li>
</ul></li>
<li><p>Size of the Test:</p></li>
</ul>
<p><span class="math display">\[
size = \alpha = P(reject \ H_0|H_0 \,\,\, true)
\]</span></p>
<ul>
<li>Type II error probability:</li>
</ul>
<p><span class="math display">\[
P(\text{type  II  error}) = \beta = P(accept \,\,\, H_0|H_0 \,\,\, false)
\]</span></p>
<ul>
<li>Power:</li>
</ul>
<p><span class="math display">\[
power = 1 − \beta = P(reject \,\,\, H_0|H_0 \,\,\, false)
\]</span></p>
<ul>
<li>Example; Consider a test of <span class="math inline">\(H_0: p = 0.4\)</span> versus <span class="math inline">\(H_a: p \ne 0.4\)</span> based on a random sample of size <span class="math inline">\(10\)</span> from <span class="math inline">\(Bern(p)\)</span>.
<ul>
<li>Let <span class="math inline">\(Y =\sum X_i\)</span>.</li>
<li>If the rejection rule is to reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(Y \le 0\)</span> or <span class="math inline">\(Y \ge 8\)</span>, then the test size is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\alpha = 1 − P(1 \le Y \le 7|p = 0.4) = 0.0183
\]</span></p>
<ul>
<li>The type II error probability and the power depend on the value of <span class="math inline">\(p\)</span> when <span class="math inline">\(H_0\)</span> is not true. Their values are</li>
</ul>
<p><span class="math display">\[
P(\text{type II error}) = P(1 \le Y \le 7|p) \,\,\, \text{and} \,\,\, power = 1 − P(1 \le Y \le 7|p)
\]</span></p>
<ul>
<li>The two plots below display the type II error probabilities and power for various values of <span class="math inline">\(p\)</span>.</li>
</ul>
<center>
<img src="fig11/fig11_1.jpg" />
</center>
<center>
<img src="fig11/fig11_2.jpg" />
</center>
<p><br></p>
</div>
<div id="the-power-function" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> The Power function<a href="chapter11.html#the-power-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Suppose that <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(f_X(x|\theta)\)</span>.
<ul>
<li>Denote the parameter space of <span class="math inline">\(\theta\)</span> by <span class="math inline">\(\theta\)</span> and let <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_a\)</span> be two disjoint subspaces of <span class="math inline">\(\theta\)</span>.</li>
<li>Consider a test of <span class="math inline">\(H_0: \theta \in \theta_0\)</span> against <span class="math inline">\(H_a: \theta \in \theta_a\)</span>.</li>
<li>The power function is a function of <span class="math inline">\(\theta\)</span> and is defined by</li>
</ul></li>
</ul>
<p><span class="math display">\[
\pi(\theta) = P(reject \ H_0|\theta)
\]</span></p>
<ul>
<li>The function usually is used when <span class="math inline">\(\theta \in \Theta_a\)</span>, but the function is defined for all <span class="math inline">\(\theta \in \Theta\)</span>.
<ul>
<li>For example, consider a test of <span class="math inline">\(H_0: \mu = \mu_0\)</span> against <span class="math inline">\(H_a: \mu &gt; \mu_0\)</span> based on a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known.</li>
<li>Let <span class="math inline">\(\Phi^{−1}(1 − \alpha) = z_{1−\alpha}\)</span> be the <span class="math inline">\(100(1 − \alpha)\)</span> percentile of the standard normal distribution.</li>
<li>Then a one-sample <span class="math inline">\(Z\)</span> test of <span class="math inline">\(H_0\)</span> will reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(Z &gt; z_{1−\alpha}\)</span>, where<br />
</li>
</ul></li>
</ul>
<p><span class="math display">\[
Z = \frac{\bar X − \mu_0}{\sigma/\sqrt{n}}
\]</span></p>
<ul>
<li>The power function is</li>
</ul>
<p><span class="math display">\[
\begin{align}
\pi(\mu_a) &amp;= P(Z &gt; z_{1−\alpha}|\mu = \mu_a) = 1 − P[\frac{\bar X − \mu_0}{\sigma/\sqrt{n}}\le z_{1−\alpha}|\mu = \mu_a] \\
&amp;= 1 − P[\frac{\bar X − \mu_a + \mu_a − \mu_0}{\sigma/\sqrt{n}}\le z_{1−\alpha}|\mu = \mu_a] \\
&amp;= 1 − P[\frac{\bar X − \mu_a}{\sigma/\sqrt{n}}\le z_{1−\alpha}-\frac{\mu_a - \mu_0}{\sigma/\sqrt{n}}|\mu = \mu_a] \\
&amp;= 1 − \Phi[z_{1−\alpha}-\frac{\mu_a − \mu_0}{\sigma/\sqrt{n}}]
\end{align}
\]</span></p>
<ul>
<li>As an illustration, if <span class="math inline">\(\sigma = 10\)</span>, <span class="math inline">\(\mu_0 = 100\)</span>, <span class="math inline">\(n = 25\)</span>, and <span class="math inline">\(\alpha = 0.05\)</span>, then the power function is</li>
</ul>
<p><span class="math display">\[
\pi(\mu_a) = 1 − \Phi[1.645 −\frac{\mu_a − 100}{2}]
\]</span></p>
<ul>
<li>This function is displayed below for various values of <span class="math inline">\(\mu_a\)</span>.</li>
</ul>
<center>
<img src="fig11/fig11_3.jpg" />
</center>
<p><br />
</p>
<p><br></p>
</div>
<div id="choosing-a-sample-size" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> Choosing a Sample Size<a href="chapter11.html#choosing-a-sample-size" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>An investigator may want to plan a study so that power will be adequate to detect a meaningful difference.
<ul>
<li>Consider the power function from the last section.</li>
<li>Suppose that the investigator decides that the minimal difference of importance is two points.</li>
<li>That is, if <span class="math inline">\(\mu_a \ge 102\)</span>, then the investigator would like to reject <span class="math inline">\(H_0\)</span>.</li>
<li>If <span class="math inline">\(\mu_a\)</span> is fixed at <span class="math inline">\(\mu_a = 102\)</span>, then the power of the test as a function of <span class="math inline">\(n\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\pi(\mu_a) = 1 − \Phi[1.645-\frac{102-100}{10\sqrt{n}}]=1 − \Phi[1.645-\frac{\sqrt{n}}{5}]
\]</span></p>
<ul>
<li>This function is plotted below for various values of <span class="math inline">\(n\)</span>.</li>
</ul>
<center>
<img src="fig11/fig11_4.jpg" />
</center>
<ul>
<li><p>If the investigator has decided that a specific power is necessary, then the required sample size can be read from the above display.</p></li>
<li><p>In general, the required sample size for a one sample <span class="math inline">\(Z\)</span> test of <span class="math inline">\(H_0: \mu = \mu_0\)</span> against <span class="math inline">\(H_0: \mu &gt; \mu_0\)</span> can be obtained by equating the power function to the desired value of <span class="math inline">\(1 − \beta\)</span> and solving for <span class="math inline">\(n\)</span>.</p></li>
<li><p>Denote the <span class="math inline">\(100\beta\)</span> percentile of the standard normal distribution by <span class="math inline">\(\Phi^{−1}(\beta) = z_\beta\)</span>. That is</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
\pi(\mu_a) &amp;= 1 − \Phi[z_{1−\alpha}-\frac{\mu_a − \mu_0}{\sigma/\sqrt{n}}]= 1 − \beta \\
&amp;\Longleftrightarrow \Phi[z_{1−\alpha}-\frac{\mu_a − \mu_0}{\sigma/\sqrt{n}}]= \beta \\
&amp;\Longleftrightarrow z_{1−\alpha} - \frac{\mu_a − \mu_0}{\sigma/\sqrt{n}} = \Phi^{−1}(\beta) = z_\beta\\
&amp;\Longleftrightarrow kn =\frac{\sigma^2(z_{1−\alpha} − z_\beta)^2}{(\mu_a − \mu_0)^2}
\end{align}
\]</span></p>
<ul>
<li>For example, if <span class="math inline">\(\mu_0 = 100\)</span>, <span class="math inline">\(\mu_a = 102\)</span>, <span class="math inline">\(\alpha = 0.05\)</span>, <span class="math inline">\(\beta = 0.10\)</span>, and <span class="math inline">\(\sigma = 10\)</span>, then</li>
</ul>
<p><span class="math display">\[
n = \frac{100(1.645+1.282)^2}{(102-100)^2}=214.18
\]</span></p>
<ul>
<li>A sample size of <span class="math inline">\(n = 215\)</span> is required.</li>
</ul>
<p><br></p>
</div>
<div id="most-powerful-tests" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Most Powerful Tests<a href="chapter11.html#most-powerful-tests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Simple Hypothesis: A simple hypothesis is one that completely specifies the joint distribution of the data.</p>
<ul>
<li>That is, there are no unknown parameters under a simple hypothesis.</li>
<li>For example <span class="math inline">\(H_0: Y \sim Bin(25,\frac{1}{3})\)</span> is a simple hypothesis.</li>
<li>In this section, we will examine the test of a simple <span class="math inline">\(H_0\)</span> against a simple <span class="math inline">\(H_a\)</span>.</li>
</ul></li>
<li><p>Most Powerful Test: A test of a simple <span class="math inline">\(H_0\)</span> versus a simple <span class="math inline">\(H_a\)</span> is a most powerful test of size <span class="math inline">\(\alpha\)</span> if no other test which has size <span class="math inline">\(\le \alpha\)</span> has greater power.</p></li>
<li><p>Neyman-Pearson Lemma: Consider the hypotheses <span class="math inline">\(H_0: X \sim f_0(x)\)</span> against <span class="math inline">\(H_a: X \sim f_1(x)\)</span>, where <span class="math inline">\(f_0\)</span> and <span class="math inline">\(f_1\)</span> are the joint pdfs (pmfs) under <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_a\)</span>, respectively.</p>
<ul>
<li>Then the most powerful test is to reject <span class="math inline">\(H_0\)</span> if</li>
</ul></li>
</ul>
<p><span class="math display">\[
\Lambda(x) &lt; K,\,\,\, \text{where} \,\,\, \Lambda(x)=\frac{f_0(x)}{f_1(x)}
\]</span></p>
<p>is the likelihood ratio.</p>
<ul>
<li>Furthermore, the size of the test is</li>
</ul>
<p><span class="math display">\[
\alpha = \int_{R^∗}^{}f_0(x) dx= \alpha^∗ \le \alpha
\]</span></p>
<p>and</p>
<p><span class="math display">\[
(1 − \beta) − (1 − \beta^∗) = \beta^∗ − \beta = \int_{R}^{}f_1(x) dx - \int_{R^∗}^{}f_1(x) dx
\]</span></p>
<ul>
<li>We will show that the above difference is greater than or equal to zero.</li>
<li>Note that <span class="math inline">\(R = (R \cap  R^∗) \cup (R \cap  R^{∗c})\)</span> and that <span class="math inline">\((R \cap  R^∗)\)</span> is disjoint from <span class="math inline">\((R \cap  R^{∗c})\)</span>.
<ul>
<li>Similarly, <span class="math inline">\(R^∗ = (R^∗ \cap  R) \cup (R^∗ \cap  R^c)\)</span> and <span class="math inline">\((R^∗ \cap  R)\)</span> is disjoint from <span class="math inline">\((R^∗ \cap  R^c)\)</span>.</li>
<li>Accordingly,</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
\int_{R}^{}f_1(x) dx &amp;=  \int_{R \cap  R^∗}^{}f_1(x) dx + \int_{R \cap  R^{∗c}}^{}f_1(x) dx,\\
   \int_{R^*}^{}f_1(x) dx &amp;=  \int_{R^∗ \cap  R^c}^{}f_1(x) dx + \int_{R^∗ \cap R}^{}f_1(x) dx, \ and\\
   \beta^∗ − \beta &amp;= \int_{R \cap  R^∗}^{}f_1(x) dx + \int_{R \cap  R^c}^{}f_1(x) dx \\
   &amp;- \int_{R^∗ \cap R}^{}f_1(x) dx-\int_{R^∗ \cap  R^c}^{}f_1(x) dx \\
   &amp;=  \int_{R \cap  R^c}^{}f_1(x) dx -  \int_{R^* \cap  R^c}^{}f_1(x) dx
\end{align}
\]</span></p>
<ul>
<li>Note that <span class="math inline">\((R \cap  R^∗) \in R\)</span> so that <span class="math inline">\(f_1(x) &gt; K−1f_0(x)\)</span> in the first integral.
<ul>
<li>Also, <span class="math inline">\((R^∗ \cap  R^c) in R^c\)</span> so that <span class="math inline">\(f1(x) &lt; K−1f_0(x)\)</span> in the second integral.</li>
<li>Therefore,</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
\beta∗ − \beta &amp;\ge \frac{1}{K} \int_{R\cap R^{∗c}}^{}f_0(x) dx −\frac{1}{K} \int_{R^*\cap R^{∗c}}^{}f_0(x) dx \\
&amp;=[\frac{1}{K} \int_{R\cap R^{∗c}}^{}f_0(x) dx +\frac{1}{K} \int_{R\cap R^{∗c}}^{}f_0(x) dx] \\
&amp;-[\frac{1}{K} \int_{R^*\cap R^{c}}^{}f_0(x) dx +\frac{1}{K} \int_{R^*\cap R}^{}f_0(x) dx] \,\,\,  \text{by  adding  zero} \\
&amp;=\frac{1}{K} \int_{R}^{}f_0(x) dx - \frac{1}{K} \int_{R^*}^{}f_0(x) dx = \frac{1}{K}(\alpha − \alpha^∗) \ge 0
\end{align}
\]</span></p>
<p>because the size of the competing test is <span class="math inline">\(\alpha^∗ \le \alpha\)</span>.</p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(NegBin(k, \theta)\)</span>, where <span class="math inline">\(k\)</span> is known.
<ul>
<li>Find the most powerful test of <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_a: \theta = \theta_a\)</span>, where <span class="math inline">\(\theta_a &gt; \theta_0\)</span>.</li>
<li>Solution: The likelihood ratio test statistic is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
\Lambda(x)&amp;= \frac{\prod_{i=1}^{n}\binom{x_i-1}{k-1}\theta^k_0(1-\theta_0)^{x_i-k}}{\prod_{i=1}^{n}\binom{x_i-1}{k-1}\theta^k_a(1-\theta_a)^{x_i-k}} \\
&amp;= \frac{\theta^{nk}_0(1-\theta_0)^{n(\bar x-k)}}{\theta^{nk}_a(1-\theta_a)^{n(\bar x-k)}} \\
&amp;= (\frac{\theta_0(1-\theta_a)}{\theta_a(1-\theta_0)})^{nk}(\frac{1-\theta_a}{1-\theta_0})^{n(\bar x-k)}
\end{align}
\]</span></p>
<ul>
<li>Note that the likelihood ratio test statistic depends on the data solely through <span class="math inline">\(\bar x\)</span> and that</li>
</ul>
<p><span class="math display">\[
\theta_a &gt; \theta_0 \Longrightarrow \frac{1-\theta_0}{1-\theta_a} &gt; 1
\]</span></p>
<ul>
<li><p>Accordingly <span class="math inline">\(\Lambda(x)\)</span> is an increasing function of <span class="math inline">\(\bar x\)</span>.</p></li>
<li><p>Rejecting <span class="math inline">\(H_0\)</span> for small values of <span class="math inline">\(\Lambda(x)\)</span> is equivalent to rejecting <span class="math inline">\(H_0\)</span> for small values of <span class="math inline">\(\bar x\)</span> and the most powerful test is to reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\bar x &lt; K^∗\)</span> where <span class="math inline">\(K^∗\)</span> is determined by the relation</p></li>
</ul>
<p><span class="math display">\[
P(\bar X &lt; K^*|\theta = \theta_0)\le \alpha.
\]</span></p>
<ul>
<li>The above probability can be evaluated without too much difficulty. Under <span class="math inline">\(H_0\)</span></li>
</ul>
<p><span class="math display">\[
n\bar X= \sum_{i=1}^{n}X_i \sim NegBin(nk, \theta_0)
\]</span></p>
<p><br></p>
</div>
<div id="uniformly-most-powerful-tests" class="section level2 hasAnchor" number="11.5">
<h2><span class="header-section-number">11.5</span> Uniformly Most Powerful Tests<a href="chapter11.html#uniformly-most-powerful-tests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Consider the problem of testing <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_0: \theta &gt; \theta_0\)</span> based on a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(f_X (x|\theta)\)</span>.</li>
</ul>
<div id="uniformly-most-powerful-ump-test" class="section level4 unnumbered hasAnchor">
<h4>Uniformly most powerful (UMP) test<a href="chapter11.html#uniformly-most-powerful-ump-test" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Definition: If a test of <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_a: \theta = \theta_a\)</span>, is a most powerful test for every <span class="math inline">\(\theta_a &gt;  \theta_0\)</span> among all tests with size <span class="math inline">\(\le \alpha\)</span>, then the test is uniformly most powerful for testing <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_a: \theta &gt; \theta_0\)</span>.</p></li>
<li><p>Approach to finding a UMP test.</p>
<ul>
<li>First use the Neyman-Pearson Lemma to find a most powerful test of <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_a: \theta = \theta_a\)</span> for some <span class="math inline">\(\theta_a &gt; \theta_0\)</span>.</li>
<li>If the form of the test is the same for all <span class="math inline">\(\theta_a\)</span>, then the test is UMP.</li>
</ul></li>
<li><p>Example: Suppose that <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(NegBin(k, \theta)\)</span>, where <span class="math inline">\(k\)</span> is known.</p>
<ul>
<li>Find the UMP test of <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_a: \theta &gt; \theta_0\)</span>.</li>
<li>Solution: The most powerful test of <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_a: \theta = \theta_a\)</span>, where <span class="math inline">\(\theta_a &gt; \theta_0\)</span> is to reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\bar x &lt; K^∗\)</span> where <span class="math inline">\(K^∗\)</span> is determined by the relation</li>
</ul></li>
</ul>
<p><span class="math display">\[
P(\bar X &lt; K^∗|\theta = \theta_0) \le \alpha.
\]</span></p>
<ul>
<li><p>Note that the form of the test does not depend on the particular value of <span class="math inline">\(\theta_a\)</span>.</p>
<ul>
<li>Accordingly, the test that rejects <span class="math inline">\(H_0\)</span> for small values of <span class="math inline">\(\bar x\)</span> is the UMP test of <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_a: \theta &gt; \theta_0\)</span>.</li>
</ul></li>
<li><p>A similar argument shows that in the negative binomial example, the UMP test of <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_a: \theta &lt; \theta_0\)</span> is to reject <span class="math inline">\(H_0\)</span> for large values of <span class="math inline">\(\bar x\)</span>.</p></li>
<li><p>The UMP idea can be extended to tests of <span class="math inline">\(H_0: \theta \le \theta_0\)</span> against <span class="math inline">\(H_a: \theta &gt; \theta_0\)</span>.</p>
<ul>
<li>If the power function is monotonic in <span class="math inline">\(\theta\)</span>, then the UMP test of <span class="math inline">\(H_0: \theta = \theta_0\)</span> against <span class="math inline">\(H_a: \theta &gt; \theta_0\)</span> also is UMP for testing <span class="math inline">\(H_0: \theta \le \theta_0\)</span> against <span class="math inline">\(H_a: \theta &gt; \theta_0\)</span>.</li>
<li>The size of the test is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\alpha =\underset{\theta \le \theta_0}{sup} \ P(reject \ H_0|\theta) = \underset{\theta \le \theta_0}{sup} \ π(\theta) =P(reject \ H_0|\theta_0)
\]</span></p>
<p>because the power function is monotonic.</p>
<ul>
<li>It can be shown that the power function is monotone in <span class="math inline">\(\theta\)</span> if the distribution of <span class="math inline">\(X\)</span> belongs to the one parameter exponential family.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="likelihood-ratio-tests" class="section level2 hasAnchor" number="11.6">
<h2><span class="header-section-number">11.6</span> Likelihood Ratio Tests<a href="chapter11.html#likelihood-ratio-tests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Consider the problem of testing <span class="math inline">\(H_0: \theta \in \theta_0\)</span> against <span class="math inline">\(Ha: \theta \in \theta_a\)</span>, where <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_a\)</span> are disjoint subspaces of the parameter space.
<ul>
<li>The parameter <span class="math inline">\(\theta\)</span> may be a vector.</li>
</ul></li>
<li>The generalized likelihood ratio test of <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_a\)</span> is to reject <span class="math inline">\(H_0\)</span> for small values of the likelihood ratio test statistic.</li>
</ul>
<p><span class="math display">\[
\Lambda(X) =\frac{L(\hat\theta_0|X)}{L(\hat\theta_a|X)}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
L(\hat \theta_0|X) = \underset{\theta\in\theta_0}{sup}L(\theta|X) \ and \ L(\hat\theta_a|X) = \underset{\theta\in\theta_0\cup\theta_a}{sup}L(\theta|X).
\]</span></p>
<ul>
<li>That is, the likelihood function is maximized twice;
<ul>
<li>first under the null, and</li>
<li>second under the union of the null and alternative.</li>
</ul></li>
<li>Properties of <span class="math inline">\(\Lambda(X)\)</span>.
<ul>
<li><span class="math inline">\(\Lambda(X)\)</span> .</li>
<li>Small values of <span class="math inline">\(\Lambda\)</span> are evidence against <span class="math inline">\(H_0\)</span>.</li>
<li>Under mild regularity conditions, the asymptotic null distribution of <span class="math inline">\(−2 ln [\Lambda(X)]\)</span> is <span class="math inline">\(\chi^2\)</span> with degrees of freedom equal to the number of restrictions under <span class="math inline">\(H_0\)</span> minus the number of restrictions under <span class="math inline">\(H_a\)</span>.</li>
<li>The decision rule is to reject <span class="math inline">\(H_0\)</span> for large values of <span class="math inline">\(−2 ln(\Lambda)\)</span>.</li>
</ul></li>
<li>Example 1: Suppose that <span class="math inline">\(X_{ij} \overset{ind}{\sim} Bern(p_i)\)</span> for <span class="math inline">\(j = 1, \ldots , n_i\)</span>.
<ul>
<li>That is, we have independent samples from each of two Bernoulli populations.</li>
<li>Consider the problem of testing <span class="math inline">\(H_0: p_1 = p_2\)</span> against <span class="math inline">\(H_a: p_1 \ne p_2\)</span>.</li>
<li>The sufficient statistics are <span class="math inline">\(Y_1=\sum_{j=1}^{n_1}X_{1j}\)</span> and <span class="math inline">\(Y_2=\sum_{j=2}^{n_2}X_{2j}\)</span>.</li>
<li>These statistics are independently distributed as <span class="math inline">\(Y_i \sim Bin(n_i, p_i)\)</span>.</li>
<li>The likelihood function is</li>
</ul></li>
</ul>
<p><span class="math display">\[
L(p_1, p_2|y_1, y_2) = p^{y_1}_1(1 − p_1)^{n_1−y_1}p^{y_2}_2(1 − p_2)^{n_2−y_2}
\]</span></p>
<ul>
<li><p>Under <span class="math inline">\(H_0\)</span>, the MLE of the common value <span class="math inline">\(p = p_1 = p_2\)</span> is <span class="math inline">\(\hat p = (y_1 + y_2)/(n_1 + n_2)\)</span>.</p></li>
<li><p>Under the union of <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_a\)</span>, there are no restrictions on <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> and the MLEs are <span class="math inline">\(\hat p_1 = y_1/n_1\)</span> and <span class="math inline">\(\hat p_2 = y_2/n_2\)</span>.</p>
<ul>
<li>The likelihood ratio test statistic is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\Lambda(y_1, y_2)= \frac{\hat p^{y_1+y_2}(1 − \hat p)^{n_1+n_2−y_1−y_2}}{\hat p^{y_1}_1(1 − \hat p_1)^{n_1−y_1}\hat p^{y_2}_2(1 − \hat p_2)^{n_2−y_2}}
\]</span></p>
<ul>
<li>If <span class="math inline">\(H_0\)</span> is true and sample sizes are large, then <span class="math inline">\(−2 ln [\Lambda(Y_1, Y_2)]\)</span> is approximately distributed as a <span class="math inline">\(\chi^2\)</span> random variable.
<ul>
<li>There are no restrictions under <span class="math inline">\(H_a\)</span> and one restriction under <span class="math inline">\(H_0\)</span>, so the <span class="math inline">\(\chi^2\)</span> random variable has 1 degree of freedom.</li>
<li>For example, if <span class="math inline">\(n_1 = 30\)</span>, <span class="math inline">\(n_2 = 40\)</span>, <span class="math inline">\(y_1 = 20\)</span>, and <span class="math inline">\(y_2 = 35\)</span>, then <span class="math inline">\(\Lambda(y_1, y_2) = 0.1103\)</span>, <span class="math inline">\(−2 ln(\Lambda) = 4.4087\)</span>, and the <span class="math inline">\(p\)</span>-value is <span class="math inline">\(0.0358\)</span>.</li>
</ul></li>
<li>For comparison, the familiar large sample test statistic is</li>
</ul>
<p><span class="math display">\[
z=\frac{\hat p_1-\hat p_2}{\sqrt{\hat p(1-\hat p)(\frac{1}{n_1}+\frac{1}{n_2})}}=2.1022
\]</span></p>
<p>and the <span class="math inline">\(p\)</span>-value is <span class="math inline">\(0.0355\)</span>.</p>
<ul>
<li><p>Note, <span class="math inline">\(Z^2 \sim \chi^2_1\)</span> and <span class="math inline">\(z^2 = 4.4192\)</span> which is very close to the LR test statistic.</p></li>
<li><p>Example 2: the usual one-sample <span class="math inline">\(t\)</span>-test of <span class="math inline">\(H_0: \mu = \mu_0\)</span> against <span class="math inline">\(H_a: \mu \ne \mu_0\)</span> when sampling from a normal distribution with unknown variance is the likelihood ratio test.</p>
<ul>
<li>Below is another version of the proof.</li>
</ul></li>
<li><p>Lemma: Let a be a constant or a variable that does not depend on <span class="math inline">\(\sigma^2\)</span> and let <span class="math inline">\(n\)</span> be a positive constant. Then, the maximizer of</p></li>
</ul>
<p><span class="math display">\[
h(\sigma^2; a) = \frac{e^{-\frac{1}{2\sigma^2}a}}{(\sigma^2)^\frac{n}{2}}
\]</span></p>
<p>with respect to <span class="math inline">\(\sigma^2\)</span> is</p>
<p><span class="math display">\[
\hat \sigma^2= \frac{a}{n}.
\]</span></p>
<ul>
<li>Also,</li>
</ul>
<p><span class="math display">\[
\underset{\sigma^2}{max}h(\sigma^2; a) = h(\frac{a}{n}, a) = \frac{e^{-\frac{n}{2}}}{(\frac{a}{n})^\frac{n}{2}}
\]</span></p>
<ul>
<li>Proof: The natural log of <span class="math inline">\(h\)</span> is</li>
</ul>
<p><span class="math display">\[
ln(h) = -\frac{1}{2\sigma^2}a-\frac{n}{2}ln(\sigma^2).
\]</span></p>
<ul>
<li>Equating the first derivative of <span class="math inline">\(ln(h)\)</span> to zero and solving for <span class="math inline">\(\sigma^2\)</span> yields</li>
</ul>
<p><span class="math display">\[
\begin{align}
\frac{\partial}{\partial \sigma^2}ln(h) &amp;=\frac{1}{2\sigma^4}a-\frac{n}{2\sigma^2},\\
\frac{\partial}{\partial \sigma^2}ln(h) &amp;= 0 =⇒ \frac{a}{\sigma^2}-n=0\\
&amp;\Longrightarrow \sigma^2 = \frac{a}{n}.
\end{align}
\]</span></p>
<ul>
<li>The solution is a maximizer because the second derivative of <span class="math inline">\(ln(h)\)</span> evaluated at <span class="math inline">\(\sigma^2 = a/n\)</span> is negative:</li>
</ul>
<p><span class="math display">\[
\begin{align}
\frac{\partial^2}{(\partial \sigma^2)^2}ln(h) &amp;=\frac{\partial}{\partial \sigma^2}(\frac{a}{2\sigma^4}-\frac{n}{2\sigma^2})= -\frac{a}{\sigma^6}+\frac{n}{2\sigma^4},\\
\frac{\partial^2}{(\partial \sigma^2)^2}ln(h)|_{\sigma2=a/n} &amp;=-\frac{a}{(a/n)^3}+\frac{n}{2(a/n)^2} = -\frac{n^3}{a^2} &lt; 0
\end{align}
\]</span></p>
<ul>
<li><p>To obtain the maximum, substitute <span class="math inline">\(a/n\)</span> for <span class="math inline">\(\sigma^2\)</span> in <span class="math inline">\(h(\sigma^2, a)\)</span>.</p></li>
<li><p>Theorem: The generalized likelihood ratio test of <span class="math inline">\(H_0: \mu = \mu_0\)</span> against <span class="math inline">\(H_a: \mu  \ne \mu_0\)</span> based on a random sample of size <span class="math inline">\(n\)</span> from a normal distribution with unknown variance is to reject <span class="math inline">\(H_0\)</span> for large <span class="math inline">\(|T|\)</span>, where</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
T &amp;=\frac{\sqrt{n}(\bar X-\mu_0)}{S_X},\\
\bar X=\frac{1}{n}\sum_{i=1}^{n}X_i, \,\,\, &amp;\text{and} \,\,\, S^2_X=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar X)^2
\end{align}
\]</span></p>
<ul>
<li>Proof: The likelihood function of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> given <span class="math inline">\(X_i \overset{iid}{\sim} N(\mu, \sigma2)\)</span> for <span class="math inline">\(i = 1, \ldots , n\)</span> is</li>
</ul>
<p><span class="math display">\[
L(\mu, \sigma^2|X) =\frac{exp\left \{ -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(X_i − \mu)^2\right \}}{(\sigma^2)^\frac{n}{2}(2π)^\frac{n}{2}}.
\]</span></p>
<ul>
<li>Under <span class="math inline">\(H_0: \mu = \mu_0\)</span>, the likelihood function is</li>
</ul>
<p><span class="math display">\[
L(\mu_0, \sigma^2|X) =\frac{exp\left \{ -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(X_i − \mu_0)^2\right \}}{(\sigma^2)^\frac{n}{2}(2π)^\frac{n}{2}}.
\]</span></p>
<p>Using the Lemma with</p>
<p><span class="math display">\[
a=\sum_{i=1}^{n}(X_i-\mu_0)^2
\]</span></p>
<p>yields</p>
<p><span class="math display">\[
\begin{align}
\hat \sigma^2_0&amp;=\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu_0)^2 \ and \\
\underset{\sigma^2}{max} \ L(\mu_0, \sigma^2|X) &amp;= L(\mu, \sigma^2_0|X)=\frac{e^{-\frac{n}{2}}}{(\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu_0)^2)^\frac{n}{2}(2π)^\frac{n}{2}}
\end{align}
\]</span></p>
<ul>
<li>Under <span class="math inline">\(H_0 \cup H_a\)</span>, the likelihood function must be maximized with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.
<ul>
<li>Note that the sign of the exponent in the numerator of <span class="math inline">\(L\)</span> is negative.</li>
<li>Accordingly, to maximize <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(\mu\)</span>, we must minimize</li>
</ul></li>
</ul>
<p><span class="math display">\[
\sum_{i=1}^{n}(X_i-\mu)^2
\]</span></p>
<p>with respect to <span class="math inline">\(\mu\)</span>.</p>
<ul>
<li>By the parallel axis theorem, it is known that the minimizer is <span class="math inline">\(\mu = \bar X\)</span>.
<ul>
<li>Substitute <span class="math inline">\(\bar X\)</span> for <span class="math inline">\(\mu\)</span> in the likelihood function and use the Lemma with</li>
</ul></li>
</ul>
<p><span class="math display">\[
a =\sum_{i=1}^{n}(X_i-\bar X)^2
\]</span></p>
<p>to obtain</p>
<p><span class="math display">\[
\begin{align}
\hat \sigma^2_a &amp;= \frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2 \ and \\
\underset{\sigma^2,\mu}{max} \ L(\mu, \sigma^2|X) &amp;= L(\mu, \hat \sigma^2_a|X)=\frac{e^{-\frac{n}{2}}}{(\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2)^\frac{n}{2}(2π)^\frac{n}{2}}
\end{align}
\]</span></p>
<ul>
<li>The likelihood ratio test statistic is</li>
</ul>
<p><span class="math display">\[
\Lambda = \frac{L(\mu_0, \hat \sigma^2_0)}{L(\bar X, \hat \sigma^2_a)}=(\frac{\sum_{i=1}^{n}(X_i-\bar X)^2}{\sum_{i=1}^{n}(X_i-\mu_0)^2})^\frac{n}{n}.
\]</span></p>
<ul>
<li>Recall, that from the parallel axis theorem,</li>
</ul>
<p><span class="math display">\[
\sum_{i=1}^{n}(X_i-\mu_0)^2=\sum_{i=1}^{n}(X_i-\bar X)^2+n(\bar X − \mu_0)^2.
\]</span></p>
<ul>
<li>Accordingly, the Likelihood Ratio Test (LRT) is to reject <span class="math inline">\(H_0\)</span> for small <span class="math inline">\(\Lambda\)</span>, where</li>
</ul>
<p><span class="math display">\[
\Lambda =(\frac{\sum_{i=1}^{n}(X_i-\bar X)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2+n(\bar X - \mu_0)^2})^\frac{n}{2}
\]</span></p>
<ul>
<li>Any monotonic transformation of <span class="math inline">\(\Lambda\)</span> also can be used as the LRT statistic. In particular,</li>
</ul>
<p><span class="math display">\[
(\Lambda^{−\frac{2}{n}} − 1)(n-1)=\frac{n(X − \mu_0)^2}{S^2_X}=T^2
\]</span></p>
<p>is a decreasing function of <span class="math inline">\(\Lambda\)</span>.</p>
<ul>
<li>Therefore, the LRT rejects <span class="math inline">\(H_0\)</span> for large <span class="math inline">\(T^2\)</span> or, equivalently, for large <span class="math inline">\(|T|\)</span>.</li>
</ul>
<p><br></p>
</div>
<div id="bayesian-testing" class="section level2 hasAnchor" number="11.7">
<h2><span class="header-section-number">11.7</span> Bayesian Testing<a href="chapter11.html#bayesian-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>To develop a Bayes Test, first make the following definitions:</p></li>
<li><p>Loss Function: <span class="math inline">\(l(\theta, act)\)</span> = loss incurred if action act is performed when the state of nature is <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>The action act will be either “reject <span class="math inline">\(H_0\)</span>” or “accept <span class="math inline">\(H_0\)</span>.”</li>
<li>For example, (<span class="math inline">\(H_0\)</span>, reject <span class="math inline">\(H_0\)</span>) is the loss incurred when a true <span class="math inline">\(H_0\)</span> is rejected.</li>
<li>It is assumed that making the correct action incurs no loss.</li>
</ul></li>
<li><p>Parameter Space: Denote the support of <span class="math inline">\(\theta\)</span> under <span class="math inline">\(H_0\)</span> by <span class="math inline">\(\theta_0\)</span> and denote the support of <span class="math inline">\(\theta\)</span> under <span class="math inline">\(H_a\)</span> by <span class="math inline">\(\theta_a\)</span>.</p>
<ul>
<li>For example, if the hypotheses are <span class="math inline">\(H_0: \mu \le 100\)</span> and <span class="math inline">\(H_a: \mu &gt; 100\)</span>, then, <span class="math inline">\(\theta_0 = (\infty, 100]\)</span> and <span class="math inline">\(\theta_a = (100, \infty)\)</span>.</li>
</ul></li>
<li><p>Prior: Before new data are collected, the prior pdf or pmf for <span class="math inline">\(\theta\)</span> is denoted by <span class="math inline">\(g(\theta)\)</span>.</p></li>
<li><p>Posterior: After new data have been collected, the posterior pdf or pmf for <span class="math inline">\(\theta\)</span> is denoted by <span class="math inline">\(g(\theta|X)\)</span>.</p></li>
<li><p>Bayes Loss: The posterior Bayes Loss for action act is</p></li>
</ul>
<p><span class="math display">\[
B(act|X) = E_\theta [l(\theta, act)]
=\left \{\begin{matrix}
\int_{\theta_0}^{} l(\theta,reject H_0)g(\theta|X) d\theta&amp; \ if \ act = reject \ H_0, &amp; \\
\int_{\theta_a}^{} l(\theta,reject H_0)g(\theta|X) d\theta&amp; \ if \ act = reject \ H_0. &amp;  &amp; \\
\end{matrix}\right.
\]</span></p>
<ul>
<li><p>Bayes Test: A Bayes test is the rule that minimizes Bayes Loss.</p></li>
<li><p>Theorem: When testing a simple null against a simple alternative, the Bayes test is a Neyman-Pearson test and a Neyman-Pearson rejection region, <span class="math inline">\(f_0/f_a &lt; K\)</span>, corresponds to a Bayes test for some prior.</p></li>
<li><p>Proof: The null and alternative can be written as <span class="math inline">\(H_0: f(x) = f_0(x)\)</span> versus <span class="math inline">\(H_1: f(x) = f_1(x)\)</span>.</p>
<ul>
<li>Also, the support of <span class="math inline">\(\theta\)</span> has only two points:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp;\theta = \left \{f_0, f_1\right \}, \ \theta_0 = \left \{f_0\right \}, \ and  \ \theta_a = \left \{f_1\right \} \ or, \ equivalently, \\
&amp;\theta = \left \{H_0, H_1\right \}, \  \theta0 = \left \{H_0\right \},  \ and \  \theta_a = \left \{H_1\right \}
\end{align}
\]</span></p>
<ul>
<li>Denote the prior probabilities of <span class="math inline">\(\theta\)</span> as</li>
</ul>
<p><span class="math display">\[
g_0 = P(H_0) \ and \ g_1 = P(H_1).
\]</span></p>
<ul>
<li>The posterior probabilities are</li>
</ul>
<p><span class="math display">\[
\begin{align}
f(H_0|x) =\frac{f(H_0, x)}{f(x)}&amp;=\frac{f(x|H0)f(H0)}{f(x|H_0)f(H_0) + f(x|H_1)f(H_1)}\\
&amp;=\frac{f_0(x)g_0}{f_0(x)g_0 + f_1(x)g_1}and\\
f(H1|x) =\frac{f(H_1, x)}{f(x)}&amp;=\frac{f(x|H1)f(H1)}{f(x|H_0)f(H_0) + f(x|H_1)f(H_1)}\\
&amp;=\frac{f_1(x)g_1}{f_0(x)g_0 + f_1(x)g_1}
\end{align}
\]</span></p>
<ul>
<li>Denote the losses for incorrect decisions by</li>
</ul>
<p><span class="math display">\[
l(H_0,\,\,\, reject \,\,\, H_0) = l_0 \,\,\, and \,\,\, l(H_1, \,\,\, accept \,\,\, H_0) = l_1
\]</span></p>
<ul>
<li>Note that <span class="math inline">\(l_0\)</span> and <span class="math inline">\(l_1\)</span> are merely scalar constants. Then, the posterior Bayes losses are</li>
</ul>
<p><span class="math display">\[
\begin{align}
B(reject \ H_0|x) &amp;= l_0 × (\frac{f_0(x)g_0}{f_0(x)g_0 + f_1(x)g_1}) \,\,\, and \\
B(reject \ H_0|x) &amp;= l_1 × (\frac{f_1(x)g_1}{f_0(x)g_0 + f_1(x)g_1})
\end{align}
\]</span></p>
<ul>
<li><p>The Bayes test consists of choosing the action that has the smallest Bayes loss.</p></li>
<li><p>Alternatively, the ratio of Bayes losses can be examined:<br />
</p></li>
</ul>
<p><span class="math display">\[
\frac{B(reject \ H_0|x)}{B(accept \ H_0|x)} = \frac {l_0f_0(x)g_0}{l_1f_1(x)g_1}.
\]</span></p>
<ul>
<li><p>If the ratio is smaller than 1, then the Bayes test is to reject <span class="math inline">\(H_0\)</span>, otherwise accept <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>That is, <span class="math inline">\(H_0\)</span> is rejected if</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp;\frac {l_0f_0(x)g_0}{l_1f_1(x)g_1}&lt;1 \,\,\, or, \,\,\,equivalently, \\
&amp;\frac{f_0(x)}{f_1(x)} &lt; K, \,\,\,where \,\,\, K=\frac{l_1g_1}{l_0g_0}
\end{align}
\]</span></p>
<ul>
<li>Accordingly, the Bayes test is a Neyman-Pearson test.
<ul>
<li>Also, a Neyman-Pearson rejection region, <span class="math inline">\(f_0/f_1 &lt; K\)</span>, corresponds to a Bayes test, where the priors and losses satisfy</li>
</ul></li>
</ul>
<p><span class="math display">\[
K=\frac{l_1g_1}{l_0g_0}.
\]</span></p>
<ul>
<li>Example: A machine that fills bags with flour is adjusted so that the mean weight in a bag is 16 ounces.
<ul>
<li>To determine whether the machine is at the correct setting, a sample of bags can be weighed.</li>
<li>There is a constant cost for readjusting the machine.</li>
<li>The cost is due to shutting down the production line, etc.</li>
<li>If the machine is not adjusted, then the company may be over-filling the bags with cost <span class="math inline">\(2(\mu − 16)\)</span> or under-filling the bags with cost <span class="math inline">\(16 − \mu\)</span>.</li>
<li>The under-filling cost is due to customer dissatisfaction.</li>
</ul></li>
<li>Consider testing <span class="math inline">\(H_0: \mu \le 16\)</span> against <span class="math inline">\(H_a: \mu &gt; 16\)</span> based on a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known.
<ul>
<li>Furthermore, suppose that the prior on is <span class="math inline">\(N(ν, τ_2)\)</span>.</li>
<li>Using the result in Example, the posterior distribution of <span class="math inline">\(\mu\)</span> is normal with</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp;E(\mu|\bar x)=(\frac{nτ^2}{nτ^2 + \sigma^2})\bar x + (1-\frac{nτ^2}{nτ^2 + \sigma^2})ν \,\,\, and \\
&amp;Var(\mu|\bar x) = (\frac{n}{\sigma^2}+\frac{1}{τ^2})^{-1}
\end{align}
\]</span></p>
<ul>
<li>If <span class="math inline">\(n = 5\)</span>, <span class="math inline">\(\bar x = 16.4\)</span>, <span class="math inline">\(sigma^2 = 0.052\)</span>, <span class="math inline">\(ν = 16\)</span>, and <span class="math inline">\(τ^2 = 0.002\)</span>, then the posterior distribution of <span class="math inline">\(\mu\)</span> is <span class="math inline">\(N(16.32, 0.02^2)\)</span>.
<ul>
<li>Suppose that the loss functions are</li>
</ul></li>
</ul>
<p><span class="math display">\[
l(\mu,reject \ H_0)=
\left\{\begin{matrix}
2(16 − \mu) &amp; if \ \mu \le 16\\
\mu − 16 &amp; if \ \mu &gt; 16,
\end{matrix}\right. \,\,\,
and \,\,\, l(\mu, \,\,\,accept \,\,\, H_0) = l_1.
\]</span></p>
<ul>
<li>The Bayes losses are</li>
</ul>
<p><span class="math display">\[
\begin{align}
B(reject \ H_0|x) &amp;= E_{\mu|x} [l(\mu,reject  \ H_0)]\\
&amp;= l_1 × P(reject \ H_0|\mu \ne 16, x) + l_1 × P(reject \ H_0|\mu = 16, x) = l_1 \\
B(accept \ H_0|x) &amp;= E_\mu|x [l(\mu, accept \ H_0)]\\
&amp;=\int_{−∞}^{16}2(16 − \mu)f_{\mu|x}(\mu|x) d\mu +\int_{16}^{∞}2(\mu-16)f_{\mu|x}(\mu|x) d\mu
\end{align}
\]</span></p>
<ul>
<li>The latter integral can be computed as follows.
<ul>
<li>Denote the conditional mean and variance of <span class="math inline">\(\mu\)</span> as <span class="math inline">\(\mu_{\mu|x}\)</span> and <span class="math inline">\(\sigma^2_{\mu|x}\)</span>. That is,</li>
</ul></li>
</ul>
<p><span class="math display">\[
\mu_{\mu|x} = E(\mu|x) \,\,\, and \,\,\, \sigma^2_{\mu|x} = Var(\mu|x).
\]</span></p>
<ul>
<li>Transform from <span class="math inline">\(\mu\)</span> to</li>
</ul>
<p><span class="math display">\[
z= \frac{\mu − \mu_{\mu|x}}{\sigma_{\mu|x}}
\]</span></p>
<ul>
<li>Denote the pdf of the standard normal distribution as <span class="math inline">\(\Phi(z)\)</span>. Then,</li>
</ul>
<p><span class="math display">\[
\begin{align}
\mu &amp;= z\sigma_{\mu|x} + \mu_{\mu|x},\\
d\mu &amp;= \sigma_{\mu|x} dz, \ and\\
B(accept \ H_0|x) &amp;=\int_{−∞}^{ (16−\mu_{\mu|x})/\sigma_{\mu|x}}2(16 − \sigma_{\mu|x}z − \mu_{\mu|x})\Phi(z) dz\\
&amp;+ \int_{(16−\mu_{\mu|x})/\sigma_{\mu|x}}^{∞}(\sigma_{\mu|x}z + \mu_{\mu|x} - 16)\Phi(z) dz\\
&amp;=-\int_{−∞}^{-16}2(0.32 + 0.02z)\Phi(z) dz + \int_{-16}^{∞}(0.02z+0.32)\Phi(z) dz \\
&amp;\approx \int_{-16}^{∞}(0.02z+0.32)\Phi(z) dz\\
&amp;\approx E(0.02Z + 0.32) = 0.032
\end{align}
\]</span></p>
<p>because essentially the entire standard normal distribution lies in the interval <span class="math inline">\((−16, \infty)\)</span> and essentially none of the distribution lies in the interval <span class="math inline">\((−\infty, −16)\)</span>.</p>
<ul>
<li><p>The Bayes test rejects <span class="math inline">\(H_0\)</span> if <span class="math inline">\(l_1 \le 0.32\)</span> and accepts <span class="math inline">\(H_0\)</span> if <span class="math inline">\(l_1 &gt; 0.32\)</span>.</p></li>
<li><p>Example: The goal is to conduct a Bayes Test of <span class="math inline">\(H_0: p \le\frac{1}{2}\)</span> against <span class="math inline">\(H_a: p &gt;\frac{1}{2}\)</span> based on a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Bern(p)\)</span>.</p>
<ul>
<li>The losses are</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
l(p, act) &amp;= 0 \text{ if the correct decision is made} \\
l(H_0,reject \,\,\, H_0) &amp;= l_0, \,\,\, and \\
l(H_a,reject \,\,\, H_0) &amp;= l_1
\end{align}
\]</span></p>
<ul>
<li>The prior on <span class="math inline">\(p\)</span> is <span class="math inline">\(Beta(\alpha, \beta)\)</span>.
<ul>
<li>Using the results in example the posterior distribution of <span class="math inline">\(p\)</span> conditional on <span class="math inline">\(x\)</span> is <span class="math inline">\(Beta(\alpha + y, \beta + n − y)\)</span>, where <span class="math inline">\(y\)</span> is the observed number of successes on the <span class="math inline">\(n\)</span> Bernoulli trials.
<ul>
<li>The Bayes losses are</li>
</ul></li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
B(reject \ H_0|x) &amp;= E_{p|x} [l(p,reject \ H_0)]\\
&amp;=\int_{H_0}^{}l_0 \frac{p^{\alpha+y−1}(1 − p)^{\beta+n−y−1}}{B(\alpha + y, \beta + n − y)}dp\\
&amp;=l_0\int_{0}^{0.5}l_0 \frac{p^{\alpha+y−1}(1 − p)^{\beta+n−y−1}}{B(\alpha + y, \beta + n − y)}dp\\
&amp;= l_0 × P (p \le 0.5|x) \ and\\
B(reject \ H_0|x) &amp;= E_{p|x} [l(p,reject \ H_0)]\\
&amp;=\int_{H_0}^{}l_1 \frac{p^{\alpha+y−1}(1 − p)^{\beta+n−y−1}}{B(\alpha + y, \beta + n − y)}dp\\
&amp;=l_1\int_{0.5}^{1}l_0 \frac{p^{\alpha+y−1}(1 − p)^{\beta+n−y−1}}{B(\alpha + y, \beta + n − y)}dp\\
&amp;= l_1 × P (p &gt; 0.5|x)
\end{align}
\]</span></p>
<ul>
<li>The required probabilities can be computed using any computer routine that calculates the CDF of a beta distribution.
<ul>
<li>If <span class="math inline">\(n = 10\)</span>, <span class="math inline">\(y = 3\)</span>, <span class="math inline">\(\alpha = 7\)</span>, <span class="math inline">\(\beta = 3\)</span>, <span class="math inline">\(l_0 = 3\)</span>, <span class="math inline">\(l_1 = 2\)</span>, then the posterior distribution of <span class="math inline">\(p\)</span> is <span class="math inline">\(Beta(10, 10)\)</span> and the Bayes losses are</li>
</ul></li>
</ul>
<p><span class="math display">\[
B(reject \ H_0|x) = 3P(W \le 0.5) \,\,\, and \,\,\, B(accept \ H_0|x) = 2P(W &gt; 0.5)
\]</span></p>
<p>where <span class="math inline">\(W \sim Beta(10, 10)\)</span>.</p>
<ul>
<li><p>This beta distribution is symmetric around <span class="math inline">\(0.5\)</span> and, therefore, each of the above probabilities is <span class="math inline">\(\frac{1}{2}\)</span>.</p></li>
<li><p>The Bayes test is to accept <span class="math inline">\(H_0\)</span> because the Bayes loss is <span class="math inline">\(1\)</span>, whereas the Bayes loss for rejection is <span class="math inline">\(1.5\)</span>.</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter10.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter12.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Mathematical Statistics.pdf", "Mathematical Statistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
