<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Discrete Random Variables | Mathematical Statistics</title>
  <meta name="description" content="This is a Mathematical Statistics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Discrete Random Variables | Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Mathematical Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Discrete Random Variables | Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is a Mathematical Statistics" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2024-08-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter1.html"/>
<link rel="next" href="chapter3.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#algebra-of-events"><i class="fa fa-check"></i><b>1.2</b> Algebra of Events</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#experiments-with-symmetries"><i class="fa fa-check"></i><b>1.3</b> Experiments with Symmetries</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#composition-of-experiments-counting-rules"><i class="fa fa-check"></i><b>1.4</b> Composition of Experiments: Counting Rules</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#sampling-at-random"><i class="fa fa-check"></i><b>1.5</b> Sampling at Random</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#binomial-multinomial-coefficients"><i class="fa fa-check"></i><b>1.6</b> Binomial &amp; Multinomial Coefficients</a></li>
<li class="chapter" data-level="1.7" data-path="chapter1.html"><a href="chapter1.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="1.8" data-path="chapter1.html"><a href="chapter1.html#subjective-probability"><i class="fa fa-check"></i><b>1.8</b> Subjective Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#probability-functions"><i class="fa fa-check"></i><b>2.1</b> Probability Functions</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Joint Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#conditional-probability"><i class="fa fa-check"></i><b>2.3</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#bayes-theorem-law-of-inverse-probability"><i class="fa fa-check"></i><b>2.4</b> Bayes Theorem (Law of Inverse Probability)</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#statistical-independence-of-random-variables"><i class="fa fa-check"></i><b>2.5</b> Statistical Independence of Random Variables</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#exchangeability"><i class="fa fa-check"></i><b>2.6</b> Exchangeability</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#application-probability-of-winning-in-craps"><i class="fa fa-check"></i><b>2.7</b> Application: Probability of Winning in Craps</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Expectations of Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#the-mean"><i class="fa fa-check"></i><b>3.1</b> The Mean</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#expectation-of-a-function"><i class="fa fa-check"></i><b>3.2</b> Expectation of a Function</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#variability"><i class="fa fa-check"></i><b>3.3</b> Variability</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#sums-of-random-variables"><i class="fa fa-check"></i><b>3.5</b> Sums of Random Variables</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#probability-generating-functions"><i class="fa fa-check"></i><b>3.6</b> Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Bernoulli and Related Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#sampling-bernoulli-populations"><i class="fa fa-check"></i><b>4.1</b> Sampling Bernoulli Populations</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#binomial-distribution"><i class="fa fa-check"></i><b>4.2</b> Binomial Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>4.3</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#negative-hypergeometric-distribution"><i class="fa fa-check"></i><b>4.6</b> Negative Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#approximating-binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Approximating Binomial Probabilities</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="chapter4.html"><a href="chapter4.html#normal-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.1</b> Normal approximation to the Binomial</a></li>
<li class="chapter" data-level="4.7.2" data-path="chapter4.html"><a href="chapter4.html#poisson-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.2</b> Poisson Approximation to the Binomial</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#poisson-distribution"><i class="fa fa-check"></i><b>4.8</b> Poisson Distribution</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#law-of-large-numbers"><i class="fa fa-check"></i><b>4.9</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="4.10" data-path="chapter4.html"><a href="chapter4.html#multinomial-distributions"><i class="fa fa-check"></i><b>4.10</b> Multinomial Distributions</a></li>
<li class="chapter" data-level="4.11" data-path="chapter4.html"><a href="chapter4.html#using-probability-generating-functions"><i class="fa fa-check"></i><b>4.11</b> Using Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.1</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#density-and-the-probability-element"><i class="fa fa-check"></i><b>5.2</b> Density and the Probability Element</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#the-median-and-other-percentiles"><i class="fa fa-check"></i><b>5.3</b> The Median and Other Percentiles</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#expected-value"><i class="fa fa-check"></i><b>5.4</b> Expected Value</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#expected-value-of-a-function"><i class="fa fa-check"></i><b>5.5</b> Expected Value of a Function</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#average-deviations"><i class="fa fa-check"></i><b>5.6</b> Average Deviations</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#bivariate-distributions"><i class="fa fa-check"></i><b>5.7</b> Bivariate Distributions</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#several-variables"><i class="fa fa-check"></i><b>5.8</b> Several Variables</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#covariance-and-correlation-1"><i class="fa fa-check"></i><b>5.9</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#independence"><i class="fa fa-check"></i><b>5.10</b> Independence</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#conditional-distributions"><i class="fa fa-check"></i><b>5.11</b> Conditional Distributions</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#moment-generating-functions"><i class="fa fa-check"></i><b>5.12</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Families of Continuous Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#normal-distributions"><i class="fa fa-check"></i><b>6.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#exponential-distributions"><i class="fa fa-check"></i><b>6.2</b> Exponential Distributions</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#gamma-distributions"><i class="fa fa-check"></i><b>6.3</b> Gamma Distributions</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#chi-squared-distributions"><i class="fa fa-check"></i><b>6.4</b> Chi Squared Distributions</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#distributions-for-reliability"><i class="fa fa-check"></i><b>6.5</b> Distributions for Reliability</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#t-f-and-beta-distributions"><i class="fa fa-check"></i><b>6.6</b> <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and Beta Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Organizing &amp; Describing Data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#frequency-distributions"><i class="fa fa-check"></i><b>7.1</b> Frequency Distributions</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#data-on-continuous-variables"><i class="fa fa-check"></i><b>7.2</b> Data on Continuous Variables</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#order-statistics"><i class="fa fa-check"></i><b>7.3</b> Order Statistics</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#data-analysis"><i class="fa fa-check"></i><b>7.4</b> Data Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#the-sample-mean"><i class="fa fa-check"></i><b>7.5</b> The Sample Mean</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#measures-of-dispersion"><i class="fa fa-check"></i><b>7.6</b> Measures of Dispersion</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#correlation"><i class="fa fa-check"></i><b>7.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Samples, Statistics, &amp; Sampling Distributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#random-sampling"><i class="fa fa-check"></i><b>8.1</b> Random Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#likelihood"><i class="fa fa-check"></i><b>8.2</b> Likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#sufficient-statistics"><i class="fa fa-check"></i><b>8.3</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#sampling-distributions"><i class="fa fa-check"></i><b>8.4</b> Sampling Distributions</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>8.5</b> Simulating Sampling Distributions</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#order-statistics-1"><i class="fa fa-check"></i><b>8.6</b> Order Statistics</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#moments-of-sample-means-and-proportionssp"><i class="fa fa-check"></i><b>8.7</b> Moments of Sample Means and Proportionssp</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>8.8</b> The Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#using-the-moment-generating-function"><i class="fa fa-check"></i><b>8.9</b> Using the Moment Generating Function</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#normal-populations"><i class="fa fa-check"></i><b>8.10</b> Normal Populations</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#updating-prior-probabilities-via-likelihood"><i class="fa fa-check"></i><b>8.11</b> Updating Prior Probabilities Via Likelihood</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#some-conjudate-families"><i class="fa fa-check"></i><b>8.12</b> Some conjudate Families</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#predictive-distributions"><i class="fa fa-check"></i><b>8.13</b> Predictive Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Estimation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#point-estimation"><i class="fa fa-check"></i><b>9.1</b> Point Estimation</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#errors-in-estimation"><i class="fa fa-check"></i><b>9.2</b> Errors in Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#consistency"><i class="fa fa-check"></i><b>9.3</b> Consistency</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#large-sample-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Large Sample Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#determining-sample-size"><i class="fa fa-check"></i><b>9.5</b> Determining Sample Size</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#small-sample-confidence-intervals-for-mu_x"><i class="fa fa-check"></i><b>9.6</b> Small Sample Confidence Intervals for <span class="math inline">\(\mu_X\)</span></a></li>
<li class="chapter" data-level="9.7" data-path="chapter9.html"><a href="chapter9.html#the-distribution-of-t"><i class="fa fa-check"></i><b>9.7</b> The Distribution of <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="9.8" data-path="chapter9.html"><a href="chapter9.html#pivotal-quantities"><i class="fa fa-check"></i><b>9.8</b> Pivotal Quantities</a></li>
<li class="chapter" data-level="9.9" data-path="chapter9.html"><a href="chapter9.html#estimating-a-mean-difference"><i class="fa fa-check"></i><b>9.9</b> Estimating a Mean Difference</a></li>
<li class="chapter" data-level="9.10" data-path="chapter9.html"><a href="chapter9.html#umvue"><i class="fa fa-check"></i><b>9.10</b> UMVUE</a></li>
<li class="chapter" data-level="9.11" data-path="chapter9.html"><a href="chapter9.html#bayes-estimators"><i class="fa fa-check"></i><b>9.11</b> Bayes Estimators</a></li>
<li class="chapter" data-level="9.12" data-path="chapter9.html"><a href="chapter9.html#efficiency"><i class="fa fa-check"></i><b>9.12</b> Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Significance Testing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chapter10.html"><a href="chapter10.html#hypotheses"><i class="fa fa-check"></i><b>10.1</b> Hypotheses</a></li>
<li class="chapter" data-level="10.2" data-path="chapter10.html"><a href="chapter10.html#assessing-the-evidence"><i class="fa fa-check"></i><b>10.2</b> Assessing the Evidence</a></li>
<li class="chapter" data-level="10.3" data-path="chapter10.html"><a href="chapter10.html#one-sample-z-tests"><i class="fa fa-check"></i><b>10.3</b> One Sample <span class="math inline">\(Z\)</span> Tests</a></li>
<li class="chapter" data-level="10.4" data-path="chapter10.html"><a href="chapter10.html#one-sample-t-tests"><i class="fa fa-check"></i><b>10.4</b> One Sample <span class="math inline">\(t\)</span> Tests</a></li>
<li class="chapter" data-level="10.5" data-path="chapter10.html"><a href="chapter10.html#some-nonparametric-tests"><i class="fa fa-check"></i><b>10.5</b> Some Nonparametric Tests</a></li>
<li class="chapter" data-level="10.6" data-path="chapter10.html"><a href="chapter10.html#probability-of-the-null-hypothesis"><i class="fa fa-check"></i><b>10.6</b> Probability of the Null Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Tests as Decision Rules</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#rejection-regions-and-errors"><i class="fa fa-check"></i><b>11.1</b> Rejection Regions and Errors</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#the-power-function"><i class="fa fa-check"></i><b>11.2</b> The Power function</a></li>
<li class="chapter" data-level="11.3" data-path="chapter11.html"><a href="chapter11.html#choosing-a-sample-size"><i class="fa fa-check"></i><b>11.3</b> Choosing a Sample Size</a></li>
<li class="chapter" data-level="11.4" data-path="chapter11.html"><a href="chapter11.html#most-powerful-tests"><i class="fa fa-check"></i><b>11.4</b> Most Powerful Tests</a></li>
<li class="chapter" data-level="11.5" data-path="chapter11.html"><a href="chapter11.html#uniformly-most-powerful-tests"><i class="fa fa-check"></i><b>11.5</b> Uniformly Most Powerful Tests</a></li>
<li class="chapter" data-level="11.6" data-path="chapter11.html"><a href="chapter11.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>11.6</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="11.7" data-path="chapter11.html"><a href="chapter11.html#bayesian-testing"><i class="fa fa-check"></i><b>11.7</b> Bayesian Testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chapter12.html"><a href="chapter12.html"><i class="fa fa-check"></i><b>12</b> Appendix</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chapter12.html"><a href="chapter12.html#greek-alphabet"><i class="fa fa-check"></i><b>12.1</b> Greek Alphabet</a></li>
<li class="chapter" data-level="12.2" data-path="chapter12.html"><a href="chapter12.html#abbreviations"><i class="fa fa-check"></i><b>12.2</b> Abbreviations</a></li>
<li class="chapter" data-level="12.3" data-path="chapter12.html"><a href="chapter12.html#practice-exams"><i class="fa fa-check"></i><b>12.3</b> PRACTICE EXAMS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter2" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Discrete Random Variables<a href="chapter2.html#chapter2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li><p>Definition: A Random Variable is a characteristic of the outcome of an experiment.</p></li>
<li><p>Notation: Use capital letters to denote random variables (rvs). Example: <span class="math inline">\(X(\omega)\)</span> is a rv. Use small letters to denote a realization of the random variable.</p></li>
<li><p>Example: Consider the experiment of choosing a student at random from a classroom. Then <span class="math inline">\(\Omega\)</span> = <span class="math inline">\(\{\)</span>Jack, Dolores, <span class="math inline">\(\dots \}\)</span>. Let <span class="math inline">\(X(\omega)\)</span> be a characteristic of student <span class="math inline">\(\omega\)</span>. Then <span class="math inline">\(X(\omega)\)</span> is a rv.</p></li>
</ul>
<div id="types-of-random-variables" class="section level4 unnumbered hasAnchor">
<h4>Types of random variables<a href="chapter2.html#types-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Categorical versus Numerical
<ul>
<li><span class="math inline">\(X(\omega)\)</span> = gender of selected student is a categorical random variable and <span class="math inline">\(X(\omega_2)\)</span> = <span class="math inline">\(x_2\)</span> = “female” is a realization of the random variable.<br />
</li>
<li><span class="math inline">\(Y(\omega)\)</span> = age of selected student is a numerical random variable and <span class="math inline">\(Y(\omega_1)\)</span> = <span class="math inline">\(y_1\)</span> = 19.62 is a realization of the random variable.</li>
</ul></li>
<li>Continuous versus Discrete
<ul>
<li>If the possible values of a rv are countable, then the rv is discrete.<br />
</li>
<li>If the possible values of a rv are contained in open subsets (or half open subsets) of the real line, then the rv is continuous.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="probability-functions" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Probability Functions<a href="chapter2.html#probability-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: The probability function (p.f.) of a discrete rv assigns a probability to the event <span class="math inline">\(X(\omega)\)</span> = <span class="math inline">\(x\)</span>. The p.f. is denoted by <span class="math inline">\(f_X(x)\)</span> and is defined by</li>
</ul>
<p><span class="math display">\[
f_X(x) \overset{def}= \Pr[X(\omega) = x] = \sum_{X(\omega) = x} \Pr(\omega).
\]</span></p>
<ul>
<li><p>This function also is called a probability mass function (pmf). The terminology pmf appears to be more often used than p.f., so I will use pmf rather than p.f.</p></li>
<li><p>The pmf can be an equation, a table, or a graph that shows how probability is assigned to possible values of the random variable.</p></li>
<li><p>The distribution of probabilities across all possible values is called the probability distribution. A probability distribution may be displayed as (a) a table, (b) a graph, or (c) an equation.</p></li>
</ul>
<div id="examples" class="section level4 unnumbered hasAnchor">
<h4>Examples<a href="chapter2.html#examples" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Example: Roll a fair four sided die twice. The face values on the die are 1, 2, 3, and 4. The sample space is <span class="math inline">\(\Omega\)</span> = <span class="math inline">\(\{\)</span>(1, 2),(1, 2), . . . ,(4, 4)<span class="math inline">\(\}\)</span>.
<ul>
<li>Note that <span class="math inline">\(\#(\Omega)\)</span> = 16 and each outcome <span class="math inline">\(\omega = (\omega_1, \omega_2)\)</span> is equally likely.</li>
<li>Let <span class="math inline">\(X(\omega) = \max(\omega_1, \omega_2)\)</span>. Find the pmf of <span class="math inline">\(X\)</span>. Solution:</li>
</ul></li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\(f_X(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">1/16</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">3/16</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">5/16</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">7/16</td>
</tr>
<tr class="odd">
<td align="center">Total</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<ul>
<li>Example: Choose a baby name at random from <span class="math inline">\(\Omega\)</span> = <span class="math inline">\(\{\)</span>John, Josh, Thomas, William<span class="math inline">\(\}\)</span>.
<ul>
<li>Let <span class="math inline">\(X(\omega)\)</span> = first letter of name then <span class="math inline">\(P(X = J) = 0.5\)</span>.</li>
</ul></li>
<li>Support of the distribution: The set of possible values of <span class="math inline">\(X\)</span> that have non-zero probability is called the support of the distribution. We will denote this set by <span class="math inline">\(S\)</span>. That is,</li>
</ul>
<p><span class="math display">\[
S = {x; f(x) ＞0}.
\]</span></p>
<ul>
<li><p>The support of a random variable is analogous to the sample space of an experiment. Note, <span class="math inline">\(f_X(x)\)</span> is abbreviated as <span class="math inline">\(f(x)\)</span>. This convention will be followed if it is clear that the pmf <span class="math inline">\(f(x)\)</span> refers to the random variable <span class="math inline">\(X\)</span>.</p></li>
<li><p>Properties of a pmf</p>
<ul>
<li><span class="math inline">\(f(x) ≥ 0\)</span> for all <span class="math inline">\(x\)</span>. This property also can be written as <span class="math inline">\(f(x) ≥ 0 \ \forall x.\)</span><br />
</li>
<li><span class="math inline">\(\displaystyle\sum_{x \in S} \Pr(X = x) = 1\)</span>.</li>
</ul></li>
<li><p>Indicator Function:</p></li>
</ul>
<p><span class="math display">\[
I_A(a) = \begin{cases}
1 &amp; \text{ if } a \in A, \\
0 &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<ul>
<li>Application of indicator function. Consider, again, the random variable <span class="math inline">\(X(\omega) = \max(\omega_1, \omega_2)\)</span>, where <span class="math inline">\((\omega_1, \omega_2)\)</span> is an outcome when rolling a fair four-sided die twice. The pmf of <span class="math inline">\(X\)</span> is</li>
</ul>
<p><span class="math display">\[ \begin{align}
f_x(x) &amp;= \begin{cases}
\frac{2x-1}{16} &amp; \text{ if } x = 1, 2, 3, 4 \\
0 &amp; \text{otherwise.}
\end{cases} \\
&amp;= \frac{2x-1}{16}I_{\{1,2,3,4\}}(x)
\end{align}
\]</span></p>
<p><br></p>
</div>
</div>
<div id="joint-distributions" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Joint Distributions<a href="chapter2.html#joint-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Joint Probability Functions: Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be discrete random variables defined on <span class="math inline">\(S_X\)</span> and <span class="math inline">\(S_Y\)</span> , respectively. Then the joint pmf (or p.f.) of <span class="math inline">\((X, Y)\)</span> is defined as</li>
</ul>
<p><span class="math display">\[
f_{X, Y}(x, y) \overset{def}= \Pr[X(\omega) = x, Y(\omega) = y] = \Pr(X = x, Y = y).
\]</span></p>
<ul>
<li>Note, joint distributions can be extended from the bivariate case (above) to the general multivariate case. A joint pmf satisfies</li>
</ul>
<p><span class="math display">\[
f(x, y) ≥ 0
\]</span></p>
<p>      for all pairs <span class="math inline">\((x, y)\)</span> and <span class="math inline">\(\sum_{(x, y) \in S} f(x, y) = 1\)</span>, where <span class="math inline">\(S = S_X × S_Y = \{(u, \upsilon); \ u \in S_X, \ \upsilon \in S_Y \}\)</span>.</p>
<ul>
<li><p>Note: The set <span class="math inline">\(S = S_X × S_Y\)</span> could include <span class="math inline">\((x, y)\)</span> pairs that have probability zero. If so, then the true support is a subset of <span class="math inline">\(S\)</span>.</p></li>
<li><p>Example: Two way table for powerball. See problem 1-R11 on page 39. Let <span class="math inline">\(X(\omega)\)</span> = number of matches out of 5 on first drawing and <span class="math inline">\(Y(\omega)\)</span> = number of matches out of 1 on second drawing. Then</p></li>
</ul>
<p><span class="math display">\[
f_{X, Y}(x, y) = P(X = x, Y = y) = \frac{\binom{5}{x}\binom{40}{5-x}\binom{1}{y}\binom{44}{1-y}}{\binom{45}{5}\binom{45}{1}}I_{\{0, 1, \dots, 5\}}(x)I_{\{0, 1\}}(y),
\]</span></p>
<p>      where <span class="math inline">\(x = 0, . . . , 5\)</span> and <span class="math inline">\(y = 0, 1\)</span>. For other values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the probability is zero.</p>
<ul>
<li>These probabilities, multiplied by 54,979,155, are given below:</li>
</ul>
<center>
<img src="fig2/fig2_2.jpg" />
</center>
<p><br></p>
<ul>
<li>In decimal form, the probabilities are</li>
</ul>
<center>
<img src="fig2/fig2_3.jpg" />
</center>
<ul>
<li>Marginal pmf: Sum the joint pmf over all other variables to obtain the marginal pmf of one random variable.
<ul>
<li><span class="math inline">\(f_X(x) = \displaystyle\sum_{y \in S_Y} f(x, y)\)</span>.<br />
</li>
<li><span class="math inline">\(f_Y(y) = \displaystyle\sum_{x \in S_x} f(x, y)\)</span>.<br />
</li>
<li><span class="math inline">\(f_Z(z) = \displaystyle\sum_{x \in S_x}\displaystyle\sum_{y \in S_Y} f_{X, Y, Z}(x, y, z)\)</span>.</li>
</ul></li>
<li>Example 1: The marginal pmfs for the powerball problem are</li>
</ul>
<p><span class="math display">\[ f_X(x) = \displaystyle\sum_{y=0}^{1}f_{X, Y}(x, y) = P(X = x) = \frac{\binom{5}{x}\binom{40}{5-x}}{\binom{45}{5}} I_{\{0, 1, \dots, 5\}}(x)
\]</span></p>
<p>      and</p>
<p><span class="math display">\[
f_Y(y) = \displaystyle\sum_{x=0}^{5}f_{X, Y}(x, y) = P(Y = y) = \frac{\binom{1}{y}\binom{44}{1-y}}{\binom{45}{1}} I_{\{0, 1\}}(y).
\]</span></p>
<ul>
<li>Example 2: Suppose that</li>
</ul>
<p><span class="math display">\[
f_{X, Y}(x, y) = \begin{cases}
\frac{2(i + 2j)}{3n(n + 1)^2} &amp; i = 0, 1, \dots, n \ \text{ and } j = 0, 1, \dots, n \\
0 &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p>      Use the result</p>
<p><span class="math display">\[
\displaystyle\sum_{i=0}^n i = \displaystyle\sum_{i=1}^n i = \frac{n(n + 1)}{2}
\]</span></p>
<p>      to obtain</p>
<p><span class="math display">\[
f_{X}(i) = \begin{cases}
\frac{2(n + i)}{3n(n + 1)} &amp; i = 0, 1, \dots, n \\
0 &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p>      and</p>
<p><span class="math display">\[
f_{Y}(j) = \begin{cases}
\frac{n + 4j}{3n(n + 1)} &amp; j = 0, 1, \ldots, n \\
0 &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p><br></p>
</div>
<div id="conditional-probability" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Conditional Probability<a href="chapter2.html#conditional-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definitions:</li>
</ul>
<p><span class="math display">\[
P(\omega|B) \overset{def}= \begin{cases}
\frac{P(\omega)}{P(B)} &amp; \text{if} \ \omega \in B \\
0 &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p><span class="math display">\[
P(A|B) \overset{def}= \frac{P(A \cap B)}{P(B)} \ \text{provided that} \ P(B) ＞ 0.
\]</span></p>
<ul>
<li>These quantities are read as “probability of <span class="math inline">\(\omega\)</span> given <span class="math inline">\(B\)</span>” and “probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>.”
<ul>
<li>Think of <span class="math inline">\(B\)</span> as the new sample space and then re-scale <span class="math inline">\(P(\omega)\)</span> and <span class="math inline">\(P(A \cap B)\)</span> relative to <span class="math inline">\(P(B)\)</span>.</li>
</ul></li>
</ul>
<div id="examples-monte-hall" class="section level4 unnumbered hasAnchor">
<h4>Examples: Monte Hall<a href="chapter2.html#examples-monte-hall" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Lets Make a Deal.
<ul>
<li>Note: this is not the same game that is described on page <span class="math inline">\(79\)</span> of the text.</li>
<li>An SUV is randomly placed behind one of three identical doors.</li>
<li>Goats are placed behind the other two doors.</li>
<li>You choose a door (say door 1) and will win the item behind the door after it is opened.</li>
<li>Before your door is opened, however, Monte Hall reveals a goat behind one of the two remaining doors (either door 2 or door 3).</li>
<li>If he reveals a goat behind door 2, then he gives you the option of switching from door 1 to door 3.</li>
<li>If he reveals a goat behind door 3, then he gives you the option of switching from door 1 to door 2.</li>
<li>To maximize the probability of winning the SUV, should you stick with your original choice or should you switch? Assume that Monte knows where the SUV is; he always reveals a goat; and he never reveals the content behind the door that you choose.</li>
</ul></li>
<li>Solution:
<ul>
<li>Let <span class="math inline">\(C = i\)</span> (C for choose) be the event that your initial choice is door <span class="math inline">\(i\)</span>.</li>
<li>Let <span class="math inline">\(S = i\)</span> (<span class="math inline">\(S\)</span> for SUV) be the event that the SUV is behind door <span class="math inline">\(i\)</span>.</li>
<li>Let <span class="math inline">\(R = i\)</span> (<span class="math inline">\(R\)</span> for reveal) be the event that Monte reveals a goat behind door <span class="math inline">\(i\)</span>.</li>
<li>Conditional on <span class="math inline">\(C = 1\)</span>, the table of joint probabilities for <span class="math inline">\((R, S)\)</span> is as follows</li>
</ul></li>
</ul>
<center>
<img src="fig2/fig2_4.jpg" />
</center>
<ul>
<li>In the above table, the value of p1 must satisfy <span class="math inline">\(p_1 \in (0,\frac{1}{3})\)</span>.
<ul>
<li>If Monte chooses a door at random when <span class="math inline">\(S = 1\)</span>, then <span class="math inline">\(p_1 = \frac{1}{6}\)</span>. Accordingly,</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
P(S = 1|C = 1) &amp;= \frac{1}{3}, \\
P(S ≠ 1|C = 1) &amp;= 1 - P(S = 1|C = 1) = \frac{2}{3}.
\end{align}
\]</span></p>
<ul>
<li>If your strategy is to stay with door 1, then you win the SUV with
probability <span class="math inline">\(\frac{1}{3}\)</span>.
<ul>
<li>If your strategy is to switch, then you will win the SUV if <span class="math inline">\(S ≠ 1\)</span> because you always switch to the correct door.</li>
<li>This event has probability <span class="math inline">\(\frac{2}{3}\)</span>.</li>
<li>Therefore, the best strategy is to switch. For more information, go to <a href="http://math.rice.edu/∼ddonovan/montyurl.html" class="uri">http://math.rice.edu/∼ddonovan/montyurl.html</a>.</li>
</ul></li>
<li>Multiplication Rule
<ul>
<li>Two events: <span class="math inline">\(P(E \cap F) = P(F|E)P(E) = P(E|F)P(F)\)</span>.<br />
</li>
<li>More than two events: <span class="math inline">\(P(\displaystyle\bigcap_{i=1}^{k} E_i) = P(E_1)\prod_{j=2}^{k}P(E_j|\bigcap_{i=1}^{j-1}E_j)\)</span>.</li>
<li>For example, with 4 events, <span class="math inline">\(P(E_1, E_2, E_3, E_4) = P(E_1) × P(E_2|E_1) × P(E_3|E_1, E_2) × P(E_4|E_1, E_2, E_3)\)</span>.</li>
</ul></li>
<li>Applications of Multiplication rule
<ul>
<li>If samples are selected at random one at a time without replacement, then all sequences are equally likely.<br />
</li>
<li>Proof: The number of distinct sequences of <span class="math inline">\(n\)</span> objects selected from <span class="math inline">\(N\)</span> objects is <span class="math inline">\((N)_n\)</span> Label the <span class="math inline">\(N\)</span> objects as <span class="math inline">\(o_1, o_2, \dots , o_N\)</span>.</li>
<li>Label the first selection as <span class="math inline">\(S_1\)</span>, the second selection as <span class="math inline">\(S_2\)</span>, etc. Then</li>
</ul></li>
</ul>
<p><span class="math display">\[
(S_1 = o_{i_1}, S_2 = o_{i_2}, S_3 = o_{i_3}, \dots , S_n = o_{i_n})
\]</span></p>
<p>      is a sequence provided that the subscripts <span class="math inline">\(i_1, i_2, \dots , i_n\)</span> are all distinct.</p>
<ul>
<li>For example, if <span class="math inline">\(N = 100\)</span> and <span class="math inline">\(n = 3\)</span>, then <span class="math inline">\((S_1 = o_{23}, S_2 = o_{14}, S_3 = o_{89})\)</span> is a sequence. Using the multiplication rule, the probability of a sequence can be written as follows:</li>
</ul>
<p><span class="math display">\[ \begin{align}
P(S_1 &amp;= o_{i_1}, S_2 = o_{i_2}, S_3 = o_{i_3}, \dots , S_n = o_{i_n}) \\
&amp;= P(S_1 = o_{i_1}) × P(S_2 = o_{i_2}|S_1 = o_{i_1}) × P(S_3 = o_{i_3}|S_1 = o_{i_1}|S_2 = o_{i_2}) \\
&amp;× \dots × P(S_n = o_{i_n}|S_1 = o_{i_1}, \dots, S_{n-1} = o_{i_{n-1}}) \\
&amp;= \frac{1}{N} × \frac{1}{N-1} × \frac{1}{N-2} × \dots × \frac{1}{N-n+1} = \frac{(N-n)!}{N!} = \frac{1}{(N)_n}.
\end{align}
\]</span></p>
<p>      Accordingly, all sequences are equally likely.</p>
<ul>
<li>If samples are selected at random without replacement, then all combinations are equally likely.
<ul>
<li>Proof: The unordered set</li>
</ul></li>
</ul>
<p><span class="math display">\[ (o_{i_1}, o_{i_2}, \dots, o_{i_n}) \]</span></p>
<p>      is a combination provided that the subscripts <span class="math inline">\(i_1, i_2, \dots , i_n\)</span> are all distinct.</p>
<ul>
<li>The number of distinct combinations is <span class="math inline">\(\binom{N}{n}\)</span> and the objects in each combination can be ordered in <span class="math inline">\(n!\)</span> ways.
<ul>
<li>Therefore, each combination corresponds to <span class="math inline">\(n!\)</span> sequences and</li>
</ul></li>
</ul>
<p><span class="math display">\[ P(o_{i_1}, o_{i_2}, \dots, o_{i_n}) = n! × (S_1 = o_{i_1}, S_2 = o_{i_2}, \dots , S_n = o_{i_n}) = \frac{n!}{(N)_n} = \frac{1}{\binom{N}{n}}. \]</span></p>
<p>      Accordingly, each combination is equally likely.</p>
<ul>
<li>Conditional pmf: Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be discrete random variables. Then,</li>
</ul>
<p><span class="math display">\[ f_{X|Y}(x|y) \overset{def}= P(X = x| Y = y) = \frac{f_{X,Y}(x,y)}{f_Y(y)} = \frac{P(X = x, Y = y)}{P(Y = y)} \]</span></p>
<p><br></p>
</div>
</div>
<div id="bayes-theorem-law-of-inverse-probability" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Bayes Theorem (Law of Inverse Probability)<a href="chapter2.html#bayes-theorem-law-of-inverse-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayes theorem answers the question—How do you express <span class="math inline">\(P(E|F)\)</span> in terms of <span class="math inline">\(P(F|E)\)</span>?</p>
<ul>
<li>Bayes theorem states that</li>
</ul>
<p><span class="math display">\[ P(E|F) = \frac{P(F|E)P(E)}{P(F|E)P(E) + P(F|E^c)P(E^c)}. \]</span></p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[ \begin{align}
P(E|F) &amp;= \frac{P(E \cap F)}{P(F)} \ \text{by the definition of conditional probability} \\
&amp;= \frac{P(F|E)P(E)}{P(F)} \ \text{by the multiplication rule} \\
&amp;= \frac{P(F|E)P(E)}{P(F \cap E) + P(F \cap E^c)} \ \text{by the law of total probability} \\
&amp;= \frac{P(F|E)P(E)}{P(F|E)P(E) + P(F|E^c)P(E^c)} \ \text{by the multiplication rule}.
\end{align}
\]</span></p>
<ul>
<li>More generally, Bayes Theorem states that if <span class="math inline">\(E_1, E_2, \dots , E_n\)</span> is a partition of <span class="math inline">\(\Omega\)</span>, then</li>
</ul>
<p><span class="math display">\[ \Pr(E_k|F) = \frac{\Pr(F|E_k)\Pr(E_k)}{\displaystyle\sum_{i=1}^{n} \Pr(F|E_i)\Pr(E_i)}. \]</span></p>
<p>      Furthermore, the conditional odds of <span class="math inline">\(E_i\)</span> to <span class="math inline">\(E_j\)</span> is</p>
<p><span class="math display">\[ \text{Odds of } E_i \text{ to } E_j \text{ conditional on } F = \frac{P(E_i|F)}{P(E_j|F)} = \frac{P(F|E_i)}{P(F|E_j)} × \frac{P(E_i)}{P(E_j)}. \]</span></p>
<p><br></p>
</div>
<div id="statistical-independence-of-random-variables" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Statistical Independence of Random Variables<a href="chapter2.html#statistical-independence-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Definition: Two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> , are independent if and only if (iff) <span class="math inline">\(f_{X,Y}(x, y) = f_X(x)f_Y(y)\)</span> for all <span class="math inline">\((x, y) \in S_{X,Y}\)</span>.</p>
<ul>
<li>To denote independence, we write <span class="math inline">\(X \Perp Y\)</span>.</li>
</ul></li>
<li><p>Definition: <span class="math inline">\(k\)</span> random variables, <span class="math inline">\(X_1, X_2, \dots , X_k\)</span>, are mutually independent iff <span class="math inline">\(f(x_1, \dots, x_k) = \displaystyle\prod_{i=1}^{k}f_i(x_i)\)</span> for all <span class="math inline">\((x_1, \dots, x_k) \in S_{X_1, \dots, X_k}\)</span>.</p></li>
<li><p>Example: Consider a random variable <span class="math inline">\(X\)</span> with pmf <span class="math inline">\(f_X(x)\)</span>. Let <span class="math inline">\(X_1, X_2, \dots , X_n\)</span> be a sequence of random variables obtained by sampling at random from <span class="math inline">\(f_X\)</span> .</p>
<ul>
<li>Then, <span class="math inline">\(X_1, \dots , X_n\)</span> are independent random variables and their joint distribution is <span class="math inline">\(f_{X_1, \dots, X_n}(x_1, \dots , x_n) = \prod_{i=1}^{n}f_X(x_i)\)</span>.</li>
</ul></li>
<li><p>Independent Events.</p>
<ul>
<li>Let <span class="math inline">\(E_1, E_2, \dots , E_k\)</span> be events.</li>
<li>A set of indicator random variables, <span class="math inline">\(X_1, \dots , X_k\)</span> can be defined as</li>
</ul></li>
</ul>
<p><span class="math display">\[X_i = \begin{cases}
1 &amp; \text{if } E_i \text{ occurs, and } \\
0 &amp; \text{otherwise.}
\end{cases} \]</span></p>
<p>      Then the events <span class="math inline">\(E_1, E_2, \dots , E_k\)</span> are mutually independent if and only if the indicator variables <span class="math inline">\(X_1, \dots , X_k\)</span> are mutually independent.</p>
<ul>
<li>Result: Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be events. Then <span class="math inline">\(A \Perp B\)</span> if and only if <span class="math inline">\(P(A \cap B) = P(A)P(B)\)</span>.
<ul>
<li>Proof: Define the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as</li>
</ul></li>
</ul>
<p><span class="math display">\[X_i = \begin{cases}
1 &amp; \text{if } A \text{ occurs, } \\
0 &amp; \text{otherwise}
\end{cases} \text{ and } Y = \begin{cases}
1 &amp; \text{if } B \text{ occurs, } \\
0 &amp; \text{otherwise.}
\end{cases}\]</span></p>
<p>      First, assume that <span class="math inline">\(A \Perp B\)</span>. Then</p>
<p><span class="math display">\[
A \Perp B \Leftrightarrow X \Perp Y
\]</span></p>
<p><span class="math display">\[
\Rightarrow P(A \cap B) = f_{X, Y}(1, 1) = f_X(1)f_Y(1) = P(A)P(B).
\]</span></p>
<p>      Second, assume that <span class="math inline">\(P(A \cap B) = P(A)P(B)\)</span>. Then,</p>
<p><span class="math display">\[ P(A \cap B) = P(A)P(B) \Rightarrow f_{X, Y}(1, 1) = f_X(1)f_Y(1). \]</span></p>
<p>      Use this result to fill in the two-by-two table of joint and marginal probabilities:</p>
<center>
<img src="fig2/fig2_5.jpg" />
</center>
<center>
<img src="fig2/fig2_6.jpg" />
</center>
<p><span class="math display">\[ \begin{align}
\Rightarrow X \Perp Y \text{ because } f_{X, Y}(x, y) &amp;= f_X(x)f_Y(y) \text{ for } x = 0, 1, \ y = 0, 1 \\
&amp;\Rightarrow A \Perp B
\end{align}
\]</span></p>
<ul>
<li>Example: : Roll two distinct fair 6-sided dice.
<ul>
<li>Let <span class="math inline">\(E_1\)</span> be the event that the first die is odd, et <span class="math inline">\(E_2\)</span> be the event that the second die is even, and let <span class="math inline">\(E_3\)</span> be the event that there exactly one odd and one even die occur.</li>
<li>Are these events mutually independent? Are there any pairs of events that are independent?</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="exchangeability" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Exchangeability<a href="chapter2.html#exchangeability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Definition: Two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, are said to be exchangeable iff <span class="math inline">\(f_{X, Y}(x, y) = f_{X, Y}(y, x)\)</span> for all <span class="math inline">\((x, y) \in S_{X, Y}\)</span>.</p>
<ul>
<li>Note, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are exchangeable, then <span class="math inline">\(S_{X, Y} = S_{Y, X}\)</span>.</li>
</ul></li>
<li><p>Definition: <span class="math inline">\(n\)</span> random variables, <span class="math inline">\(X_1, \dots , X_n\)</span> are said to be exchangeable iff <span class="math inline">\(f_{X_1, \dots, X_n}(x_1, \dots, x_n) = f_{X_1, \dots, X_n}(x^*_1, \dots, x^*_n)\)</span> for all <span class="math inline">\((x_1, \dots , x_n) \in S_{X_1, \dots ,X_n}\)</span> and for all <span class="math inline">\((x^*_1, \dots, x^*_n)\)</span>, where <span class="math inline">\((x^*_1, \dots, x^*_n)\)</span> is a permutation of <span class="math inline">\((x_1, \dots, x_n)\)</span>. Note, the equality must be satisfied for all <span class="math inline">\(n!\)</span> permutations.</p></li>
<li><p>Result: If <span class="math inline">\(X_1, \dots , X_n\)</span> are exchangeable, then the marginal distributions of each <span class="math inline">\(X_i\)</span> are identical.</p>
<ul>
<li>Also, the joint distributions of any subset of <span class="math inline">\(k \ Xs\)</span> is thesame as the distribution of any other set of <span class="math inline">\(k \ Xs\)</span>, where <span class="math inline">\(k\)</span> can be <span class="math inline">\(1, 2, \dots , n\)</span>.<br />
</li>
<li>Proof that bivariate marginals are identical when 3 random variables are exchangeable: Recall, that the joint pmf of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is obtained from the joint pmf of <span class="math inline">\(X_1, X_2\)</span>, and <span class="math inline">\(X_3\)</span> as follows:</li>
</ul></li>
</ul>
<p><span class="math display">\[ f_{X_1, X_2}(x_1, x_2) = \sum_{x_3 \in S_{X_3}}f_{X_1, X_2, X_3}(x_1, x_2, x_3). \]</span></p>
<ul>
<li>If <span class="math inline">\(X_1, X_2\)</span>, and <span class="math inline">\(X_3\)</span> are exchangeable, then</li>
</ul>
<p><span class="math display">\[ \begin{align}
f_{X_1, X_2, X_3}(x_1, x_2, x_3) &amp;= f_{X_1, X_2, X_3}(x_1, x_3, x_2) \ \textrm{and} \\
f_{X_1, X_2}(x_1, x_2) &amp;= \sum_{x_3 \in S_{X_3}}f_{X_1, X_2, X_3}(x_1, x_3, x_2) = f_{X_1, X_3}(x_1, x_2)
\end{align}
\]</span></p>
<p>      Also,</p>
<p><span class="math display">\[ \begin{align}
f_{X_1, X_2, X_3}(x_1, x_2, x_3) &amp;= f_{X_1, X_2, X_3}(x_3, x_1, x_2) \ \textrm{and} \\
f_{X_1, X_2}(x_1, x_2) &amp;= \sum_{x_3 \in S_{X_3}}f_{X_1, X_2, X_3}(x_3, x_1, x_2) = f_{X_2, X_3}(x_1, x_2)
\end{align}
\]</span></p>
<p>      Accordingly, exchangeability implies that</p>
<p><span class="math display">\[ f_{X_1, X_2}(x_1, x_2) = f_{X_1, X_3}(x_1, x_2) = f_{X_2, X_3}(x_1, x_2) \]</span></p>
<p>      In the same manner, it can be shown that exchangeability implies that</p>
<p><span class="math display">\[ f_{X_1}(x_1) = f_{X_2}(x_1) = f_{X_3}(x_1) \]</span></p>
<ul>
<li><p>Example 1: If <span class="math inline">\(X_1, \dots , X_n\)</span> are independently and identically distributed (iid), then <span class="math inline">\(X_1, \dots , X_n\)</span> are exchangeable.</p></li>
<li><p>Example 2: Consider the procedure of sampling at random without replacement from a finite population of size <span class="math inline">\(N\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(X_1, X_2, \dots , X_n\)</span> be the first, second, etc selection and let <span class="math inline">\(x_1, \dots , x_n\)</span> be the population units.</li>
<li>The random variables are not independent but the random variables are exchangeable. For example</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
f_{X_1, X_2}(x_i, x_j) &amp;= f_{X_1}(x_i)f{X_2|X_1}(x_j|X_1 = x_i) \\
&amp;= \bigg(\frac{1}{N} \bigg) \bigg(\frac{1}{N-1} \bigg) = \frac{1}{N(N-1)} \ \textrm{ and } \\
f_{X_1, X_2}(x_j, x_i) &amp;= f_{X_1}(x_j)f{X_2|X_1}(x_i|X_1 = x_j) \\
&amp;= \bigg(\frac{1}{N} \bigg) \bigg(\frac{1}{N-1} \bigg) = \frac{1}{N(N-1)}.
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="application-probability-of-winning-in-craps" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Application: Probability of Winning in Craps<a href="chapter2.html#application-probability-of-winning-in-craps" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Recall, the rules of the game are as follows.
<ul>
<li>Roll a pair of dice (i.e., two die). If the sum of the dice is 7 or 11, then the player wins and the game is over.</li>
<li>If the sum of the dice is 2, 3, or 12, then the player loses and the game is over.</li>
<li>If the sum of the dice is anything else, then the sum is called “the point” and the game continues.</li>
<li>In this case, a player repeatedly rolls the pair of dice until either the sum is either 7 or equal to the point. If a 7 occurs first, then the player loses.</li>
<li>If the point occurs first, then the player wins.</li>
</ul></li>
<li>The sample space when rolling two dice is</li>
</ul>
<p><span class="math display">\[ \Omega = \{(1,1), (1,2), (2,1), \dots, (6,6) \}. \]</span></p>
<ul>
<li>If the dice are fair, then the 36 outcomes are equally likely.
<ul>
<li>Let <span class="math inline">\(Y(\omega)\)</span> be the sum of the two dice on the first roll. It is easy to show that the pmf for <span class="math inline">\(Y\)</span> is</li>
</ul></li>
</ul>
<center>
<img src="fig2/fig2_7.jpg" />
</center>
<p>      Alternatively,</p>
<p><span class="math display">\[ f_Y(y) = \frac{6 - |x - 7|}{36}I_{\{2, 3, \dots, 12\}}(x). \]</span></p>
<ul>
<li>Let <span class="math inline">\(X(\omega)\)</span> be the sum of the dice on the last roll of the game.
<ul>
<li>Then the joint support for <span class="math inline">\((Y, X)\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
S_{Y, X} = &amp;{ \{(2, 2),(3, 3),(4, 4),(4, 7),(5, 5),(5, 7),(6, 6),(6, 7),} \\
&amp;{(7, 7),(8, 8),(8, 7),(9, 9),(9, 7),(10, 10),(10, 7),(11, 11),(12, 12)\}}.
\end{align}
\]</span></p>
<ul>
<li>The winning <span class="math inline">\((Y, X)\)</span> values are</li>
</ul>
<p><span class="math display">\[ (4, 4),(5, 5),(6, 6),(7, 7),(8, 8),(9, 9),(10, 10), \text{ and } (11, 11). \]</span></p>
<ul>
<li>The losing (Y, X) values are</li>
</ul>
<p><span class="math display">\[ (2, 2),(3, 3),(4, 7),(5, 7),(6, 7),(8, 7),(9, 7),(10, 7), \text{ and } (12, 12). \]</span></p>
<ul>
<li>Suppose that the first roll yields a 4.
<ul>
<li>Then the game continues until a 7 or another 4 is rolled.</li>
<li>Denote a non-4, non-7 by <span class="math inline">\(N\)</span>.</li>
<li>Then the game is won if a sequence such as <span class="math inline">\(\{4, 4\}, \{4, N, 4\}, \{4, N, N, 4\}, \{4, N, N, N, 4\}\)</span> etc is observed.</li>
<li>Note that <span class="math inline">\(P(N) = 1 − P(4) − P(7) = 27/36\)</span>.</li>
<li>In any case, the first roll is a 4 and the last role is a 4.</li>
<li>That is, <span class="math inline">\(Y = 4\)</span> and <span class="math inline">\(X = 4\)</span>.</li>
<li>The probability that <span class="math inline">\(Y = 4\)</span> and <span class="math inline">\(X = 4\)</span> can be computed as follows:</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
f_{Y, X}(4, 4) &amp;= P\{4, 4\} + P\{4, N, 4\} + P\{4, N, N, 4\} + P\{4, N, N, N, 4\} + \cdots \\
&amp;= \bigg(\frac{3}{36} \bigg)^2 + \bigg(\frac{3}{36} \bigg)^2\bigg(\frac{27}{36} \bigg) + \bigg(\frac{3}{36} \bigg)^2 \bigg(\frac{27}{36} \bigg)^2 + \bigg(\frac{3}{36} \bigg)^2 \bigg(\frac{27}{36} \bigg)^3 \cdots \\
&amp;= \bigg(\frac{3}{36} \bigg)^2 \displaystyle\sum_{i=0}^{\infty} \bigg(\frac{27}{36} \bigg)^i \\
&amp;= \bigg(\frac{3}{36} \bigg)^2 \frac{1}{1-\Big(\frac{27}{36} \Big)} \ \text{ by the geometric series result } \\
&amp;= \frac{1}{36}.
\end{align}
\]</span></p>
<p>      Furthermore, the conditional probability of winning, given that the point is 4 is</p>
<p><span class="math display">\[ f_{X|Y}(4|4) = \frac{f_{Y, X}(4, 4)}{f_Y(4)} = \frac{1/36}{3/36} = \frac{1}{3}. \]</span></p>
<p>      Accordingly,</p>
<p><span class="math display">\[ f_{X|Y}(x|4) = \begin{cases}
\frac{1}{3} &amp; \text{if } x = 4 \\
\frac{2}{3} &amp; \text{if } x = 7 \\
0 &amp; \text{otherwise.}
\end{cases} .\]</span></p>
<ul>
<li>The probabilities <span class="math inline">\(f_{Y, X}(y, y)\)</span> are summarized in the following table.
<ul>
<li>The probabilities that correspond to a win are summed. The result is</li>
</ul></li>
</ul>
<p><span class="math display">\[ P(\text{Win}) = \frac{244}{495} = 0.5 − \frac{7}{990} ≈ 0.4929. \]</span></p>
<center>
<img src="fig2/fig2_8.jpg" />
</center>
<!-------------------------------------->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Mathematical Statistics.pdf", "Mathematical Statistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
