<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Expectations of Discrete Random Variables | Mathematical Statistics</title>
  <meta name="description" content="This is a Mathematical Statistics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Expectations of Discrete Random Variables | Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Mathematical Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Expectations of Discrete Random Variables | Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is a Mathematical Statistics" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2024-08-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter2.html"/>
<link rel="next" href="chapter4.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#algebra-of-events"><i class="fa fa-check"></i><b>1.2</b> Algebra of Events</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#experiments-with-symmetries"><i class="fa fa-check"></i><b>1.3</b> Experiments with Symmetries</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#composition-of-experiments-counting-rules"><i class="fa fa-check"></i><b>1.4</b> Composition of Experiments: Counting Rules</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#sampling-at-random"><i class="fa fa-check"></i><b>1.5</b> Sampling at Random</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#binomial-multinomial-coefficients"><i class="fa fa-check"></i><b>1.6</b> Binomial &amp; Multinomial Coefficients</a></li>
<li class="chapter" data-level="1.7" data-path="chapter1.html"><a href="chapter1.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="1.8" data-path="chapter1.html"><a href="chapter1.html#subjective-probability"><i class="fa fa-check"></i><b>1.8</b> Subjective Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#probability-functions"><i class="fa fa-check"></i><b>2.1</b> Probability Functions</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Joint Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#conditional-probability"><i class="fa fa-check"></i><b>2.3</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#bayes-theorem-law-of-inverse-probability"><i class="fa fa-check"></i><b>2.4</b> Bayes Theorem (Law of Inverse Probability)</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#statistical-independence-of-random-variables"><i class="fa fa-check"></i><b>2.5</b> Statistical Independence of Random Variables</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#exchangeability"><i class="fa fa-check"></i><b>2.6</b> Exchangeability</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#application-probability-of-winning-in-craps"><i class="fa fa-check"></i><b>2.7</b> Application: Probability of Winning in Craps</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Expectations of Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#the-mean"><i class="fa fa-check"></i><b>3.1</b> The Mean</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#expectation-of-a-function"><i class="fa fa-check"></i><b>3.2</b> Expectation of a Function</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#variability"><i class="fa fa-check"></i><b>3.3</b> Variability</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#sums-of-random-variables"><i class="fa fa-check"></i><b>3.5</b> Sums of Random Variables</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#probability-generating-functions"><i class="fa fa-check"></i><b>3.6</b> Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Bernoulli and Related Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#sampling-bernoulli-populations"><i class="fa fa-check"></i><b>4.1</b> Sampling Bernoulli Populations</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#binomial-distribution"><i class="fa fa-check"></i><b>4.2</b> Binomial Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>4.3</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#negative-hypergeometric-distribution"><i class="fa fa-check"></i><b>4.6</b> Negative Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#approximating-binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Approximating Binomial Probabilities</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="chapter4.html"><a href="chapter4.html#normal-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.1</b> Normal approximation to the Binomial</a></li>
<li class="chapter" data-level="4.7.2" data-path="chapter4.html"><a href="chapter4.html#poisson-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.2</b> Poisson Approximation to the Binomial</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#poisson-distribution"><i class="fa fa-check"></i><b>4.8</b> Poisson Distribution</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#law-of-large-numbers"><i class="fa fa-check"></i><b>4.9</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="4.10" data-path="chapter4.html"><a href="chapter4.html#multinomial-distributions"><i class="fa fa-check"></i><b>4.10</b> Multinomial Distributions</a></li>
<li class="chapter" data-level="4.11" data-path="chapter4.html"><a href="chapter4.html#using-probability-generating-functions"><i class="fa fa-check"></i><b>4.11</b> Using Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.1</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#density-and-the-probability-element"><i class="fa fa-check"></i><b>5.2</b> Density and the Probability Element</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#the-median-and-other-percentiles"><i class="fa fa-check"></i><b>5.3</b> The Median and Other Percentiles</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#expected-value"><i class="fa fa-check"></i><b>5.4</b> Expected Value</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#expected-value-of-a-function"><i class="fa fa-check"></i><b>5.5</b> Expected Value of a Function</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#average-deviations"><i class="fa fa-check"></i><b>5.6</b> Average Deviations</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#bivariate-distributions"><i class="fa fa-check"></i><b>5.7</b> Bivariate Distributions</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#several-variables"><i class="fa fa-check"></i><b>5.8</b> Several Variables</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#covariance-and-correlation-1"><i class="fa fa-check"></i><b>5.9</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#independence"><i class="fa fa-check"></i><b>5.10</b> Independence</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#conditional-distributions"><i class="fa fa-check"></i><b>5.11</b> Conditional Distributions</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#moment-generating-functions"><i class="fa fa-check"></i><b>5.12</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Families of Continuous Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#normal-distributions"><i class="fa fa-check"></i><b>6.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#exponential-distributions"><i class="fa fa-check"></i><b>6.2</b> Exponential Distributions</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#gamma-distributions"><i class="fa fa-check"></i><b>6.3</b> Gamma Distributions</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#chi-squared-distributions"><i class="fa fa-check"></i><b>6.4</b> Chi Squared Distributions</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#distributions-for-reliability"><i class="fa fa-check"></i><b>6.5</b> Distributions for Reliability</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#t-f-and-beta-distributions"><i class="fa fa-check"></i><b>6.6</b> <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and Beta Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Organizing &amp; Describing Data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#frequency-distributions"><i class="fa fa-check"></i><b>7.1</b> Frequency Distributions</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#data-on-continuous-variables"><i class="fa fa-check"></i><b>7.2</b> Data on Continuous Variables</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#order-statistics"><i class="fa fa-check"></i><b>7.3</b> Order Statistics</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#data-analysis"><i class="fa fa-check"></i><b>7.4</b> Data Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#the-sample-mean"><i class="fa fa-check"></i><b>7.5</b> The Sample Mean</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#measures-of-dispersion"><i class="fa fa-check"></i><b>7.6</b> Measures of Dispersion</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#correlation"><i class="fa fa-check"></i><b>7.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Samples, Statistics, &amp; Sampling Distributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#random-sampling"><i class="fa fa-check"></i><b>8.1</b> Random Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#likelihood"><i class="fa fa-check"></i><b>8.2</b> Likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#sufficient-statistics"><i class="fa fa-check"></i><b>8.3</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#sampling-distributions"><i class="fa fa-check"></i><b>8.4</b> Sampling Distributions</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>8.5</b> Simulating Sampling Distributions</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#order-statistics-1"><i class="fa fa-check"></i><b>8.6</b> Order Statistics</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#moments-of-sample-means-and-proportionssp"><i class="fa fa-check"></i><b>8.7</b> Moments of Sample Means and Proportionssp</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>8.8</b> The Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#using-the-moment-generating-function"><i class="fa fa-check"></i><b>8.9</b> Using the Moment Generating Function</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#normal-populations"><i class="fa fa-check"></i><b>8.10</b> Normal Populations</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#updating-prior-probabilities-via-likelihood"><i class="fa fa-check"></i><b>8.11</b> Updating Prior Probabilities Via Likelihood</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#some-conjudate-families"><i class="fa fa-check"></i><b>8.12</b> Some conjudate Families</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#predictive-distributions"><i class="fa fa-check"></i><b>8.13</b> Predictive Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Estimation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#point-estimation"><i class="fa fa-check"></i><b>9.1</b> Point Estimation</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#errors-in-estimation"><i class="fa fa-check"></i><b>9.2</b> Errors in Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#consistency"><i class="fa fa-check"></i><b>9.3</b> Consistency</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#large-sample-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Large Sample Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#determining-sample-size"><i class="fa fa-check"></i><b>9.5</b> Determining Sample Size</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#small-sample-confidence-intervals-for-mu_x"><i class="fa fa-check"></i><b>9.6</b> Small Sample Confidence Intervals for <span class="math inline">\(\mu_X\)</span></a></li>
<li class="chapter" data-level="9.7" data-path="chapter9.html"><a href="chapter9.html#the-distribution-of-t"><i class="fa fa-check"></i><b>9.7</b> The Distribution of <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="9.8" data-path="chapter9.html"><a href="chapter9.html#pivotal-quantities"><i class="fa fa-check"></i><b>9.8</b> Pivotal Quantities</a></li>
<li class="chapter" data-level="9.9" data-path="chapter9.html"><a href="chapter9.html#estimating-a-mean-difference"><i class="fa fa-check"></i><b>9.9</b> Estimating a Mean Difference</a></li>
<li class="chapter" data-level="9.10" data-path="chapter9.html"><a href="chapter9.html#umvue"><i class="fa fa-check"></i><b>9.10</b> UMVUE</a></li>
<li class="chapter" data-level="9.11" data-path="chapter9.html"><a href="chapter9.html#bayes-estimators"><i class="fa fa-check"></i><b>9.11</b> Bayes Estimators</a></li>
<li class="chapter" data-level="9.12" data-path="chapter9.html"><a href="chapter9.html#efficiency"><i class="fa fa-check"></i><b>9.12</b> Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Significance Testing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chapter10.html"><a href="chapter10.html#hypotheses"><i class="fa fa-check"></i><b>10.1</b> Hypotheses</a></li>
<li class="chapter" data-level="10.2" data-path="chapter10.html"><a href="chapter10.html#assessing-the-evidence"><i class="fa fa-check"></i><b>10.2</b> Assessing the Evidence</a></li>
<li class="chapter" data-level="10.3" data-path="chapter10.html"><a href="chapter10.html#one-sample-z-tests"><i class="fa fa-check"></i><b>10.3</b> One Sample <span class="math inline">\(Z\)</span> Tests</a></li>
<li class="chapter" data-level="10.4" data-path="chapter10.html"><a href="chapter10.html#one-sample-t-tests"><i class="fa fa-check"></i><b>10.4</b> One Sample <span class="math inline">\(t\)</span> Tests</a></li>
<li class="chapter" data-level="10.5" data-path="chapter10.html"><a href="chapter10.html#some-nonparametric-tests"><i class="fa fa-check"></i><b>10.5</b> Some Nonparametric Tests</a></li>
<li class="chapter" data-level="10.6" data-path="chapter10.html"><a href="chapter10.html#probability-of-the-null-hypothesis"><i class="fa fa-check"></i><b>10.6</b> Probability of the Null Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Tests as Decision Rules</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#rejection-regions-and-errors"><i class="fa fa-check"></i><b>11.1</b> Rejection Regions and Errors</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#the-power-function"><i class="fa fa-check"></i><b>11.2</b> The Power function</a></li>
<li class="chapter" data-level="11.3" data-path="chapter11.html"><a href="chapter11.html#choosing-a-sample-size"><i class="fa fa-check"></i><b>11.3</b> Choosing a Sample Size</a></li>
<li class="chapter" data-level="11.4" data-path="chapter11.html"><a href="chapter11.html#most-powerful-tests"><i class="fa fa-check"></i><b>11.4</b> Most Powerful Tests</a></li>
<li class="chapter" data-level="11.5" data-path="chapter11.html"><a href="chapter11.html#uniformly-most-powerful-tests"><i class="fa fa-check"></i><b>11.5</b> Uniformly Most Powerful Tests</a></li>
<li class="chapter" data-level="11.6" data-path="chapter11.html"><a href="chapter11.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>11.6</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="11.7" data-path="chapter11.html"><a href="chapter11.html#bayesian-testing"><i class="fa fa-check"></i><b>11.7</b> Bayesian Testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chapter12.html"><a href="chapter12.html"><i class="fa fa-check"></i><b>12</b> Appendix</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chapter12.html"><a href="chapter12.html#greek-alphabet"><i class="fa fa-check"></i><b>12.1</b> Greek Alphabet</a></li>
<li class="chapter" data-level="12.2" data-path="chapter12.html"><a href="chapter12.html#abbreviations"><i class="fa fa-check"></i><b>12.2</b> Abbreviations</a></li>
<li class="chapter" data-level="12.3" data-path="chapter12.html"><a href="chapter12.html#practice-exams"><i class="fa fa-check"></i><b>12.3</b> PRACTICE EXAMS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter3" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Expectations of Discrete Random Variables<a href="chapter3.html#chapter3" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="the-mean" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> The Mean<a href="chapter3.html#the-mean" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Definition: The expected value of <span class="math inline">\(X\)</span> is defined as <span class="math inline">\(E(X) \overset{def}= \displaystyle\sum_{x \in S} xf_X(x)\)</span> if the expectation exists.</p></li>
<li><p>Alternative definition: <span class="math inline">\(E(X) \overset{def}= \displaystyle\sum_{\omega \in \Omega} X(\omega)P(\omega)\)</span> if the expectation exists.</p></li>
<li><p>Properties of expectations: Let <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> be constants. Then</p>
<ul>
<li><span class="math inline">\(E(c) = c\)</span>.<br />
</li>
<li><span class="math inline">\(E(aX + c) = aE(X) + c\)</span> if the expectation exists.<br />
</li>
<li><span class="math inline">\(E(X + Y) = E(X) + E(Y)\)</span> if the expectations exist.<br />
</li>
<li><span class="math inline">\(E(aX + bY + c) = aE(X) + bE(Y) + c\)</span> if the expectations exist.<br />
</li>
<li>Proof: Assume that the expectations exist. Then,</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E(aX + bY + c) &amp;= \displaystyle\sum_{\omega \in \Omega}[aX(\omega) + bY(\omega) + c] P(\omega) \\
&amp;= a \displaystyle\sum_{\omega \in \Omega} X(\omega) P(\omega) + b \displaystyle\sum_{\omega \in \Omega} Y(\omega) P(\omega) + c \displaystyle\sum_{\omega \in \Omega} P(\omega) \\
&amp;= aE(X) + bE(Y) + c.
\end{align}
\]</span></p>
<ul>
<li>Examples
<ul>
<li>If <span class="math inline">\(X_i \text{ for } i = 1, \dots , n\)</span> are random variables, then <span class="math inline">\(E \Bigg( \displaystyle\sum_{i=1}^n X_i \Bigg) = \displaystyle\sum_{i=1}^n E(X_i)\)</span>.<br />
</li>
<li>Example: Play 100 games of craps at <span class="math inline">\(\$1\)</span> per game. Let <span class="math inline">\(X_i\)</span> be the amount won on game <span class="math inline">\(i\)</span>. Find the total expected return.</li>
</ul></li>
</ul>
<p><span class="math display">\[ X_i = \begin{cases}
1 &amp; \text{ if game } i \text{ is won, and } \\
-1 &amp; \text{ if game } i \text{ is lost. }
\end{cases} \]</span></p>
<ul>
<li>More Examples: <span class="math inline">\(f_{X,Y} (i, j) = 2(i + 2j)/[3n(n + 1)^2] \text{ for } i = 0, \dots , n \text{ and } j = 0, \dots , n\)</span>.
<ul>
<li>It can be shown that <span class="math inline">\(f_X (i) = 2(i + n)/[3n(n + 1)] \text{ for } i = 0, \dots , n \text{ and that } f_Y (j) = (n + 4j)/[3n(n + 1)] \text{ for } j = 0, \dots , n\)</span>.</li>
<li>Accordingly,</li>
</ul></li>
</ul>
<p><span class="math display">\[ E(X) = \displaystyle\sum_{i = 0}^n \frac{2(i^2 + ni)}{3n(n + 1)} = \frac{5n + 1}{9} \text{ and } E(Y) = \displaystyle\sum_{i = 0}^n \frac{nj + 4j^2}{3n(n + 1)} = \frac{11n + 4}{18}. \]</span></p>
<ul>
<li><p>To verify the above results, use <span class="math inline">\(\displaystyle\sum_{i = 0}^n i^2= n(n + 1)(2n + 1)/6\)</span>.</p></li>
<li><p>Center of gravity interpretation:</p>
<ul>
<li>Denote <span class="math inline">\(E(X)\)</span> by <span class="math inline">\(\mu\)</span>.</li>
<li>Then, <span class="math inline">\(E(X − \mu) = 0\)</span>.</li>
<li>The sum of the positive deviations and the sum of the negative deviations are equal in absolute value.</li>
<li>To balance the distribution, place the fulcrum at <span class="math inline">\(\mu\)</span>.</li>
</ul></li>
<li><p>Symmetric distributions.</p>
<ul>
<li>The distribution of <span class="math inline">\(X\)</span> is said to be symmetric around a if <span class="math inline">\(f_X (a − y) = f_X (a + y) \text{ for all } y\)</span>.<br />
</li>
<li>Suppose that <span class="math inline">\(f_X (a − y) = f_X (a + y) \text{ for all } y\)</span>. Then, <span class="math inline">\(E(X) = a\)</span>.</li>
<li>Proof: First, note that symmetry implies that</li>
</ul></li>
</ul>
<p><span class="math display">\[ a − y \in S_X \Leftrightarrow a + y \in S_X. \]</span></p>
<ul>
<li>The expected value of <span class="math inline">\(X\)</span> is</li>
</ul>
<p><span class="math display">\[
E(X) = \sum_{x \in S_X} xf_X(x) = \sum_{x \in S_X, x ＜ a} xf_X(x) + af_X(a) +  \sum_{x \in S_X, x ＞ a} xf_X(x).
\]</span></p>
<ul>
<li>Write <span class="math inline">\(x\)</span> as <span class="math inline">\(x = (a − y)\)</span> in the first sum.
<ul>
<li>Then <span class="math inline">\(a − y &lt; a \Leftrightarrow y &gt; 0\)</span>.</li>
<li>Write <span class="math inline">\(x\)</span> as <span class="math inline">\(a + y\)</span> in the second sum. Then, <span class="math inline">\(a + y &gt; a \Leftrightarrow y &gt; 0\)</span>.</li>
<li>Accordingly, the expectation is</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E(X) &amp;= \displaystyle\sum_{a - y \in S_X,  y ＞ 0} (a - y)f_X(a - y) + af_X(a) + \displaystyle\sum_{a + y \in S_X, y ＞ 0} (a + y)f_X(a + y) \\
&amp;= \displaystyle\sum_{a - y \in S_X, y ＞ 0} (a - y + a + y)f_X(a - y) + af_X(a) \\
\text{ because } a - y \in S_X &amp;\Leftrightarrow a + y \in S_X \text{ and } f_X(a - y) = f_X(a + Y) \\
&amp;= 2a \displaystyle\sum_{a - y \in S_X,  y ＞ 0} f_X(a - y) + af_X(a) \\
&amp;= a \displaystyle\sum_{a - y \in S_X, y ＞ 0} f_X(a - y) + af_X(a) + a \displaystyle\sum_{a + y \in S_X , y ＞ 0} f_X(a + y) \\
\text{ because } a - y \in S_X &amp;\Leftrightarrow a + y \in S_X \text{ and } f_X(a - y) = f_X(a + Y) \\
&amp;= a \displaystyle\sum_{x \in S_X , x ＜ a} f_X(x) + af_X(a) + a \displaystyle\sum_{x \in S_X , x ＞ a} f_X(x) \\
&amp;= a \Bigg[ \displaystyle\sum_{x \in S_X , x ＜ a} f_X(x) + f_X(a) + \displaystyle\sum_{x \in S_X , x ＞ a} f_X(x) \Bigg] \\
&amp;= a \displaystyle\sum_{x \in S_X} f_X(x) = a × 1 = a.
\end{align}
\]</span></p>
<ul>
<li><span class="math inline">\(E(X)\)</span> need not exist.
<ul>
<li>Example: double or nothing gamble.</li>
<li>Play a game in which the probability of winning is <span class="math inline">\(\theta\)</span>.</li>
<li>Bet <span class="math inline">\(\$1\)</span> on the game.</li>
<li>If you win, then collect <span class="math inline">\(\$2\)</span>.</li>
<li>If you lose, double the bet and play again. Continue to play the game until you win.</li>
<li>Let <span class="math inline">\(X\)</span> = total amount bet before you finally win. Find <span class="math inline">\(E(X)\)</span>.<br />
</li>
<li>Solution: The probability of winning on the <span class="math inline">\(i^{th}\)</span> game is the probability of losing on each of the first <span class="math inline">\(i − 1\)</span> games and winning on the <span class="math inline">\(i^{th}\)</span> game.</li>
<li>The games are independent, so this probability is <span class="math inline">\((1 − \theta)^{i−1}\theta\)</span>.</li>
<li>The support set for <span class="math inline">\(X\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[ S_X = \{1, 3, 7, 15, 31, \dots\} = \{2^1 − 1, 2^2 − 1, 2^3 − 1, 2^4 − 1, \dots\}. \]</span></p>
<ul>
<li>For example, if you win on game 3, then you bet <span class="math inline">\(\$1\)</span> on game 1, <span class="math inline">\(\$2\)</span> on game 2, and <span class="math inline">\(\$4\)</span> on game 3.
<ul>
<li>The total amount bet is <span class="math inline">\(1 + 2 + 4 = \$7\)</span>.</li>
<li>The table below summarizes the pmf of <span class="math inline">\(X\)</span>.</li>
</ul></li>
</ul>
<center>
<img src="fig3/fig3_1.jpg" />
</center>
<ul>
<li>The expected value of <span class="math inline">\(X\)</span> is</li>
</ul>
<p><span class="math display">\[ \begin{align}
E(X) &amp;= \displaystyle\sum_{i=1}^{\infty}x_if_X(x_i) = \displaystyle\sum_{i=1}^{\infty}(2^i-1)(1-\theta)^{i-1}\theta \\
&amp;=\theta \displaystyle\sum_{i=1}^{\infty}2^i(1-\theta)^{i-1} - \theta \displaystyle\sum_{i=1}^{\infty}(1-\theta)^{i-1} \\
&amp;= 2\theta \displaystyle\sum_{i=1}^{\infty}2^i(1-\theta)^{i-1} - \theta \displaystyle\sum_{i=1}^{\infty}(1-\theta)^i \\
&amp;= \begin{cases}
2\theta \Big( \frac{1}{1-2(1-\theta)} \Big) &amp; \text{ if } 2(1-\theta) ＜ 1, \\
\infty - 1 &amp; \text{ if } 2(1-\theta) ≥ 1
\end{cases} \\
&amp;= \begin{cases}
\frac{1}{2\theta - 1} &amp; \text{ if } 2(1-\theta) ＜ 1, \\
\infty &amp; \text{ if } 2(1-\theta) ≥ 1.
\end{cases}
\end{align}
\]</span></p>
<ul>
<li>Conditional Expectation: <span class="math inline">\(E(X|Y = y) = \displaystyle\sum_{x \in S_X} xf_{X|Y}(x|y).\)</span> Note that <span class="math inline">\(E(X|Y = y)\)</span> is a function of <span class="math inline">\(y\)</span>.</li>
</ul>
<p><br></p>
</div>
<div id="expectation-of-a-function" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Expectation of a Function<a href="chapter3.html#expectation-of-a-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>If <span class="math inline">\(X\)</span> is a discrete random variable, then <span class="math inline">\(g(X)\)</span> is just another discrete random variable. That is <span class="math inline">\(g(X) = g[X(\omega)] = Y(\omega)\)</span> for some function <span class="math inline">\(Y\)</span>.<br />
</li>
<li><span class="math inline">\(E[g(X)] = E(Y) = \displaystyle\sum_{\omega}Y(\omega)P(\omega) = \displaystyle\sum_{y \in S_Y} y \displaystyle\sum_{\omega, Y = y} P(\omega) = \displaystyle\sum_{y \in S_Y} yf_Y(y)\)</span><br />
</li>
<li>Result:</li>
</ul>
<p><span class="math display">\[ E[g(X)] = \displaystyle\sum_{x \in S_X} g(x)f_X(x). \]</span></p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[ \begin{align}
E[g(X)] &amp;= \displaystyle\sum_{\omega \in \Omega} g[X(\omega)]P(\omega) \text{ by definition } \\
&amp;= \displaystyle\sum_{x \in S_X} g(x) \displaystyle\sum_{\omega, X(\omega) = x} P(\omega) \text{ by reordering the terms in the summation } \\
&amp;= \displaystyle\sum_{x \in S_X} g(x)f_X(x).
\end{align}
\]</span></p>
<ul>
<li>Caution: In general <span class="math inline">\(E[g(X)] ≠ g[E(X)]\)</span>. For example, if <span class="math inline">\(g(X) = 1/X\)</span>, then</li>
</ul>
<p><span class="math display">\[ E[g(X)] = \displaystyle\sum_{x \in S_X}\frac{1}{x}f_X(x) ≠ g[E(X)] = \frac{1}{E(X)} = \frac{1}{\displaystyle\sum_{x \in S_X}xf_X(x)}. \]</span></p>
<ul>
<li>If <span class="math inline">\(g(X)\)</span> is a linear function, however, then <span class="math inline">\(E[g(X)] = g[E(X)]\)</span>. That is, if <span class="math inline">\(g(X) = a + bX\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, then</li>
</ul>
<p><span class="math display">\[ E[g(X)] = E(a + bX) = a + bE(X) = g[E(X)]. \]</span></p>
<ul>
<li>Utilities: The subjective value of the random variable <span class="math inline">\(X\)</span> is called the utility of <span class="math inline">\(X\)</span> and is denoted by <span class="math inline">\(u(X)\)</span>.
<ul>
<li>The expected utility is</li>
</ul></li>
</ul>
<p><span class="math display">\[
E(U) = \displaystyle\sum_{x \in S_X} u(x)f_X(x).
\]</span></p>
<ul>
<li>Expectations of Conditional pmf: <span class="math inline">\(E[f_{X|Y} (x|Y)] = f_X(x)\)</span>.
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E[f_{X|Y} (x|Y)] &amp;= \displaystyle\sum_{y \in S_Y} f_{X|Y} (x|y) f_Y(y) \\
&amp;= \displaystyle\sum_{y \in S_Y} f_{X,Y}(x,y) \text{ by the definition of a conditional pmf } \\
&amp;= f_X(x).
\end{align}
\]</span></p>
<ul>
<li><p>This expectation is sometimes written as <span class="math inline">\(E_Y [f_{X|Y} (x|Y)] = f_X(x)\)</span> to remind us that the expectation is taken with respect to the distribution of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Iterated Expectation: <span class="math inline">\(E_Y[E(X|Y)] = E(X)\)</span>.</p>
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E_Y[E(X|Y)] &amp;= \displaystyle\sum_{y \in S_Y} [E(X|y)] f_Y(y) \text{ by results on expectation of a function } \\
&amp;= \displaystyle\sum_{y \in S_Y} \Bigg[ \displaystyle\sum_{x \in S_X} xf_{X|Y}(x|y) \Bigg] f_Y(y) \text{ by definition of conditional expectation } \\
&amp;= \displaystyle\sum_{y \in S_Y} \displaystyle\sum_{x \in S_X} xf_{X|Y}(x|y)f_Y(y) \\
&amp;= \displaystyle\sum_{y \in S_Y} \displaystyle\sum_{x \in S_X} xf_{X,Y}(x,y) \text{ by definition of conditional pmf } \\
&amp;= \displaystyle\sum_{x \in S_X} x \displaystyle\sum_{y \in S_Y} f_{X,Y}(x,y) \\
&amp;= \displaystyle\sum_{x \in S_X} x f_X(x) = E(X).
\end{align}
\]</span></p>
<ul>
<li>Example of iterated expectation.
<ul>
<li>Suppose that a coin has probability <span class="math inline">\(\theta\)</span> of landing heads.</li>
<li>Define the random variable <span class="math inline">\(Y\)</span> to be 1 if a head is tossed and 0 if a tail is tossed.</li>
<li>Note that <span class="math inline">\(E(Y) = θ\)</span>.</li>
<li>Toss the coin and then roll a fair six-sided die <span class="math inline">\(2Y + 1\)</span> times.</li>
<li>Let <span class="math inline">\(X\)</span> be the total number of pips on the <span class="math inline">\(2Y +1\)</span>
rolls.</li>
<li>Find <span class="math inline">\(E(X)\)</span>.<br />
</li>
<li>Solution: The expected number of pips on a single roll is 3.5.
<ul>
<li>Therefore, <span class="math inline">\(E(X|Y) = (2Y + 1)(3.5)\)</span> and</li>
</ul></li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E(X) &amp;= E_Y[E(X|Y)] \\
&amp;= E_Y[(2Y + 1)(3.5)] \\
&amp;= [2E(Y) + 1](3.5) \\
&amp;= (2\theta + 1)(3.5).
\end{align}
\]</span></p>
<ul>
<li><p>If the coin is fair, then <span class="math inline">\(E(Y) = \frac{1}{2} \text{ and } E(X) = 7\)</span>.</p></li>
<li><p>Expected value of a function of several random variables: <span class="math inline">\(E[g(X_1, X_2, \dots , X_k) = \displaystyle\sum_{(x_1, x_2, \dots, x_k) \in S}g(x_1, x_2, \dots, x_k)f_X(x_1, x_2, \dots, x_k)\)</span>.</p>
<ul>
<li>Example: suppose the joint support of <span class="math inline">\((X, Y)\)</span> is <span class="math inline">\(S = \{(0, 0),(0, 1),(1, 0),(1, 1)\}\)</span>.</li>
<li>Find the expectation of <span class="math inline">\(1/f_{X,Y}(X, Y)\)</span>.<br />
</li>
<li>Solution:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
E \Bigg( \frac{1}{f_{X, Y}(X, Y)} \Bigg) &amp;= \displaystyle\sum_{x=0}^1 \displaystyle\sum_{y=0}^1 \Bigg( \frac{1}{f_{X, Y}(X, Y)} \Bigg) f_{X, Y}(X, Y) \\
&amp;= \displaystyle\sum_{x=0}^1 \displaystyle\sum_{y=0}^1 1 = 4.
\end{align}
\]</span></p>
<ul>
<li>Expectation under independence:
<ul>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(E[g(X)h(Y)] = E[g(X)]E[h(Y)]\)</span>, provided that the expectations exist.</li>
<li>Proof: Suppose that <span class="math inline">\(X ㅛ Y\)</span> .</li>
<li>Then <span class="math inline">\(f_{X,Y}(x, y) = f_X(x)f_Y(y)\)</span> and</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E[g(X)h(Y)] &amp;= \displaystyle\sum_{x \in S_X} \displaystyle\sum_{y \in S_Y} g(x)h(y)f_{X,Y}(x,y) \\
&amp;= \displaystyle\sum_{x \in S_X} \displaystyle\sum_{y \in S_Y} g(x)h(y)f_X(x)f_Y(y) \\
&amp;= \displaystyle\sum_{x \in S_X}g(x)f_X(x) \displaystyle\sum_{y \in S_Y}h(y)f_Y(y)\\
&amp;= E[g(X)]E[h(Y)].
\end{align}
\]</span></p>
<ul>
<li>Note: a much stronger result can be established. If <span class="math inline">\(X ㅛ Y\)</span>, then <span class="math inline">\(g(X) ㅛ h(Y)\)</span> for any functions <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span>.</li>
</ul>
<p><br></p>
</div>
<div id="variability" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Variability<a href="chapter3.html#variability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Mean Absolute Deviation: <span class="math inline">\(\text{MAD } \overset{def}= E(|X - \mu_X|) = \displaystyle\sum_{x \in S}|x - \mu_X| f_X(x)\)</span>.</p></li>
<li><p>Variance: The variance of <span class="math inline">\(X\)</span> is defined as</p></li>
</ul>
<p><span class="math display">\[ Var(X) \overset{def}= E(|X - \mu_X|)^2 = \displaystyle\sum_{x \in S}(x - \mu_X)^2 f_X(x). \]</span></p>
<ul>
<li><p>It is conventional to denote the variance of the random variable <span class="math inline">\(X\)</span> by <span class="math inline">\(σ^2_X\)</span>.</p></li>
<li><p>Result: <span class="math inline">\(Var(X) = E(X^2) − [E(X)]^2\)</span>.</p>
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
Var(X) &amp;= E\{[X − E(X)]\}^2 = E\{X^2 − 2XE(X) + [E(X)]^2\} \\
&amp;= E(X^2) − 2E(X)E(X) + [E(X)]^2 = E(X^2) − [E(X)]^2.
\end{align}
\]</span></p>
<ul>
<li><p>Standard Deviation: <span class="math inline">\(\sigma \overset{def}= + \sqrt{\sigma^2}\)</span>.</p></li>
<li><p>Variance of a uniform distribution:</p>
<ul>
<li>Suppose that the support of <span class="math inline">\(X\)</span> is <span class="math inline">\(S = \{1, 2, \dots , N\}\)</span> and that each value in the support set has equal probability.</li>
<li>This situation can be denoted as <span class="math inline">\(X ∼ \text{ Discrete Uniform}(1, 2, \dots , N)\)</span>. - For this distribution,</li>
</ul></li>
</ul>
<p><span class="math display">\[ \mu_X = \frac{N + 1}{2} \text{ and } \sigma^2_X = \frac{(N + 1)(N - 1)}{12}. \]</span></p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[ \begin{align}
E(X) &amp;= \displaystyle\sum_{x \in S} xf_X(x) = \displaystyle\sum_{x = 1}^N x \frac{1}{N} \\
&amp;= \frac{1}{N} \displaystyle\sum_{i = 1}^N i = \bigg( \frac{1}{N} \bigg) \bigg(\frac{N(N + 1)}{2} \bigg) \\
&amp;= \frac{N + 1}{2}.
\end{align}
\]</span></p>
<p>      Also,</p>
<p><span class="math display">\[ \begin{align}
E(X^2) &amp;= \displaystyle\sum_{x \in S} x^2f_X(x) = \displaystyle\sum_{x = 1}^N x^2 \frac{1}{N} \\
&amp;= \frac{1}{N} \displaystyle\sum_{i = 1}^N i^2 = \bigg( \frac{1}{N} \bigg) \bigg(\frac{N(N + 1)(2N + 1)}{6} \bigg) \\
&amp;= \frac{(N + 1)(2N + 1)}{6}.
\end{align}
\]</span></p>
<p>      Accordingly,</p>
<p><span class="math display">\[ Var(X) = \frac{(N + 1)(2N + 1)}{6} - \bigg( \frac{N + 1}{2} \bigg)^2 = \frac{(N + 1)(N - 1)}{12}. \]</span></p>
<ul>
<li>Example: Toss a fair die once. Let <span class="math inline">\(X\)</span> be the number of pips on the top face. Then, <span class="math inline">\(X ∼ \text{ Discrete Uniform}(1, 2, \dots ,)\)</span>. Therefore,</li>
</ul>
<p><span class="math display">\[ E(X) = \frac{7}{2} \text{ and } Var(X) = \frac{(7)(5)}{12} = 2.9167. \]</span></p>
<ul>
<li>Parallel axis theorem: Let <span class="math inline">\(c\)</span> be a constant and let <span class="math inline">\(X\)</span> be a random variable
with mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(σ^2_x &lt; \infty\)</span>.
<ul>
<li>Then, <span class="math inline">\(E(X − c)^2 = \sigma^2_X + (c − \mu)^2\)</span>.</li>
<li>Note that <span class="math inline">\(E(X − c)^2\)</span> is minimized with respect to <span class="math inline">\(c\)</span> when <span class="math inline">\(c = \mu_X\)</span>.<br />
</li>
<li>Proof: Use the add zero trick.</li>
<li>Write <span class="math inline">\(c\)</span> as <span class="math inline">\(c = \mu_X + (c − \mu_X)\)</span>. Therefore,</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E(X − c)^2 &amp;= E [(X − \mu_X) − (c − \mu_X)]^2 \\
&amp;= E[(X − \mu_X)^2 − 2(X − \mu_X)(c − \mu_X) + (c − \mu_X)^2] \\
&amp;= E(X − \mu_X)^2 − 2(c − \mu_X)E(X − \mu_X) + E(c − \mu_X)^2 \\
&amp;= \sigma^2_X + 0 + (c − \mu_X)^2 = \sigma^2_X + (c − \mu_X)^2
\end{align}
\]</span></p>
<ul>
<li>Why is this called the parallel axis theorem? My guess is that the parallel axes refer to two vertical lines drawn on the graph of the pmf of <span class="math inline">\(X\)</span>.
<ul>
<li>One is drawn at <span class="math inline">\(x = c\)</span> and one is drawn at <span class="math inline">\(x = \mu_X\)</span>.</li>
</ul></li>
<li>Alternative proof that <span class="math inline">\(\mu_X\)</span> is the minimizer of <span class="math inline">\(g(c) = E(X − c)^2\)</span>:
<ul>
<li>Take the derivative of <span class="math inline">\(g(c)\)</span> with respect to <span class="math inline">\(c\)</span> and set it to zero to find critical points:</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
\frac{d}{d \ c} g(c) &amp;= \frac{d}{d \ c} E(X^2 - 2cX + c^2) \\
&amp;= \frac{d}{d \ c} [E(X^2) - 2c\mu_X + C^2] = -2\mu_X + 2C, \text{ and } \\
\frac{d \ g(c)}{d \ c} &amp;= 0 \Rightarrow c = \mu_X
\end{align}
\]</span></p>
<ul>
<li>Use the second derivative test to show that a minimizer has been found:</li>
</ul>
<p><span class="math display">\[ \frac{d^2}{(d \ c)^2} g(c) \bigg|_{c = \mu_x} = 2 ＞ 0 \Rightarrow \mu_X \text{ is a minimizer }. \]</span></p>
<p><br></p>
</div>
<div id="covariance-and-correlation" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Covariance and Correlation<a href="chapter3.html#covariance-and-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Covariance:</li>
</ul>
<p><span class="math display">\[ Cov(X, Y) \overset{def}= E[(X - \mu_X)(Y - \mu_Y)]. \]</span></p>
<ul>
<li><p>It is conventional to denote the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> by <span class="math inline">\(σ_{X,Y}\)</span>.</p></li>
<li><p>Result: <span class="math inline">\(E[(X - \mu_X)(Y - \mu_Y)] = E(XY) - E(X)E(Y)\)</span>.</p>
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E[(X − \mu_X)(Y − \mu_Y)] &amp;= E [XY − XE(Y) − E(X)Y + E(X)E(Y)] \\
&amp;= E(XY) − E(X)E(Y) − E(X)E(Y) + E(X)E(Y) \\
&amp;= E(XY) − E(X)E(Y).
\end{align}
\]</span></p>
<ul>
<li><p>Example: Suppose that <span class="math inline">\(f_{X,Y}(i, j) = 2(i + 2j)/[3n(n + 1)^2] \text{ for } i = 0, \dots , n \text{ and } j = 0, \dots , n\)</span>. Then <span class="math inline">\(E(XY) = n(2n + 1)/6 \text{ and } Cov(X, Y) = −(n + 2)2/162\)</span>.</p></li>
<li><p>Result: <span class="math inline">\(Cov(a + bX, c + dY) = bdCov(X, Y)\)</span>.</p></li>
<li><p>Special case of above result: <span class="math inline">\(Var(aX + b) = a^2 Var(X)\)</span>.</p></li>
<li><p>Correlation: <span class="math inline">\(Cor(X, Y) \overset{def}= σ_{X,Y}/(σ_Xσ_Y)\)</span>.</p>
<ul>
<li>It is conventional to denote the correlation between <span class="math inline">\(X \text{ and } Y \text{ by } \rho_{X,Y}\)</span>.</li>
</ul></li>
<li><p>Cauchy-Schwartz Inequality: Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables.</p>
<ul>
<li>Then <span class="math inline">\(E(X^2)E(Y^2) ≥ [E(XY)]^2\)</span>, provided that the expectations exist.</li>
<li>Proof: <span class="math inline">\(E(Y − \alpha X)^2 ≥ 0 \text{ for all } \alpha\)</span>.</li>
<li>Now minimize with respect to <span class="math inline">\(\alpha\)</span>.</li>
</ul></li>
</ul>
<p><span class="math display">\[ \frac{\partial E(Y - \alpha X)^2}{\partial \alpha} = 0 \Rightarrow \alpha = \frac{E(XY)}{E(X)^2} \Rightarrow E \bigg[ Y - \frac{E(XY)}{E(X^2)}X \bigg]^2 ≥ 0. \]</span></p>
<ul>
<li><p>Simplify to obtain the Cauchy-Schwartz inequality.</p></li>
<li><p>Application of Cauchy-Schwartz: The Cauchy-Schwartz inequality says that if <span class="math inline">\(X^∗ \text{ and } Y^∗\)</span> are random variables and the required expectations exist, then <span class="math inline">\([E(X^∗Y^∗)]^2 ≤ E(X^{∗2})E(Y^{∗2})\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(X^∗ = (X − \mu_X)/\sigma_X\)</span> and let <span class="math inline">\(Y^∗ = (Y − \mu_Y )/\sigma_Y\)</span>.Then,</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
\bigg\{E \bigg[\frac{(X-\mu_x)(Y-\mu_Y)}{\sigma_X \sigma_Y} \bigg] \bigg\}^2 &amp;≤ E(X^{*2})E(Y^{*2}) \\
&amp;= E \bigg[\frac{(X-\mu_X)^2}{\sigma^2_X} \bigg] E \bigg[\frac{(Y-\mu_Y)^2}{\sigma^2_Y} \bigg] \\
&amp;\Rightarrow \rho^2_{X,Y} ≤ \bigg(\frac{\sigma^2_X}{\sigma^2_X}\bigg)\bigg(\frac{\sigma^2_Y}{\sigma^2_Y}\bigg) = 1 \\
&amp;\Rightarrow \rho_{X,Y} \in [-1. 1].
\end{align}
\]</span></p>
<ul>
<li>Example: Consider a very small insurance company.
<ul>
<li>Let <span class="math inline">\(X\)</span> be the number of policies sold and let <span class="math inline">\(Y\)</span> be the number of claims made.</li>
<li>Suppose that the joint pmf for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the following:</li>
</ul></li>
</ul>
<center>
<img src="fig3/fig3_2.jpg" />
</center>
<ul>
<li>If the premium on each policy is <span class="math inline">\(\$1,000\)</span> and each claim amount is <span class="math inline">\(\$2,000\)</span>, then the net revenue is <span class="math inline">\(1000X − 2000Y\)</span>.
<ul>
<li>Find the expected net revenue.<br />
</li>
<li>Solution:</li>
</ul></li>
</ul>
<p><span class="math display">\[
E(1000X − 2000Y) = 1000E(X) − 2000E(Y) = 1000(1) − 2000(.5) = 0.
\]</span></p>
<ul>
<li>Find the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
<ul>
<li>Solution: <span class="math inline">\(E(X) = 1, E(Y) = 0.5, E(X^2) = 1.5, E(Y^2) = \frac{5}{8}\)</span>, and <span class="math inline">\(E(XY) = \frac{3}{4}\)</span>. Therefore,</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
Var(X) &amp;= 1.5 - 1^2 = 0.5, \\
Var(Y) &amp;= \frac{5}{8} - 0.5^2 = 0.375, \\
Cov(X, Y) &amp;= 0.75 - (1)(0.5) = 0.25, \text{ and } \\
\rho_{X, Y} &amp;= \frac{0.25}{\sqrt{(0.5)(0.375)}} = \frac{1}{\sqrt{3}} \approx 0.5774. \end{align}
\]</span></p>
<ul>
<li>Result: <span class="math inline">\(Cor(a + bX, c + dY) = \text{sign}(bd)\rho_{X,Y}\)</span>.
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
Cor(a + bX, c + dY) &amp;= \frac{Cov(a + bX, c + dY)}{\sqrt{Var(a + bX)Var(c + dY)}} \\  
&amp;= \frac{bd\sigma_{X, Y}}{\sqrt{b^2 \sigma^2_X d^2 \sigma^2_Y}} \\
&amp;= \bigg(\frac{bd}{|bd|} \bigg) \frac{\sigma_{X, Y}}{\sigma_X \sigma_Y} \\
&amp;= \text{sign}(bd)\rho_{X,Y}.
\end{align}
\]</span></p>
<ul>
<li>Example: Suppose that <span class="math inline">\(f_{X,Y}(i, j) = 2(i + 2j)/[3n(n + 1)^2] \text{ for } i = 0, \dots , n \text{ and } j = 0, \dots , n\)</span>.
<ul>
<li>Then <span class="math inline">\(E(XY) = n(2n + 1)/6, E(X) = (5n + 1)/9\)</span>;
<span class="math inline">\(E(X^2) = n(7n + 5)/18, E(Y) = (11n + 4)/18, \text{ and } E(Y^2) = n(8n + 7)/18\)</span>.</li>
<li>To obtain <span class="math inline">\(E(X^2)\)</span> and <span class="math inline">\(E(Y^2)\)</span>, use <span class="math inline">\(\sum_{i=0}^n i^3 = n^2(n+1)^2/4\)</span>.</li>
<li>It follows that <span class="math inline">\(\sigma^2_X = (n + 2)(13n − 1)/162, \sigma^2_Y = (n + 2)(23n − 8)/324, \sigma_{X,Y} = −(n + 2)^2/162, \text{ and } \rho_{X,Y} = -\sqrt{2}(n+2)/\sqrt{(13n-1)(23n-8)}.\)</span></li>
<li>Note that <span class="math inline">\(\text{lim}_{n \rightarrow \infty}\rho_{X,Y} = -\sqrt{2}/\sqrt{299} = -0.08179\)</span>.</li>
</ul></li>
<li>Result: if <span class="math inline">\(X ㅛ Y\)</span> , then <span class="math inline">\(\rho_{X,Y} = 0\)</span>.
<ul>
<li>Proof: From result 10 on page 42, we know that <span class="math inline">\(X ㅛ Y \Rightarrow E[g(X)h(Y)] = E[g(x)]E[h(Y)]\)</span>.</li>
<li>Accordingly,</li>
</ul></li>
</ul>
<p><span class="math display">\[ Cov(X, Y) = E [(X − \mu_X )(Y − \mu_Y)] = E(X − \mu_X)E(Y − \mu_Y) = 0. \]</span></p>
<ul>
<li>Result: <span class="math inline">\(\rho_{X,Y} = 0 \ne\Rightarrow X ㅛ Y\)</span>.
<ul>
<li>A counter example is sufficient to establish this result.</li>
<li>Consider the joint probability function below:</li>
</ul></li>
</ul>
<center>
<img src="fig3/fig3_3.jpg" />
</center>
<ul>
<li>Computation shows that <span class="math inline">\(E(X) = 2.5, E(Y) = 3, \text{ and } E(XY) = 7.5\)</span>.
<ul>
<li>Therefore, <span class="math inline">\(Cov(X, Y) = 0 \text{ and } \rho_{X,Y} = 0, \text{ but } X \text{ and } Y\)</span> are not independent.</li>
<li>Correlation is a measure of the linear dependence between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li>In this case, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not linearly related, but they are quadratically related.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="sums-of-random-variables" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Sums of Random Variables<a href="chapter3.html#sums-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Suppose that <span class="math inline">\(X_1, X_2, \dots , X_k\)</span> are random variables, where <span class="math inline">\(E(X_i) = \mu_i, Var(X_i) = \sigma^2_i, \text{ and } Cov(X_i, X_j) = \sigma_{ij}\)</span>.
<ul>
<li>Furthermore, suppose that <span class="math inline">\(c_1, c_2, \dots , c_k\)</span> are known constants.</li>
<li>Let <span class="math inline">\(T = \displaystyle\sum_{i=1}^k c_iX_i\)</span>. Then,</li>
</ul></li>
</ul>
<p><span class="math display">\[ E(T) = \displaystyle\sum_{i=1}^{k}c_i\mu_i \text{ and } Var(T) = \displaystyle\sum_{i=1}^k c_i^2 \sigma_i^2 + 2\displaystyle\sum_{i ＜ j}^n c_ic_j\sigma_{ij}. \]</span></p>
<ul>
<li>Proof: The result concerning <span class="math inline">\(E(T)\)</span> is trivial. To prove the variance result, begin with the definition of variance:</li>
</ul>
<p><span class="math display">\[ \begin{align}
Var(T) &amp;= E[T - E(T)]^2 \\
&amp;= E \Bigg[ \displaystyle\sum_{i=1}^{k}c_iX_i - \displaystyle\sum_{i=1}^{k}c_i\mu_i \Bigg]^2 \\
&amp;= E \Bigg[ \displaystyle\sum_{i=1}^{k}c_i(X_i - \mu_i) \Bigg]^2 \\
&amp;= E \Bigg\{ \Bigg[ \displaystyle\sum_{i=1}^{k}c_i(X_i - \mu_i) \Bigg]\Bigg[ \displaystyle\sum_{j=1}^{k}c_j(X_j - \mu_j) \Bigg] \Bigg\} \\
&amp;= E \Bigg\{ \displaystyle\sum_{i=1}^{k}\displaystyle\sum_{j=1}^{k}c_ic_j(X_i - \mu_i)(X_j - \mu_j) \Bigg\} \\
&amp;= \displaystyle\sum_{i=1}^{k}c_i^2E(X_i - \mu_i)^2 + \displaystyle\sum_{i≠j}c_ic_jE[(X_i-\mu_i)(X_j-\mu_j)] \\
&amp;= \displaystyle\sum_{i=1}^{k}c_i^2\sigma_i^2 + \displaystyle\sum_{i≠j}c_ic_j\sigma_{ij} \\
&amp;= \displaystyle\sum_{i=1}^{k}c_i^2\sigma_i^2 + 2\displaystyle\sum_{i＜j}c_ic_j\sigma_{ij} \text{ because } \sigma_{ij} = \sigma_{ji}.
\end{align}
\]</span></p>
<ul>
<li>Special case: <span class="math inline">\(c_i = 1\)</span> for all <span class="math inline">\(i\)</span>. Then,</li>
</ul>
<p><span class="math display">\[ Var \Bigg( \displaystyle\sum_{i=1}^{k} X_i \Bigg) = \displaystyle\sum_{i=1}^{k} \sigma_i^2 + 2\displaystyle\sum_{i＜j} \sigma_{ij}. \]</span></p>
<ul>
<li>Special case: <span class="math inline">\(k = 2, c1 = 1, c2 = 1\)</span>. Then</li>
</ul>
<p><span class="math display">\[ Var(X_1 + X_2) = \sigma_1^2 + \sigma_2^2 + 2\sigma_1\sigma_2. \]</span></p>
<ul>
<li>Special case: <span class="math inline">\(k = 2, c1 = 1, c2 = −1\)</span>. Then</li>
</ul>
<p><span class="math display">\[ Var(X_1 + X_2) = \sigma_1^2 + \sigma_2^2 - 2\sigma_1\sigma_2. \]</span></p>
<ul>
<li>Special case: <span class="math inline">\(c_i = 1\)</span> for all <span class="math inline">\(i\)</span> and variables are pairwise uncorrelated. Then,</li>
</ul>
<p><span class="math display">\[ Var \Bigg( \displaystyle\sum_{i=1}^{k} X_i \Bigg) = \displaystyle\sum_{i=1}^{k} \sigma_i^2. \]</span></p>
<ul>
<li>Application: Take a simple random sample of size n with replacement from a population having mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma_X^2\)</span>.
<ul>
<li>Let <span class="math inline">\(T = \sum_{i=1}^n X_i\)</span> and let <span class="math inline">\(\bar{X} = T/n\)</span>. Then</li>
</ul></li>
</ul>
<p><span class="math display">\[ E(T) = n\mu_X, \ Var(T) = n\sigma_X^2,  \ E(\bar{X}) = \mu_X, \text{ and }  \ Var(\bar{X}) = \sigma_X^2/n.  \]</span></p>
<ul>
<li>Proof: The random variables are <span class="math inline">\(iid\)</span>.
<ul>
<li>Therefore, <span class="math inline">\(E(X_i) = \mu_X\)</span> for all <span class="math inline">\(i\)</span>, <span class="math inline">\(Var(X_i) = \sigma_X^2\)</span> for all <span class="math inline">\(i\)</span>, and <span class="math inline">\(Cov(X_i, X_j) = 0\)</span> for all <span class="math inline">\(i ≠ j\)</span>. Therefore,</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E(T) &amp;= E \Bigg( \displaystyle\sum_{i=1}^{n} X_i \Bigg) = \displaystyle\sum_{i=1}^{n}E(X_i) = n\mu_X \text{ and } \\
Var(T) &amp;= Var \Bigg( \displaystyle\sum_{i=1}^{n} X_i \Bigg) = \displaystyle\sum_{i=1}^{n}\sigma_X^2 = n\sigma_X^2.
\end{align}
\]</span></p>
<ul>
<li>Furthermore, <span class="math inline">\(\bar{X} = \displaystyle\sum_{i=1}^n X_i\)</span>, so. <span class="math inline">\(c_i = 1/n\)</span> for all <span class="math inline">\(i\)</span> in the formula for <span class="math inline">\(\bar{X}\)</span>.
<ul>
<li>Accordingly,</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E(\bar{X}) &amp;= \frac{1}{N}E(T) = \mu_X \text{ and } \\
Var(\bar{X}) &amp;= Var\bigg(\frac{1}{N} T \bigg) =  \frac{1}{n^2} Var(T) =  \frac{\sigma_X^2}{n}.
\end{align}
\]</span></p>
<ul>
<li>Application: Take a simple random sample of size <span class="math inline">\(n\)</span> from the pmf <span class="math inline">\(f_X (x)\)</span> having expectation <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma_X^2\)</span>.
<ul>
<li>Let <span class="math inline">\(T = \sum_{i=1}^n X_i\)</span> and let <span class="math inline">\(\bar{X} = T/n\)</span>. Then</li>
</ul></li>
</ul>
<p><span class="math display">\[ E(T) = n\mu_X, \ Var(T) = n\sigma_X^2, \ E(\bar{X}) = \mu_X, \text{ and } \ Var(\bar{X}) = \sigma_X^2/n. \]</span></p>
<p>            - Proof: This situation is essentially the same as the situation above.
- The random variables are iid because taking an observation from <span class="math inline">\(f_X(x)\)</span> does not change the pmf.</p>
<ul>
<li>Application: Take a simple random sample of size n without replacement from a finite population of size <span class="math inline">\(N\)</span> having mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma_X^2\)</span>.
<ul>
<li>Let <span class="math inline">\(T = \sum_{i=1}^n X_i\)</span> and let <span class="math inline">\(\bar{X} = T/n\)</span>. Then,</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
E(T) &amp;= n\mu_X, \ Var(T) = n\sigma_X^2 \Bigg[ 1 - \frac{(n-1)}{(N-1)} \Bigg], \\
E(\bar{X}) &amp;= \mu_X, \ Var(\bar{X}) = \frac{\sigma_X^2}{n} \Bigg[ 1 - \frac{(n-1)}{(N-1)} \Bigg].
\end{align}
\]</span></p>
<p>            - Proof: Item 5 on page 33 shows that <span class="math inline">\(X_1, \dots , X_n\)</span> are exchangeable random variables, even though they are not independent. Therefore,</p>
<p><span class="math display">\[ \begin{align}
E (X_i) &amp;= E(X_1) = \mu_X \text{ for } i = 1, \dots , n; \\
Var(X_i) &amp;= Var(X_1) = \sigma_X^2 \text{ for } i = 1, \dots , n; \text { and } \\
Cov(X_i, X_j) &amp;= Cov(X_1, X_2) = \sigma_{12} \text{ for all } i ≠ j.
\end{align}
\]</span></p>
<ul>
<li>To find the value for <span class="math inline">\(\sigma_{12}\)</span>, use</li>
</ul>
<p><span class="math display">\[ Var \Bigg( \displaystyle\sum_{i=1}^{n} X_i \Bigg) = Var(N\mu_X) = 0 \]</span></p>
<p>      together with exchangeability. That is,</p>
<p><span class="math display">\[ \begin{align}
Var \Bigg( \displaystyle\sum_{i=1}^{n} X_i \Bigg) &amp;= \displaystyle\sum_{i=1}^{n} Var(X_i) + 2\displaystyle\sum_{i＜j}Cov(X_i, X_j) \\
&amp;= N\sigma_X^2 + N(N - 1)\sigma_{12} = 0 \\
&amp;\Rightarrow \sigma_{12} = -\frac{\sigma_X^2}{N-1}.
\end{align}
\]</span></p>
<p>      Accordingly,</p>
<p><span class="math display">\[ \begin{align}
E(T) &amp;= \displaystyle\sum_{i=1}^{n} E(X_i) = n\mu_X, \\
E(\bar{X}) &amp;= E \bigg(\frac{1}{n}T \bigg) = \frac{1}{n}E(T) = \mu_X, \\
Var(T) &amp;= Var \bigg( \displaystyle\sum_{i=1}^{n} X_i \bigg) = \displaystyle\sum_{i=1}^{n} Var(X_i) + \displaystyle\sum_{i＜j}Cov(X_i, X_j) \\
&amp;= n\sigma_X^2 + n(n-1)\bigg(- \frac{\sigma_X^2}{N-1} \bigg) = n\sigma_X^2 \bigg[ 1 - \frac{(n-1)}{(N-1)} \bigg], \text{ and } \\
Var(\bar{X}) &amp;= Var \bigg( \frac{1}{n}T \bigg) = \bigg( \frac{1}{n} \bigg)^2 Var(T) = \frac{\sigma_X^2}{n} \bigg[ 1 - \frac{(n-1)}{(N-1)} \bigg].
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="probability-generating-functions" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Probability Generating Functions<a href="chapter3.html#probability-generating-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: Suppose that X is a discrete random variable with support a subset of the natural numbers.
<ul>
<li>That is, <span class="math inline">\(S_X \in \{0, 1, 2, \dots , \infty \}\)</span>.</li>
<li>Let <span class="math inline">\(t\)</span> be a real number. Then the probability generating function (pgf) of <span class="math inline">\(X\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[ \eta_X(t) \overset{def}= E(t^X), \]</span></p>
<p>      where <span class="math inline">\(t\)</span> is chosen to be small enough in absolute value so that the expectation exists. If <span class="math inline">\(|t| &lt; 1\)</span>, then the expectation always exists.</p>
<ul>
<li>Result (without proof): Probability Generating Functions are Unique.
<ul>
<li>This result reveals that there is a one-to-one relationship between the pmf and the pgf.</li>
<li>That is, each pmf is associated with exactly one pgf and each pgf is associated with exactly one pmf.</li>
<li>The importance of this result is that we can use the pgf to find the pmf.<br />
</li>
<li>More specifically, the uniqueness results says that if <span class="math inline">\(Y\)</span> is a random variable
with support <span class="math inline">\(S_Y \in \{0, 1, 2, \dots , \infty \}\)</span> and</li>
</ul></li>
</ul>
<p><span class="math display">\[ E(t_Y) = p_0t^0 + p_1t^1 + \dots + p_kt^k + \cdots, \]</span></p>
<p>      then the pmf of <span class="math inline">\(Y\)</span> is <span class="math inline">\(f_Y (i) = p_i\)</span> for <span class="math inline">\(i = 0, 1, \dots , \infty\)</span>. Of course, it also is true that if <span class="math inline">\(Y\)</span> is a random variable with support <span class="math inline">\(S_Y \in \{0, 1, 2, \dots , \infty \}\)</span> and the pmf of <span class="math inline">\(Y\)</span> is <span class="math inline">\(f_Y (i) = p_i\)</span> for <span class="math inline">\(i = 0, 1, \dots , \infty\)</span>; then <span class="math inline">\(E(t_Y) = p_0t^0 + p_1t^1 + \dots + p_kt^k + \cdots.\)</span></p>
<ul>
<li>Example: Find the pmf of <span class="math inline">\(Y\)</span> if <span class="math inline">\(E(t^Y) = .2t^4 + .3t^8 + .5t^19\)</span>.
<ul>
<li>Solution:</li>
</ul></li>
</ul>
<center>
<img src="fig3/fig3_4.jpg" />
</center>
<ul>
<li>Result: Suppose that <span class="math inline">\(X_1, \dots , X_n\)</span> are independent.
<ul>
<li>Denote the pgf of <span class="math inline">\(X_i\)</span> by <span class="math inline">\(\eta X_i(t)\)</span> for <span class="math inline">\(i = 1, \dots , n\)</span>.</li>
<li>If</li>
</ul></li>
</ul>
<p><span class="math display">\[ U = \displaystyle\sum_{i=1}^n X_i, \text{ then } \eta_U(t) = \prod_{i=1}^n \eta_{X_i}(t). \]</span></p>
<ul>
<li>Proof: Use results on expectations of functions of independent random variables. That is,</li>
</ul>
<p><span class="math display">\[ \begin{align}
\eta_U(t) &amp;= E(t^U) = E(t^{\sum_{i=1}^n X_i}) \\
&amp;= E \Bigg(\prod_{i=1}^n t^{X_i} \Bigg) \\
&amp;= \prod_{i=1}^n E(t^{X_i}) = \prod_{i=1}^n \eta X_i(t),
\end{align}
\]</span></p>
<ul>
<li>Result: Suppose that <span class="math inline">\(X_1, \dots , X_n\)</span> are iid.
<ul>
<li>Denote the pgf of <span class="math inline">\(X_i\)</span> by <span class="math inline">\(\eta_X (t)\)</span> for <span class="math inline">\(i = 1, \dots , n\)</span>. If</li>
</ul></li>
</ul>
<p><span class="math display">\[ U = \displaystyle\sum_{i=1}^n X_i, \text{ then } \eta_U(t) = [\eta_X(t)]^n. \]</span></p>
<ul>
<li><p>Proof: This result follows directly from the result above.</p></li>
<li><p>Application 1: Bernoulli → Binomial.</p>
<ul>
<li>Suppose that <span class="math inline">\(X_i\)</span> for <span class="math inline">\(i = 1, \dots , n\)</span> are iid Bernoulli random variables each with probability of success <span class="math inline">\(\theta\)</span>.</li>
<li>Find the distribution of <span class="math inline">\(Y = \sum_{i=1} ^n Xi\)</span>.</li>
<li>A Bernoulli random variable is a random variable that has support <span class="math inline">\(S = \{0, 1\}\)</span>.</li>
</ul></li>
<li><p>The pmf of a Bernoulli random variable, <span class="math inline">\(B\)</span>, is</p></li>
</ul>
<p><span class="math display">\[ f_B(b) = \begin{cases}
\theta &amp; \text{ if  } b = 1, \\
1 - \theta &amp; \text{ if  } b = 0, \\
0 &amp; \text{ otherwise.}
\end{cases}
\]</span></p>
<ul>
<li>The pmf of a Bernoulli random variable also can be written as</li>
</ul>
<p><span class="math display">\[ f_B(b) = \theta^b(1 - \theta)^{(1 - b)}I_{\{0,1\}}(b). \]</span></p>
<ul>
<li>Solution: If <span class="math inline">\(X_i\)</span> are iid Bernoulli random variables each with probability of
success <span class="math inline">\(\theta\)</span>, then</li>
</ul>
<p><span class="math display">\[ \begin{align}
f_{X_i}(x) &amp;= \theta^x(1 - \theta)^{(1 - x)}I_{\{0,1\}}(x) \text{ and } \\
\eta_{X_i}(t) &amp;= (1 - \theta)t^0 + \theta t^1 = (1- \theta) + \theta t \text{ for } i = 1, \dots, n.
\end{align}
\]</span></p>
<ul>
<li>Using the result from above, the pmf of <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is</li>
</ul>
<p><span class="math display">\[ \eta_Y(t) = [\theta t + (1 - \theta)]^n. \]</span></p>
<ul>
<li>Using the binomial theorem, the pmf of <span class="math inline">\(Y\)</span> can be written as</li>
</ul>
<p><span class="math display">\[ \eta_Y(t) = \displaystyle\sum_{i=0}^n \binom{n}{i}(\theta t)^i(1 - \theta)^{(n-i)}. \]</span></p>
<ul>
<li>Accordingly,</li>
</ul>
<p><span class="math display">\[ f_Y(y) = \binom{n}{y} \theta^y (1 - \theta)^{n-y} \text{ for } y = 0, 1, \dots, n. \]</span></p>
<p>This pmf is called the binomial pmf.</p>
<ul>
<li>Result: A Useful Expansion: Suppose that a is a constant whose value is in <span class="math inline">\([−1, 1]\)</span>, <span class="math inline">\(n\)</span> is an integer constant that satisfies <span class="math inline">\(n ≥ 1\)</span>, and <span class="math inline">\(t\)</span> is a variable that satisfies <span class="math inline">\(t \in (−1, 1)\)</span>.
<ul>
<li>Define <span class="math inline">\(h(t)\)</span> as</li>
</ul></li>
</ul>
<p><span class="math display">\[ h(t) \overset{def}= (1 - at)^{-n}. \]</span></p>
<p>      - Then,</p>
<p><span class="math display">\[ h(t) = \displaystyle\sum_{r=0}^{\infty} \binom{n+r-1}{r} a^rt^r. \]</span></p>
<ul>
<li>Proof: Expand <span class="math inline">\(h(t)\)</span> in a Taylor series around <span class="math inline">\(t = 0\)</span>.
<ul>
<li>The first few derivatives of <span class="math inline">\(h(t)\)</span>, evaluated at <span class="math inline">\(t = 0\)</span> are</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
\frac{d}{dt}h(t) &amp;= -n(1 - at)^{-(n+1)}(-1)a, \\
\frac{d}{dt} \bigg|_{t=0} &amp;= na, \\
\frac{d^2}{(dt)^2}h(t) &amp;= -n(n+1)(1-at)^{-(n+1)}(-1)a^2, \\
\frac{d^2}{(dt)^2} \bigg|_{t=0} &amp;= n(n+1)a^2, \\
\frac{d^3}{(dt)^3}h(t) &amp;= -n(n+1)(n+2)(1-at)^{-(n+3)}(-1)a^3, \\
\frac{d^3}{(dt)^3} \bigg|_{t=0} &amp;= n(n+1)(n+2)a^3, \\
\vdots \\
\frac{d^r}{(dt)^r}h(t) &amp;= -n(n+1)(n+2) \dots (n+r-1)(1-at)^{-(n+r)}(-1)a^r, \\
\frac{d^r}{(dt)^r} \bigg|_{t=0} &amp;= n(n+1)(n+2) \dots (n+r-1)a^r.
\end{align}\]</span></p>
<ul>
<li>Accordingly, the Taylor series is</li>
</ul>
<p><span class="math display">\[ \begin{align}
h(t) &amp;= \displaystyle\sum_{r=0}^{\infty} \frac{d^r}{(dt)^r} h(t) \bigg|_{t=0} \frac{1}{r!}(t-0)^r \\
&amp;= \displaystyle\sum_{r=0}^{\infty} \binom{n+r-1}{r}a^rt^r.
\end{align}\]</span></p>
<ul>
<li>The ratio test verifies that the series converges because</li>
</ul>
<p><span class="math display">\[ \lim_{r \rightarrow 0} \frac{\binom{n+r+1-1}{r+1}(at)^{r+1}}{\binom{n+r+-1}{r}(at)^r} = \lim_{r \rightarrow 0} \frac{n+r}{r+1}at = at \text{ and } |at| ＜ 1. \]</span></p>
<ul>
<li>Application 2: Geometric → Negative Binomial.
<ul>
<li>Suppose that <span class="math inline">\(U_1, U_2, \dots\)</span> is a sequence of iid Bernoulli random variables, each with probability of success <span class="math inline">\(\theta \in (0, 1)\)</span>.</li>
<li>Let <span class="math inline">\(X\)</span> be the number of Bernoulli trials to the first success.</li>
<li>For example, if <span class="math inline">\(U_1 = 1\)</span>, then <span class="math inline">\(X = 1\)</span>. If <span class="math inline">\(U_1 = 0, U_2 = 1\)</span>, then <span class="math inline">\(X = 2\)</span>. If <span class="math inline">\(U_1 = 0, U_2 = 0, \dots , U_x−1 = 0, U_x = 1\)</span>, then <span class="math inline">\(X = x\)</span>.</li>
<li>The random variable <span class="math inline">\(X\)</span> is called a geometric random variable and its pmf is</li>
</ul></li>
</ul>
<p><span class="math display">\[ f_X(x) = (1-\theta)^{x-1}\theta I_{\{ 1,2, \dots, \infty \}}(x). \]</span></p>
<ul>
<li>The pmf follows from independence among the Bernoulli random variables.
<ul>
<li>Suppose that <span class="math inline">\(X_1, X_2, \dots , X_n\)</span> are i.i.d geometric random variables, each with parameter <span class="math inline">\(\theta\)</span>. Let <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.</li>
<li>The random variable <span class="math inline">\(Y\)</span> is called a negative binomial random variable with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span>.</li>
<li>The random variable <span class="math inline">\(Y\)</span> is the number of Bernoulli trials to the <span class="math inline">\(n^{\text{th}}\)</span> success.</li>
<li>The pmf of <span class="math inline">\(Y\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[ f_Y(y) = \binom{y-1}{y-n} \theta^n (1-\theta)^{y-n} I_{\{n, n+1, \dots, \infty \}}(y). \]</span></p>
<ul>
<li>Proof: The pgf of <span class="math inline">\(X_i\)</span> for <span class="math inline">\(t \in (−1, 1)\)</span> is</li>
</ul>
<p><span class="math display">\[ \begin{align}
\eta_{X_i}(t) &amp;= E(t^{X_i}) = \displaystyle\sum_{x=1}^{\infty}(1-\theta)^{x-1}\theta t^x \\
&amp;= \theta t \displaystyle\sum_{x=1}^{\infty}[(1-\theta)t]^{x-1} = \theta t \displaystyle\sum_{x=1}^{\infty} [(1-\theta)t]^x \\
&amp;= \frac{\theta t}{1 - (1-\theta)t} \text{ because } |(1-\theta)t| ＜ 1.
\end{align}
\]</span></p>
<ul>
<li>Accordingly, the pgf of <span class="math inline">\(Y\)</span> is</li>
</ul>
<p><span class="math display">\[ \eta_{Y}(t) = \bigg[ \frac{\theta t}{1 - (1-\theta)t} \bigg]^n = (\theta t)^n[1 - (1-\theta)t]^{-n}. \]</span></p>
<ul>
<li>Using the expansion above, where <span class="math inline">\(a = (1 − \theta)\)</span>, the pgf of <span class="math inline">\(Y\)</span></li>
</ul>
<p><span class="math display">\[ \eta_Y(t) = (\theta t)^n \displaystyle\sum_{r=0}^{\infty} \binom{n+r-1}{r} (1-\theta)^r t^r.  \]</span></p>
<ul>
<li>Accordingly,</li>
</ul>
<p><span class="math display">\[ \Pr(Y = n+r) = \binom{n+r-1}{r}\theta^n(1-\theta)^r. \]</span></p>
<ul>
<li>Let <span class="math inline">\(y = n + r\)</span> and, therefore, <span class="math inline">\(r = y − n\)</span> to obtain</li>
</ul>
<p><span class="math display">\[ f_Y(y) = \binom{y-1}{y-n}(1-\theta)^{y-n}\theta^n I_{\{ n, n+1, \dots, \infty \}}(y). \]</span></p>
<ul>
<li>Application 3: Game of Razzle Dazzle.
<ul>
<li>Toss 8 fair six-sided dice. Let <span class="math inline">\(Y\)</span> be the sum of the pips.<br />
</li>
<li>Let <span class="math inline">\(X_i\)</span> be the number of pips shown on die <span class="math inline">\(i\)</span> for <span class="math inline">\(i = 1, \dots , 8\)</span>. The total score is <span class="math inline">\(Y = \sum_{i=1}^8 X_i\)</span>. The goal is to find the pmf of <span class="math inline">\(Y\)</span><br />
</li>
<li>The random variable <span class="math inline">\(X_i\)</span> has a discrete uniform distribution on <span class="math inline">\(S_X = \{1, 2, \dots , 6\}\)</span>.</li>
<li>The pgf of <span class="math inline">\(X_i\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[ \eta_{X_i}(t) = E(t^{X_i}) = \displaystyle\sum_{i=1}^6 t^i \bigg(\frac{1}{6} \bigg) = \bigg(\frac{1}{6} \bigg) \frac{t-t^7}{1-t} = \bigg(\frac{t}{6} \bigg) \frac{1-t^6}{1-t}
\]</span>
      by using the result <span class="math inline">\(\sum_{i=1}^N a^i = \frac{a-a^{N+1}}{1-a}.\)</span></p>
<p>      - The pgf of <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[ \begin{align}
\eta_{Y}(t) &amp;= E(t^{Y}) \\
&amp;= E(t^{X_1 + X_2 + \dots + X_8}) \\
&amp;= E(t^{X_1})E(t^{X_2}) \cdots E(t^{X_8}) \\
&amp;= \prod_{i=1}^{8} E(t^{X_i}) \\
&amp;= \bigg( \frac{t}{6} \bigg)^8 \bigg( \frac{1-t^6}{1-t} \bigg)^8,
\end{align}
\]</span></p>
<p>      using the mutual independence of <span class="math inline">\(X_1, \dots , X_n\)</span>.</p>
<ul>
<li>Use the binomial theorem to write <span class="math inline">\((1 − t^6)^8\)</span> as</li>
</ul>
<p><span class="math display">\[ (1 − t^6)^8 = (-t^6+1)^8 = \sum_{i=0}^8 \binom{8}{i}(-t^6)^i 1^{8-i} = \sum_{i=0}^8 \binom{8}{i}(-t^6)^i. \]</span></p>
<ul>
<li>If <span class="math inline">\(|t| &lt; 1\)</span>, then the Taylor series expansion of <span class="math inline">\((1 − t)^{−8}\)</span> around <span class="math inline">\(t = 0\)</span> is</li>
</ul>
<p><span class="math display">\[ \frac{1}{(1-t)^8} = \sum_{r=0}^{\infty} \binom{7+r}{r}t^r \]</span></p>
<p>      using the result in item 7, where <span class="math inline">\(a = 1\)</span>.</p>
<ul>
<li>Accordingly, the pgf of <span class="math inline">\(Y\)</span> is</li>
</ul>
<p><span class="math display">\[ \eta_Y = \bigg( \frac{1}{6} \bigg)^8 \sum_{i=0}^8 \sum_{j=0}^8 \binom{8}{i} (-1)^i \binom{7+j}{j} t^{8+6i+j}. \]</span></p>
<ul>
<li><p>To find <span class="math inline">\(\Pr(Y = y)\)</span>, one can sum all coefficients in the pgf for which <span class="math inline">\(t\)</span> is raised to the <span class="math inline">\(y^{\text{th}}\)</span> power. That is, sum all coefficients for which <span class="math inline">\(8 + 6i + j = y\)</span>.</p></li>
<li><p>Example: <span class="math inline">\(\Pr(Y = 13)\)</span> is equal to the coefficient that corresponds to <span class="math inline">\((i, j) = (0, 5)\)</span> because <span class="math inline">\(6 × 0 + 5 + 8 = 13\)</span>. That is,</p></li>
</ul>
<p><span class="math display">\[ \Pr(Y = 5) = \frac{1}{6^8} \binom{8}{0}(-1)^0 \binom{7+5}{5} = \frac{22}{6^8} \approx 0.00047154. \]</span></p>
<ul>
<li>Example: <span class="math inline">\(\Pr(Y = 20)\)</span> is found by summing coefficients corresponding to <span class="math inline">\((i, j) = (0, 12), (1, 6)\)</span>, and <span class="math inline">\((2, 0)\)</span>. That is,</li>
</ul>
<p><span class="math display">\[ \begin{align}
\Pr(Y = 20) &amp;= \frac{1}{6^8} \Bigg[ \binom{8}{0}(-1)^0 \binom{7+12}{12} + \binom{8}{1}(-1)^1 \binom{7+6}{6} +\binom{8}{2}(-1)^2 \binom{7+0}{0} \Bigg] \\
&amp;= \frac{1}{6^8} (50388 - 13728 + 28) = \frac{36,688}{6^8} \approx 0.0218431.
\end{align}
\]</span></p>
<!-------------------------------------->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Mathematical Statistics.pdf", "Mathematical Statistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
