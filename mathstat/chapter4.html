<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Bernoulli and Related Random Variables | Mathematical Statistics</title>
  <meta name="description" content="This is a Mathematical Statistics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Bernoulli and Related Random Variables | Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Mathematical Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Bernoulli and Related Random Variables | Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is a Mathematical Statistics" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2024-08-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter3.html"/>
<link rel="next" href="chapter5.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#algebra-of-events"><i class="fa fa-check"></i><b>1.2</b> Algebra of Events</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#experiments-with-symmetries"><i class="fa fa-check"></i><b>1.3</b> Experiments with Symmetries</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#composition-of-experiments-counting-rules"><i class="fa fa-check"></i><b>1.4</b> Composition of Experiments: Counting Rules</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#sampling-at-random"><i class="fa fa-check"></i><b>1.5</b> Sampling at Random</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#binomial-multinomial-coefficients"><i class="fa fa-check"></i><b>1.6</b> Binomial &amp; Multinomial Coefficients</a></li>
<li class="chapter" data-level="1.7" data-path="chapter1.html"><a href="chapter1.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="1.8" data-path="chapter1.html"><a href="chapter1.html#subjective-probability"><i class="fa fa-check"></i><b>1.8</b> Subjective Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#probability-functions"><i class="fa fa-check"></i><b>2.1</b> Probability Functions</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Joint Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#conditional-probability"><i class="fa fa-check"></i><b>2.3</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#bayes-theorem-law-of-inverse-probability"><i class="fa fa-check"></i><b>2.4</b> Bayes Theorem (Law of Inverse Probability)</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#statistical-independence-of-random-variables"><i class="fa fa-check"></i><b>2.5</b> Statistical Independence of Random Variables</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#exchangeability"><i class="fa fa-check"></i><b>2.6</b> Exchangeability</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#application-probability-of-winning-in-craps"><i class="fa fa-check"></i><b>2.7</b> Application: Probability of Winning in Craps</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Expectations of Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#the-mean"><i class="fa fa-check"></i><b>3.1</b> The Mean</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#expectation-of-a-function"><i class="fa fa-check"></i><b>3.2</b> Expectation of a Function</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#variability"><i class="fa fa-check"></i><b>3.3</b> Variability</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#sums-of-random-variables"><i class="fa fa-check"></i><b>3.5</b> Sums of Random Variables</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#probability-generating-functions"><i class="fa fa-check"></i><b>3.6</b> Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Bernoulli and Related Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#sampling-bernoulli-populations"><i class="fa fa-check"></i><b>4.1</b> Sampling Bernoulli Populations</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#binomial-distribution"><i class="fa fa-check"></i><b>4.2</b> Binomial Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>4.3</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#negative-hypergeometric-distribution"><i class="fa fa-check"></i><b>4.6</b> Negative Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#approximating-binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Approximating Binomial Probabilities</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="chapter4.html"><a href="chapter4.html#normal-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.1</b> Normal approximation to the Binomial</a></li>
<li class="chapter" data-level="4.7.2" data-path="chapter4.html"><a href="chapter4.html#poisson-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.2</b> Poisson Approximation to the Binomial</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#poisson-distribution"><i class="fa fa-check"></i><b>4.8</b> Poisson Distribution</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#law-of-large-numbers"><i class="fa fa-check"></i><b>4.9</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="4.10" data-path="chapter4.html"><a href="chapter4.html#multinomial-distributions"><i class="fa fa-check"></i><b>4.10</b> Multinomial Distributions</a></li>
<li class="chapter" data-level="4.11" data-path="chapter4.html"><a href="chapter4.html#using-probability-generating-functions"><i class="fa fa-check"></i><b>4.11</b> Using Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.1</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#density-and-the-probability-element"><i class="fa fa-check"></i><b>5.2</b> Density and the Probability Element</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#the-median-and-other-percentiles"><i class="fa fa-check"></i><b>5.3</b> The Median and Other Percentiles</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#expected-value"><i class="fa fa-check"></i><b>5.4</b> Expected Value</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#expected-value-of-a-function"><i class="fa fa-check"></i><b>5.5</b> Expected Value of a Function</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#average-deviations"><i class="fa fa-check"></i><b>5.6</b> Average Deviations</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#bivariate-distributions"><i class="fa fa-check"></i><b>5.7</b> Bivariate Distributions</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#several-variables"><i class="fa fa-check"></i><b>5.8</b> Several Variables</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#covariance-and-correlation-1"><i class="fa fa-check"></i><b>5.9</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#independence"><i class="fa fa-check"></i><b>5.10</b> Independence</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#conditional-distributions"><i class="fa fa-check"></i><b>5.11</b> Conditional Distributions</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#moment-generating-functions"><i class="fa fa-check"></i><b>5.12</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Families of Continuous Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#normal-distributions"><i class="fa fa-check"></i><b>6.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#exponential-distributions"><i class="fa fa-check"></i><b>6.2</b> Exponential Distributions</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#gamma-distributions"><i class="fa fa-check"></i><b>6.3</b> Gamma Distributions</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#chi-squared-distributions"><i class="fa fa-check"></i><b>6.4</b> Chi Squared Distributions</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#distributions-for-reliability"><i class="fa fa-check"></i><b>6.5</b> Distributions for Reliability</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#t-f-and-beta-distributions"><i class="fa fa-check"></i><b>6.6</b> <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and Beta Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Organizing &amp; Describing Data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#frequency-distributions"><i class="fa fa-check"></i><b>7.1</b> Frequency Distributions</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#data-on-continuous-variables"><i class="fa fa-check"></i><b>7.2</b> Data on Continuous Variables</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#order-statistics"><i class="fa fa-check"></i><b>7.3</b> Order Statistics</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#data-analysis"><i class="fa fa-check"></i><b>7.4</b> Data Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#the-sample-mean"><i class="fa fa-check"></i><b>7.5</b> The Sample Mean</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#measures-of-dispersion"><i class="fa fa-check"></i><b>7.6</b> Measures of Dispersion</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#correlation"><i class="fa fa-check"></i><b>7.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Samples, Statistics, &amp; Sampling Distributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#random-sampling"><i class="fa fa-check"></i><b>8.1</b> Random Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#likelihood"><i class="fa fa-check"></i><b>8.2</b> Likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#sufficient-statistics"><i class="fa fa-check"></i><b>8.3</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#sampling-distributions"><i class="fa fa-check"></i><b>8.4</b> Sampling Distributions</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>8.5</b> Simulating Sampling Distributions</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#order-statistics-1"><i class="fa fa-check"></i><b>8.6</b> Order Statistics</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#moments-of-sample-means-and-proportionssp"><i class="fa fa-check"></i><b>8.7</b> Moments of Sample Means and Proportionssp</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>8.8</b> The Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#using-the-moment-generating-function"><i class="fa fa-check"></i><b>8.9</b> Using the Moment Generating Function</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#normal-populations"><i class="fa fa-check"></i><b>8.10</b> Normal Populations</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#updating-prior-probabilities-via-likelihood"><i class="fa fa-check"></i><b>8.11</b> Updating Prior Probabilities Via Likelihood</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#some-conjudate-families"><i class="fa fa-check"></i><b>8.12</b> Some conjudate Families</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#predictive-distributions"><i class="fa fa-check"></i><b>8.13</b> Predictive Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Estimation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#point-estimation"><i class="fa fa-check"></i><b>9.1</b> Point Estimation</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#errors-in-estimation"><i class="fa fa-check"></i><b>9.2</b> Errors in Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#consistency"><i class="fa fa-check"></i><b>9.3</b> Consistency</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#large-sample-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Large Sample Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#determining-sample-size"><i class="fa fa-check"></i><b>9.5</b> Determining Sample Size</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#small-sample-confidence-intervals-for-mu_x"><i class="fa fa-check"></i><b>9.6</b> Small Sample Confidence Intervals for <span class="math inline">\(\mu_X\)</span></a></li>
<li class="chapter" data-level="9.7" data-path="chapter9.html"><a href="chapter9.html#the-distribution-of-t"><i class="fa fa-check"></i><b>9.7</b> The Distribution of <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="9.8" data-path="chapter9.html"><a href="chapter9.html#pivotal-quantities"><i class="fa fa-check"></i><b>9.8</b> Pivotal Quantities</a></li>
<li class="chapter" data-level="9.9" data-path="chapter9.html"><a href="chapter9.html#estimating-a-mean-difference"><i class="fa fa-check"></i><b>9.9</b> Estimating a Mean Difference</a></li>
<li class="chapter" data-level="9.10" data-path="chapter9.html"><a href="chapter9.html#umvue"><i class="fa fa-check"></i><b>9.10</b> UMVUE</a></li>
<li class="chapter" data-level="9.11" data-path="chapter9.html"><a href="chapter9.html#bayes-estimators"><i class="fa fa-check"></i><b>9.11</b> Bayes Estimators</a></li>
<li class="chapter" data-level="9.12" data-path="chapter9.html"><a href="chapter9.html#efficiency"><i class="fa fa-check"></i><b>9.12</b> Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Significance Testing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chapter10.html"><a href="chapter10.html#hypotheses"><i class="fa fa-check"></i><b>10.1</b> Hypotheses</a></li>
<li class="chapter" data-level="10.2" data-path="chapter10.html"><a href="chapter10.html#assessing-the-evidence"><i class="fa fa-check"></i><b>10.2</b> Assessing the Evidence</a></li>
<li class="chapter" data-level="10.3" data-path="chapter10.html"><a href="chapter10.html#one-sample-z-tests"><i class="fa fa-check"></i><b>10.3</b> One Sample <span class="math inline">\(Z\)</span> Tests</a></li>
<li class="chapter" data-level="10.4" data-path="chapter10.html"><a href="chapter10.html#one-sample-t-tests"><i class="fa fa-check"></i><b>10.4</b> One Sample <span class="math inline">\(t\)</span> Tests</a></li>
<li class="chapter" data-level="10.5" data-path="chapter10.html"><a href="chapter10.html#some-nonparametric-tests"><i class="fa fa-check"></i><b>10.5</b> Some Nonparametric Tests</a></li>
<li class="chapter" data-level="10.6" data-path="chapter10.html"><a href="chapter10.html#probability-of-the-null-hypothesis"><i class="fa fa-check"></i><b>10.6</b> Probability of the Null Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Tests as Decision Rules</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#rejection-regions-and-errors"><i class="fa fa-check"></i><b>11.1</b> Rejection Regions and Errors</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#the-power-function"><i class="fa fa-check"></i><b>11.2</b> The Power function</a></li>
<li class="chapter" data-level="11.3" data-path="chapter11.html"><a href="chapter11.html#choosing-a-sample-size"><i class="fa fa-check"></i><b>11.3</b> Choosing a Sample Size</a></li>
<li class="chapter" data-level="11.4" data-path="chapter11.html"><a href="chapter11.html#most-powerful-tests"><i class="fa fa-check"></i><b>11.4</b> Most Powerful Tests</a></li>
<li class="chapter" data-level="11.5" data-path="chapter11.html"><a href="chapter11.html#uniformly-most-powerful-tests"><i class="fa fa-check"></i><b>11.5</b> Uniformly Most Powerful Tests</a></li>
<li class="chapter" data-level="11.6" data-path="chapter11.html"><a href="chapter11.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>11.6</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="11.7" data-path="chapter11.html"><a href="chapter11.html#bayesian-testing"><i class="fa fa-check"></i><b>11.7</b> Bayesian Testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chapter12.html"><a href="chapter12.html"><i class="fa fa-check"></i><b>12</b> Appendix</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chapter12.html"><a href="chapter12.html#greek-alphabet"><i class="fa fa-check"></i><b>12.1</b> Greek Alphabet</a></li>
<li class="chapter" data-level="12.2" data-path="chapter12.html"><a href="chapter12.html#abbreviations"><i class="fa fa-check"></i><b>12.2</b> Abbreviations</a></li>
<li class="chapter" data-level="12.3" data-path="chapter12.html"><a href="chapter12.html#practice-exams"><i class="fa fa-check"></i><b>12.3</b> PRACTICE EXAMS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter4" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Bernoulli and Related Random Variables<a href="chapter4.html#chapter4" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="sampling-bernoulli-populations" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Sampling Bernoulli Populations<a href="chapter4.html#sampling-bernoulli-populations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>A Bernoulli random variable is a random variable with support <span class="math inline">\(S=\{0, 1\}\)</span>.
<ul>
<li>That is, if <span class="math inline">\(X\)</span> is a Bernoulli rv., then <span class="math inline">\(X=1\)</span> (success) or <span class="math inline">\(X=0\)</span> (failure).</li>
</ul></li>
<li>Probability mass function (pmf): Denote the probability of success by <span class="math inline">\(p\)</span>. Then,</li>
</ul>
<p><span class="math display">\[
\begin{align}
f_X(x)=\begin{cases}
p \,\,\, &amp;\mathrm{if} \,\,\, x=1, \,\,\, p\in[0,1] \\
1-p \,\,\, &amp;\mathrm{if} \,\,\, x=0, \,\,\, p\in[0,1]\\
0 \,\,\, &amp;\mathrm{otherwise}
\end{cases}
\end{align}
\]</span></p>
<ul>
<li><p>This is a family of pmfs indexed by the parameter <span class="math inline">\(p\)</span>.</p></li>
<li><p>Indicator function: An indicator function is a function that has range <span class="math inline">\(\{0, 1\}\)</span>.</p>
<ul>
<li>We will denote indicator functions by the letter <span class="math inline">\(I\)</span>. Specifically, <span class="math inline">\(I_A(a)\)</span> is defined as</li>
</ul></li>
</ul>
<p><span class="math display">\[
I_A(a)=\begin{cases}
1 \,\,\, \mathrm{if} \,\,\, a\in A \\
0 \,\,\, \mathrm{otherwise}
\end{cases}
\]</span></p>
<ul>
<li>Example
<ul>
<li><span class="math inline">\(I_{(0,100)}(-2.6)=0\)</span></li>
<li><span class="math inline">\(I_{(0,100)}(2.6)=1\)</span></li>
<li><span class="math inline">\(I_{\{0,100\}}(2.6)=0\)</span></li>
</ul></li>
<li>Alternative expressions for pmf:</li>
</ul>
<p><span class="math display">\[
f_{X}(x)=p^{x}(1-p)^{1-x},\,\,\, {\mathrm{for}} \,\,\, x=0,1 \,\,\,{\mathrm{and}}\,\,\, p\in[0,1]
\]</span></p>
<p><span class="math display">\[
f_{X}(x)=p^{x}(1-p)^{1-x}I_{\{0,1\}}(x)I_{[0,1]}(p)
\]</span></p>
<ul>
<li><p>Moments:
<span class="math display">\[
\operatorname{E}(X^{k})=p\ \operatorname{for}\ k=1,2,\dots
\]</span></p></li>
<li><p>Result:</p></li>
</ul>
<p><span class="math display">\[
\mu_{X}=p\,\,\, \mathrm{and}\,\,\,\sigma_{X}^{2}=p(1-p)
\]</span></p>
<ul>
<li>Abbreviation: The notation i.i.d means “independently and identically distributed”<br />
</li>
</ul>
<div id="sequences-of-bernoulli-random-variables" class="section level4 unnumbered hasAnchor">
<h4>Sequences of Bernoulli Random Variables<a href="chapter4.html#sequences-of-bernoulli-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Sampling With Replacement: The Bernoulli Process
<ul>
<li>Characteristics (these follow from <span class="math inline">\(X_{i}\stackrel{\mathrm{iid}}{\sim}\mathrm{Bern}(p)\)</span> for <span class="math inline">\(i = 1, 2, \ldots\)</span>)
– Non-overlapping sequences of trials are independent
– The distribution of a set of consecutive trials is identical to the distribution of any other set of trials of the same length (this is called the stationary property)
– The distribution of future trials is independent of the results of past trials.</li>
<li>Joint pmf of <span class="math inline">\(X_{1},\cdot\cdot\cdot,X_{n}\)</span>:</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_{X_{1},X_{2},\dots,X_{n}}(x_{1},x_{2},\dots,x_{n}|p)=p^{y}(1-p)^{n-y}I_{\{0,1,\dots,n\}}(y)I_{\{0,1\}}(p),
\]</span>,</p>
<p>      where <span class="math inline">\(y=\sum_{i=1}^{n}x_{i}.\)</span></p>
<ul>
<li>Sampling Without Replacement
<ul>
<li>Population contains <span class="math inline">\(N\)</span> items, <span class="math inline">\(M\)</span> of which are 1s (successes) and <span class="math inline">\(N − M\)</span> of which are <span class="math inline">\(0\)</span>s (failures).</li>
<li>Consecutively sample <span class="math inline">\(n \ne N\)</span> items at random without replacement.</li>
<li>Let <span class="math inline">\(X_{i}\)</span> be the value of the sampled item on trial <span class="math inline">\(i\)</span>. Note that <span class="math inline">\(X_{i}\)</span>s a Bernoulli rv.</li>
<li>Joint pmf:</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_{X_{1},X_{2},\ldots,X_{n}}(x_{1},x_{2},\ldots,x_{n}|M,N,n)={\frac{(M)_{y}(N-M)_{n-y}}{(N)_{n}}}I_{S_{Y}}(y)
\]</span></p>
<p><span class="math display">\[
=\frac{\binom{M}{y}\binom{N-M}{n-y}}{\binom{n}{y}\binom{N}{n}} I_{SY}(y),
\]</span>
      where <span class="math inline">\(y=\sum_{i=1}^{n}x_{i}\)</span> and
<span class="math inline">\(S_{Y}=\{\mathrm{max}(0,n+M-N),\cdots,\mathrm{min}(n,M)\}.\)</span></p>
<ul>
<li>Note, the support of <span class="math inline">\(Y\)</span> follows from <span class="math inline">\(y\geq0;\;y\leq\,n;\;y\leq\;M;\)</span> and <span class="math inline">\(n-y\leq N-M.\)</span></li>
</ul>
<p><br></p>
</div>
</div>
<div id="binomial-distribution" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Binomial Distribution<a href="chapter4.html#binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Suppose that <span class="math inline">\(X_{1},X_{2},\cdots,X_{n}\)</span> are iid <span class="math inline">\(Bern(p)\)</span>. Then</li>
</ul>
<p><span class="math display">\[
Y=\sum_{i=1}^{n}X_{i}\sim\mathrm{Bin}(n,p).
\]</span></p>
<ul>
<li>Probability mass function:</li>
</ul>
<p><span class="math display">\[
f_{Y}(y)=\ {\binom{n}{y}}p^{y}(1-p)^{n-y}
\]</span></p>
<p>      where <span class="math inline">\(y=0,1,\cdots,n\)</span> and <span class="math inline">\(p\in[0,1].\)</span></p>
<ul>
<li><p>To justify this result, note that Pr(<span class="math inline">\(Y=y\)</span>) is the probability of a specific sequence of <span class="math inline">\(X_{i}\)</span>s such that <span class="math inline">\(\sum x_{i}=y\)</span> multiplied by the number of possible sequences that satisfy <span class="math inline">\(\sum x_{i}=y\)</span>. - The pmf of <span class="math inline">\(Y\)</span> also can be obtained from the pmf of <span class="math inline">\(X_{i}\)</span> by using probability generating functions.</p></li>
<li><p>Moments</p>
<ul>
<li><span class="math inline">\(E(Y)=E(\sum_{i=1}^{n}X_{i})=nE(X)=np.\)</span></li>
<li><span class="math inline">\(Var(Y)=Var(\sum_{i=1}^{n}X_{i})=nVar(X)=n p(1-p).\)</span></li>
<li><span class="math inline">\({\sum}_{y=k}^{n}f_{Y}(y)=\mathrm{P}(Y\geq k)\)</span>.</li>
<li>Caution, most tables of the cumulative binomial distribution give <span class="math inline">\(\mathrm{Pr}(Y\leq k)\)</span>.</li>
</ul></li>
<li><p>Reproductive property: If <span class="math inline">\(\ Y_{1},Y_{2},\ldots,Y_{k}\)</span> are independently distributed as <span class="math inline">\(Y_{i}\sim\mathrm{Bin}(n_{i},p)\)</span>, then <span class="math inline">\(\sum_{i=1}^{k}Y_{i}\sim\mathrm{Bin}(n,p)\)</span>, where <span class="math inline">\(n=\sum_{i=1}^{k}n_{i}\)</span>.</p>
<ul>
<li>To justify this result, write each <span class="math inline">\(Y_i\)</span> as the sum of iid Bernoulli random variables.</li>
<li>An alternative justification is to use probability generating functions.</li>
<li>If <span class="math inline">\(Y_{i}\sim\mathrm{Bin}(n_{i},p)\)</span>, then the pgf of <span class="math inline">\(Y\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
\eta_{Y}(t)&amp;=E(t^{Y})=\sum_{y=0}^{n}{\binom{n}{y}}p^{y}(1-p)^{n-y}t^{y}\\
&amp;=\sum_{y=0}^{n}{\binom{n}{y}}(p t)^{y}(1-p)^{n-y}=[p t+(1-p)]^{n}
\end{align}
\]</span></p>
<ul>
<li>Suppose that <span class="math inline">\(Y_{i}\stackrel{\mathrm{ind}}{\sim}\mathrm{Bin}(n_{i},p)\)</span> for <span class="math inline">\(i=1,\ldots\,k.\)</span>. Let <span class="math inline">\(W=\sum_{i=1}^{k}Y_{i}.\)</span> The pgf of <span class="math inline">\(W\)</span> is</li>
</ul>
<p><span class="math display">\[
\eta_{W}(t)=\operatorname{E}(t^{W})=\prod_{i=1}^{k}\eta_{Y_{i}}(t)=[p t+(1-p)]^{\sum_{i=1}^{k}n_{i}}.
\]</span></p>
<ul>
<li>The pgf for <span class="math inline">\(W\)</span> has the form of the pgf of a binomial random variable with parameters <span class="math inline">\(\sum_{i=1}^{k}\,n_{i}\)</span> and <span class="math inline">\(p\)</span>. Therfore,</li>
</ul>
<p><span class="math display">\[
\{Y_{i}\}_{i=1}^{k}\stackrel{ind}{\sim}\ \mathrm{Bin}(n_{i},p)\Longrightarrow\sum_{i=1}^{k}Y_{i}\sim\mathrm{Bin}\left(\sum_{i=1}^{k}n_{i},p\right).
\]</span></p>
<p><br></p>
</div>
<div id="hypergeometric-distribution" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Hypergeometric Distribution<a href="chapter4.html#hypergeometric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Population contains <span class="math inline">\(N\)</span> items, <span class="math inline">\(M\)</span> of which are 1s (successes) and <span class="math inline">\(N − M\)</span> of which are <span class="math inline">\(0\)</span>s (failures).</li>
<li>Sample <span class="math inline">\(n\leq N\)</span> items at random without replacement.</li>
<li>Let <span class="math inline">\(X_{i}\)</span> be the value of the sampled item on trial <span class="math inline">\(i\)</span> and let <span class="math inline">\(Y=\sum_{i=1}^{n}X_{i}=\)</span> total number of successes. Then,</li>
</ul>
<p><span class="math display">\[
Y\sim\mathrm{HyperG}(N,M,n).
\]</span></p>
<ul>
<li>Probability mass function:</li>
</ul>
<p><span class="math display">\[
f_{y}(y|N,M,n)=\frac{{\binom{M}{y}\ {\binom{N-M}{n-y}}}}{{\binom{N}{n}}}
\]</span></p>
<p>      where <span class="math inline">\(y\)</span> is an integer that satisfies <span class="math inline">\(max(0,n+M-N)\leq y\leq min(n,M)\)</span>.</p>
<ul>
<li><p>Note: this probability is <span class="math inline">\(\binom{n}{y}\)</span> times the probability of a specific sequence of <span class="math inline">\(X\)</span>s that result in <span class="math inline">\(Y = y\)</span> successes. the number of possible sequences is <span class="math inline">\(\binom{n}{y}\)</span> .</p></li>
<li><p>Result:</p></li>
</ul>
<p><span class="math display">\[E(Y)=\sum_{i=1}^{n}E(X_{i})=\ n p,
\]</span></p>
<p>      where <span class="math inline">\(p=M/N\)</span>.</p>
<ul>
<li><p>Result: Suppose that <span class="math inline">\(n = N\)</span>. Then <span class="math inline">\(\mathrm{Var}(\sum_{i=1}^{N}X_{i})=\mathrm{Var}(M)=0.\)</span> Use exchangeability to obtain Cov<span class="math inline">\((X_{i},X_{j})=-p(1-p)/(N-1)\)</span>.</p></li>
<li><p>Result:</p></li>
</ul>
<p><span class="math display">\[
\mathrm{Var}(Y)=n p(1-p)\left(1-\frac{n-1}{N-1}\right).
\]</span></p>
<ul>
<li>Proof: By exchangeability,</li>
</ul>
<p><span class="math display">\[ \begin{align}
\mathrm{Var}(Y)&amp;=\mathrm{Var}(\textstyle{\sum_{i=1}^{n}X_{i}})=n\,\mathrm{Var}(X_{1})+n(n-1)\mathrm{Cov}(X_{1},X_{2})\\
&amp;=n p(1-p)-n(n-1)p(1-p)/(N-1)\\
&amp;=n p(1-p)(1-{\frac{n-1}{N-1}}).
\end{align}
\]</span></p>
<ul>
<li>Application: Fisher’s exact test for <span class="math inline">\(H_0 :p_{1}=p_{2}\)</span> in <span class="math inline">\(2 \times 2\)</span> tables.
<ul>
<li>Let <span class="math inline">\(Y_{1}\sim\mathrm{Bin}(n_{1},p_{1})\)</span> and <span class="math inline">\(Y_{2}\sim\mathrm{Bin}(n_{2},p_{2})\)</span> be independent random variables.</li>
<li>Assume that <span class="math inline">\(p_{1}=p_{2}\)</span> and find the conditional distribution of <span class="math inline">\(Y_{1}\)</span> given that <span class="math inline">\(Y_{1}+Y_{2}=m_{1}.\)</span></li>
<li>Solution: write <span class="math inline">\(p_{1}\)</span> and <span class="math inline">\(p_{2}\)</span> as <span class="math inline">\(p\)</span> (the common value). Then</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
Pr(Y_{1}=y_{1}|Y_{1}+Y_{2}=m_{1})&amp;={\frac{Pr(Y_{1}=y_{1}\ \mathrm{and}\ Y_{1}+Y_{2}=m_{1})}{Pr(Y_{1}+Y_{2}=m_{1})}} \\
&amp;=\frac{\mathrm{Pr}(Y_{1}=y_{1},Y_{2}=m_{1}-y_{1})}{\mathrm{Pr}(Y_{1}+Y_{2}=m_{1})}\\
&amp;=\frac{{\binom{n_{1}}{y_{1}}}p^{y_{1}}(1-p)^{n_{1}-y_{1}}{\binom{n_{2}}{m_{1}-y_{1}}}p^{m_{1}-y_{1}}(1-p)^{n_{2}-m_{1}+y_{1}}}{\binom{n_{1}+n_{2}}{m_{1}}p^{m_{1}}(1-p)^{n_{1}+n_{2}-m_{1}}}\\
&amp;=\frac{{\binom{n_{1}}{y_{1}}}{\binom{n_{2}}{m_{1}-y_{1}}}}
{\binom{n_{1}+n_{2}}{m_{1}}}
\end{align}
\]</span></p>
<ul>
<li><p>Accordingly, conditional on <span class="math inline">\(Y1 + Y2 = m1\)</span>, the rv <span class="math inline">\(Y_1\)</span> is distributed as <span class="math inline">\(hyperG(n_{1}+n_{2},n_{1},m_{1})\)</span>.</p></li>
<li><p>Binomial approximation to hypergeometric. If <span class="math inline">\(N\)</span> is large and <span class="math inline">\(p = M/N\)</span> is not near <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, then</p></li>
</ul>
<p><span class="math display">\[
Y\sim HyperG(N,M,n) {\implies} Y\sim{\mathrm{Bin}}(n,p)
\]</span></p>
<ul>
<li>Proof: Suppose that both <span class="math inline">\(p = M/N\)</span> and <span class="math inline">\(n\)</span> remain constant while <span class="math inline">\(N\)</span> goes to infinity. Then</li>
</ul>
<p><span class="math display">\[
\begin{align}
lim_{N\rightarrow \infty}f_{y}(y|N,M,n)&amp;= lim_{N\rightarrow \infty} \frac{\binom{M}{y}\binom{N-M}{n-y}}{\binom{N}{n}}\\
&amp;=lim_{N\rightarrow\infty} \binom{n}{y} \frac{(M)_y(N-M)_{n-y}}{(N)_n}\\
&amp;=lim_{N\to\infty}\binom{n}{y}\left({\frac{M}{N}}\right)\left({\frac{M-1}{N-1}}\right)\cdots \left({\frac{M-y+1}{N-y+1}}\right) \\
&amp;\times\left({\frac{N-M}{N-y}}\right)\left({\frac{N-M-1}{N-y-1}}\right)\cdots\left({\frac{N-M-n+y+1}{N-n+1}}\right) \\
&amp;lim_{N\rightarrow \infty} \binom{n}{y} \left({\frac{M}{N}}\right) \left({\frac{\frac{M}{N}-{\frac{1}{N}}}{1-{\frac{1}{N}}}} \right) \cdots \left({\frac{\frac{M}{N}-{\frac{y-1}{N}}}{1-{\frac{y-1}{N}}}} \right) \\
&amp; \times \left({\frac{1-{\frac{M}{N}}}{1-{\frac{y}{N}}}} \right) \left({\frac{1-{\frac{M}{N}}-{\frac{1}{N}}}{1-{\frac{y-1}{N}}}}\right)\cdots \left({\frac{1-{\frac{M}{N}}-{\frac{n-y-1}{N}}}{1-{\frac{n-1}{N}}}} \right) \\
&amp;={\binom{n}{y}}p^{y}(1-p)^{n-y}
\end{align}
\]</span></p>
<ul>
<li>Illustration: Suppose <span class="math inline">\(n = 20, y = 6\)</span>, and <span class="math inline">\(p = 0.1\)</span> or <span class="math inline">\(p = 0.5\)</span>. The binomial probabilities are</li>
</ul>
<p><span class="math display">\[
\mathrm{Pr}(Y=6|n=20,p=0.1)=0.0089
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathrm{Pr}(Y=6|n=20,p=0.5)=0.0370
\]</span></p>
<ul>
<li>The hypergeometric probabilities for various population sizes are displayed below.
<ul>
<li>It can be seen that as <span class="math inline">\(N\rightarrow\infty\)</span>, the hypergeometric probabilities converge to the binomial probabilities.</li>
</ul></li>
</ul>
<center>
<img src="fig4/3.png" style="width:80.0%" />
</center>
<p><br></p>
</div>
<div id="geometric-distribution" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Geometric Distribution<a href="chapter4.html#geometric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Let <span class="math inline">\(X_{1},X_{2},\dots\)</span> be an infinite sequence of iid <span class="math inline">\(\operatorname{Bern}(p)\)</span> random variables.</li>
<li>Let <span class="math inline">\(Z\)</span> be the trial number in which the first success occurs. Then,</li>
</ul>
<p><span class="math display">\[
Z\sim\mathrm{Geo}(p).
\]</span></p>
<p><span class="math display">\[
f_{Z}(z)=(1-p)^{z-1}p
\]</span></p>
<p>      where <span class="math inline">\(z=1,2,\ldots\)</span> and <span class="math inline">\(p\in(0,1]\)</span>.</p>
<ul>
<li>Result: If <span class="math inline">\(Z\sim\mathrm{Geo}(p)\)</span>, then</li>
</ul>
<p><span class="math display">\[
E(Z) = 1/p
\]</span></p>
<ul>
<li>Proof:<span class="math inline">\(\mathrm{let}\ q=1-p.\)</span>Then</li>
</ul>
<p><span class="math display">\[
\begin{align}
E(Z)&amp;=\sum_{i=1}^{\infty}i p q^{i-1}=p\sum_{i=1}^{\infty}i q^{i-1}=p\frac{d}{d q}\sum_{i=1}^{\infty}q^{i}\\
&amp;=\;p\frac{d}{dq}\left(\frac{1}{1-q}-1\right)=p\frac{1}{(1-q)^{2}}=\frac{1}{p}
\end{align}
\]</span></p>
<ul>
<li>Result: <span class="math inline">\(Z\sim\mathrm{Geo}(p),\)</span> then</li>
</ul>
<p><span class="math display">\[
\mathrm{Var}(Z)=q/p^{2}
\]</span></p>
<ul>
<li>Hint on proof: let <span class="math inline">\(q=1-p\)</span> Then,</li>
</ul>
<p><span class="math display">\[
E[Z(Z-1)]=\sum_{i=1}^{\infty}i(i-1)p q^{i-1}=p q\sum_{i=1}^{\infty}i(i-1)q^{i-2}=p q{\frac{d^{2}}{dq^{2}}}\sum_{i=1}^{\infty}q^{i}\]</span></p>
<p><br></p>
</div>
<div id="negative-binomial-distribution" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Negative Binomial Distribution<a href="chapter4.html#negative-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Let <span class="math inline">\(X_{1},X_{2},\ldots\)</span> be an infinite sequence of iid Bern(<span class="math inline">\(p\)</span>) random variables.</p></li>
<li><p>Let W be the trial number in which the <span class="math inline">\(r\)</span>th success occurs.
<span class="math display">\[
W\sim\mathrm{Negbin}(r,p)
\]</span></p></li>
</ul>
<p><span class="math display">\[f_{W}(w)={\binom{w-1}{r-1}}p^{r}(1-p)^{w-r}
\]</span></p>
<p>      where <span class="math inline">\(w=r,r+1,r+2,\ldots\)</span> and <span class="math inline">\(p\in(0,1].\)</span></p>
<ul>
<li><p>Justification: if the <span class="math inline">\(r\)</span>th success occurs on trial <span class="math inline">\(w\)</span>, then there must be <span class="math inline">\(r − 1\)</span> successes on the first <span class="math inline">\(w − 1\)</span> trials and one success on trial <span class="math inline">\(w\)</span>.</p></li>
<li><p>These events are independent and have
<span class="math inline">\(\textstyle{\binom{w-1}{r-1}}p^{r-1}(1-p)^{w-r}\)</span> probabilities and <span class="math inline">\(p\)</span>, respectively.</p></li>
<li><p>Result: If <span class="math inline">\(W\sim\mathrm{NegBin}(r,p),\)</span> then <span class="math inline">\(W\sim\sum_{i=1}^{r}Z_{i}\)</span>, where <span class="math inline">\(Z_{i}\stackrel{iid}{\sim}\mathrm{Geo}(p)\)</span>, for <span class="math inline">\(i=1,2,\ldots\)</span>.</p></li>
<li><p>Accordingly,</p></li>
</ul>
<p><span class="math display">\[ E(W) = r/p\]</span>
<span class="math display">\[ Var(W) = rq/p^{2}\]</span></p>
<ul>
<li>Alternative definition of Negative Binomial
<ul>
<li>Let <span class="math inline">\(X_{1},X_{2},\ldots\)</span> be an infinite sequence of iid Bern(<span class="math inline">\(p\)</span>) random variables.</li>
<li>Let <span class="math inline">\(Y\)</span> be the number of failures before the <span class="math inline">\(r\)</span>th success occurs.</li>
<li>Then, <span class="math inline">\(Y\sim\mathrm{NegBin}^{*}(r,p).\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[
f_{Y}(y)={\binom{r+y-1}{y}}p^{r}(1-p)^{y}
\]</span></p>
<p>      where <span class="math inline">\(y=0,1,\dots\)</span> and <span class="math inline">\(p\in(0,1]\)</span>.</p>
<ul>
<li><p>Justification: if the <span class="math inline">\(r\)</span>th success occurs on trial <span class="math inline">\(y + r\)</span>, then there must be <span class="math inline">\(y\)</span> failures on the first <span class="math inline">\(r + y − 1\)</span> trials and one success on trial <span class="math inline">\(r + y\)</span>.</p></li>
<li><p>These events are independent and have probabilities <span class="math inline">\(\binom{r+y-1}{y}p^{r-1}(1-p)^{y}\)</span> and <span class="math inline">\(p\)</span>, respectively.</p></li>
<li><p>Result: If <span class="math inline">\(Y\sim\mathrm{NegBin}^{*}(r,p),\)</span> then <span class="math inline">\(Y=W-r.\)</span> Accordingly,</p></li>
</ul>
<p><span class="math display">\[
E(Y)=E(W)-r=r/p-r=r(1-p)/p
\]</span></p>
<p><span class="math display">\[
Var(Y)=\mathrm{{Var}}(W-r)=\mathrm{{Var}}(W)=r q/p^{2}
\]</span>
<br></p>
</div>
<div id="negative-hypergeometric-distribution" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Negative Hypergeometric Distribution<a href="chapter4.html#negative-hypergeometric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Population contains <span class="math inline">\(N\)</span> items, <span class="math inline">\(M\)</span> of which are 1s (successes) and <span class="math inline">\(N − M\)</span> of which are <span class="math inline">\(0\)</span>s (failures).</p></li>
<li><p>Sample items at random without replacement.</p></li>
<li><p>Let <span class="math inline">\(X_{i}\)</span> be the value of the sampled item on trial <span class="math inline">\(i\)</span> and let <span class="math inline">\(W\)</span> be the trial number in which the <span class="math inline">\(r\)</span>th success occurs.</p>
<ul>
<li>Note: <span class="math inline">\(r\)</span> must satisfy <span class="math inline">\(r\leq M\)</span>.</li>
</ul></li>
<li><p><span class="math inline">\(W\sim\mathrm{NegHyperG}(N,M,r)\)</span></p></li>
</ul>
<p><span class="math display">\[
f_{W}(w)=\frac{\binom{M}{r-1} \binom{N-M}{w-r}}{\binom{N}{w-1}}\left({\frac{M-r+1}{N-w+1}}\right)
\]</span></p>
<p>      where <span class="math inline">\(w\equiv\,r,r+\,1,\ldots,M\)</span>.</p>
<ul>
<li><p>Justification: if the <span class="math inline">\(r\)</span>th success occurs on trial <span class="math inline">\(w\)</span>, then there must be <span class="math inline">\(r − 1\)</span> successes on the first <span class="math inline">\(w − 1\)</span> trials and one success on trial <span class="math inline">\(w\)</span>.</p></li>
<li><p>These events have probabilities
Pr(<span class="math inline">\(r − 1\)</span> successes on first <span class="math inline">\(w − 1\)</span> trials) <span class="math inline">\(={\binom{M}{r-1}}{\binom{N-M}{w-r}}/ \binom{N}{w-1}\)</span> and Pr(success on trial <span class="math inline">\(r|r-1\)</span> successes on first <span class="math inline">\(w − 1\)</span> trials) <span class="math inline">\(=(M-r+1)/(N-w+1)\)</span>.</p></li>
<li><p>Alternative expression:
<span class="math display">\[
f_{W}(w)= \frac{{\binom{w-1}{r-1}}\ {\binom{N-w}{M-r}}}{\binom{N}{M}}
\]</span></p></li>
<li><p>Justification: There are <span class="math inline">\(\binom{N}{M}\)</span> ways of arranging the <span class="math inline">\(M\)</span> successes.</p>
<ul>
<li>If <span class="math inline">\(W = w\)</span>, then the first <span class="math inline">\(w − 1\)</span> trials must contain <span class="math inline">\(r − 1\)</span> successes, the <span class="math inline">\(r\)</span>th trial contains <span class="math inline">\(1\)</span> success, and the last <span class="math inline">\(N − w\)</span> trials contain <span class="math inline">\(M − r\)</span> successes.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="approximating-binomial-probabilities" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Approximating Binomial Probabilities<a href="chapter4.html#approximating-binomial-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="normal-approximation-to-the-binomial" class="section level3 hasAnchor" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Normal approximation to the Binomial<a href="chapter4.html#normal-approximation-to-the-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>If <span class="math inline">\(Y\sim\mathrm{Bin}(n,p)\)</span> and <span class="math inline">\(n p(1-p)\gt 5,\)</span> then <span class="math inline">\(Y \approx {N}\left[n p,n p(1-p)\right].\)</span>
<ul>
<li>This is an application of the central limit theorem (CLT).</li>
<li>Below are displays of four binomial distributions.</li>
<li>It can be seen that if <span class="math inline">\(n\)</span> is small and <span class="math inline">\(p\)</span> is near zero (or one), then the normal approximation is not very accurate.</li>
<li>Generally, it recommends that the normal approximation be used only if <span class="math inline">\(n p(1-p)\geq 5\)</span>.</li>
</ul></li>
</ul>
<center>
<img src="fig4/4.png" style="width:80.0%" />
</center>
<ul>
<li>Using a correction for continuity, will improve the accuracy of the normal approximation. The continuity correction consists of adding approximating <span class="math inline">\(P(Y ≤ y)\)</span> as</li>
</ul>
<p><span class="math display">\[
P(Y\leq y) \approx \Phi \left(\frac{y+0.5-n p}{\sqrt{n p(1-p)}}\right)
\]</span></p>
<p>      where <span class="math inline">\(\Phi\)</span> is the standard normal cumulative distribution function, rather than as</p>
<p><span class="math display">\[
P(Y\leq y)\approx\Phi\left(\frac{y-n p}{\sqrt{n p(1-p)}}\right)
\]</span></p>
<ul>
<li>Below is a comparison of the approximation with continuity correction and without continuity correction for the case <span class="math inline">\(Y\sim\mathrm{Bin}(10,0.5)\)</span>
<ul>
<li>Note that <span class="math inline">\(n p(1-p)\geq 5\)</span> is not satisfied.</li>
</ul></li>
</ul>
<center>
<img src="fig4/5.png" style="width:70.0%" />
</center>
</div>
<div id="poisson-approximation-to-the-binomial" class="section level3 hasAnchor" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> Poisson Approximation to the Binomial<a href="chapter4.html#poisson-approximation-to-the-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>If <span class="math inline">\(Y\sim\mathrm{Bin}(n,p)\)</span>, where <span class="math inline">\(n\)</span> is large and <span class="math inline">\(p\)</span> is small, then</li>
</ul>
<p><span class="math display">\[
P(Y=y)\approx\frac{e^{-n p}(n p)^{y}}{y!} \,\,\, y=0,1,2,\ldots
\]</span></p>
<ul>
<li>Proof: Let <span class="math inline">\(\lambda=n p\)</span> and write <span class="math inline">\(p\)</span> as <span class="math inline">\(p=\lambda/n\)</span>
<ul>
<li>Examine the binomial pmf as <span class="math inline">\(n\)</span> goes to infinity, but <span class="math inline">\(\lambda\)</span> remains constant:</li>
</ul></li>
</ul>
<p><span class="math display">\[ \begin{align}
P(Y=y)&amp;={\binom{n}{y}}p^{y}(1-p)^{n-y}
=\frac{n!}{y!(n-y)!}\left({\frac{\lambda}{n}}\right)^{y}\left(1-{\frac{\lambda}{n}}\right)^{n-y} \\
&amp;=\frac{\lambda^{y}}{y!}\frac{(n)_{y}}{n^{y}}\left(1-\frac{\lambda}{n}\right)^{n}\left(1-\frac{\lambda}{n}\right)^{-y}
\end{align}
\]</span></p>
<ul>
<li><p>Examine the components individually:</p></li>
<li><p><span class="math inline">\(lim_{n\to\infty}{\frac{(n)_{y}}{n^{y}}}=lim_{n\to\infty}\left({\frac{n}{n}}\right)\left({\frac{n-1}{n}}\right)\left({\frac{n-2}{n}}\right)\cdots \left({\frac{n-y+1}{n}}\right)=1\)</span></p></li>
<li><p><span class="math inline">\(lim_{n\rightarrow\infty}\left(1-\frac{\lambda}{n}\right)^{-y}=(1-0)^{-y}=1\)</span></p></li>
<li><p><span class="math inline">\(lim_{n\rightarrow\infty}\left(1-\frac{\lambda}{n}\right)^{n}=e^{-\lambda}\)</span></p></li>
<li><p>Accordingly, as <span class="math inline">\(n\)</span> goes to <span class="math inline">\(\infty\)</span>, <span class="math inline">\(P(Y=y)\)</span> goes to</p></li>
</ul>
<p><span class="math display">\[
lim_{n\to\infty}{\binom{n}{y}}p^{y}(1-p)^{n-y}={\frac{\lambda^{y}}{y!}}\times1\times e^{-\lambda}\times1={\frac{e^{-\lambda}\lambda^{y}}{y!}}
\]</span></p>
<p><br></p>
</div>
</div>
<div id="poisson-distribution" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Poisson Distribution<a href="chapter4.html#poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>If <span class="math inline">\(Y\sim\mathrm{{Poi}}(\lambda)\)</span>, then</li>
</ul>
<p><span class="math display">\[
P(Y=y)=\frac{e^{-\lambda}\lambda^{y}}{y!}, \,\,\, y=0,1,2,\ldots
\]</span></p>
<ul>
<li><p>The pmf sums to one because <span class="math inline">\(\sum_{i=0}^{\infty}{\lambda^{i} / i!} = e^{\lambda}\)</span></p></li>
<li><p>The pgf of <span class="math inline">\(Y\)</span> is</p></li>
</ul>
<p><span class="math display">\[
\eta_{Y}(t)=E(t^{Y})=\sum_{i=0}^{\infty}{\frac{e^{-\lambda}(\lambda t)^{i}}{i!}}=e^{-\lambda}e^{\lambda t}=e^{(t-1)\lambda}
\]</span></p>
<div id="poisson-process" class="section level4 unnumbered hasAnchor">
<h4>Poisson Process<a href="chapter4.html#poisson-process" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Stationary Assumption: The probability of <span class="math inline">\(y\)</span> events in a region or time period does not depend on the location of the region or time period.</p></li>
<li><p>Independence Assumption: Events in non-overlapping regions (time periods) are independent.</p></li>
<li><p>In a small region (time period), the probability of one event is approximately proportional to the size of the region (time period).</p></li>
<li><p>In a small region (time period), the probability of two or more events in a small region (time period) is negligible compared to the probability of one event in the region.</p></li>
<li><p>Technical Details about Assumptions 3rd and 4th above.</p>
<ul>
<li>Consider an interval <span class="math inline">\((t_{0},t_{0}+h]\)</span>. The interval length is <span class="math inline">\(h\)</span></li>
<li>Let <span class="math inline">\(X\)</span> = number of events that occur in the interval.</li>
<li>Assumption #3 says that <span class="math inline">\(\mathrm{Pr}(X=1)=\lambda h+o(h),\)</span> where <span class="math inline">\(o(h)\)</span> is a small remainder term. That is, the probability is approximately proportional to the size of the area.</li>
<li><span class="math inline">\(o(h)\)</span> is read as little <span class="math inline">\(o\)</span> of <span class="math inline">\(h\)</span> and is defined as a term that satisfies
<span class="math inline">\(lim_{h\rightarrow 0}{\frac{o(h)}{h}}=0\)</span></li>
<li>Assumption #3 says that
<span class="math inline">\(lim_{h\to0}{\frac{Pr(X=1)}{h}}=lim_{h\to 0}{\frac{\lambda h+o(h)}{h}}=\lambda.\)</span></li>
<li>Assumption #4 says that <span class="math inline">\(\mathrm{Pr}(X\geq 2)=o(h).\)</span></li>
<li>Assumptions #3 and #4 together imply that <span class="math inline">\(\mathrm{Pr}(X=0)=1-\lambda h+o(h).\)</span></li>
</ul></li>
<li><p>Let <span class="math inline">\(Y\)</span> be the total number of events in a region of size <span class="math inline">\(t\)</span>.</p>
<ul>
<li>If the events follow a Poisson process, then <span class="math inline">\(Y\sim\mathrm{Poi}(\lambda t)\)</span>.</li>
<li>Divide the total region into <span class="math inline">\(n\)</span> equal parts, each of size <span class="math inline">\(h = t/n.\)</span></li>
<li>Let <span class="math inline">\(X_{i}\)</span> = number of events in the <span class="math inline">\(i^{th}\)</span> part.</li>
<li>By the Poisson process assumptions, the <span class="math inline">\(X_{i}\)</span>s are independent and identically distributed. Furthermore, <span class="math inline">\(X_{i}\overset{\cdot}{\sim}\)</span> iid Bern(<span class="math inline">\(p\)</span>) for <span class="math inline">\(i=1,\cdots ,n\)</span>, where <span class="math inline">\(p=\lambda t/n\)</span></li>
<li>The Bernoulli approximation becomes more accurate as <span class="math inline">\(n\)</span> increases.</li>
<li>Let <span class="math inline">\(Y=\sum_{i=1}^nX_{i}\)</span>. By the Poisson process assumptions, <span class="math inline">\(Y\sim\mathrm{Bin}(n,p)\)</span>, where <span class="math inline">\(p = λt/n\)</span>. The binomial approximation becomes more accurate as <span class="math inline">\(n\)</span> goes to infinity.</li>
<li>Let <span class="math inline">\(n\)</span> go to infinity. Note that <span class="math inline">\(p\)</span> goes to zero as <span class="math inline">\(n\)</span> goes to infinity and <span class="math inline">\(np=\lambda t\)</span>. Accordingly, by the Poisson approximation to the binomial, the limiting distribution of <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathrm{Poi}(\lambda t)\)</span>.</li>
</ul></li>
<li><p>Moments: if <span class="math inline">\(Y\sim\mathrm{{Poi}}(\lambda)\)</span>, then</p>
<ul>
<li><span class="math inline">\(E(Y)=\lambda\)</span></li>
<li><span class="math inline">\(\mathrm{Var}(Y)=\lambda\)</span></li>
<li>Justification: Show that <span class="math inline">\(\mathrm{E}\left[(Y)_{r}\right]=\lambda^{r}:\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[
E\left[(Y)_{r}\right]=\sum_{i=0}^{\infty}(i)_{r}e^{-\lambda}\lambda^{i}/i!=e^{-\lambda}\sum_{i=r}^{\infty}(i)_{r}\lambda^{i}/i!=e^{-\lambda}\lambda^{r}\sum_{j=0}^{\infty}\lambda^{j}/j!=\lambda^{r}
\]</span></p>
<ul>
<li><p>Reproductive property of Poisson random variables.</p>
<ul>
<li>If <span class="math inline">\(Y_{1},Y_{2},\ldots,Y_{k}\)</span> are independently distributed as <span class="math inline">\(Y_{i}\sim Poi(\lambda_{i})\)</span>, then <span class="math inline">\(\sum_{i=1}^{k} Y_{i}\sim \mathrm{Poi}(\lambda)\)</span> where <span class="math inline">\(\lambda=\sum_{i=1}^{k}\lambda_{i}\)</span>.</li>
</ul></li>
<li><p>Justification: Recall that if <span class="math inline">\(Y_{i}\sim\mathrm{Poi}(\lambda_{i})\)</span>, then the pgf of <span class="math inline">\(Y_{i}\)</span> is <span class="math inline">\(\eta_{Y_{i}}(t)=exp\{(t-1)\lambda_{i}\}.\)</span></p></li>
<li><p>Let <span class="math inline">\(W=\sum_{i=1}^{k}Y_{i}\)</span> then, by independence, the pgf of <span class="math inline">\(W\)</span> is</p></li>
</ul>
<p><span class="math display">\[
\eta_{W}(t)=\prod_{i=1}^{k}e^{(t-1)\lambda_{i}}=e^{(t-1)\sum_{i=1}^{k}\lambda i}=e^{(t-1)\lambda}
\]</span></p>
<p>Therefore, <span class="math inline">\(W\sim\mathrm{Poi}(\lambda),\)</span> where <span class="math inline">\(\lambda=\sum_{i=1}^{k}\lambda_{i}\)</span>.</p>
<ul>
<li>Proof that <span class="math inline">\(Y\sim Poi(\lambda t)\)</span> if events follow a Poisson process.
<ul>
<li>Let <span class="math inline">\(Y\)</span> be the total number of events that occur in a region of size <span class="math inline">\(t\)</span>.</li>
<li>Let <span class="math inline">\(P_{n}(t)=\mathrm{Pr}(Y=n)\)</span> in a region of size <span class="math inline">\(t\)</span>. Note that
<span class="math inline">\(P_{0}(0)=lim_{h\to 0}1-\lambda h+o(h)=1\)</span>.<br />
</li>
<li>Accordingly, <span class="math inline">\(P_{n}(0)=0\)</span> if <span class="math inline">\(n\geq1\)</span>.</li>
<li><span class="math inline">\(P_{0}(t+h)=P_{0}(t)P_{0}(h)\)</span> by the stationary and independence assumptions. To verify this result, divide the region of size <span class="math inline">\(t + h\)</span> into a region of size <span class="math inline">\(t\)</span> and a region of size <span class="math inline">\(h\)</span>. Then <span class="math inline">\(Y = 0\)</span> if and only if there are zero events in each sub-region. These events are independent by the Poisson process assumptions.</li>
<li>The derivative of <span class="math inline">\(P_{0}(t)\)</span> with respect to <span class="math inline">\(t\)</span> can be obtained as follows. By definition, the derivative is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\frac{d P_{0}(t)}{d t}=lim_{h\to0}\frac{P_{0}(t+h)-P_{0}(t)}{h}
\]</span></p>
<p>      Write <span class="math inline">\(P_{0}(t+h)-P_{0}(t)\)</span> as</p>
<p><span class="math display">\[
\begin{align}
P_{0}(t+h)-P_{0}(t)&amp;=P_{0}(t)P_{0}(h)-P_{0}(t)=P_{0}(t)\,[1-P_{0}(h)]\\
&amp;=P_{0}(t)\left\{1-[1-\lambda h+o(h)]\right\}=P_{0}(t)\left[-\lambda h+o(h)\right]
\end{align}
\]</span></p>
<p>      Accordingly,</p>
<p><span class="math display">\[
\frac{d P_{0}(t)}{d t}=lim_{h\rightarrow 0}\frac{P_{0}(t+h)-P_{0}(t)}{h}=lim_{h\rightarrow  0}\frac{-\lambda h P_{0}(t)+o(h)}{h}=-\lambda P_{0}(t)
\]</span></p>
<p>      Note that,</p>
<p><span class="math display">\[
\frac{d\ln[P_{0}(t)]}{d t}=\frac{1}{P_{0}(t)}\frac{d P_{0}(t)}{d t}=-\lambda
\]</span></p>
<ul>
<li>Integrate to obtain</li>
</ul>
<p><span class="math display">\[
ln[P_{0}(t)]=\int\frac{d\ln[P_{0}(t)]}{d t}d t=-\int\lambda d t=-\lambda t+c
\]</span></p>
<p>      It follows that <span class="math inline">\(P_{0}(t)=K e^{-\lambda t}\)</span>, where <span class="math inline">\(K=e^{c}\)</span>. Furthermore, <span class="math inline">\(K = 1\)</span> because <span class="math inline">\(P_{0}(0)=1=K e^{0}.\)</span> Thus, <span class="math inline">\(P_{0}(t)=e^{-\lambda t}\)</span>.</p>
<ul>
<li><span class="math inline">\(P_{n}(t+h)=\sum_{i=0}^{n}P_{i}(h)P_{n-i}(t)\)</span> by the independence and stationary assumptions.
<ul>
<li>Furthermore, <span class="math inline">\(P_{n}(t+h)=P_{0}(h)P_{n}(t)+P_{1}(h)P_{n-1}(t)+o(h)\)</span> because <span class="math inline">\(\sum_{i=2}^{n}P_{i}(h)=o(h)\)</span>.</li>
</ul></li>
<li>The derivative of <span class="math inline">\(P_{n}(t)\)</span> with respect to <span class="math inline">\(t\)</span> can be obtained as follows. By definition, the derivative is</li>
</ul>
<p><span class="math display">\[
\frac{d P_{n}(t)}{d t}=lim_{h\to 0}\frac{P_{n}(t+h)-P_{n}(t)}{h}
\]</span></p>
<p>      Write <span class="math inline">\(P_{n}(t+h)-P_{n}(t)\)</span> as</p>
<p><span class="math display">\[
\begin{align}
P_{n}(t+h)-P_{n}(t)&amp;=P_{n}(t)P_{0}(h)+P_{n-1}(t)P_{1}(h)+o(h)-P_{0}(t)\\
&amp;=P_{n}(t)\,[1-\lambda h+o(h)]+P_{n-1}(t)\,[\lambda h+o(h)]-P_{n}(t)\\
&amp;=-\lambda h P_{n}(t)+P_{n-1}(t)\lambda h+o(h)
\end{align}
\]</span></p>
<p>      Accordingly,</p>
<p><span class="math display">\[
\begin{align}
\frac{dP_n(t)}{dt}&amp;=lim_{h\rightarrow 0} \frac{P_0(t+h)-P_0(t)}{h}\\
&amp;=lim_{h\rightarrow 0}\frac{-\lambda h P_n(t)+P_{n-1}(t)\lambda h+o(h)}{h}=-\lambda P_n(t)+\lambda P_{n-1}(t)
\end{align}
\]</span></p>
<ul>
<li>Note that <span class="math inline">\(e^{\lambda t}\left[\frac{d P_{n}(t)}{d t}+\lambda P_{n}(t)\right]=e^{\lambda t}\lambda P_{n-1}(t)\)</span>.
<ul>
<li>It follows that</li>
</ul></li>
</ul>
<p><span class="math display">\[
\frac{d}{d t}e^{\lambda t}P_{n}(t)=e^{\lambda t}\lambda P_{n-1}(t)
\]</span></p>
<ul>
<li><p>For <span class="math inline">\(n=1\)</span> <span class="math inline">\(\frac{d}{d t}e^{\lambda t}P_{1}(t)=e^{\lambda t}\lambda P_{0}(t)=\lambda\)</span>.</p></li>
<li><p>Integrate to obtain <span class="math inline">\(e^{\lambda t}P_{1}(t)=\int\frac{d}{d t}e^{\lambda t}P_{1}(t)d t=\lambda t+c\)</span>.</p>
<ul>
<li>Use <span class="math inline">\(P_{1}(0)=0\)</span> to obtain <span class="math inline">\(P_{1}(t)=e^{-\lambda t}\lambda t\)</span>.</li>
</ul></li>
<li><p>Use induction to prove that <span class="math inline">\(P_{n}(t)=e^{-\lambda t}(\lambda t)^{n}/n!\)</span>.</p>
<ul>
<li>Suppose that <span class="math inline">\(P_{i}(t)=e^{-\lambda t}(\lambda t)^{i}/i!\)</span> for <span class="math inline">\(i=0.1\ldots n-1\)</span> Then</li>
</ul></li>
</ul>
<p><span class="math display">\[
\frac{d}{d t}e^{\lambda t}P_{n}(t)=e^{\lambda t}\lambda P_{n-1}(t)=\frac{e^{\lambda t}\lambda e^{-\lambda t}(\lambda t)^{n-1}}{(n-1)!}=\frac{\lambda^{n}t^{n-1}}{(n-1)!}
\]</span></p>
<ul>
<li>Integrate to obtain <span class="math inline">\(e^{\lambda t}P_{n}(t)=(\lambda t)^{n}/n!+c\)</span>.
<ul>
<li>Use <span class="math inline">\(P_{n}(0)=0\)</span> to obtain <span class="math inline">\(P_{n}(t)=e^{-\lambda t}(\lambda t)^{n}/n!\)</span>.</li>
</ul></li>
</ul>
<p><br></p>
</div>
</div>
<div id="law-of-large-numbers" class="section level2 hasAnchor" number="4.9">
<h2><span class="header-section-number">4.9</span> Law of Large Numbers<a href="chapter4.html#law-of-large-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Let <span class="math inline">\(X_{1},X_{2}, \ldots ,X_{n}\)</span> be a sequence of iid random variables having mean <span class="math inline">\(E(X)=\mu\)</span> and <span class="math inline">\(Var(X)=\sigma^{2}.\)</span>
<ul>
<li>Let <span class="math inline">\(\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\)</span> be the sample mean of the sequence. Then</li>
</ul></li>
</ul>
<p><span class="math display">\[
\bar{X}_{n}\stackrel{\mathrm{~Prob}}{\longrightarrow}\mu \,\,\, \text{as} \,\,\, n\longrightarrow \infty
\]</span></p>
<p>      Read this result as “<span class="math inline">\(X\)</span> bar converges in probability to <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n\)</span> goes to <span class="math inline">\(\infty\)</span>.” Convergence to <span class="math inline">\(\mu\)</span> in probability means that</p>
<p><span class="math display">\[
\lim_{n\rightarrow\infty}\mathrm{Pr}(|\bar{X}_{n}-\mu|\gt \epsilon)=0 \,\,\, \text{for any} \,\,\, \epsilon\gt 0
\]</span></p>
<ul>
<li><p>Justification:<span class="math inline">\(lim_{n\rightarrow\infty}\sigma_{\bar{X}}^{2}=lim_{n\rightarrow\infty}\sigma^{2}/n=0\)</span>. A formal proof: using Chebyshev’s inequality</p></li>
<li><p>Special case: If <span class="math inline">\(X_{1},X_{2}, \ldots ,X_{n}\)</span> is a sequence of iid <span class="math inline">\(Bern(p)\)</span> random variables and <span class="math inline">\(\hat{p}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\)</span>, then <span class="math inline">\(\hat{p}_{n}\,\stackrel{\mathrm{prob}}{\longrightarrow}p\)</span>.</p></li>
<li><p>Beware of the gamblers fallacy.</p>
<ul>
<li>Consider a sequence of iid <span class="math inline">\(Bern(0.5)\)</span> random variables.</li>
<li>Some gamblers believe that the law of large numbers means that a string of <span class="math inline">\(0\)</span>s is likely to be followed by 1s because the long run proportion of <span class="math inline">\(1\)</span>s is <span class="math inline">\(0.5\)</span>.</li>
<li>This interpretation is not correct. The random variables are independent, so the probability of a <span class="math inline">\(1\)</span> given a string of <span class="math inline">\(0\)</span>s is still <span class="math inline">\(0.5\)</span>.</li>
</ul></li>
</ul>
</div>
<div id="multinomial-distributions" class="section level2 hasAnchor" number="4.10">
<h2><span class="header-section-number">4.10</span> Multinomial Distributions<a href="chapter4.html#multinomial-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Let <span class="math inline">\(X\)</span> be a categorical random variable with support set <span class="math inline">\(S=\{A_{1},A_{2},\cdots ,A_{k}\}\)</span> and probability function <span class="math inline">\(\mathrm{Pr}(X=A_{j})= p_{j}\)</span> for <span class="math inline">\(j=1,\ldots,k\)</span>. That is,</li>
</ul>
<p><span class="math display">\[
f_X(a)=\begin{cases} p_j \,\,\, \text{if} \,\,\, a=A_j \,\,\, \text{for} \,\,\, j=1, \ldots , k \\
0 \,\,\, \text{otherwise}
\end{cases}
\]</span></p>
<ul>
<li>Sequence of iid random variables:
<ul>
<li>Let <span class="math inline">\(X_{1},X_{2}, \ldots ,X_{n}\)</span> be a sequence of iid random variables, each with p.f.<span class="math inline">\(f_{X}(a)\)</span>.</li>
<li>Let <span class="math inline">\(y_{j}\)</span> be the number of occurrences of <span class="math inline">\(A_{j}\)</span> in the sequence for <span class="math inline">\(j=1,\ldots,k\)</span>.</li>
<li>By exchangeability, the probability of the sequence is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\mathrm{Pr}(X_{1}=a_{1},X_{2}=a_{2}, \ldots , X_{n}=a_{n})=\prod_{j=1}^{k}p_{j}^{y_{j}}
\]</span></p>
<ul>
<li>Let <span class="math inline">\(Y_{j}\)</span> for <span class="math inline">\(j=1,\ldots ,k\)</span> be a set of <span class="math inline">\(k\)</span> random variables defined by</li>
</ul>
<p><span class="math inline">\(Y_{j}=\sum_{i=1}^{n}\delta_{A_{j}}(X_{i})=\)</span> number of occurances of ${A_j} in the sequence,
where</p>
<p><span class="math display">\[
\delta_{A_j}(X_i)=\begin{cases} 1 \,\,\, \text{if} \,\,\, X_i=A_j \\
0 \,\,\, \text{otherwise}
\end{cases}
\]</span></p>
<p>      Then <span class="math inline">\(Y_{1},Y_{2},\ldots,Y_{k}\)</span> have a multinomial distribution.</p>
<ul>
<li>If <span class="math inline">\(Y_{1},Y_{2},\ldots,Y_{k} \sim Multinom(n, p_1, \ldots , p_k)\)</span>, then</li>
</ul>
<p><span class="math display">\[
f_{Y_{1},Y_{2},\ldots,Y_{k}}(y_{1},y_{2},\dots,y_{k})=\binom{n}{y_{1},\ldots , y_{k}} \prod_{j=1}^{k}p_{j}^{y_{j}}
\]</span></p>
<p>      where <span class="math inline">\(y_{1},\ldots,y_{k}\)</span> are non-negative integers that satisfy <span class="math inline">\(\sum_{j=1}^{k}y_{j}=n\)</span>. Justification: each sequence with <span class="math inline">\(y_{1}\)</span> occurrences of <span class="math inline">\(A_{1}\)</span>, <span class="math inline">\(y_{2}\)</span> occurrences of <span class="math inline">\(A_{2}\)</span>, etc has the same probability. There are <span class="math inline">\(\binom{n}{y_{1},\ldots,y_{k}}\)</span> such sequences.</p>
<ul>
<li><p>Special case: <span class="math inline">\(k = 2\)</span>. Suppose that <span class="math inline">\(Y_{1},Y_{2}\sim Multinom(n,p_{1},p_{2})\)</span>. Then <span class="math inline">\(Y_{1}\sim\mathrm{Bin}(n,p_{1})\)</span>.</p></li>
<li><p>Marginal Distributions</p>
<ul>
<li>Result: Any set of two or more <span class="math inline">\(Y\)</span> s also has a multinomial distribution. For example, Consider the set <span class="math inline">\(Y_{1},\ldots, Y_{k}\)</span>. Let <span class="math inline">\(Z=Y_{4}+Y_{5}+\cdots +Y_{k}.\)</span> Then the triplet <span class="math inline">\(Y_{1},Y_{2},Y_{3}\)</span> has distribution <span class="math inline">\(Y_{1},Y_{2},Y_{3},Z \sim \mathrm{Multinom}(n,p_{1},p_{2},p_{3},\sum_{i=4}^{k}p_{i})\)</span>.</li>
<li>Justification: collapse categories 4 to <span class="math inline">\(k\)</span>.</li>
<li>Example: <span class="math inline">\(Y_{j}\sim Bin(n,p_{j})\)</span>. Accordingly, <span class="math inline">\(Y_{1},\ldots , Y_{k}\)</span> is a set of non-independent binomial random variables.</li>
<li><span class="math inline">\(E(Y_{j})=n p_{j}\)</span> and <span class="math inline">\(Var(Y_{j})=n p_{j}(1-p_{j})\)</span>.</li>
<li><span class="math inline">\(Y_{i}+Y_{j}\sim Bin(n,p_{i}+p_{j})\)</span> if <span class="math inline">\(i\neq j\)</span>.</li>
<li><span class="math inline">\(Cov(Y_{i},Y_{j})=-n p_{i}p_{j}\)</span> if <span class="math inline">\(i\neq j.\)</span> Justification:</li>
</ul></li>
</ul>
<p><span class="math display">\[
Var(Y_{i}+Y_{i})=n(p_{i}+p_{i})(1-p_{i}-p_{i})=Var(Y_{i})+Var(Y_{i})+2Cov(Y_{i},Y_{i})
\]</span></p>
<ul>
<li>Conditional Multinomial Distributions
<ul>
<li>Result: Suppose that</li>
</ul></li>
</ul>
<p><span class="math display">\[
(X_{1},\ldots,X_{k_{1}},Y_{1},\ldots,Y_{k_{2}},Z_{1},\ldots,Z_{k_{3}}) \sim Multinom(n,p_{11},\cdot\cdot\cdot,p_{1k_{1}},p_{21},\cdot\cdot\cdot,p_{2k_{2}},p_{31},\cdot\cdot\cdot,p_{3k_{3}})
\]</span></p>
<p>      Then the conditional distribution of <span class="math inline">\(X_{1},\ldots,X_{k_{1}}\)</span> given
<span class="math inline">\(Z_{1}=z_{1},Z_{2}=z_{2},\ldots,Z_{k_{3}}=z_{k_{3}}\)</span> is</p>
<p><span class="math display">\[
\begin{align}
(X_{1},\cdot\cdot\cdot,X_{k_{1}},Y|Z_{1}=z_{1},Z_{2}=z_{2},\cdots ,Z_{k_{3}}=z_{k_{3}}) \\ \sim\mathrm{Multinom}\left(n-\sum_{i=1}^{k_{3}}z_{i},\frac{p_{11}}{p_{1\cdot}+p_{2\cdot}},\cdots,\frac{p_{1k_{1}}}{p_{1\cdot}+p_{2\cdot}},\frac{p_{2\cdot}}{p_{1\cdot}+p_{2\cdot}}\right)
\end{align}
\]</span></p>
<p>      where <span class="math inline">\(Y=n-\sum z_{i}-\sum X_{i}\)</span>, <span class="math inline">\(p_{1\cdot}=\sum p_{1i}\)</span>, and <span class="math inline">\(p_{2\cdot}=\sum p_{2i\cdot}\)</span>.</p>
<ul>
<li>Example: Suppose <span class="math inline">\(X_{1},\cdots X_{4}\sim Multinom \left(30,{\frac{1}{2}},{\frac{1}{5}},{\frac{1}{4}},{\frac{1}{20}}\right)\)</span>. Then, the distribution of <span class="math inline">\(X_{1},X_{2}\)</span> given <span class="math inline">\(X_{4}=10\)</span> is</li>
</ul>
<p><span class="math display">\[
(X_{1},X_{2},Y | X_{4}=10)\sim\mathrm{Multinom}\left(20,\frac{0.5}{0.95},\frac{0.2}{0.95},\frac{0.25}{0.95}\right)
\]</span></p>
<ul>
<li>Example: Suppose <span class="math inline">\(X_{1},\cdots X_{4}\sim Multinom \left(30,{\frac{1}{2}},{\frac{1}{5}},{\frac{1}{4}},{\frac{1}{20}}\right)\)</span>. Then, the distribution of <span class="math inline">\(X_{1}\)</span> given <span class="math inline">\(X_{3}=2,X_{4}=12\)</span> is</li>
</ul>
<p><span class="math display">\[
(X_{1},Y|X_{3}=2,X_{4}=12)\sim\mathrm{Multinom}\left(16,\frac{0.5}{0.7},\frac{0.2}{0.7}\right)
\]</span></p>
<p><span class="math display">\[
\Longrightarrow X_{1}\sim\mathrm{{Bin}}\left(16,{\frac{5}{7}}\right)
\]</span></p>
<p><br></p>
</div>
<div id="using-probability-generating-functions" class="section level2 hasAnchor" number="4.11">
<h2><span class="header-section-number">4.11</span> Using Probability Generating Functions<a href="chapter4.html#using-probability-generating-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="factorial-moment-generating-function" class="section level4 unnumbered hasAnchor">
<h4>Factorial Moment Generating Function<a href="chapter4.html#factorial-moment-generating-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The f.m.g.f. is the same as the p.g.f., except that the expectation <span class="math inline">\(E(t^{X})\)</span> is required to exist for <span class="math inline">\(T\)</span> in an open neighborhood containing <span class="math inline">\(1\)</span>.</p></li>
<li><p>Differentiate the f.m.g.f. with respect to <span class="math inline">\(t\)</span> and evaluate at <span class="math inline">\(t = 1\)</span> to obtain</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp;\frac{d}{dt}\eta_{X}(t)\biggl|_{t=1}=\frac{d}{d t}\sum_{x\in S}t^{x}f_{X}(x)\biggl|_{t=1}=\sum_{x\in S}{\frac{d}{d t}}t^{x}f_{X}(x)\biggr|_{t=1}\\
&amp;=\sum_{x\in S}x t^{x-1}f_{X}(x)\biggr|_{t=1}=\sum_{x\in S}x f_{X}(x)=E(X)
\end{align}
\]</span></p>
<ul>
<li>Take the second derivative of the f.m.g.f. with respect to t and evaluate at <span class="math inline">\(t = 1\)</span> to obtain</li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp;\frac{d^{2}}{d t^{2}}\eta_{X}(t)\bigg|_{t=1}=\frac{d^{2}}{d t^{2}}\sum_{x\in S}t^{x}f_{X}(x)\bigg|_{t=1}=\sum_{x\in S}\frac{d^{2}}{d t^{2}}t^{x}f_{X}(x)\bigg|_{t=1}\\
&amp;=\sum_{x\in S}x(x-1)t^{x-2}f_{X}(x)\bigg|_{t=1}=\sum_{x\in S}x(x-1)f_{X}(x)\\
&amp;=E[X(X-1)]
\end{align}
\]</span></p>
<ul>
<li>In general,</li>
</ul>
<p><span class="math display">\[
\mathrm{E}\left[(X)_{r}\right]={\frac{d^{r}}{d t^{r}}}\eta_{X}(t)\biggl|_{t=1}\operatorname{for}\ r=1,2,\ldots
\]</span>
      if the expectation exists.</p>
<ul>
<li><span class="math inline">\(Var(X)=E[(X)_{2}]+ E(X)-[E(X)]^2\)</span>.</li>
</ul>
<!-------------------------------------->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Mathematical Statistics.pdf", "Mathematical Statistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
