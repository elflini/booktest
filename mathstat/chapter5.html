<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Continuous Random Variables | Mathematical Statistics</title>
  <meta name="description" content="This is a Mathematical Statistics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Continuous Random Variables | Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Mathematical Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Continuous Random Variables | Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is a Mathematical Statistics" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2024-08-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter4.html"/>
<link rel="next" href="chapter6.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#algebra-of-events"><i class="fa fa-check"></i><b>1.2</b> Algebra of Events</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#experiments-with-symmetries"><i class="fa fa-check"></i><b>1.3</b> Experiments with Symmetries</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#composition-of-experiments-counting-rules"><i class="fa fa-check"></i><b>1.4</b> Composition of Experiments: Counting Rules</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#sampling-at-random"><i class="fa fa-check"></i><b>1.5</b> Sampling at Random</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#binomial-multinomial-coefficients"><i class="fa fa-check"></i><b>1.6</b> Binomial &amp; Multinomial Coefficients</a></li>
<li class="chapter" data-level="1.7" data-path="chapter1.html"><a href="chapter1.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="1.8" data-path="chapter1.html"><a href="chapter1.html#subjective-probability"><i class="fa fa-check"></i><b>1.8</b> Subjective Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#probability-functions"><i class="fa fa-check"></i><b>2.1</b> Probability Functions</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Joint Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#conditional-probability"><i class="fa fa-check"></i><b>2.3</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#bayes-theorem-law-of-inverse-probability"><i class="fa fa-check"></i><b>2.4</b> Bayes Theorem (Law of Inverse Probability)</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#statistical-independence-of-random-variables"><i class="fa fa-check"></i><b>2.5</b> Statistical Independence of Random Variables</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#exchangeability"><i class="fa fa-check"></i><b>2.6</b> Exchangeability</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#application-probability-of-winning-in-craps"><i class="fa fa-check"></i><b>2.7</b> Application: Probability of Winning in Craps</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Expectations of Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#the-mean"><i class="fa fa-check"></i><b>3.1</b> The Mean</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#expectation-of-a-function"><i class="fa fa-check"></i><b>3.2</b> Expectation of a Function</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#variability"><i class="fa fa-check"></i><b>3.3</b> Variability</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#sums-of-random-variables"><i class="fa fa-check"></i><b>3.5</b> Sums of Random Variables</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#probability-generating-functions"><i class="fa fa-check"></i><b>3.6</b> Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Bernoulli and Related Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#sampling-bernoulli-populations"><i class="fa fa-check"></i><b>4.1</b> Sampling Bernoulli Populations</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#binomial-distribution"><i class="fa fa-check"></i><b>4.2</b> Binomial Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>4.3</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#negative-hypergeometric-distribution"><i class="fa fa-check"></i><b>4.6</b> Negative Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#approximating-binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Approximating Binomial Probabilities</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="chapter4.html"><a href="chapter4.html#normal-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.1</b> Normal approximation to the Binomial</a></li>
<li class="chapter" data-level="4.7.2" data-path="chapter4.html"><a href="chapter4.html#poisson-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.2</b> Poisson Approximation to the Binomial</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#poisson-distribution"><i class="fa fa-check"></i><b>4.8</b> Poisson Distribution</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#law-of-large-numbers"><i class="fa fa-check"></i><b>4.9</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="4.10" data-path="chapter4.html"><a href="chapter4.html#multinomial-distributions"><i class="fa fa-check"></i><b>4.10</b> Multinomial Distributions</a></li>
<li class="chapter" data-level="4.11" data-path="chapter4.html"><a href="chapter4.html#using-probability-generating-functions"><i class="fa fa-check"></i><b>4.11</b> Using Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.1</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#density-and-the-probability-element"><i class="fa fa-check"></i><b>5.2</b> Density and the Probability Element</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#the-median-and-other-percentiles"><i class="fa fa-check"></i><b>5.3</b> The Median and Other Percentiles</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#expected-value"><i class="fa fa-check"></i><b>5.4</b> Expected Value</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#expected-value-of-a-function"><i class="fa fa-check"></i><b>5.5</b> Expected Value of a Function</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#average-deviations"><i class="fa fa-check"></i><b>5.6</b> Average Deviations</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#bivariate-distributions"><i class="fa fa-check"></i><b>5.7</b> Bivariate Distributions</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#several-variables"><i class="fa fa-check"></i><b>5.8</b> Several Variables</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#covariance-and-correlation-1"><i class="fa fa-check"></i><b>5.9</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#independence"><i class="fa fa-check"></i><b>5.10</b> Independence</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#conditional-distributions"><i class="fa fa-check"></i><b>5.11</b> Conditional Distributions</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#moment-generating-functions"><i class="fa fa-check"></i><b>5.12</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Families of Continuous Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#normal-distributions"><i class="fa fa-check"></i><b>6.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#exponential-distributions"><i class="fa fa-check"></i><b>6.2</b> Exponential Distributions</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#gamma-distributions"><i class="fa fa-check"></i><b>6.3</b> Gamma Distributions</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#chi-squared-distributions"><i class="fa fa-check"></i><b>6.4</b> Chi Squared Distributions</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#distributions-for-reliability"><i class="fa fa-check"></i><b>6.5</b> Distributions for Reliability</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#t-f-and-beta-distributions"><i class="fa fa-check"></i><b>6.6</b> <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and Beta Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Organizing &amp; Describing Data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#frequency-distributions"><i class="fa fa-check"></i><b>7.1</b> Frequency Distributions</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#data-on-continuous-variables"><i class="fa fa-check"></i><b>7.2</b> Data on Continuous Variables</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#order-statistics"><i class="fa fa-check"></i><b>7.3</b> Order Statistics</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#data-analysis"><i class="fa fa-check"></i><b>7.4</b> Data Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#the-sample-mean"><i class="fa fa-check"></i><b>7.5</b> The Sample Mean</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#measures-of-dispersion"><i class="fa fa-check"></i><b>7.6</b> Measures of Dispersion</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#correlation"><i class="fa fa-check"></i><b>7.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Samples, Statistics, &amp; Sampling Distributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#random-sampling"><i class="fa fa-check"></i><b>8.1</b> Random Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#likelihood"><i class="fa fa-check"></i><b>8.2</b> Likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#sufficient-statistics"><i class="fa fa-check"></i><b>8.3</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#sampling-distributions"><i class="fa fa-check"></i><b>8.4</b> Sampling Distributions</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>8.5</b> Simulating Sampling Distributions</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#order-statistics-1"><i class="fa fa-check"></i><b>8.6</b> Order Statistics</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#moments-of-sample-means-and-proportionssp"><i class="fa fa-check"></i><b>8.7</b> Moments of Sample Means and Proportionssp</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>8.8</b> The Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#using-the-moment-generating-function"><i class="fa fa-check"></i><b>8.9</b> Using the Moment Generating Function</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#normal-populations"><i class="fa fa-check"></i><b>8.10</b> Normal Populations</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#updating-prior-probabilities-via-likelihood"><i class="fa fa-check"></i><b>8.11</b> Updating Prior Probabilities Via Likelihood</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#some-conjudate-families"><i class="fa fa-check"></i><b>8.12</b> Some conjudate Families</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#predictive-distributions"><i class="fa fa-check"></i><b>8.13</b> Predictive Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Estimation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#point-estimation"><i class="fa fa-check"></i><b>9.1</b> Point Estimation</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#errors-in-estimation"><i class="fa fa-check"></i><b>9.2</b> Errors in Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#consistency"><i class="fa fa-check"></i><b>9.3</b> Consistency</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#large-sample-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Large Sample Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#determining-sample-size"><i class="fa fa-check"></i><b>9.5</b> Determining Sample Size</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#small-sample-confidence-intervals-for-mu_x"><i class="fa fa-check"></i><b>9.6</b> Small Sample Confidence Intervals for <span class="math inline">\(\mu_X\)</span></a></li>
<li class="chapter" data-level="9.7" data-path="chapter9.html"><a href="chapter9.html#the-distribution-of-t"><i class="fa fa-check"></i><b>9.7</b> The Distribution of <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="9.8" data-path="chapter9.html"><a href="chapter9.html#pivotal-quantities"><i class="fa fa-check"></i><b>9.8</b> Pivotal Quantities</a></li>
<li class="chapter" data-level="9.9" data-path="chapter9.html"><a href="chapter9.html#estimating-a-mean-difference"><i class="fa fa-check"></i><b>9.9</b> Estimating a Mean Difference</a></li>
<li class="chapter" data-level="9.10" data-path="chapter9.html"><a href="chapter9.html#umvue"><i class="fa fa-check"></i><b>9.10</b> UMVUE</a></li>
<li class="chapter" data-level="9.11" data-path="chapter9.html"><a href="chapter9.html#bayes-estimators"><i class="fa fa-check"></i><b>9.11</b> Bayes Estimators</a></li>
<li class="chapter" data-level="9.12" data-path="chapter9.html"><a href="chapter9.html#efficiency"><i class="fa fa-check"></i><b>9.12</b> Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Significance Testing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chapter10.html"><a href="chapter10.html#hypotheses"><i class="fa fa-check"></i><b>10.1</b> Hypotheses</a></li>
<li class="chapter" data-level="10.2" data-path="chapter10.html"><a href="chapter10.html#assessing-the-evidence"><i class="fa fa-check"></i><b>10.2</b> Assessing the Evidence</a></li>
<li class="chapter" data-level="10.3" data-path="chapter10.html"><a href="chapter10.html#one-sample-z-tests"><i class="fa fa-check"></i><b>10.3</b> One Sample <span class="math inline">\(Z\)</span> Tests</a></li>
<li class="chapter" data-level="10.4" data-path="chapter10.html"><a href="chapter10.html#one-sample-t-tests"><i class="fa fa-check"></i><b>10.4</b> One Sample <span class="math inline">\(t\)</span> Tests</a></li>
<li class="chapter" data-level="10.5" data-path="chapter10.html"><a href="chapter10.html#some-nonparametric-tests"><i class="fa fa-check"></i><b>10.5</b> Some Nonparametric Tests</a></li>
<li class="chapter" data-level="10.6" data-path="chapter10.html"><a href="chapter10.html#probability-of-the-null-hypothesis"><i class="fa fa-check"></i><b>10.6</b> Probability of the Null Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Tests as Decision Rules</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#rejection-regions-and-errors"><i class="fa fa-check"></i><b>11.1</b> Rejection Regions and Errors</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#the-power-function"><i class="fa fa-check"></i><b>11.2</b> The Power function</a></li>
<li class="chapter" data-level="11.3" data-path="chapter11.html"><a href="chapter11.html#choosing-a-sample-size"><i class="fa fa-check"></i><b>11.3</b> Choosing a Sample Size</a></li>
<li class="chapter" data-level="11.4" data-path="chapter11.html"><a href="chapter11.html#most-powerful-tests"><i class="fa fa-check"></i><b>11.4</b> Most Powerful Tests</a></li>
<li class="chapter" data-level="11.5" data-path="chapter11.html"><a href="chapter11.html#uniformly-most-powerful-tests"><i class="fa fa-check"></i><b>11.5</b> Uniformly Most Powerful Tests</a></li>
<li class="chapter" data-level="11.6" data-path="chapter11.html"><a href="chapter11.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>11.6</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="11.7" data-path="chapter11.html"><a href="chapter11.html#bayesian-testing"><i class="fa fa-check"></i><b>11.7</b> Bayesian Testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chapter12.html"><a href="chapter12.html"><i class="fa fa-check"></i><b>12</b> Appendix</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chapter12.html"><a href="chapter12.html#greek-alphabet"><i class="fa fa-check"></i><b>12.1</b> Greek Alphabet</a></li>
<li class="chapter" data-level="12.2" data-path="chapter12.html"><a href="chapter12.html#abbreviations"><i class="fa fa-check"></i><b>12.2</b> Abbreviations</a></li>
<li class="chapter" data-level="12.3" data-path="chapter12.html"><a href="chapter12.html#practice-exams"><i class="fa fa-check"></i><b>12.3</b> PRACTICE EXAMS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter5" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Continuous Random Variables<a href="chapter5.html#chapter5" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="cumulative-distribution-function-cdf" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Cumulative Distribution Function (CDF)<a href="chapter5.html#cumulative-distribution-function-cdf" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: Let X be a random variable. Then the cdf of <span class="math inline">\(X\)</span> is denoted by <span class="math inline">\(F_{X}(x)\)</span> and defined by</li>
</ul>
<p><span class="math display">\[
F_{X}(x)=P(X\leq x)
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(X\)</span> is the only random variable under consideration, then <span class="math inline">\(F_{X}(x)\)</span> can be written as <span class="math inline">\(F(x)\)</span>.</p></li>
<li><p>Example: Discrete Distribution.</p>
<ul>
<li>Suppose that <span class="math inline">\(X\sim\mathrm{Bin}(3, 0.5)\)</span>. Then <span class="math inline">\(F(x)\)</span> is a step function and can be written as</li>
</ul></li>
</ul>
<p><span class="math display">\[
F(x)=\begin{cases}
{0}&amp;{x\in(-\infty,0)}\\ {{\frac{1}{2}}}&amp;{x\in[0,1)}\\ {{\frac{1}{8}}}&amp;{x\in[2,3)}\\ {1}&amp;{x\in[3,\infty)}
\end{cases}
\]</span></p>
<ul>
<li>Example: Continuous Distribution.
<ul>
<li>Consider modeling the probability of vehicle accidents on I-94 in the Gallatin Valley by a Poisson process with rate <span class="math inline">\(\lambda\)</span> per year.</li>
<li>Let <span class="math inline">\(T\)</span> be the time until the first accident. Then
<span class="math inline">\(P(T\leq t)=P\)</span>(at least one accident in time <span class="math inline">\(t\)</span>)
=<span class="math inline">\(1-P\)</span>(no accidents in time <span class="math inline">\(t\)</span>)<span class="math inline">\(=1-\frac{e^{-\lambda}\lambda^{0}}{0!}=1-e^{-\lambda t}\)</span>.</li>
<li>Therefore,</li>
</ul></li>
</ul>
<p><span class="math display">\[
F(t)=\begin{cases}
{{0}}&amp;{{t\lt 0}}\\ {{1-e^{-\lambda t}}}&amp;{{t\geq 0}}
\end{cases}
\]</span></p>
<ul>
<li>Example: Uniform Distribution.
<ul>
<li>Suppose that <span class="math inline">\(X\)</span> is a r.v with support <span class="math inline">\(S = [a, b]\)</span>, where <span class="math inline">\(b &gt; a\)</span>.</li>
<li>Further, suppose that the probability that <span class="math inline">\(X\)</span> falls in an interval in <span class="math inline">\(S\)</span> is proportional to the length of the interval. That is, <span class="math inline">\(P(x_{1}\leq X\leq x_{2})=\lambda(x_{2}-x_{1})\)</span> for <span class="math inline">\(a\leq x_{1}\leq x_{2}\leq b\)</span>.</li>
<li>To solve for <span class="math inline">\(\lambda\)</span>, let <span class="math inline">\(x_{1} = a\)</span> and <span class="math inline">\(x_{2} = b\)</span>. then</li>
</ul></li>
</ul>
<p><span class="math display">\[
P(a\leq X\leq b)=1=\lambda(b-a)\Longrightarrow\lambda=\frac{1}{b-a}
\]</span></p>
<ul>
<li>Accordingly, the cdf is</li>
</ul>
<p><span class="math display">\[
F(x)=P(X\leq x)=P(a\leq X\leq x)=\begin{cases}
{0}&amp;{x\lt a}\\ {\frac{x-a}{b-a}}&amp;{x\in[a,b]}\\ {1}&amp;{x\gt b}
\end{cases}
\]</span></p>
<ul>
<li>In this case, <span class="math inline">\(X\)</span> is said to have a uniform distribution: <span class="math inline">\(X\sim\mathrm{Unit}(a,b)\)</span>.</li>
</ul>
<div id="properties-of-a-cdf" class="section level4 unnumbered hasAnchor">
<h4>Properties of a cdf<a href="chapter5.html#properties-of-a-cdf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(F(-\infty)=0\)</span> and <span class="math inline">\(F(\infty)=1\)</span>.</p></li>
<li><p><span class="math inline">\(F\)</span> is non-decreasing; i.e., <span class="math inline">\(F(a)\leq F(b)\)</span> whenever <span class="math inline">\(a\leq b\)</span>.</p></li>
<li><p><span class="math inline">\(F(x)\)</span> is right continuous. That is,</p></li>
</ol>
<p><span class="math display">\[
\mathrm{lim}_{\epsilon\to0^{+}}F(x+\epsilon)=F(x)
\]</span></p>
<ul>
<li><p>Let <span class="math inline">\(X\)</span> be a r.v with cdf <span class="math inline">\(F(x)\)</span>.</p>
<ul>
<li>If <span class="math inline">\(b \ge a\)</span>, then <span class="math inline">\(P(a\lt X\leq b)=F(b)-F(a)\)</span>.</li>
<li>For any <span class="math inline">\(x\)</span>, <span class="math inline">\(P(X=x)=\mathrm*{lim}_{\epsilon\rightarrow0+}P(x-\epsilon\lt X\leq x)=F(x)-F(x-)\)</span>, where <span class="math inline">\(F(x−)\)</span> is <span class="math inline">\(F\)</span> evaluated as <span class="math inline">\(x −\epsilon\)</span> and <span class="math inline">\(\epsilon\)</span> is an infinitesimally small positive number. If the cdf of <span class="math inline">\(X\)</span> is continuous from the left, then <span class="math inline">\(F(x−) = F(x)\)</span> and <span class="math inline">\(P(X = x) = 0\)</span>.</li>
<li>If the cdf of <span class="math inline">\(X\)</span> has a jump at x, then <span class="math inline">\(F(x) − F(x−)\)</span> is the size of the jump.</li>
</ul></li>
<li><p>Definition of Continuous Distribution: The distribution of the r.v <span class="math inline">\(X\)</span> is said to be continuous if the cdf is continuous at each <span class="math inline">\(x\)</span> and the cdf is differentiable (except, possibly, at a countable number of points).</p></li>
<li><p>Monotonic transformations of a continuous rv: Let <span class="math inline">\(X\)</span> be a continuous rv with cdf <span class="math inline">\(F_{X}(x)\)</span>.</p></li>
<li><ol style="list-style-type: lower-alpha">
<li>Suppose that <span class="math inline">\(g(X)\)</span> is a continuous one-to-one increasing function.</li>
</ol>
<ul>
<li>Then for <span class="math inline">\(y\)</span> in the counter-domain (range) of <span class="math inline">\(g\)</span>, the inverse function <span class="math inline">\(x=g^{-1}(y)\)</span> exists.</li>
<li>Let <span class="math inline">\(Y = g(X)\)</span>. Find the cdf of <span class="math inline">\(Y\)</span>.</li>
<li>Solution:</li>
</ul></li>
</ul>
<p><span class="math display">\[
P(Y\leq y)=P[g(X)\leq y]=P(X\leq g^{-1}(y)]=F_{X}[g^{-1}(y)]
\]</span></p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Suppose that <span class="math inline">\(g(X)\)</span> is a continuous one-to-one decreasing function.</li>
</ol>
<ul>
<li>Then for <span class="math inline">\(y\)</span> in the counter-domain (range) of <span class="math inline">\(g\)</span>, the inverse function <span class="math inline">\(x=g^{-1}(y)\)</span> exists.</li>
<li>Let <span class="math inline">\(Y=g(X)\)</span>. Find the cdf of <span class="math inline">\(Y\)</span>.</li>
<li>Solution:</li>
</ul></li>
</ul>
<p><span class="math display">\[
P(Y\le y)=P[g(X)\le y]=P(X\gt g^{-1}(y)]=1-F_{X}[g^{-1}(y)]
\]</span></p>
<ul>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Example: Suppose that <span class="math inline">\(X\sim{\mathrm{Unit}}(0,1)\)</span>, and <span class="math inline">\(Y=g(X)=hX+k\)</span> where <span class="math inline">\(h&lt; 0\)</span>. Then,</li>
</ol></li>
</ul>
<p><span class="math display">\[X=g^{-1}(Y)=(Y-k)/h\]</span></p>
<p><span class="math display">\[F_{X}(x)=\left\{ \begin{cases}{\int_{0}}&amp;{x\lt 0;}\\ {x}&amp;{{}x\in[0,1];}\\ {1}&amp;{x\gt 1,}\end{cases} \right.\]</span></p>
<p><span class="math display">\[F_{Y}(y)=1-F_{x}[(y-k)/h]=\left\{\begin{cases}{{0}}&amp;{{y\lt h+k;}}\\ {{y-(h+k)}}&amp;{{y\in[h+k,k];}}\\ {{1}}&amp;{{y\gt k,}}\end{cases}\right.\]</span></p>
<ul>
<li><p>That is, <span class="math inline">\(Y\sim\mathrm{Unif}(h+k,k)\)</span>.</p></li>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Inverse CDF Transformation.</li>
</ol>
<ul>
<li>Suppose that <span class="math inline">\(X\)</span> is a continuous rv having a strictly increasing cdf <span class="math inline">\(F_{X}(x)\)</span>.
<ul>
<li><ol style="list-style-type: lower-roman">
<li>Recall that a strictly monotone function has an inverse.</li>
</ol>
<ul>
<li>Denote the inverse of the cdf by <span class="math inline">\(F_{X}^{-1}\)</span>. That is, if <span class="math inline">\(F_{X}(x) = y\)</span>, then <span class="math inline">\(F_{X}^{-1}(y)=x\)</span>. Let <span class="math inline">\(Y =F_{X}(X)\)</span>. Then the distribution of <span class="math inline">\(Y\)</span> is <span class="math inline">\(Unif(0, 1)\)</span>.</li>
<li>Proof: If W <span class="math inline">\(\sim Unif(0, 1)\)</span>, then the cdf of <span class="math inline">\(W\)</span> is <span class="math inline">\(F_W(w) = w\)</span>. The cdf of <span class="math inline">\(Y\)</span> is <span class="math inline">\(F_{Y}(y) = P(Y\leq y)=P(F_{X}(X)\leq y)=P[X\leq F_{X}^{-1}(y)]=\mathrm{F}_{X}\left[F_{X}^{-1}(y)\right]=y\)</span>.</li>
<li>If <span class="math inline">\(Y\)</span> has support [0, 1] and <span class="math inline">\(F_{Y}(y) = y\)</span>, then it must be true that <span class="math inline">\(Y \sim\)</span> Unif(0, 1).</li>
</ul></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li>Let <span class="math inline">\(U\)</span> be a rv with distribution <span class="math inline">\(U \sim\)</span> Unif(0, 1).</li>
</ol>
<ul>
<li>Suppose that <span class="math inline">\(F_{X}(x)\)</span> is a strictly increasing cdf for a continuous random variable <span class="math inline">\(X\)</span>. Then the cdf of the rv <span class="math inline">\(F_{X}^{-1}(U)\)</span> is <span class="math inline">\(F_{X}(x)\)</span>.</li>
<li>Proof: <span class="math inline">\(P\left[F_{X}^{-1}(U)\le x\right]=P\left[U\le F_{X}(x)\right]=F_{U}\left[F_{X}(x)\right]=F_{X}(x)\)</span> because <span class="math inline">\(F_{U}(u) = u\)</span>.</li>
</ul></li>
</ul></li>
</ul></li>
<li><ol start="5" style="list-style-type: lower-alpha">
<li>Application of inverse cdf transformation: Given <span class="math inline">\(U_{1},U_{2},\ldots,U_{n},\)</span> a random sample from Unif(0, 1), generate a random sample from <span class="math inline">\(F_{X}(x)\)</span>..</li>
</ol>
<ul>
<li>Solution: Let <span class="math inline">\(X_{i}=F_{X}^{-1}(U_{i})\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span>.
<ul>
<li><ol style="list-style-type: lower-roman">
<li>Example 1: Suppose that <span class="math inline">\(F_{X}(x)=1-e^{-\lambda x}\)</span> for <span class="math inline">\(x\gt 0,\)</span> where<span class="math inline">\(\lambda\gt 0\)</span>. Then <span class="math inline">\(X_{i}=-\ln(1-U_{i})/{\lambda}\)</span> for<span class="math inline">\(i=1,\ldots,n\)</span> is a random sample from <span class="math inline">\(F_X\)</span>.</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li>Example 2: Suppose that
<span class="math inline">\(F_{X}(x)=\left[1-\left(\frac{a}{x}\right)^{b}\right]I_{(a,\infty)}(x)\)</span>, where <span class="math inline">\(a &gt; 0\)</span> and <span class="math inline">\(b &gt; 0\)</span> are constants. Then <span class="math inline">\(X_{i}=a(1-U_{i})^{-b}\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span> is a random sample from <span class="math inline">\(F_X\)</span>.</li>
</ol></li>
</ul></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li>Non-monotonic transformations of a continuous rv.
<ul>
<li>Let <span class="math inline">\(X\)</span> be a continuous rv with cdf <span class="math inline">\(F_{X}(x)\)</span>. Suppose that <span class="math inline">\(Y = g(X)\)</span> is a continuous but non-monotonic function. As in the case of monotonic functions, <span class="math inline">\(F_{Y}(y)=P(Y\leq y)=P[g(X)\leq y],\)</span> but in this case each inverse solution <span class="math inline">\(x=g^{-1}(y)\)</span> must be used to find an expression for <span class="math inline">\(F_{Y}(y)\)</span> in terms of <span class="math inline">\(F_{X}[g^{-1}(y)]\)</span>.</li>
<li>For example, suppose that <span class="math inline">\(X \sim\)</span> Unif(−1, 2) and <span class="math inline">\(g(X) = Y = X^2\)</span>.</li>
<li>Note that <span class="math inline">\(x=\pm\sqrt{y}\)</span> for <span class="math inline">\(y\in[0,1]\)</span> and <span class="math inline">\(x=+{\sqrt{y}}\)</span> for <span class="math inline">\(y\in(1,4]\)</span>.</li>
<li>The cdf of <span class="math inline">\(Y\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[F_{Y}(y)=P(X^{2}\leq y)=\left\{ \begin{cases}{l l l r}{P(-\sqrt{y}\leq X\leq\sqrt{y})}&amp;{y\in[0,1];}\\ {P(X\leq\sqrt{y})}&amp;{y\in(1,4]}\end{cases}\right.\]</span></p>
<p><span class="math display">\[
=\left \{ \begin{cases}{l l l r}{{ F_{X}(\sqrt{y})-F_{X}(-\sqrt{y})}}&amp;{y\in[0,1];}\\{{F_{X}(\sqrt{y})}}&amp;{{y\in(1,4]}}\end{cases} \right.
\]</span></p>
<p><span class="math display">\[
=\left\{\begin{cases}{l l}{{0}}&amp;{{y\lt 0;}}\\ {{2\sqrt{y}/3}}&amp;{{y\in[0,1];}}\\ {{(\sqrt{y}+1)/3}}&amp;{{y\gt 4;}}\\ {{1}}&amp;{{y\gt 4;}}\end{cases}\right.
\]</span></p>
<ul>
<li>Plot the function <span class="math inline">\(g(x)\)</span> over <span class="math inline">\(x\in S x\)</span> as an aid to finding the inverse solutions <span class="math inline">\(x=g^{-1}(y)\)</span>.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="density-and-the-probability-element" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Density and the Probability Element<a href="chapter5.html#density-and-the-probability-element" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Mathematical Result: Assume that <span class="math inline">\(F(x)\)</span> is a continuous cdf. Let <span class="math inline">\(g(x + m)\)</span> be a differentiable function and let <span class="math inline">\(y = x + m\)</span>. Then, by the chain rule;</li>
</ul>
<p><span class="math display">\[
\left.\frac{d}{d m}g(x+m)\right|_{m=0}=\left.\frac{d}{d y}g(y)\times\frac{d}{d m}y\right|_{m=0}=\frac{d}{d y}g(y)\biggr|_{m=0}=\frac{d}{d x}g(x)
\]</span></p>
<ul>
<li>Probability Element: Suppose that X is a continuous rv. Let <span class="math inline">\(\Delta x\)</span> be a small positive number. Define <span class="math inline">\(h(a, b)\)</span> as</li>
</ul>
<p><span class="math display">\[
h(a,b)\ {\stackrel{\mathrm{def}}{=}}\ P(a\leq X\leq a+b)=F_{X}(a+b)-F_{X}(a)
\]</span></p>
<ul>
<li>Expand <span class="math inline">\(h(x,\Delta x)=P(x\leq X\leq x+\Delta x)\)</span> in a Taylor series around <span class="math inline">\(\Delta x=0\)</span>:</li>
</ul>
<p><span class="math display">\[h(x,\Delta x)={F(x+\Delta x)-F(x)}=\left. h(x,0)+\frac{d}{d\Delta x}h(x,\Delta x)\right|_{\Delta x=0}\Delta x+o(\Delta x)\]</span></p>
<p><span class="math display">\[=\left. 0+\frac{d}{d\Delta x}h(x,\Delta x)\right|_{\Delta x=0}\Delta x+o(\Delta x)= \left[\frac{d}{d x}F(x)\right]\Delta x+o(\Delta x)\]</span></p>
<p>where</p>
<p><span class="math display">\[\mathrm{lim}_{\Delta x\to0}{\frac{o(\Delta x)}{\Delta x}}=0\]</span></p>
<ul>
<li>The function <span class="math inline">\(d F(x)=\left[\frac{d}{d x}F(x)\right]\Delta x\)</span> is called the differential. In the field of statistics, the differential of a cdf is called the probability element.
<ul>
<li>The probability element is an approximation to <span class="math inline">\(h(x,\Delta x)\)</span>. Note that the probability element is a linear function of the derivative <span class="math inline">\({\frac{d}{d x}}F(x)\)</span>.</li>
</ul></li>
<li>Example; Suppose that <span class="math inline">\(F(x)=\begin{cases}{{0}}&amp;{{x\lt 0;}}\\ {{1-e^{-3x}}}&amp;{{\mathrm{otherwise.}}}\end{cases}\)</span>.
<ul>
<li>Note that <span class="math inline">\(F(x)\)</span> is a cdf.</li>
<li>Find the probability element at <span class="math inline">\(x = 2\)</span> and approximate the probability <span class="math inline">\(P(2\leq X\leq2.01)\)</span>.</li>
<li>Solution: <span class="math inline">\({\frac{d}{d x}}F(x)=3e^{-3x}\)</span> so the probability element is <span class="math inline">\(3e^{-6}\Delta x\)</span> and <span class="math inline">\(P(2\leq X\leq2.01)\approx3e^{-6}\times0.01=0.0007436\)</span>.</li>
<li>The exact probability is <span class="math inline">\(F(2.01) − F(2) = 0.00007326\)</span>.</li>
</ul></li>
<li>The average density in the interval <span class="math inline">\((x, x + \Delta x)\)</span> is defined as
Average density</li>
</ul>
<p><span class="math display">\[{\overset{\mathrm*{def}}{=}}{\frac{P(x\lt X\lt x+\Delta x)}{\Delta x}}\]</span></p>
<ul>
<li><p>Density: The probability density function (pdf) at <span class="math inline">\(x\)</span> is the limit of the average density as <span class="math inline">\(\Delta x\rightarrow0;\)</span></p></li>
<li><p>pdf
<span class="math display">\[f(x)\ {\stackrel{\mathrm{def}}{=}}\ \mathrm*{lim}_{\Delta x\to0}{\frac{P(x\leq X\leq x+\Delta x)}{\Delta x}}\]</span></p></li>
</ul>
<p><span class="math display">\[{\mathrm*{lim}_{x\to0}{\frac{F_{X}(x+\Delta x)-F_{X}(x)}{\Delta x} }}\]</span></p>
<p><span class="math display">\[=\frac {\left[{\frac{d}{d x}}F(x)\right]{\Delta x}+o(\Delta x)}{\Delta x}\]</span></p>
<p><span class="math display">\[={\frac{d}{d x}}F(x)\]</span></p>
<ul>
<li><p>Note that the probability element can be written as <span class="math inline">\(dF(x)=f(x)\Delta x\)</span>.</p></li>
<li><p>Example: Suppose that <span class="math inline">\(\lambda\)</span> is a positive real number. If</p></li>
</ul>
<p><span class="math display">\[F(x)=\left\{\begin{cases}{l l}{{1- e^{-\lambda x}}}&amp;{{x\geq0;}}\\ {{0}}&amp;{{\mathrm{otherwise.}}}\end{cases}\right.\]</span></p>
<p>then</p>
<p><span class="math display">\[F(x)={\frac{d}{d x}}F(x)=\left\{\begin{cases}{l l}{{\lambda e^{-\lambda x}}}&amp;{{x\geq0;}}\\ {{0}}&amp;{{\mathrm{otherwise.}}}\end{cases}\right.\]</span></p>
<ul>
<li>Example: If <span class="math inline">\(X \sim Unif(a, b)\)</span>, then</li>
</ul>
<p><span class="math display">\[F(x)={\left\{\begin{cases}{l l}{0}&amp;{x\lt a;}\\ {{\frac{x-a}{b-a}}}&amp;{x\in[a,b];}\\ {1}&amp;{x\gt b}\end{cases}\right.}\]</span></p>
<p>and</p>
<p><span class="math display">\[F(x)=\frac{d}{dx}F(x)={\left\{\begin{cases}{l l}{0}&amp;{x\lt a;}\\ {{\frac{1}{b-a}}}&amp;{x\in[a,b];}\\ {0}&amp;{x\gt b}\end{cases}\right.}\]</span></p>
<ul>
<li>Properties of a pdf
<ul>
<li><ol style="list-style-type: lower-roman">
<li><span class="math inline">\(f(x)\geq0\)</span> for all <span class="math inline">\(x\)</span>.</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li><span class="math inline">\(\int_{-\infty}^{\infty}f(x)=1\)</span></li>
</ol></li>
</ul></li>
</ul>
<p><br></p>
<ul>
<li>Relationship between pdf and cdf: If <span class="math inline">\(X\)</span> is a continuous rv with pdf <span class="math inline">\(f(x)\)</span> and cdf <span class="math inline">\(F(x)\)</span>, then</li>
</ul>
<p><span class="math display">\[f(x)=\frac{d}{d x}F(x), \,\,\,\, F(x)=\int_{-\infty}^{x}f(u)d u\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align}
P(a\lt X\lt b)&amp;=P(a\leq X\leq b)=P(a\lt X\leq b) \\
&amp;=P(a\leq X&lt;b)={F}(b)-{F}(a) \\
&amp;=\int_{a}^{b}f(x)d x
\end{align}
\]</span></p>
<ul>
<li>PDF example - Cauchy distribution. Let <span class="math inline">\(f(x) = c/(1 + x^2)\)</span> for <span class="math inline">\(-\infty\lt x\lt \infty\)</span> and where <span class="math inline">\(c\)</span> is a constant. Note that <span class="math inline">\(f(x)\)</span> is nonnegative and</li>
</ul>
<p><span class="math display">\[\int_{-\infty}^{\infty}\frac{1}{1+x^{2}}d x=\arctan(x){\bigg|}_{-\infty}^{\infty}=\frac{\pi}{2}-\frac{-\pi}{2}=\pi\]</span></p>
<p>Accordingly, if we let <span class="math inline">\(c = 1/\pi\)</span>, then</p>
<p><span class="math display">\[f(x)=\frac{1}{\pi(1+x^{2})}\]</span></p>
<p>is a pdf. It is called the Cauchy pdf. The corresponding CDF is</p>
<p><span class="math display">\[F(x)\left. = int_{-\infty}^{x}\frac{1}{\pi(1+u^{2})}d u=\frac{\arctan(u)}{\pi}\right|_{-\infty}^{x}\]</span></p>
<p><span class="math display">\[={\frac{1}{\pi}}\left[\mathrm{arctan}(x)+{\frac{\pi}{2}}\right]={\frac{\mathrm{arctan}(x)}{\pi}}+{\frac{1}{2}}\]</span></p>
<ul>
<li>PDF example - Gamma distribution: A more general waiting time distribution: - Let <span class="math inline">\(T\)</span> be the time of arrival of the <span class="math inline">\(r^{\mathrm{th}}\)</span> event in a Poisson process with rate parameter <span class="math inline">\(\lambda\)</span>. Find the pdf of <span class="math inline">\(T\)</span>.
<ul>
<li>Solution:<span class="math inline">\(T\in(t,t+\Delta t)\)</span> if and only if (a) <span class="math inline">\(r − 1\)</span> events occur before time <span class="math inline">\(t\)</span> and (b) one event occurs in the interval <span class="math inline">\((t,t+ \Delta t)\)</span>. The probability that two or more events occur in <span class="math inline">\((t,t+ \Delta t)\)</span> is <span class="math inline">\(o\Delta t\)</span> and can be ignored. By the Poisson assumptions, outcomes (a) and (b) are independent and the probability of outcome (b) is <span class="math inline">\(\lambda\Delta t+o(\Delta t)\)</span>. Accordingly,</li>
</ul></li>
</ul>
<p><span class="math display">\[P(t\lt T\lt t+\Delta t)\approx f(t)\Delta t=\frac{e^{-\lambda t}(\lambda t)^{r-1}}{(r-1)!}\times\lambda\Delta t\]</span></p>
<p><span class="math display">\[=\left[\frac{e^{-\lambda t}\lambda^{r}t^{r-1}}{(r-1)!}\right]\Delta t\]</span></p>
<p>and the pdf is</p>
<p><span class="math display">\[f(t)=\left\{{\begin{cases}{l l}{0}&amp;{t\lt 0;}\\ {\frac{e^{-\lambda t}\lambda^{r}t^{r-1}}{(r-1)}}&amp;{t\ge 0}\end{cases}}\right.\]</span></p>
<p><span class="math display">\[=\frac{e^{-\lambda t}\lambda^{r}t^{r-1}}{\Gamma(r)} I_{[0,\infty)}(t)\]</span></p>
<ul>
<li>Transformations with Single-Valued Inverses: If <span class="math inline">\(X\)</span> is a continuous random variable with pdf <span class="math inline">\(f_{X}(x)\)</span> and <span class="math inline">\(Y = g(X)\)</span> is a single-valued differentiable function of <span class="math inline">\(X\)</span>, then the pdf of <span class="math inline">\(Y\)</span> is</li>
</ul>
<p><span class="math display">\[f_{Y}(y)=f_{X}\left[g^{-1}(y)\right]\left|\frac{d}{d y}g^{-1}(y)\right|\]</span></p>
<p>for <span class="math inline">\(y\in{S}_{g(x)}\)</span> (i.e., support of <span class="math inline">\(Y = g(X)\)</span>). The term</p>
<p><span class="math display">\[J(y)={\frac{d}{d y}}g^{-1}(y)\]</span></p>
<p>is called the Jacobian of the transformation.</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Justification 1: Suppose that<span class="math inline">\(Y = g(X)\)</span> is strictly increasing. Then <span class="math inline">\(F_{Y}(y)=F_{X}[g^{-1}(y)]\)</span> and</li>
</ol></li>
</ul>
<p><span class="math display">\[f_{Y}(y)={\frac{d}{d y}}F_{Y}(y)=f_{X}[g^{-1}(y)]\frac{d}{d y}g^{-1}(y)\]</span></p>
<p><span class="math display">\[= f_{X}[g^{-1}(y)]\left|\frac{d}{d y}g^{-1}(y)\right|\]</span></p>
<p>because the Jacobian is positive.</p>
<ul>
<li>Suppose that <span class="math inline">\(Y = g(X)\)</span> is strictly decreasing. Then <span class="math inline">\(F_{Y}(y)=1-F_{X}[g^{-1}(y)]\)</span> and</li>
</ul>
<p><span class="math display">\[f_{Y}(y)={\frac{d}{d y}}[1-F_{Y}(y)]=-f_{X}[g^{-1}(y)]\frac{d}{d y}g^{-1}(y)\]</span></p>
<p><span class="math display">\[= f_{X}[g^{-1}(y)]\left|\frac{d}{d y}g^{-1}(y)\right|\]</span></p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Justification 2: Suppose that <span class="math inline">\(g(x)\)</span> is strictly increasing. Recall that</li>
</ol></li>
</ul>
<p><span class="math display">\[P(x\leq X\leq x+\Delta x)=f_{X}(x)\Delta x+o(\Delta x)\]</span></p>
<p>Note that</p>
<p><span class="math display">\[x\leq X\leq x+\Delta x\Longleftrightarrow g(x)\leq g(x)\leq g(x+\Delta x)\]</span></p>
<p>Accordingly,</p>
<p><span class="math display">\[P(x\leq X\leq x+\Delta x)=P(y\leq Y\leq y+\Delta y)=f_{Y}(y)\Delta y+o(\Delta y)=f_{X}(x)\Delta x+o(\Delta x)\]</span></p>
<p>where <span class="math inline">\(y+\Delta y=g(x+\Delta x)\)</span>.</p>
<p>Expanding <span class="math inline">\(g(x+\Delta x)\)</span> around <span class="math inline">\(\Delta x=0\)</span> reveals that</p>
<p><span class="math display">\[y+\Delta y=g(x+\Delta x)=g(x)+{\frac{dg(x)}{dx}}\Delta x+o(\Delta x)\]</span></p>
<p>Also,</p>
<p><span class="math display">\[y=g(x)\Longrightarrow g^{-1}(y)=x\]</span></p>
<p><span class="math display">\[\Longrightarrow{\frac{d{g}^{-1}(y)}{d y}}={\frac{d x}{d y}}\]</span></p>
<p><span class="math display">\[\Longrightarrow{\frac{d y}{d x}}={\frac{d\,g(x)}{d\,x}}=\left[{\frac{dg^{-1}(y)}{dy}}\right]^{-1}\]</span></p>
<p><span class="math display">\[\Longrightarrow y+\Delta y=g(x)+\left[{\frac{d g^{-1}(y)}{d y}}\right]^{-1}\Delta x\]</span></p>
<p><span class="math display">\[\Longrightarrow \Delta y=\left[{\frac{d g^{-1}(y)}{d y}}\right]^{-1}\Delta x\]</span></p>
<p><span class="math display">\[\Longrightarrow\Delta x=\frac{dg^{-1}(\,y)}{d\nu}\Delta y\]</span></p>
<p>Lastly, equating <span class="math inline">\(f_{X}(x)\Delta x\)</span> to <span class="math inline">\(f_{Y}(y)\Delta y\)</span> reveals that</p>
<p><span class="math display">\[f_{Y}(y)\Delta y=f_{X}(x)\Delta x=f_{X}\left[g^{-1}(y)\right]\Delta xf_{X}\left[g^{-1}(y)\right]\frac{d g^{-1}(y)}{d y}\Delta y\]</span></p>
<p><span class="math display">\[\Longrightarrow f_{Y}(y)=f_{X}\left[g^{-1}(y)\right]\frac{dg^{-1}(y)}{dy}\]</span></p>
<ul>
<li>The Jacobian <span class="math inline">\(\textstyle{\frac{d g^{-1}+(y)}{d y}}\)</span> is positive for an increasing function, so the absolute value operation is not necessary.
<ul>
<li>A similar argument can be made for the case when g(x) is strictly decreasing.</li>
</ul></li>
<li>Transformations with Multiple-Valued Inverses: If <span class="math inline">\(g(x)\)</span> has more than one inverse function, then a separate probability element must be calculated for each of the inverses.
<ul>
<li>For example, suppose that <span class="math inline">\(X \sim\)</span> Unif(−1, 2) and<span class="math inline">\(Y = g(X) = X^{2}\)</span>.</li>
<li>There are two inverse functions for<span class="math inline">\(y\in[0,1]\)</span>, namely <span class="math inline">\(x=-\sqrt{y}\)</span> and <span class="math inline">\(x=+\sqrt{y}\)</span>.</li>
<li>There is a single inverse function for <span class="math inline">\(y\in[1,4]\)</span>.</li>
<li>The pdf of <span class="math inline">\(Y\)</span> is found as</li>
</ul></li>
</ul>
<p><span class="math display">\[f(y)=\left\{\begin{cases}{l l}{{0}}&amp;{{y\lt 0;}}\\{f(-{\sqrt{y}})\left|{\frac{-d{\sqrt{y}}}{dy}}\right|+f({\sqrt{y}})\left|{\frac{d{\sqrt{y}}}{dy}}\right|}&amp;{{y\in[0,1];}}\\{f({\sqrt{y}})\left|{\frac{d{\sqrt{y}}}{d y}}\right|}&amp;{{y\in(1,4];}}\\{0}&amp;{y\gt4}\end{cases}\right.\]</span></p>
<p><span class="math display">\[=\left\{\begin{cases}{c c}{{0}}&amp;{{y\lt 0;}}\\{{\frac{1}{8\sqrt{y}}}}&amp;{{y\in[0,1];}}\\{{\frac{1}{6\sqrt{y}}}}&amp;{{y\in(1,4];}}\\{0}&amp;{y\gt4}\end{cases}\right.\]</span></p>
<p><br></p>
</div>
<div id="the-median-and-other-percentiles" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> The Median and Other Percentiles<a href="chapter5.html#the-median-and-other-percentiles" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: The number xp is said to be the <span class="math inline">\(100p^{\mathrm{th}}\)</span> percentile of the distribution of <span class="math inline">\(X\)</span> if <span class="math inline">\(x_p\)</span> satisfies</li>
</ul>
<p><span class="math display">\[F_{X}(x_{p})=P(X\leq x_{p})=p\]</span></p>
<ul>
<li>If the cdf <span class="math inline">\(F_{X}{(x)}\)</span> is strictly increasing, then <span class="math inline">\(x_{p}=F_{X}^{-1}(p)\)</span> and <span class="math inline">\(x_{p}\)</span> is unique.</li>
<li>If <span class="math inline">\(F_{X}{(x)}\)</span> is not strictly increasing, then <span class="math inline">\(x_{p}\)</span> may not be unique.</li>
<li>Median: The median is the <span class="math inline">\(50^{\mathrm{th}}\)</span> percentile (i.e., p = 0.5).</li>
<li>Quartiles: The first and third quartiles are <span class="math inline">\(x_{0.25}\)</span> and <span class="math inline">\(x_{0.75}\)</span> respectively.</li>
<li>Example: If <span class="math inline">\(X\sim\)</span> Unif(a, b), then <span class="math inline">\((x_{p}-a)/(b-a)=p;\,x_{p}=a+p(b-a);\)</span> and <span class="math inline">\(x_{0.5}=(a+b)/2\)</span>.</li>
<li>Example: If <span class="math inline">\(F_{X}(x)=1-e^{-\lambda x}\)</span> (i.e., waiting time distribution), then <span class="math inline">\(1-e^{-\lambda x_{p}}=p;\,x_{p}=-\ln(1-p)/\lambda;\)</span> and <span class="math inline">\(x_{0.5}=\mathrm{ln}(2)/\lambda\)</span>.</li>
<li>Example—Cauchy: Suppose that <span class="math inline">\(X\)</span> is a random variable with pdf <span class="math inline">\(f(x)={\frac{1}{\sigma\pi\left[1+{\frac{(x-\mu)^{2}}{\sigma^{2}}}\right]}},\)</span> where <span class="math inline">\(-\infty\lt x\lt \infty ;\sigma\gt 0;\)</span> and <span class="math inline">\(\mu\)</span> is a finite number. Then</li>
</ul>
<p><span class="math display">\[F(x)~~=~\int_{-\infty}^{x}f(u)d u=\int_{-\infty}^{{\frac{x-\mu}{\pi}}}{\frac{1}{\pi(1+z^{2})}}d z\]</span></p>
<p><span class="math display">\[\left(\mathrm*{make}\,\mathrm{the\ change\ of\ variable\ from\ x\ to\ z={\frac{x-\mu}{\sigma}}}\right)\]</span></p>
<p><span class="math display">\[=\;\frac{1}{\pi}\arctan\Bigg(\frac{x-\mu}{\sigma}\Bigg)+\frac{1}{2}\]</span></p>
<p>Accordingly,
<span class="math display">\[F(x_{p})=p\Longrightarrow x_{p}=\mu+\sigma\tan\left[\pi(p-0.5)\right];\]</span></p>
<p><span class="math display">\[x_{0.25}=\mu+\sigma\:\mathrm{tan}\left[\pi(0.25-0.5)\right]=\mu-\sigma;\]</span></p>
<p><span class="math display">\[x_{0.5}=\mu+\sigma\:\mathrm{tan}(0)=\mu;\]</span></p>
<p><span class="math display">\[x_{0.75}=\mu+\sigma\:\mathrm{tan}\left[\pi(0.75-0.5)\right]=\mu+\sigma\]</span></p>
<ul>
<li><p>Definition Symmetric Distribution: A distribution is said to be symmetric around <span class="math inline">\(c\)</span> if <span class="math inline">\(F_{X}(c-\delta)=1-F_{X}(c+\delta)\)</span> for all <span class="math inline">\(\delta\)</span>.</p></li>
<li><p>Definition Symmetric Distribution: A distribution is said to be symmetric around <span class="math inline">\(c\)</span> if <span class="math inline">\(f_{X}(c-\delta)=f_{X}(c+\delta)\)</span> for all <span class="math inline">\(\delta\)</span>.</p></li>
<li><p>Median of a symmetric distribution. Suppose that the distribution of <span class="math inline">\(X\)</span> is symmetric around <span class="math inline">\(c\)</span>. Then, set <span class="math inline">\(\delta\)</span> to <span class="math inline">\(c-x_{0.5}\)</span> to obtain</p></li>
</ul>
<p><span class="math display">\[F_{X}(x_{0,5})={\frac{1}{2}}=1-F_{X}(2c-x_{0.5})\Longrightarrow F_{X}(2c-x_{0.5})={\frac{1}{2}}\Longrightarrow c=x_{0,.5}\]</span></p>
<ul>
<li>That is, if the distribution of <span class="math inline">\(X\)</span> is symmetric around <span class="math inline">\(c\)</span>, then the median of the distribution is <span class="math inline">\(c\)</span>.</li>
</ul>
<p><br></p>
</div>
<div id="expected-value" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Expected Value<a href="chapter5.html#expected-value" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Definition: Let <span class="math inline">\(X\)</span> be a rv with pdf <span class="math inline">\(f(x)\)</span>. Then the expected value (or mean)</li>
</ol>
<p>of <span class="math inline">\(X\)</span>, if it exists, is</p>
<p><span class="math display">\[\mathrm{E}(X)=\mu_{X}=\int_{-\infty}^{\infty}x f(x)d x\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The expectation is said to exist if the integral of the positive part of the function is finite and the integral of the negative part of the function is finite.</li>
</ol>
</div>
<div id="expected-value-of-a-function" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Expected Value of a Function<a href="chapter5.html#expected-value-of-a-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Let <span class="math inline">\(X\)</span> be a rv with pdf <span class="math inline">\(f(x)\)</span>. Then the expected value of <span class="math inline">\(g(X)\)</span>, if it exists, is</li>
</ul>
<p><span class="math display">\[\mathrm{E}[g(X)]=\int_{-\infty}^{\infty}g(x)f(x)d x\]</span></p>
<ul>
<li>Linear Functions. The integral operator is linear. If <span class="math inline">\(g_{1}(X)\)</span> and <span class="math inline">\(g_{2}(X)\)</span> are functions whose expectation exists and <span class="math inline">\(a, b, c\)</span> are constants, then</li>
</ul>
<p><span class="math display">\[\mathrm{E}\left[a g_{1}(X)+b g_{2}(X)+c\right]=a\mathrm{E}\left[g_{1}(X)\right]+b\mathrm{E}\left[g_{2}(X)\right]+c\]</span></p>
<ul>
<li>Symmetric Distributions: If the distribution of <span class="math inline">\(X\)</span> is symmetric around <span class="math inline">\(c\)</span> and the expectation exists, then <span class="math inline">\(\mathrm{E}(X) = c\)</span>. Proof. Assume that the mean exists. First, show that <span class="math inline">\(\mathrm{E}(X-c) = 0:\)</span></li>
</ul>
<p><span class="math display">\[\mathrm{E}(X-c)=\int_{-\infty}^{\infty}(x-c)f(x)d x=\int_{-\infty}^{c}(x-c)f(x)d x+\int_{c}^{\infty}(x-c)f(x)d x\]</span></p>
<p>( let <span class="math inline">\(x = c − u\)</span> in integral 1 and let <span class="math inline">\(x = c + u\)</span> in integral 2)</p>
<p><span class="math display">\[=-\int_{0}^{\infty}u f(c-u)d u+\int_{0}^{\infty}u f(c+u)d u=\int_{0}^{\infty}u\left[f(c+u)-f(c-u)\right]d u=0\]</span></p>
<p>by symmetry of the pdf around c. Now use <span class="math inline">\(\mathrm{E}(X-c)=0\Longleftrightarrow \mathrm{E}(X)=c\)</span>.</p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X\sim\)</span> Unif<span class="math inline">\((a, b)\)</span>. That is,</li>
</ul>
<p><span class="math display">\[\left\{\begin{cases}{cc}{\frac{1}{b-a}} &amp; {x\in[a,b];} \\{0} &amp; {\mathrm{otherwise}.} \\\end{cases}\right.\]</span></p>
<ul>
<li>A sketch of the pdf shows that the distribution is symmetric around <span class="math inline">\((a + b)/2\)</span>. More formally,</li>
</ul>
<p><span class="math display">\[f\left({\frac{a+b}{2}}-\delta\right)=
f\left({\frac{a+b}{2}}+\delta\right)=
\left\{ \begin{cases}{ll}{\frac{1}{b-a}} &amp; {\delta\in[-\frac{b-a}{2},\frac{b-a}{2}];} \\ {0} &amp; {\mathrm{otherwise}.} \end{cases} \right.\]</span></p>
<ul>
<li>Accordingly, <span class="math inline">\(\mathrm{E}(X)=(a+b)/2\)</span>. Alternatively, the expectation can be found by integrating <span class="math inline">\(xf(x)\)</span>:</li>
</ul>
<p><span class="math display">\[\mathrm{E}(X)=\int_{-\infty}^{\infty}x f(x)\,d x=\ \int_{a}^{b}{\frac{x}{b-a}}d x=\left.{\frac{x^{2}}{2(b-a)}}\right|_{a}^{b}={\frac{b^{2}-a^{2}}{2(b-a)}}=\frac{(b-a)(b+a)}{2(b-a)}=\frac{a+b}{2}\]</span></p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X\)</span> has a Cauchy distribution. The pdf is</li>
</ul>
<p><span class="math display">\[f(x)=\frac{1}{\sigma\pi\left[1+\frac{(x-\mu)^{2}}{\sigma^{2}}\right]},\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are constants that satisfy <span class="math inline">\(|\mu|\lt \infty\)</span> and <span class="math inline">\(\sigma\in(0,\infty)\)</span>.</p>
<ul>
<li>By inspection, it is apparent that the pdf is symmetric around <span class="math inline">\(\mu\)</span>. Nonetheless, the expectation is not <span class="math inline">\(\mu\)</span>, because the expectation does not exist. That is,</li>
</ul>
<p><span class="math display">\[\int_{-\infty}^{\infty}x f(x)d x=\int_{-\infty}^{\infty}\frac{x}{\sigma\pi\left[1+\frac{(x-\mu)^{2}}{\sigma^{2}}\right]}d x=\mu+\sigma\int_{-\infty}^{\infty}\frac{z}{\pi(1+z^{2})}d z, \,\,\,\, z=\frac{x-\mu}{\sigma}\]</span></p>
<p><span class="math display">\[=\mu+\sigma\int_{-\infty}^{0}\frac{z}{\pi(1+z^{2})}dz+\sigma\int_{0}^{\infty}\frac{z}{\pi(1+z^{2})}d z\]</span></p>
<p><span class="math display">\[=\left. \mu+\sigma\frac{\mathrm{ln}(1+z^{2})}{2\pi} \right|_{-\infty}^{0}+\left.\sigma\frac{\mathrm{ln}(1+z^{2})}{2\pi}\right|_{0}^{\infty}\]</span></p>
<p>and neither the positive nor the negative part is finite.</p>
<ul>
<li>Example: Waiting time distribution. Suppose that <span class="math inline">\(X\)</span> is a rv with pdf <span class="math inline">\(\lambda e^{-\lambda x}\)</span> for <span class="math inline">\(x\gt 0\)</span> and where <span class="math inline">\(\lambda\gt 0\)</span>. Then, using integration by parts,</li>
</ul>
<p><span class="math display">\[{\mathrm{E}(X)}={\int_{0}^{\infty}x\lambda e^{-\lambda x}d x=-x e^{-\lambda x}{\bigg|}_{0}^{\infty}+\int_{0}^{\infty}e^{-\lambda x}d x}=0-\frac{1}{\lambda}e^{-\lambda x}\bigg|_{0}^{\infty}=\frac{1}{\lambda}\]</span></p>
<p><br></p>
</div>
<div id="average-deviations" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Average Deviations<a href="chapter5.html#average-deviations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="variance" class="section level4 unnumbered hasAnchor">
<h4>Variance<a href="chapter5.html#variance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Definition:</li>
</ol></li>
</ul>
<p><span class="math display">\[\mathrm{Var}(X)\stackrel{\mathrm{def}}{=}\mathrm{E}(X-\mu_{X})^{2}=\int_{-\infty}^{\infty}(x-\mu_{X})^{2}f(x)d x\]</span></p>
<p>if the expectation exists. It is conventional to denote the variance of <span class="math inline">\(X\)</span> by <span class="math inline">\(\sigma_{X}^{2}\)</span>.</p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Computational formula: Be able to verify that
<span class="math inline">\(Var(X)= E(X^2) − [E(X)]^2\)</span></li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Example: Suppose that <span class="math inline">\(X \sim Unif(a, b)\)</span>. Then</li>
</ol></li>
</ul>
<p><span class="math display">\[\mathrm{E}(X^{r})=\int_{a}^{b}{\frac{x^{r}}{b-a}}dx={\frac{x^{r+1}}{(r+1)(b-a)}}{\bigg|}_{a}^{b}={\frac{b^{r+1}-a^{r+1}}{(r+1)(b-a)}}\]</span></p>
<p>Accordingly, <span class="math inline">\(\mu_{X}=(a+b)/2,\)</span></p>
<p><span class="math display">\[\mathrm{E}(X^{2}){=}{\frac{b^{3}-a^{3}{3(b-a)}}={\frac{(b-a)(b^{2}+a b+a^{2})}{3(b-a)}}={\frac{b^{2}+a b+a}{3}}}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathrm{Var}(X)=\frac{b^{2}+a b+a^{2}}{3}-\frac{(b+a)^{2}}{4}=\frac{b^{2}-2a b+a^{2}}{12}=\frac{(b-a)^{2}}{12}\]</span></p>
<ul>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Example: Suppose that <span class="math inline">\(f(x)=\lambda e^{-\lambda x}\)</span> for <span class="math inline">\(x\gt 0\)</span> and where <span class="math inline">\(\lambda\gt 0\)</span>. Then <span class="math inline">\(E(X) = 1/λ\)</span>,</li>
</ol></li>
</ul>
</div>
<div id="mad" class="section level4 unnumbered hasAnchor">
<h4>MAD<a href="chapter5.html#mad" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Definition:</li>
</ol></li>
</ul>
<p><span class="math display">\[\mathrm{Mad}(X){\stackrel{\mathrm{def}}{=}}\mathrm{E}(|X-\mu_{X}|)=\int_{-\infty}^{\infty}|x-\mu_{X}|f(x)d x\]</span></p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Alternative expression: First, note that</li>
</ol></li>
</ul>
<p><span class="math display">\[\mathrm{E}(|X-c|)=\int_{-\infty}^{c}\left(c-x\right)f(x)d x+\int_{c}^{\infty}(x-c)f(x)d x\]</span></p>
<p><span class="math display">\[=c\left[2F_{X}(c)-1\right]-\int_{-\infty}^{c}x f(x)d x+\int_{c}^{\infty}x f(x)d x\]</span></p>
<p>Accordingly,</p>
<p><span class="math display">\[\mathrm{Mad}(X)=\mu_{X}\left[2F_{X}(\mu_{X})-1\right]-\int_{-\infty}^{\mu_{X}}x f(x)d x+\int_{\mu_{X}}^{\infty}x f(x)d x\]</span></p>
<ul>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Leibnitz’s Rule: Suppose that <span class="math inline">\(a(\theta)\)</span>, <span class="math inline">\(b(\theta)\)</span>, and <span class="math inline">\(g(x,\theta)\)</span> are differentiable functions of <span class="math inline">\(\theta\)</span>. Then</li>
</ol></li>
</ul>
<p><span class="math display">\[{\frac{d}{d\theta}}\int_{a(\theta)}^{b(\theta)}g(x,\theta)d x=g\left[b(\theta),\theta\right]{\frac{d}{d\theta}}b(\theta)-g\left[a(\theta),\theta\right]{\frac{d}{d\theta}}a(\theta)+\int_{a(\theta)}^{b(\theta)}\frac{d}{d\theta}g(x,\theta)d x\]</span></p>
<ul>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Result: If the expectation E(<span class="math inline">\(|X − c|\)</span>) exists, then the minimizer of E(<span class="math inline">\(|X − c|\)</span>) with respect to <span class="math inline">\(c\)</span> is <span class="math inline">\(c=F_{X}^{-1}(0.5)\)</span> = median of <span class="math inline">\(X\)</span>.
Proof:: Set the derivative of E(<span class="math inline">\(|X − c|)\)</span> to zero and solve for <span class="math inline">\(c\)</span>:</li>
</ol></li>
</ul>
<p><span class="math display">\[{\frac{d}{d c}}\mathbf{E}(\left|X-c\right|)={\frac{d}{d c}}\left\{c\left[2F_{X}(c)-1\right]-\int_{-\infty}^{c}x f_{X}(x)d x+\int_{c}^{\infty}x f_{X}(x)d x\right\}\]</span></p>
<p><span class="math display">\[=2F_{X}(c)-1+2c f_{X}(c)-c f_{X}(c)-c f_{X}(c)-c f_{X}(c)=2F_{X}(c)-1\]</span></p>
<ul>
<li>Equating the derivative to zero and solving for c reveals that <span class="math inline">\(c\)</span> is a solution to <span class="math inline">\(F_{X}(c)=0.5\)</span>. That is, <span class="math inline">\(c\)</span> is the median of <span class="math inline">\(X\)</span>. Use the second derivative test to verify that the solution is a minimizer:</li>
</ul>
<p><span class="math display">\[\frac{d^{2}}{d c^{2}}\mathrm{E}(|X-c|)=\frac{d}{d c}\left[2F_{X}(c)-1\right]=2f_{X}(c)\gt 0\]</span></p>
<p><span class="math inline">\(\Longrightarrow c\)</span> is a minimizer.</p>
<ul>
<li><ol start="5" style="list-style-type: lower-alpha">
<li>Example: Suppose that <span class="math inline">\(X\sim Unif(a, b)\)</span>. Then <span class="math inline">\(\textstyle F_{\,X}\left(\frac{a+b}{2}\right) =0.5\)</span> and</li>
</ol></li>
</ul>
<p><span class="math display">\[Mad(X) = -\int_{a}^{\frac{a+b}{2}}{\frac{x}{b-a}}d x+\int_{\frac{a+b}{2}}^{b}{\frac{x}{b-a}}d x={\frac{b-a}{4}}\]</span></p>
<ul>
<li><ol start="6" style="list-style-type: lower-alpha">
<li>Example: Suppose that <span class="math inline">\(f_{X}(x)=\lambda e^{-\lambda x}\)</span> for <span class="math inline">\(x\gt 0\)</span> and where <span class="math inline">\(\lambda\gt 0\)</span>. Then E(X) = <span class="math inline">\(1/\lambda\)</span>, Median(<span class="math inline">\(X\)</span>) = ln(2)/<span class="math inline">\(\lambda\)</span>, <span class="math inline">\(F_{X}(x)\)</span> =<span class="math inline">\(1-e^{-\lambda x}\)</span>, and</li>
</ol></li>
</ul>
<p><span class="math display">\[\mathrm{Mad}(X)=\frac{1}{\lambda}\left[2-2e^{-1}-1\right]-\int_{0}^{\lambda^{-1}}x\lambda e^{-\lambda x}d x+\int_{\lambda^{-1}}^{\infty}x\lambda e^{-\lambda x}d x=\frac{2}{\lambda e},\]</span></p>
<p>where <span class="math inline">\(\textstyle\int x\lambda e^{-\lambda x}d x=-x e^{-\lambda x}-\lambda^{-1}e^{-\lambda x}\)</span> has been used.</p>
<ul>
<li>The mean absolute deviation from the median is</li>
</ul>
<p><span class="math display">\[\mathrm{E}\left|X-{\frac{\ln(2)}{\lambda}}\right|=-\int_{0}^{\ln(2)\lambda^{-1}}x\lambda e^{-\lambda x}d x+\int_{\ln(2)\lambda^{-1}}^{\infty}x\lambda e^{-\lambda x}d x={\frac{\ln(2)}{\lambda}}\]</span></p>
</div>
<div id="standard-scores" class="section level4 unnumbered hasAnchor">
<h4>Standard Scores<a href="chapter5.html#standard-scores" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Let <span class="math inline">\(Z={\frac{X-\mu_{X}}{\sigma_{X}}}\)</span>.</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Moments: <span class="math inline">\(E(Z) = 0\)</span> and <span class="math inline">\(Var(Z) = 1\)</span>.</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Interpretation: <span class="math inline">\(Z\)</span> scores are scaled in standard deviation units.</li>
</ol></li>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Inverse Transformation:<span class="math inline">\(X=\mu_{X}+\sigma_{X}Z\)</span>.</li>
</ol></li>
</ul>
<p><br></p>
</div>
</div>
<div id="bivariate-distributions" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Bivariate Distributions<a href="chapter5.html#bivariate-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: A function <span class="math inline">\(f_{X,Y}(x,y)\)</span> is a bivariate pdf if
<ul>
<li><ol style="list-style-type: lower-roman">
<li><span class="math inline">\(f_{X,Y}(x,y)\geq0\)</span> for all <span class="math inline">\(x,y\)</span> and</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li><span class="math inline">\(\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X,Y}(x,y)d x d y=1\)</span>.</li>
</ol></li>
</ul></li>
<li>Bivariate CDF: If <span class="math inline">\(f_{X,Y}(x,y)\)</span> is a bivariate pdf, then</li>
</ul>
<p><span class="math display">\[
F_{X,Y}(x,y)=P(X\leq x,Y\leq y)=\int_{-\infty}^{x}\int_{-\infty}^{y}f_{X,Y}(u,v)d v d u\]</span></p>
<ul>
<li>Properties of a bivariate cdf:
<ul>
<li><ol style="list-style-type: lower-roman">
<li><span class="math inline">\(F_{X.Y}(x,\infty)=F_{X}(x)\)</span></li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li><span class="math inline">\(F_{X,Y}(\infty,y)=F_{Y}(y)\)</span></li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-roman">
<li><span class="math inline">\(F_{X,Y}(\infty,\infty)=1\)</span></li>
</ol></li>
<li><ol start="4" style="list-style-type: lower-roman">
<li><span class="math inline">\(F_{X,Y}(-\infty,y)=F_{X,Y}(x,-\infty)=F_{X,Y}(-\infty,-\infty)=0\)</span></li>
</ol></li>
<li><ol start="22" style="list-style-type: lower-alpha">
<li><span class="math inline">\(f_{X,Y}(x,y)={\frac{\partial^{2}}{\partial x\partial y}}F_{X,Y}(x,y)\)</span>.</li>
</ol></li>
</ul></li>
<li>Joint pdfs and joint cdfs for three or more random variables are obtained as straightforward generalizations of the above definitions and conditions.</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Probability Element:<span class="math inline">\(f_{X,Y}(x,y)\Delta x\Delta y\)</span> is the joint probability element. That is,</li>
</ol>
<p><span class="math display">\[P(x\leq X\leq x+\Delta x,y\leq Y\leq y+\Delta y)=f_{X,Y}(x,y)\Delta x\Delta y+o(\Delta x\Delta y)\]</span></p>
<ul>
<li>Example: Bivariate Uniform. If <span class="math inline">\((X, Y ) \sim Unif(a, b, c, d)\)</span>, then</li>
</ul>
<p><span class="math display">\[
f_{X,Y}(x,y) =
\begin{cases}
{\frac{1}{(b-a)(d-c)}}&amp;{x\in(a,b),\quad y\in(c,d);}\\ {0}&amp;{\mathrm{otherwise}}\end{cases}
\]</span></p>
<ul>
<li>For this density, the probability<span class="math inline">\(P(x_{1}\leq X\leq x_{2},y_{1}\leq Y\leq y_{2})\)</span> is the volume of the rectangle.
<ul>
<li>For example, if <span class="math inline">\((X, Y ) \sim Unif(0, 4, 0, 6)\)</span>, then <span class="math inline">\(P(2.5\leq X\leq3.5,1\leq Y\leq4)= (3.5 − 2.5)(4 − 1)/(4 × 6) = 3/24\)</span>.</li>
<li>Another example is <span class="math inline">\(P(X^{2}+Y^{2}\gt 16)=1-P(X^{2}+Y^{2}\leq16)=1-4\pi/24=1-\pi/6\)</span> because the area of a circle is <span class="math inline">\(\pi r^2\)</span> and therefore, the area of a circle with radius 4 is <span class="math inline">\(16\pi\)</span> and the area of the quarter circle in the support set is <span class="math inline">\(4\pi\)</span></li>
</ul></li>
<li>Example: <span class="math inline">\(f_{X,Y}(x,y)=\textstyle{\frac{6}{5}}(x+y^{2})\)</span> for <span class="math inline">\(x \in (0, 1)\)</span> and <span class="math inline">\(y \in (0, 1)\)</span>. Find <span class="math inline">\(P(X + Y &lt; 1)\)</span>.
<ul>
<li>Solution: First sketch the region of integration, then use calculus:</li>
</ul></li>
</ul>
<p><span class="math display">\[P(X+Y\lt 1)\ =\ P(X\lt 1-Y)=\int_{0}^{1}\int_{0}^{1-y}{\frac{6}{5}}(x+y^{2})d x d y\]</span></p>
<p><span class="math display">\[=\left.\frac{6}{5}\int_{0}^{1}\left(\frac{x^{2}}{2}+x y^{2}\right)\right|_{0}^{1-y}d y=\begin{cases}{c}{{6}}\\ {{5}}\end{cases}\!\int_{0}^{1}\frac{(1-y)^{2}}{2}+(1-y)y^{2}d y\]</span></p>
<p><span class="math display">\[=\left.{\frac{6}{5}}\left({\frac{y}{2}}-{\frac{y^{2}}{2}}+{\frac{y^{3}}{6}}+{\frac{y^{3}}{3}}-{\frac{y^{4}}{4}}\right)\right|_{0}^{1}={\frac{3}{10}}\]</span></p>
<ul>
<li>Example: Bivariate standard normal</li>
</ul>
<p><span class="math display">\[f_{X,Y}(x,y)=\frac{e^{-\frac{4}{2}(x^{2}+y^{2})}}{2\pi}=\frac{e^{-\frac{4}{2}x^{2}}}{\sqrt{2\pi}}\frac{e^{-\frac{4}{2}y^{2}}}{\sqrt{2\pi}}=F_{X}(x)f_{Y}(y)\]</span></p>
<div id="marginal-densities" class="section level4 unnumbered hasAnchor">
<h4>Marginal Densities:<a href="chapter5.html#marginal-densities" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Integrate out unwanted variables to obtain marginal densities. For example,</li>
</ol></li>
</ul>
<p><span class="math display">\[f_{X}(x)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)d y; f_{Y}(y)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)d x;\]</span></p>
<p>and</p>
<p><span class="math display">\[f_{X,Y}(x,y)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{W,X,Y,Z}(w,x,y,z)d w d z\]</span></p>
<ul>
<li>Example: If <span class="math inline">\(f_{X,Y}(x,y={\frac{6}{5}}(x+y^{2}))\)</span> for <span class="math inline">\(x ∈ (0, 1)\)</span> and <span class="math inline">\(y ∈ (0, 1)\)</span>, then</li>
</ul>
<p><span class="math display">\[f_{X}(x)={\frac{6}{5}}\int_{0}^{1}(x+y^{2})d y={\frac{6x+2}{5}}\;\mathrm{for} \ x\in\left(0,1\right)\]</span></p>
<p>and</p>
<p><span class="math display">\[f_{Y}(y)={\frac{6}{5}}\int_{0}^{1}(x+y^{2})d x={\frac{6y^{2}+3}{5}}\mathrm{for}\ y\in(0,1)\]</span></p>
</div>
<div id="expected-values" class="section level4 unnumbered hasAnchor">
<h4>Expected Values<a href="chapter5.html#expected-values" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>The expected value of a function <span class="math inline">\(g(X,Y)\)</span> is</li>
</ol></li>
</ul>
<p><span class="math display">\[\mathrm{E}\left[g(X,Y)\right]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)f_{X,Y}(x,y)d x d y\]</span></p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Example: If <span class="math inline">\(f_{X,Y}(x,y)=\textstyle{\frac{6}{5}}(x+y^{2})\)</span> for <span class="math inline">\(x \in (0,1)\)</span> and <span class="math inline">\(y \in (0,1)\)</span>, then</li>
</ol></li>
</ul>
<p><span class="math display">\[\mathrm{E}(X)=\int_{0}^{1}\int_{0}^{1}x{\frac{6}{5}}(x+y^{2})d x d y=\int_{0}^{1}{\frac{3y^{2}+2}{5}}d y={\frac{3}{5}}\]</span></p>
<p><br></p>
</div>
</div>
<div id="several-variables" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Several Variables<a href="chapter5.html#several-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>The joint pdf of <span class="math inline">\(n\)</span> continuous random variables, <span class="math inline">\(X_{1},\ldots,X_{n}\)</span> is a function that satisfies
<ul>
<li><ol style="list-style-type: lower-roman">
<li><span class="math inline">\(f(x_{1},\ldots,x_{n})\geq0,\)</span> and</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li><span class="math inline">\(\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}f(x_{1},\ldots,x_{n})\,d x_{1}\cdots d x_{n}=1\)</span>.</li>
</ol></li>
</ul></li>
<li>Expectations are linear regardless of the number of variables:</li>
</ul>
<p><span class="math display">\[\mathrm{E}\left[\sum_{i=1}^{k}a_{i}g_{i}(X_{1},X_{2},\cdot\cdot\cdot,X_{n})\right]=\sum_{i=1}^{k}a_{i}\mathrm{E}\left[g_{i}(X_{1},X_{2},\cdot\cdot,X_{n})\right]\]</span></p>
<p>if the expectations exist.</p>
<div id="exchangeable-random-variables" class="section level4 unnumbered hasAnchor">
<h4>Exchangeable Random variables<a href="chapter5.html#exchangeable-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Let <span class="math inline">\(x_{1}^{*},\ldots,x_{n}^{*}\)</span> be a permutation of <span class="math inline">\(x_{1},\ldots,x_{n}\)</span>. Then, the joint density of <span class="math inline">\(X_{1},\ldots,X_{n}\)</span> is said to be exchangeable if</li>
</ol></li>
</ul>
<p><span class="math display">\[f_{X_{1},\ldots,X_{n}}(x_{1},\ldots,x_{n})=f_{X_{1},\ldots,X_{n}}(x_{1}^{*},\ldots,x_{n}^{*})\]</span></p>
<p>for all <span class="math inline">\(x_{1},\ldots,x_{n}\)</span> and for all permutations <span class="math inline">\(x_{1}^{*},\ldots,x_{n}^{*}\)</span>.</p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Result: If the joint density is exchangeable, then all marginal densities are identical. For example,</li>
</ol></li>
</ul>
<p><span class="math display">\[f_{X_{1},X_{2}}(x_{1},x_{2})=\int_{-\infty}^{\infty}f_{X_{1},X_{2},X_{3}}(x_{1},x_{2},x_{3})\,d x_{3}\]</span></p>
<p><span class="math display">\[=\int_{-\infty}^{\infty}f_{X_{1},X_{2},X_{3}}(x_{3},x_{2},x_{1})\,d x_{3}\]</span></p>
<p>by exchangeability</p>
<p><span class="math display">\[\int_{-\infty}^{\infty}f_{X_{1},X_{2},X_{3}}(x_{1},x_{2},x_{3})\,d x_{1}\]</span></p>
<p>by relabeling variables</p>
<p><span class="math display">\[=f_{X_{2},X_{3}}(x_{2},x_{3})\]</span></p>
<ul>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Result: If the joint density is exchangeable, then all bivariate marginal densities are identical, and so forth.</li>
</ol></li>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Result: If the joint density is exchangeable, then the moments of <span class="math inline">\(X_{i}\)</span> (if they exist) are identical for all <span class="math inline">\(i\)</span>.</li>
</ol></li>
<li><ol start="5" style="list-style-type: lower-alpha">
<li>Example Suppose that <span class="math inline">\(f_{X,Y}(x,y) = 2\)</span> for<span class="math inline">\(x ≥ 0, y ≥ 0\)</span>, and <span class="math inline">\(x + y ≤ 1\)</span>. Then</li>
</ol></li>
</ul>
<p><span class="math display">\[f_{X}(x)=\int_{0}^{1-x}2d y=2(1-x)~\mathrm{for}~x\in(0,1)\]</span></p>
<p><span class="math display">\[f_{Y}(y)=\int_{0}^{1-y}2d x=2(1-y)~\mathrm{for}~y\in(0,1)\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathrm{E}(X)\ =\mathrm{E}(Y)={\frac{1}{3}}\]</span></p>
<p><br></p>
</div>
</div>
<div id="covariance-and-correlation-1" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Covariance and Correlation<a href="chapter5.html#covariance-and-correlation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definitions</li>
</ul>
<p><span class="math display">\[\mathrm{Cov}(X,Y)~{\stackrel{\mathrm{def}}{=}}~\mathrm{E}\left[(X-\mu_{X})(Y-\mu_{Y})\right]\]</span></p>
<ul>
<li><p>Cov(<span class="math inline">\(X, Y\)</span>) is denoted by <span class="math inline">\(\sigma_{X,Y}\)</span>.</p></li>
<li><p>Var(<span class="math inline">\(X\)</span>) = Cov(<span class="math inline">\(X,X\)</span>).</p></li>
</ul>
<p><span class="math display">\[\textstyle\mathrm{Corr}(X,Y){\stackrel{\mathrm{def}}{=}}\mathrm{Cov}(X,Y)/{\sqrt{\mathrm{Var}(X)\mathrm{Var}(Y)}}\]</span></p>
<ul>
<li><p>Corr(<span class="math inline">\(X,Y\)</span>) is denoted by <span class="math inline">\(\rho_{X,Y}\)</span>.</p></li>
<li><p>Covariance and Correlation Results (be able to prove any of these).</p></li>
<li><p><span class="math inline">\(Cov(X, Y) = E(XY) − E(X)E(Y)\)</span>.</p></li>
<li><p>Cauchy-Schwartz Inequality:</p></li>
</ul>
<p><span class="math display">\[[\mathrm E(X Y)]^{2}\,\leq\,{\mathrm E}(X^{2}){\mathrm E}({Y}^{2})\]</span></p>
<ul>
<li><p><span class="math inline">\(\rho_{X,Y}\in[-1,1]\)</span> To proof, use the Cauchy-Schwartz inequality.</p></li>
<li><p><span class="math inline">\(Cov(a+bX, c + dY) = bdCov(X,Y)\)</span>.</p></li>
<li><p><span class="math inline">\(Cov \left(\sum_{i}a_{i}X_{i},\sum_{i}b_{i}Y_{i}\right)=\sum_{i}\sum_{j}a_{i}b_{j} Cov(X_{i},Y_{j})\)</span>.</p></li>
<li><p>For example,</p></li>
</ul>
<p><span class="math display">\[Cov(aW + bX, cY + dZ) =ac Cov(W, Y) + adCov(W, Z) + bcCov(X, Y) + bdCov(X, Z)\]</span></p>
<ul>
<li><span class="math inline">\(Corr(a + bX, c + dY) = sign(bd) Corr(X, Y)\)</span>.</li>
</ul>
<p><span class="math display">\[Var\left(\sum_{i}X_{i}\right)=\sum_{i}\sum_{j}\mathrm{Cov}(X_{i},X_{j})=\sum_{i}\mathrm{Var}(X_{i})+\sum_{i\neq j}\mathrm{Cov}(X_{i},X_{j})\]</span></p>
<ul>
<li><p>Parallel axis theorem: <span class="math inline">\(\mathrm{E}(X-c)^2=\mathrm{Var}(X)+(\mu_{X}-c)^2\)</span>. Hint on proof: first add zero <span class="math inline">\(X − c = (X − \mu X ) + (\mu X − c)\)</span>, then take expectation.</p></li>
<li><p>Example (simple linear regression with correlated observations): Suppose that <span class="math inline">\(Y_{i}=\alpha+\beta x_{i}+\varepsilon_{i}\)</span> for <span class="math inline">\(i=1,\ldots ,n\)</span> and where <span class="math inline">\(\varepsilon_{1},\ldots ,\varepsilon_{n}\)</span> have an exchangeable distribution with <span class="math inline">\(E(\varepsilon_{1}) = 0\)</span>, <span class="math inline">\(Var(\varepsilon_{1}) = \sigma^2\)</span> and <span class="math inline">\(Cov(\varepsilon_{1},\varepsilon_{2}) = \rho\sigma^2\)</span>.</p></li>
<li><p>The ordinary least squares estimator of <span class="math inline">\(\beta\)</span> is</p></li>
</ul>
<p><span class="math display">\[\widehat{\beta}=\frac{\displaystyle\sum_{i=1}^{n}(x_{i}-{\bar{x}})Y_{i}}{\displaystyle\sum_{i=1}^{n}(x_{i}-{\bar{x}})^{2}}\]</span></p>
<ul>
<li>Find the expectation and variance of <span class="math inline">\(\hat{\beta}\)</span>.
<ul>
<li>Solution: Write <span class="math inline">\(\hat{\beta}\)</span> as</li>
</ul></li>
</ul>
<p><span class="math display">\[\hat{\beta}=\sum_{i=1}^{n}w_{i}Y_{i}, \text{ where } w_{i}=\frac{(x_{i}-\bar{x})}{\displaystyle\sum_{j=1}^{n}(x_{j}-\bar{x})^{2}}\]</span></p>
<ul>
<li>Then, the expectation of <span class="math inline">\(\hat{\beta}\)</span> is</li>
</ul>
<p><span class="math display">\[\mathrm{E}({\widehat{\beta}})=\sum_{i=1}^{n}w_{i}\mathrm{E}(Y_{i})=\sum_{i=1}^{n}w_{i}(\alpha+\beta x_{i})=\frac{\displaystyle\sum_{i=1}^{n}(x_{i}-{\bar{x}})(\alpha+\beta x_{i})}{\displaystyle\sum_{j=1}^{n}(x_{i}-{\bar{x}})^2}\]</span></p>
<p><span class="math display">\[=\beta\frac{\displaystyle \sum_{i=1}^{n}(x_{i}-{\bar{x}})x_{i}}{\displaystyle \sum_{j=1}^{n}(x_{j}-{\bar{x}})^{2}}\]</span></p>
<p>because <span class="math inline">\(\sum_{i=1}^{n}(x_{i}-{\bar{x}})=0\)</span></p>
<ul>
<li>because <span class="math inline">\(\sum(x_{i}-{\bar{x}})x_{i}=\sum(x_{i}-{\bar{x}})^{2}\)</span> The variance of <span class="math inline">\(\widehat{\beta}\)</span> is</li>
</ul>
<p><span class="math display">\[\mathrm{Var}(\widehat{\beta}) = \sum_{i=1}^{n}w_{i}^{2}\mathrm{Var}(Y_{i})+\sum_{i\neq j}w_{i}w_{j}\mathrm{Cov}(Y_{i},Y_{j})\]</span></p>
<p><span class="math display">\[=\sigma^{2}\sum_{i=1}^{n}w_{i}^{2}+\rho\sigma^{2}\sum_{i\neq j}w_{i}w_{j}\]</span></p>
<p><span class="math display">\[=\frac{\sigma^{2}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}+\rho\sigma^{2}\left[\sum_{i=1}^{n}\sum_{j=1}^{n}w_{i}w_{j}-\sum_{i=1}^{n}w_{i}^{2}\right]\]</span></p>
<p><span class="math display">\[=\frac{\sigma^{2}}{\displaystyle\sum_{i=1}^{n}(x_{i}-{\bar{x}})^{2}}+\rho\sigma^{2}\left[\left(\sum_{i=1}^{n}w_{i}\right)\left(\sum_{j=1}^{n}w_{j}\right)-\sum_{i=1}^{n}w_{i}^{2}\right]\]</span></p>
<p><span class="math display">\[=\frac{\sigma^{2}(1-\rho)}{\displaystyle\sum_{i=1}^{n}(x_{i}-{\bar{x}})^{2}}\]</span></p>
<p><br></p>
</div>
<div id="independence" class="section level2 hasAnchor" number="5.10">
<h2><span class="header-section-number">5.10</span> Independence<a href="chapter5.html#independence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: Continuous random variables X and Y are said to be independent if their joint pdf factors into a product of the marginal pdfs. That is,</li>
</ul>
<p><span class="math display">\[f_{X.Y}(x,y)=f_{X}(x)f_{Y}(y)\Longleftrightarrow X⫫Y\]</span></p>
<ul>
<li>Example: if <span class="math inline">\(f_{X,Y}(x,y) = 2\)</span> for <span class="math inline">\(x \in (0, 0.5)\)</span> and <span class="math inline">\(y \in (0, 1)\)</span> then <span class="math inline">\(X ⫫ Y\)</span>. Note, the joint pdf can be written as</li>
</ul>
<p><span class="math display">\[f_{X,Y}=2I_{(0.0.5)}(x)I_{(0.1)}(y)=2I_{(0.0.5)}(x)\times I_{(0.1)}(y)= f_{X}(x)\times f_{Y}(y)\]</span></p>
<p>where</p>
<p><span class="math display">\[I_{A}(x)=\left\{\begin{cases}{l l}{1}&amp;{x\in A;}\\ {0}&amp;{\mathrm{otherwise}}\end{cases}\right.\]</span></p>
<ul>
<li>Example: if <span class="math inline">\(f_{X,Y}(x,y) = 8xy\)</span> for <span class="math inline">\(0 ≤ x ≤ y ≤ 1\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not independent. Note</li>
</ul>
<p><span class="math display">\[f_{X,Y}(x,y)=8x y I_{(0,1)}(y)I_{(0,y)}(x),\]</span></p>
<p>but</p>
<p><span class="math display">\[f_{X}(x)=\int_{x}^{1}f_{X,Y}(x,y)\,d y=4x(1-x^{2})I_{(0,1)}(x)\]</span></p>
<p>and</p>
<p><span class="math display">\[f_{Y}(y)=\int_{0}^{y}f_{X,Y}(x,y)\,d x=4y^{3}I_{(0,1)}(y)\]</span></p>
<ul>
<li>Note: <span class="math inline">\(Cov(X,Y) = 0\nRightarrow X⫫ Y\)</span>. For example, if</li>
</ul>
<p><span class="math display">\[f_{X,Y}(x,y)={\frac{1}{3}}I_{(1,2)}(x)I_{(-x,x)}(y)\]</span></p>
<p>then</p>
<p><span class="math display">\[E(X) = \int_{1}^{2}\int_{-x}^{x}\frac{x}{3}d y d x=\int_{1}^{2}=\frac{2x^{2}}{3}d x=\frac{14}{9},\]</span></p>
<p><span class="math display">\[E(Y) = \int_{1}^{2}\int_{-x}^{x}\frac{y}{3}d y d x=\int_{1}^{2}\frac{x^{2}-x^{2}}{6}d x=0,\]</span></p>
<p>and</p>
<p><span class="math display">\[E(XY) = \int_{1}^{2}\int_{-x}^{x}\frac{x y}{3}d y d x=\int_{1}^{2}\frac{x(x^{2}-x^{2})}{6}d x=0\]</span></p>
<p>Accordingly, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have correlation <span class="math inline">\(0\)</span>, but they are not independent.</p>
<ul>
<li>Result: Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be subsets of the real line. Then random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if and only if</li>
</ul>
<p><span class="math display">\[
P(X \in A, Y \in B) = P(X \in A) P(Y \in B)
\]</span></p>
<p>for all choices of sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<ul>
<li>Result: If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then so are <span class="math inline">\(g(X)\)</span> and <span class="math inline">\(h(Y)\)</span> for any <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span>.
<ul>
<li>Proof: Let <span class="math inline">\(A\)</span> be any set of intervals in the range of <span class="math inline">\(g(x)\)</span> and let <span class="math inline">\(B\)</span> be any set of intervals in the range of <span class="math inline">\(h(y)\)</span>.</li>
<li>Denote by <span class="math inline">\(g^{-1}(A)\)</span> the set of all intervals in the support of <span class="math inline">\(X\)</span> that satisfy <span class="math inline">\(x \in g^{-1}(A) \iff g(x)\in A\)</span>.</li>
<li>Similarly, denote by <span class="math inline">\(h^{-1}(B)\)</span> the set of all intervals in the support of <span class="math inline">\(Y\)</span> that satisfy <span class="math inline">\(y \in h^{-1}(B)\iff  h(y) \in B\)</span>.</li>
<li>If <span class="math inline">\(X⫫Y\)</span>, then,</li>
</ul></li>
</ul>
<p><span class="math display">\[
P[g(X)\in A,h(Y)\in B]=P\left[X\in g^{-1}(A),Y\in h^{-1}(B)\right]
\]</span></p>
<p><span class="math display">\[
=P\left[X\in{\mathfrak{g}}^{-1}(A)\right]\times P\left[Y\in h^{-1}(B)\right]=P[g(X)\in A]\times P[h(Y)\in B]
\]</span></p>
<ul>
<li><p>The above equality implies that <span class="math inline">\(g(X)⫫h(Y)\)</span> because the factorization is satisfied for all <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in the range spaces of <span class="math inline">\(g(X)\)</span> and <span class="math inline">\(h(Y)\)</span>.</p></li>
<li><p>The previous two results readily extend to any number of random variables (not just two).</p></li>
<li><p>Suppose that <span class="math inline">\(X_{i}\)</span> for <span class="math inline">\(i=1,\ldots ,n\)</span> are independent. Then</p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(g_{1}(X_{1}),\ldots,g_{n}(X_{n})\)</span> are independent,</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>The <span class="math inline">\(X\)</span>s in any subset are independent,</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li><span class="math inline">\(\mathrm{Var}\left(\sum a_{i}X_{i}\right)=\sum a_{i}^{2}\mathrm{Var}(X_{i})\)</span>, and</li>
</ol></li>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>if the <span class="math inline">\(X\)</span>s are iid with variance <span class="math inline">\(\sigma^2\)</span>, then <span class="math inline">\(\mathrm{Var}\left(\sum a_{i}X_{i}\right)=\sigma^{2}\sum a_{i}^{2}\)</span>.</li>
</ol></li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="conditional-distributions" class="section level2 hasAnchor" number="5.11">
<h2><span class="header-section-number">5.11</span> Conditional Distributions<a href="chapter5.html#conditional-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: If <span class="math inline">\(f_{X,Y}(x,y)\)</span> is a joint pdf, then the pdf of <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X = x\)</span> is</li>
</ul>
<p><span class="math display">\[f_{Y\mid X}(y\mid x)~{\stackrel{\mathrm{def}}{=}}~{\frac{f_{X,Y}\left(x,y\right)}{f_{X}\left(x\right)}}\]</span></p>
<p>provided that <span class="math inline">\(f_{X}(x)\gt 0\)</span>.</p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint distribution</li>
</ul>
<p><span class="math display">\[f_{X,Y}(x,y) = 8xy \,\,\, \mathrm{for} \,\,\, 0 &lt; x &lt; y &lt; 1\]</span></p>
<p>Then,</p>
<p><span class="math display">\[f_{X}(x)=\int_{x}^{1}f_{X,Y}(x,y)d y=4x(1-x^{2}),~~0\lt x\lt 1;\]</span></p>
<p><span class="math display">\[\mathrm{E}(X^{r})=\int_{\alpha}^{1}4x(1-x^{2})x^{r}d x=\frac{8}{(r+2)(r+4)};\]</span></p>
<p><span class="math display">\[f_{Y}(y)=4y^{3},~~0\lt y\lt 1;\]</span></p>
<p><span class="math display">\[\mathrm{E}(Y^{r})=\int_{0}^{1}4y^{3}y^{r}d y={\frac{4}{r+4}}\]</span></p>
<p><span class="math display">\[f_{X\mid Y}(x\mid y) = \frac{8x y}{4y^{3}}=\frac{2x}{y^{2}},~~0\lt x\lt y;\]</span></p>
<p><span class="math display">\[f_{Y|X}(y|x)={\frac{8x y}{4x(1-x^{2})}}={\frac{2y}{1-x^{2}}},~~ x\lt y\lt 1\]</span></p>
<p>Furthermore,</p>
<p><span class="math display">\[\mathrm{E}(X^{r}|Y=y)=\int_{0}^{y}x^{r}{\frac{2x}{y^{2}}}d x={\frac{2y^{r}}{r+2}}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathrm{E}(Y^{r}|X=x)=\int_{x}^{1}y^{r}{\frac{2y}{1-x^{2}}}dy={\frac{2(1-x^{r+2})}{(r+2)(1-x^{2})}}\]</span></p>
<ul>
<li>Regression Function: Let <span class="math inline">\((X, Y)\)</span> be a pair of random variables with joint pdf <span class="math inline">\(f_{X,Y}(x,y)\)</span>.
<ul>
<li>Consider the problem of predicting Y after observing <span class="math inline">\(X = x\)</span>.</li>
<li>Denote the predictor as <span class="math inline">\(\hat{y}(x)\)</span>.</li>
<li>The best predictor is defined as the function <span class="math inline">\(\hat{Y}(X)\)</span> that minimizes</li>
</ul></li>
</ul>
<p><span class="math display">\[SSE=\mathrm{E}\left[Y-{\hat{Y}}(X)\right]^{2}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\left[y-{\hat{y}}(x)\right]^{2}f_{X,Y}(x,y)d y d x\]</span></p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Result: The best predictor is <span class="math inline">\(\hat{y}(x)=\mathrm{E}(Y|X=x)\)</span>.</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Proof: Write <span class="math inline">\(f_{X,Y}(x,y)\)</span> as <span class="math inline">\(f_{Y|X}(y|x)f_{X}(x)\)</span>. Accordingly,</li>
</ol></li>
</ul>
<p><span class="math display">\[
SEE=\int_{-\infty}^{\infty}\left\{\int_{-\infty}^{\infty}\left[y-\hat{y}(x)\right]^{2}f_{Y,|X}(y,x)d y\right\}f_{X}(x)d x
\]</span></p>
<ul>
<li>To minimize <span class="math inline">\(SSE\)</span>, minimize the quantity in { } for each value of <span class="math inline">\(x\)</span>.
<ul>
<li>Note that <span class="math inline">\(\hat{y}(x)\)</span> is a constant with respect to the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span>.</li>
<li>By the parallel axis` theorem, the quantity in { } is minimized by</li>
</ul></li>
</ul>
<p><span class="math display">\[\hat{y}(x)=\mathrm{E}(Y|X=x)\]</span></p>
<ul>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Example: Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint distribution</li>
</ol></li>
</ul>
<p><span class="math display">\[f_{X,Y}(x,y) = 8xy \,\,\, \mathrm{for} \,\,\, 0 &lt; x &lt; y &lt; 1\]</span></p>
<p>Then,</p>
<p><span class="math display">\[f_{Y|X}(y|x)=\frac{8x y}{4x t1-x^{2}}=\frac{2y}{1-x^{2}},~~x\lt y\lt 1\]</span></p>
<p>and</p>
<p><span class="math display">\[\hat{y}(x)=\mathrm{E}(Y|X=x)\int_{x}^{1}y{\frac{2y}{1-x^{2}}}d y={\frac{2(1-x^{3})}{3(1-x^{2})}}\]</span></p>
<ul>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Example; Suppose that <span class="math inline">\((Y, X)\)</span> has a bivariate normal distribution with moments <span class="math inline">\(E(Y) = \mu+{Y}\)</span>, <span class="math inline">\(E(X) = \mu_{X}\)</span>, <span class="math inline">\(Var(X) =\sigma_{X}^{2}\)</span>, <span class="math inline">\(Var(Y) = \sigma_{Y}^{2}\)</span> and <span class="math inline">\(Cov(X,Y) ==\rho_{X,Y}\sigma_{X}\sigma_{Y}\)</span>. Then the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is</li>
</ol></li>
</ul>
<p><span class="math display">\[(Y|X=x)\sim\mathrm{N}(\alpha+\beta x,\sigma^{2})\]</span></p>
<p>where</p>
<p><span class="math display">\[\beta={\frac{\mathrm{Cov}(X,Y)}{\mathrm{Var}(X)}}={\frac{\rho_{X,Y}\sigma_{Y}}{\sigma_{X}}};~~\alpha=\mu_{Y}-\beta\mu_{X}\]</span></p>
<p>and</p>
<p><span class="math display">\[\sigma^{2}=\sigma_{Y}^{2}\left(1-\rho_{X.Y}^{2}\right)\]</span></p>
<ul>
<li>Averaging Conditional pdfs and Moments (be able to prove any of these results)
<ul>
<li><ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\mathrm{E}_{X}\left[f_{Y\mid X}(y|X)\right]=f_{Y}(y)\)</span>. Hint: <span class="math inline">\(f_{X,Y}(x,y)=f_{Y|X}(y|x)f_{X}(x)\)</span>.</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li><span class="math inline">\(\mathrm{E}_{X}\{\mathrm{E}[h(Y)|X]\}=\mathrm{E}[h(Y)]\)</span>. This is the rule of iterated expectation. A special case is <span class="math inline">\(\mathrm{E}_{X}\left[\mathrm{E}(Y|X)\right]=\mathrm{E}(Y)\)</span>.</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Var(<span class="math inline">\(Y\)</span>) = <span class="math inline">\(\mathrm{E}_{X}\left[\mathrm{Var}(Y|X)\right]+\mathrm{Var}\left[\mathrm{E}(Y|X)\right]\)</span>.</li>
</ol></li>
<li>That is, the variance of <span class="math inline">\(Y\)</span> is equal to the expectation of the conditional variance plus the variance of the conditional expectation.</li>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[\mathrm{Var}(Y)=\mathrm{E}(Y^{2})-\left[\mathrm{E}(Y)\right]^{2}=\mathrm{E}_{X}\left[\mathrm{E}(Y^{2}|X)\right]-\{\mathrm{E}_{X}\left[\mathrm{E}(Y|X)\}\right\}^{2}\]</span></p>
<p>by the rule of iterated expectation</p>
<p><span class="math display">\[=\mathrm{E}_{X}\{\mathrm{Var}(Y|X)+[\mathrm{E}(Y|X)]^{2}\}-\{\mathrm{E}_{X}[\mathrm{E}(Y|X)]\}^{2}\]</span></p>
<p>because <span class="math inline">\(\mathrm{Var}(Y|X)=\mathrm{E}(Y^{2}|X)-[\mathrm{E}(Y|X)]^{2}\)</span></p>
<p><span class="math display">\[\mathrm{E}_{X}\left[\mathrm{Var}(Y|X)\right]+\mathrm{E}_{X}[\mathrm{E}(Y|X)]^{2}-\{\mathrm{E}_{X}[\mathrm{E}(Y|X)]\}^{2}=\mathrm{E}_{X}\left[\mathrm{Var}(Y|X)\right]+\mathrm{Var}[\mathrm{E}(Y|X)]\]</span></p>
<p>because <span class="math inline">\(\mathrm{Var}[\mathrm{E}(Y|X)]=\mathrm{E}_{X}\left[\mathrm{E}(Y|X)\right]^{2}+-\{\mathrm{E}_{X}[\mathrm{E}(Y|X)]\}^{2}\)</span>.</p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint distribution
<span class="math inline">\(f_{X,Y}(x,y)={\frac{3y^{2}}{x^{3}}}\mathrm{for}\ 0\lt y\lt x\lt 1\)</span>. Then,</li>
</ul>
<p><span class="math display">\[f_{Y}(y)=\int_{v}^{1}\frac{3y^{2}}{x^{3}}d x=\frac{3}{2}(1-y^{2}),~\mathrm{for}~0\lt y\lt 1;\]</span></p>
<p><span class="math display">\[\mathrm{E}(Y^{r})=\int_{0}^{1}\frac{3}{2}(1-y^{2})y^{r}d y=\frac{3}{(r+1)(r+3)};\]</span></p>
<p><span class="math display">\[\Longrightarrow\mathrm{E}(Y)={\frac{3}{8}}~{\mathrm{and}}~\mathrm{Var}(Y)={\frac{19}{320}};\]</span></p>
<p><span class="math display">\[f_{X}(x)=\int_{0}^{x}\frac{3y^{2}}{x^{2}}d y=1,\;\mathrm{for}\;0\lt x\lt 1;\]</span></p>
<p><span class="math display">\[f_{Y|X}(y|x)~=~\frac{3y^{2}}{x^{3}}, ~\mathrm{for}~ 0\lt y \lt x\lt 1;\]</span></p>
<p><span class="math display">\[\mathrm{E}(Y^{r}|X=x)\ \ =\ \int_{0}^{x}{\frac{3y^{2}}{x^{3}}}y^{r}d y={\frac{3x^{r}}{3+r}}\]</span></p>
<p><span class="math display">\[\Longrightarrow\mathrm{E}(Y|X=x)={\frac{3x}{4}}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathrm{Var}(Y|X=x)={\frac{3x^{2}}{80}};\]</span></p>
<p><span class="math display">\[\mathrm{Var}\left[\mathrm{E}(Y|X)\right]={\mathrm{Var}}\left({\frac{3X}{4}}\right)={\frac{9}{16}}\times{\frac{1}{12}}={\frac{3}{64}};\]</span></p>
<p><span class="math display">\[\begin{cases}{l}{\mathrm{E}\left[\mathrm{Var}(Y|X)\right]}\end{cases}\,=\ \mathrm{E}\left({\frac{3X^{2}}{80}}\right)={\frac{1}{80}};\]</span></p>
<p><span class="math display">\[{\frac{19}{320}}\ =\ {\frac{3}{64}}+{\frac{1}{80}}\]</span></p>
<p><br></p>
</div>
<div id="moment-generating-functions" class="section level2 hasAnchor" number="5.12">
<h2><span class="header-section-number">5.12</span> Moment Generating Functions<a href="chapter5.html#moment-generating-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: If <span class="math inline">\(X\)</span> is a continuous random variable, then the moment generating function (mgf) of <span class="math inline">\(X\)</span> is</li>
</ul>
<p><span class="math display">\[\psi_{X}(t)\ {\stackrel{\mathrm{def}}{=}}\mathrm{E}\left(e^{t X}\right)=\int_{-\infty}^{\infty}e^{t x}f_{X}(x)d x\]</span></p>
<p>provided that the expectation exists for t in a neighborhood of 0.</p>
<ul>
<li>If <span class="math inline">\(X\)</span> is discrete, then replace integration by summation. If all of the moments of <span class="math inline">\(X\)</span> do not exist, then the mgf will not exist. Note that the mgf is related to the pgf by</li>
</ul>
<p><span class="math display">\[\psi_{X}(t)=\eta_{X}(e^{t})\]</span></p>
<p>Also note that if <span class="math inline">\(\psi_{X}(t)\)</span> is a mgf, then <span class="math inline">\(\psi_{X}(0)=1\)</span>.</p>
<ul>
<li>Example: Exponential Distribution. If <span class="math inline">\(f_{X}(x)=\lambda e^{-\lambda x}I_{(0.\infty)}(x),\)</span> then</li>
</ul>
<p><span class="math display">\[\psi_{X}(t)=\int_{0}^{\infty}e^{t x}\lambda e^{-\lambda x}dx={\frac{\lambda}{\lambda-t}}\int_{0}^{\infty}(\lambda-t)e^{-(\lambda-t)x}dx={\frac{\lambda}{\lambda-t}}\]</span></p>
<ul>
<li>Example: Geometric Distribution. If <span class="math inline">\(X\sim Geom(p)\)</span>, then</li>
</ul>
<p><span class="math display">\[\psi_{X}(t)=\sum_{x=1}^{\infty}e^{tx}(1-p)^{x-1}p=pe^{t}\sum_{x=1}^{\infty}(1-p)^{x-1}e^{t(x-1)}\]</span></p>
<p><span class="math display">\[=p e^{t}\sum_{x=0}^{\infty}\left[(1-p)e^{t}\right]^{x}={\frac{p e^{t}}{1-(1-p)e^{t}}}\]</span></p>
<ul>
<li>mgf of a linear function:</li>
</ul>
<p><span class="math display">\[\psi_{a+b X}(t)=\mathrm{E}\left[e^{t(a+b X)}\right]=e^{a t}\psi_{X}(t b)\]</span></p>
<ul>
<li>For example, if <span class="math inline">\(Z=(X-\mu_{X})/\sigma_{X},\)</span> then</li>
</ul>
<p><span class="math display">\[\psi_{Z}(t)=e^{-t\mu_{X}/\sigma_{X}}\psi_{X}(t/\sigma_{X})\]</span></p>
<ul>
<li>Independent Random Variables: If <span class="math inline">\(X_{i}\)</span> for <span class="math inline">\(i=1,\ldots ,n\)</span> are independent, and <span class="math inline">\(S=\textstyle\sum X_{i},\)</span> then</li>
</ul>
<p><span class="math display">\[\psi_{S}(t)=\mathrm{E}\left(e^{t\sum X_{i}}\right)=\mathrm{E}\left[\prod_{i=1}^{n}e^{t X_{i}}\right]=\prod_{i=1}^{n}\psi_{X_{i}}(t)\]</span></p>
<ul>
<li>If the <span class="math inline">\(X\)</span>s are iid random variables and <span class="math inline">\(S=\textstyle\sum_{i=1}^{n}X_{i},\)</span> then</li>
</ul>
<p><span class="math display">\[\psi_{S}(t)=[\psi_{X}(t)]^{n}\,\]</span></p>
<ul>
<li>Result: Moment generating functions, if they exist, uniquely determine the distribution. For example, if the mgf of <span class="math inline">\(Y\)</span> is</li>
</ul>
<p><span class="math display">\[\psi_{Y}(t)={\frac{e^{t}}{2-e^{t}}}={\frac{\frac{1}{2}e^{t}}{1-{\frac{1}{2}}e^{t}}},\]</span></p>
<p>then <span class="math inline">\(Y\sim Geom(0.5)\)</span>.</p>
<ul>
<li>Computing Moments. Consider the derivative of <span class="math inline">\(\psi_{X}(t)\)</span> with respect to <span class="math inline">\(t\)</span> evaluated at <span class="math inline">\(t = 0\)</span>:</li>
</ul>
<p><span class="math display">\[\left.\frac{d}{d t}\psi_{X}(t)\right|_{t=0}= \left.\int_{-\infty}^{\infty}\frac{d}{d t}e^{t x}\right|_{t=0}f_{X}(x)d x\]</span></p>
<ul>
<li>Similarly, higher order moments can be found by taking higher order derivatives:</li>
</ul>
<p><span class="math display">\[\left.\mathrm{E}(X^{r})={\frac{d^{r}}{(d t)^{r}}}\psi_{X}(t)\right|_{t=0}\]</span></p>
<ul>
<li>Alternatively, expand <span class="math inline">\(e^{tx}\)</span> around <span class="math inline">\(t = 0\)</span> to obtain</li>
</ul>
<p><span class="math display">\[e^{t x}=\sum_{r=0}^{\infty}{\frac{(t x)^{r}}{r!}}\]</span></p>
<ul>
<li><p>Therefore
<span class="math display">\[\psi_{X}(t)=\mathrm{E}\left(e^{t X}\right)=\mathrm{E}\left[\sum_{r=0}^{\infty}{\frac{(t X)^{r}}{r!}}\right]=\sum_{r=0}^{\infty}\mathrm{E}(X^{r}){\frac{t^{r}}{r!}}\]</span></p></li>
<li><p>Accordingly, <span class="math inline">\(E(X^{r})\)</span> is the coefficient of <span class="math inline">\(t^{r}/r!\)</span> in the expansion of the mgf.</p></li>
<li><p>Example: Suppose that <span class="math inline">\(X\sim Geom(p)\)</span>. Then the moments of <span class="math inline">\(X\)</span> are</p></li>
</ul>
<p><span class="math display">\[\mathrm{E}(X^{r})=\left.{\frac{d^{r}}{(d t)^{r}}}\psi_{X}(t)\right|_{t=0}={\frac{d}{d t}}\left.\left[{\frac{p e^{t}}{1-(1-p)e^{t}}}\right]\right|_{t=0}\]</span></p>
<p>Specifically,</p>
<p><span class="math display">\[{\frac{d}{d t}}\psi_{X}(t)={\frac{d}{d t}}\left[{\frac{p e^{t}}{1-(1-p)e^{t}}}\right]=\psi_{X}(t)+{\frac{1-p}{p}}\psi_{X}(t)^{2}\]</span></p>
<p>and</p>
<p><span class="math display">\[\frac{d^{2}}{(d t)^{2}}\psi_{X}(t)=\frac{d}{d t}\left[\psi_{X}(t)+\frac{1-p}{p}\psi_{X}(t)^{2}\right]\]</span></p>
<p><span class="math display">\[=\psi_{X}(t)+{\frac{1-p}{p}}\psi_{X}(t)^{2}+{\frac{1-p}{p}}2\psi_{X}(t)\left[\psi_{X}(t)+{\frac{1-p}{p}}\psi_{X}(t)^{2}\right]\]</span></p>
<p>Therefore, <span class="math inline">\(E(X) = 1+{\frac{1-p}{p}}={\frac{1}{p}};\)</span>, <span class="math inline">\(E(X^{2}) = 1+{\frac{1-p}{p}}+{\frac{1-p}{p}}2\left[1+{\frac{1-p}{p}}\right]={\frac{2-p}{p^{2}}}\)</span> and <span class="math inline">\(Var(X) ={\frac{2-p}{p^{2}}}-{\frac{1}{p^{2}}}={\frac{1-p^{*}}{p^{2}}}\)</span>.</p>
<ul>
<li>Example: Suppose that <span class="math inline">\(Y \sim Unif(a, b)\)</span>.
<ul>
<li>Use the MGF to find the central moments <span class="math inline">\(\textstyle\mathrm{E}[(Y-\mu Y)^{r}]=\mathrm{E}[(Y-{\frac{a+b}{2}})^{r}]\)</span>.</li>
<li>Solution:</li>
</ul></li>
</ul>
<p><span class="math display">\[\psi_{Y}(t)=\frac{1}{b-a}\int_{a}^{b}e^{t y}d y=\frac{e^{t b}-e^{t a}}{t(b-a)}\]</span></p>
<p><span class="math display">\[\psi_{Y-\mu,Y}(t)=e^{-t(a+b)/2}\psi_{Y}(t)=\frac{e^{-t(a+b)/2}[e^{t b}-e^{t a}]}{t(b-a)}\;\]</span></p>
<p><span class="math display">\[=\left(\frac{2}{t(b-a)}\right)\frac{e^{\frac{t}{2}(b-a)}-e^{-\frac{t}{2}(b-a)}}{2}\]</span></p>
<p><span class="math display">\[=\left({\frac{2}{t(b-a)}}\right)\sinh\left({\frac{t(b-a)}{2}}\right)\]</span></p>
<p><span class="math display">\[=\frac{2}{t(b-a)}\sum_{i=0}^{\infty}\left(\frac{t(b-a)}{2}\right)^{2i+1}\frac{1}{(2i+1)!}\]</span></p>
<p><span class="math display">\[=\sum_{i=0}^{\infty}\left(\frac{t(b-a)}{2}\right)^{2i}\frac{1}{(2i+1)!}=\sum_{i=0}^{\infty}\left(\frac{t^{2i}}{(2i)!}\right)\frac{(b-a)^{2i}}{2^{2i}(2i+1)!}\]</span></p>
<ul>
<li>Therefore, the odd moments are zero, and</li>
</ul>
<p><span class="math display">\[\mathrm{E}(Y-\mu_{Y})^{2i}={\frac{(b-a)^{2i}}{2^{2i}(2i+1)}}\]</span></p>
<ul>
<li>For example,</li>
</ul>
<p><span class="math display">\[\mathrm{E}(Y-\mu_Y)^{2}=(b-a)^{2}/12\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathrm{E}(Y-\mu_{Y})^{4}=(b-a)^{4}/80\]</span></p>
<!-------------------------------------->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Mathematical Statistics.pdf", "Mathematical Statistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
