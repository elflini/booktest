<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Families of Continuous Distributions | Mathematical Statistics</title>
  <meta name="description" content="This is a Mathematical Statistics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Families of Continuous Distributions | Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Mathematical Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Families of Continuous Distributions | Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is a Mathematical Statistics" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2024-08-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter5.html"/>
<link rel="next" href="chapter7.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#algebra-of-events"><i class="fa fa-check"></i><b>1.2</b> Algebra of Events</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#experiments-with-symmetries"><i class="fa fa-check"></i><b>1.3</b> Experiments with Symmetries</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#composition-of-experiments-counting-rules"><i class="fa fa-check"></i><b>1.4</b> Composition of Experiments: Counting Rules</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#sampling-at-random"><i class="fa fa-check"></i><b>1.5</b> Sampling at Random</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#binomial-multinomial-coefficients"><i class="fa fa-check"></i><b>1.6</b> Binomial &amp; Multinomial Coefficients</a></li>
<li class="chapter" data-level="1.7" data-path="chapter1.html"><a href="chapter1.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="1.8" data-path="chapter1.html"><a href="chapter1.html#subjective-probability"><i class="fa fa-check"></i><b>1.8</b> Subjective Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#probability-functions"><i class="fa fa-check"></i><b>2.1</b> Probability Functions</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Joint Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#conditional-probability"><i class="fa fa-check"></i><b>2.3</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#bayes-theorem-law-of-inverse-probability"><i class="fa fa-check"></i><b>2.4</b> Bayes Theorem (Law of Inverse Probability)</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#statistical-independence-of-random-variables"><i class="fa fa-check"></i><b>2.5</b> Statistical Independence of Random Variables</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#exchangeability"><i class="fa fa-check"></i><b>2.6</b> Exchangeability</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#application-probability-of-winning-in-craps"><i class="fa fa-check"></i><b>2.7</b> Application: Probability of Winning in Craps</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Expectations of Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#the-mean"><i class="fa fa-check"></i><b>3.1</b> The Mean</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#expectation-of-a-function"><i class="fa fa-check"></i><b>3.2</b> Expectation of a Function</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#variability"><i class="fa fa-check"></i><b>3.3</b> Variability</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#sums-of-random-variables"><i class="fa fa-check"></i><b>3.5</b> Sums of Random Variables</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#probability-generating-functions"><i class="fa fa-check"></i><b>3.6</b> Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Bernoulli and Related Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#sampling-bernoulli-populations"><i class="fa fa-check"></i><b>4.1</b> Sampling Bernoulli Populations</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#binomial-distribution"><i class="fa fa-check"></i><b>4.2</b> Binomial Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>4.3</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#negative-hypergeometric-distribution"><i class="fa fa-check"></i><b>4.6</b> Negative Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#approximating-binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Approximating Binomial Probabilities</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="chapter4.html"><a href="chapter4.html#normal-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.1</b> Normal approximation to the Binomial</a></li>
<li class="chapter" data-level="4.7.2" data-path="chapter4.html"><a href="chapter4.html#poisson-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.2</b> Poisson Approximation to the Binomial</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#poisson-distribution"><i class="fa fa-check"></i><b>4.8</b> Poisson Distribution</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#law-of-large-numbers"><i class="fa fa-check"></i><b>4.9</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="4.10" data-path="chapter4.html"><a href="chapter4.html#multinomial-distributions"><i class="fa fa-check"></i><b>4.10</b> Multinomial Distributions</a></li>
<li class="chapter" data-level="4.11" data-path="chapter4.html"><a href="chapter4.html#using-probability-generating-functions"><i class="fa fa-check"></i><b>4.11</b> Using Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.1</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#density-and-the-probability-element"><i class="fa fa-check"></i><b>5.2</b> Density and the Probability Element</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#the-median-and-other-percentiles"><i class="fa fa-check"></i><b>5.3</b> The Median and Other Percentiles</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#expected-value"><i class="fa fa-check"></i><b>5.4</b> Expected Value</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#expected-value-of-a-function"><i class="fa fa-check"></i><b>5.5</b> Expected Value of a Function</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#average-deviations"><i class="fa fa-check"></i><b>5.6</b> Average Deviations</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#bivariate-distributions"><i class="fa fa-check"></i><b>5.7</b> Bivariate Distributions</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#several-variables"><i class="fa fa-check"></i><b>5.8</b> Several Variables</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#covariance-and-correlation-1"><i class="fa fa-check"></i><b>5.9</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#independence"><i class="fa fa-check"></i><b>5.10</b> Independence</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#conditional-distributions"><i class="fa fa-check"></i><b>5.11</b> Conditional Distributions</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#moment-generating-functions"><i class="fa fa-check"></i><b>5.12</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Families of Continuous Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#normal-distributions"><i class="fa fa-check"></i><b>6.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#exponential-distributions"><i class="fa fa-check"></i><b>6.2</b> Exponential Distributions</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#gamma-distributions"><i class="fa fa-check"></i><b>6.3</b> Gamma Distributions</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#chi-squared-distributions"><i class="fa fa-check"></i><b>6.4</b> Chi Squared Distributions</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#distributions-for-reliability"><i class="fa fa-check"></i><b>6.5</b> Distributions for Reliability</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#t-f-and-beta-distributions"><i class="fa fa-check"></i><b>6.6</b> <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and Beta Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Organizing &amp; Describing Data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#frequency-distributions"><i class="fa fa-check"></i><b>7.1</b> Frequency Distributions</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#data-on-continuous-variables"><i class="fa fa-check"></i><b>7.2</b> Data on Continuous Variables</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#order-statistics"><i class="fa fa-check"></i><b>7.3</b> Order Statistics</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#data-analysis"><i class="fa fa-check"></i><b>7.4</b> Data Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#the-sample-mean"><i class="fa fa-check"></i><b>7.5</b> The Sample Mean</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#measures-of-dispersion"><i class="fa fa-check"></i><b>7.6</b> Measures of Dispersion</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#correlation"><i class="fa fa-check"></i><b>7.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Samples, Statistics, &amp; Sampling Distributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#random-sampling"><i class="fa fa-check"></i><b>8.1</b> Random Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#likelihood"><i class="fa fa-check"></i><b>8.2</b> Likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#sufficient-statistics"><i class="fa fa-check"></i><b>8.3</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#sampling-distributions"><i class="fa fa-check"></i><b>8.4</b> Sampling Distributions</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>8.5</b> Simulating Sampling Distributions</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#order-statistics-1"><i class="fa fa-check"></i><b>8.6</b> Order Statistics</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#moments-of-sample-means-and-proportionssp"><i class="fa fa-check"></i><b>8.7</b> Moments of Sample Means and Proportionssp</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>8.8</b> The Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#using-the-moment-generating-function"><i class="fa fa-check"></i><b>8.9</b> Using the Moment Generating Function</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#normal-populations"><i class="fa fa-check"></i><b>8.10</b> Normal Populations</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#updating-prior-probabilities-via-likelihood"><i class="fa fa-check"></i><b>8.11</b> Updating Prior Probabilities Via Likelihood</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#some-conjudate-families"><i class="fa fa-check"></i><b>8.12</b> Some conjudate Families</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#predictive-distributions"><i class="fa fa-check"></i><b>8.13</b> Predictive Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Estimation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#point-estimation"><i class="fa fa-check"></i><b>9.1</b> Point Estimation</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#errors-in-estimation"><i class="fa fa-check"></i><b>9.2</b> Errors in Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#consistency"><i class="fa fa-check"></i><b>9.3</b> Consistency</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#large-sample-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Large Sample Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#determining-sample-size"><i class="fa fa-check"></i><b>9.5</b> Determining Sample Size</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#small-sample-confidence-intervals-for-mu_x"><i class="fa fa-check"></i><b>9.6</b> Small Sample Confidence Intervals for <span class="math inline">\(\mu_X\)</span></a></li>
<li class="chapter" data-level="9.7" data-path="chapter9.html"><a href="chapter9.html#the-distribution-of-t"><i class="fa fa-check"></i><b>9.7</b> The Distribution of <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="9.8" data-path="chapter9.html"><a href="chapter9.html#pivotal-quantities"><i class="fa fa-check"></i><b>9.8</b> Pivotal Quantities</a></li>
<li class="chapter" data-level="9.9" data-path="chapter9.html"><a href="chapter9.html#estimating-a-mean-difference"><i class="fa fa-check"></i><b>9.9</b> Estimating a Mean Difference</a></li>
<li class="chapter" data-level="9.10" data-path="chapter9.html"><a href="chapter9.html#umvue"><i class="fa fa-check"></i><b>9.10</b> UMVUE</a></li>
<li class="chapter" data-level="9.11" data-path="chapter9.html"><a href="chapter9.html#bayes-estimators"><i class="fa fa-check"></i><b>9.11</b> Bayes Estimators</a></li>
<li class="chapter" data-level="9.12" data-path="chapter9.html"><a href="chapter9.html#efficiency"><i class="fa fa-check"></i><b>9.12</b> Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Significance Testing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chapter10.html"><a href="chapter10.html#hypotheses"><i class="fa fa-check"></i><b>10.1</b> Hypotheses</a></li>
<li class="chapter" data-level="10.2" data-path="chapter10.html"><a href="chapter10.html#assessing-the-evidence"><i class="fa fa-check"></i><b>10.2</b> Assessing the Evidence</a></li>
<li class="chapter" data-level="10.3" data-path="chapter10.html"><a href="chapter10.html#one-sample-z-tests"><i class="fa fa-check"></i><b>10.3</b> One Sample <span class="math inline">\(Z\)</span> Tests</a></li>
<li class="chapter" data-level="10.4" data-path="chapter10.html"><a href="chapter10.html#one-sample-t-tests"><i class="fa fa-check"></i><b>10.4</b> One Sample <span class="math inline">\(t\)</span> Tests</a></li>
<li class="chapter" data-level="10.5" data-path="chapter10.html"><a href="chapter10.html#some-nonparametric-tests"><i class="fa fa-check"></i><b>10.5</b> Some Nonparametric Tests</a></li>
<li class="chapter" data-level="10.6" data-path="chapter10.html"><a href="chapter10.html#probability-of-the-null-hypothesis"><i class="fa fa-check"></i><b>10.6</b> Probability of the Null Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Tests as Decision Rules</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#rejection-regions-and-errors"><i class="fa fa-check"></i><b>11.1</b> Rejection Regions and Errors</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#the-power-function"><i class="fa fa-check"></i><b>11.2</b> The Power function</a></li>
<li class="chapter" data-level="11.3" data-path="chapter11.html"><a href="chapter11.html#choosing-a-sample-size"><i class="fa fa-check"></i><b>11.3</b> Choosing a Sample Size</a></li>
<li class="chapter" data-level="11.4" data-path="chapter11.html"><a href="chapter11.html#most-powerful-tests"><i class="fa fa-check"></i><b>11.4</b> Most Powerful Tests</a></li>
<li class="chapter" data-level="11.5" data-path="chapter11.html"><a href="chapter11.html#uniformly-most-powerful-tests"><i class="fa fa-check"></i><b>11.5</b> Uniformly Most Powerful Tests</a></li>
<li class="chapter" data-level="11.6" data-path="chapter11.html"><a href="chapter11.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>11.6</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="11.7" data-path="chapter11.html"><a href="chapter11.html#bayesian-testing"><i class="fa fa-check"></i><b>11.7</b> Bayesian Testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chapter12.html"><a href="chapter12.html"><i class="fa fa-check"></i><b>12</b> Appendix</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chapter12.html"><a href="chapter12.html#greek-alphabet"><i class="fa fa-check"></i><b>12.1</b> Greek Alphabet</a></li>
<li class="chapter" data-level="12.2" data-path="chapter12.html"><a href="chapter12.html#abbreviations"><i class="fa fa-check"></i><b>12.2</b> Abbreviations</a></li>
<li class="chapter" data-level="12.3" data-path="chapter12.html"><a href="chapter12.html#practice-exams"><i class="fa fa-check"></i><b>12.3</b> PRACTICE EXAMS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter6" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Families of Continuous Distributions<a href="chapter6.html#chapter6" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="normal-distributions" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Normal Distributions<a href="chapter6.html#normal-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>PDF and CDF of Standard Normal Distribution</li>
</ul>
<p><span class="math display">\[ f_Z(z)=\frac{e^{-z^2/2}}{\sqrt {2\pi}}\]</span></p>
<p><span class="math display">\[ F_Z(z)=P(Z≤ z)= Φ(z) =\int_{-\infty}^zf_z(u)du\]</span></p>
<ul>
<li>To verify that <span class="math inline">\(f_Z(z)\)</span> integrates to one, examine <span class="math inline">\(K^2\)</span>, where
<span class="math inline">\(K=\int _{-\infty}^{\infty}e^{-u^2/2}du\)</span>:</li>
</ul>
<p><span class="math display">\[\begin{align}
K^2 &amp;= \Bigg(\int_{-\infty}^{\infty}e^{-u^2/2}du\Bigg)^2 \\
&amp;=\Bigg(\int_{-\infty}^{\infty}e^{-u_1^2/2}du_1\Bigg)\Bigg(\int_{-\infty}^{\infty}e^{-u_2^2/2}du_2\Bigg) \\
&amp;=\Bigg(\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-\frac{1}{2}(u_1^2+u_2^2)}du_1du_2 .\Bigg)
\end{align}
\]</span></p>
<ul>
<li>Now transform to polar coordinates:</li>
</ul>
<p><span class="math display">\[\begin{align}
u_1&amp;=rsin\theta; u_2= rcos\theta \ and \\
K^2&amp;=\int_0^{2\pi}\int_0^\infty e^{-\frac{1}{2}(r^2)}r \ drd\theta \\
&amp;= \int_0^{2\pi}\Bigg[-e^{\frac{1}{2}(r^2)}r \ drd\theta \\
&amp;= \int_0^{2\pi}1d\theta \ =2\pi
\end{align}
\]</span></p>
<ul>
<li><p>Therefore <span class="math inline">\(K =\sqrt{2\pi} \ and \ f_Z(z)\)</span> integrates to one.</p></li>
<li><p>Other Normal Distributions: Transform from Z to <span class="math inline">\(X = \mu + \sigma Z\)</span>, where
<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are constants that satisfy <span class="math inline">\(|\mu| &lt; \infty \,\,\,  and \,\,\, 0 &lt; \sigma &lt; \infty\)</span></p></li>
<li><p>The inverse transformation is <span class="math inline">\(z = (x − \mu)/sigma\)</span> and the Jacobian of the
transformation is</p></li>
</ul>
<p><span class="math display">\[  |J|= \Bigg|\frac{dz}{dx}\Bigg| =\frac{1}{\sigma}\]</span></p>
<p>Accordingly, the pdf of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[f_X(x)=f_Z(\frac{x-\mu}{\sigma})\frac{1}{\sigma}=\frac{e^{-\frac{1}{2\sigma^2}(x-\mu)^2}}{\sqrt{2\pi\sigma^2}}.\]</span></p>
<ul>
<li><p>We will use the notation <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> to mean that <span class="math inline">\(X\)</span> has a
normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>Moment Generating Function: Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>. THen</p></li>
</ul>
<p><span class="math display">\[\psi_X(t)=e^{\mu t+t^2\sigma^2/2.}\]</span></p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[
\psi_X(t)=E(e^{tX})=\int_{-\infty}^\infty e^{tX}\frac{e^{-\frac{1}{2\sigma^2}(x-\mu)^2}}{\sqrt{2\pi\sigma^2}}dx=\int_{-\infty}^\infty \frac{e^{-\frac{1}{2\sigma^2}[-2t\sigma^2x+(x-\mu)^2]}}{\sqrt{2\pi\sigma^2}}dx
\]</span></p>
<ul>
<li>The trick is to complete the square in the exponent:</li>
</ul>
<p><span class="math display">\[=−2tσ^2+(x-\mu)^2=-2t\sigma^2x+x^2-2x\mu+\mu^2=x^2-2x(\mu+t\sigma^2)+\mu^2\]</span></p>
<p><span class="math display">\[=[x-(\mu+t\sigma^2)]^2-(\mu+t\sigma^2)^2+\mu^2=[x-(\mu+t\sigma^2)]^2-2\mu\sigma^2-t^2\sigma^4\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
=\int_{-\infty}^\infty \frac{e^{-\frac{1}{2\sigma^2}[-2t\sigma^2x+(x-\mu)^2]}}{\sqrt{2\pi\sigma^2}}dx \ where \ \mu^*=\mu + t\sigma^2\]</span></p>
<p><span class="math display">\[
=e^{t\mu+t^2\sigma^2/2}
\]</span></p>
<ul>
<li>because the second term is the integral of the pdf of a random variable
with distribution <span class="math inline">\(N(\mu^*, \sigma^2)\)</span> and this integral is one.</li>
</ul>
<div id="moments-of-normal-distributions" class="section level4 unnumbered hasAnchor">
<h4>Moments of Normal Distributions<a href="chapter6.html#moments-of-normal-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Moments of the standard normal distribution: Let <span class="math inline">\(Z\)</span> be a normal
random variable with <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma = 1\)</span>. That is, <span class="math inline">\(Z ∼ N(0, 1).\)</span></li>
</ol>
<ul>
<li>The moment generating function of Z is <span class="math inline">\(\psi_Z(t) = e^{t^2/2}\)</span>.</li>
<li>The Taylor series expansion of <span class="math inline">\(\psi_Z(t)\)</span> around <span class="math inline">\(t = 0\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[\psi_Z(t)=e^{t^2/2}=\Sigma_{i=0}^\infty\frac{1}{i!}\Bigg(\frac{t^2}{2}\Bigg)^i =\Sigma_{i=0}^\infty\Bigg(\frac{(2i)!}{2^ii!}\Bigg)\Bigg(\frac{t^{2i}}{(2i)!}\Bigg)\]</span></p>
<ul>
<li>Note that all odd powers in the expansion are zero. Accordingly,</li>
</ul>
<p><span class="math display">\[
E(Z^r)=\begin{cases}0 &amp; if\ r\ is\ odd \\ \frac {r!}{2^{r/2}(\frac{r}{2})!} &amp; if \ r \ is \ even.\end{cases}
\]</span></p>
<ul>
<li>It can be shown by induction that if r is even, then</li>
</ul>
<p><span class="math display">\[ \frac{r!}{2^{r/2}}(\frac{r}{2})!=(r-1)(r-3)(r-5)···1\]</span></p>
<ul>
<li><p>In particular, <span class="math inline">\(E(Z) = 0 \ and\ Var(Z) = E(Z^2) = 1\)</span>.</p></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Moments of Other Normal Distributions: Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>. Then <span class="math inline">\(X\)</span> can be written as <span class="math inline">\(X = \mu + Z\sigma\)</span>, where <span class="math inline">\(Z ∼ N(0, 1)\)</span>. To obtain the moments of <span class="math inline">\(X\)</span>, one may use the moments of <span class="math inline">\(Z\)</span> or one may differentiate the moment generating function of <span class="math inline">\(X\)</span>. For example, using the moments of <span class="math inline">\(Z\)</span>, the first two moments of <span class="math inline">\(X\)</span> are</li>
</ol></li>
</ul>
<p><span class="math display">\[E(X) = E(µ + σZ) = µ + σE(Z) = µ\]</span></p>
<p>and</p>
<p><span class="math display">\[E(X^2)=E[(µ + σZ)^2]=E[µ^2+2µσZ+\sigma^2Z^2]=\mu^2+\sigma^2\]</span></p>
<ul>
<li><p>Note that <span class="math inline">\(Var(X) = E(X^2) − [E(X)]^2 = σ^2\)</span>.</p></li>
<li><p>The alternative approach is to use the moment generating function:</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
E(X)&amp;=ψ_X(t)\frac{d}{dt}\Bigg|_{t=0}=\frac{d}{dt}e^{\mu t+t^2\sigma^2/2}\Bigg|_{t=0} \\
&amp;=(\mu+ t\sigma^2)e^{\mu t+t^2\sigma^2/2}\Bigg|_{t=0}=\mu \ and
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
E(X^2)&amp;=\frac{d^2}{dt^2}ψ_X(t)\Bigg|_{t=0}=\frac{d}{dt}(\mu+ t\sigma^2)e^{\mu t+t^2\sigma^2/2}\Bigg|_{t=0}  \\
&amp;=\sigma^2e^{t\mu+t^2\sigma^2/2}+(\mu+ t\sigma^2)e^{\mu t+t^2\sigma^2/2}\Bigg|_{t=0}=\mu^2+\sigma^2
\end{align}
\]</span></p>
<ul>
<li><p>Box-Muller method for generating standard normal variables. Let <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> be iid random variables with distributions <span class="math inline">\(Z_i \sim N(0, 1)\)</span>.</p></li>
<li><p>The joint pdf of <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> is</p></li>
</ul>
<p><span class="math display">\[
f_{Z_1},f_{Z_2}(z_1,z_2)=\frac{e^{\frac{1}{2}(z_1^2+z_2^2)}}{2\pi}
\]</span></p>
<ul>
<li>Transform to polar coordinates: <span class="math inline">\(Z_1 = R sin(T)\)</span> and <span class="math inline">\(Z_2 = R cos(T)\)</span>. The joint distribution of <span class="math inline">\(R\)</span> and <span class="math inline">\(T\)</span> is</li>
</ul>
<p><span class="math display">\[
\begin{align}
f_{(R_1,T)}(r,t)&amp;=\frac{re^{-\frac{1}{2}r^2}}{2\pi}I_{(0,\infty)}(r)I_{(0,2\pi)}(T)=f_R(r)×f_T(t) where \\
f_R(r)&amp;=re^{-\frac{1}{2}r^2}I_{(0,\infty)}(r)I_{(0,2\pi)}\ and f_T(t)=\frac{1}{2\pi}I_{(0,2\pi)}(t).
\end{align}
\]</span></p>
<ul>
<li>Factorization of the joint pdf reveals that <span class="math inline">\(RㅛT\)</span>. Their respective cdfs are</li>
</ul>
<p><span class="math display">\[
F_R(r)=1-e^{\frac{1}{2}r^2} \text{ and } F_T(t)=\frac{t}{2\pi}
\]</span></p>
<ul>
<li>Let <span class="math inline">\(U_1 = F_R(R)\)</span> and <span class="math inline">\(U_2 = F_T(T)\)</span>. Recall that <span class="math inline">\(U_i \sim Unif(0, 1)\)</span>.
<ul>
<li>Solving the cdf equations for <span class="math inline">\(R\)</span> and <span class="math inline">\(T\)</span> yields</li>
</ul></li>
</ul>
<p><span class="math display">\[
R=\sqrt{-2ln(1-U_1)}\ and\ T=2\pi U_2
\]</span></p>
<p>Lastly, express <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> as functions of <span class="math inline">\(R\)</span> and <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[
\begin{align}
Z_1&amp;=Rsin(T)=\sqrt{-2ln(1-U_1)}sin(2\pi U_2) \ and  \\
Z_2&amp;=Rcost(T)=\sqrt{-2ln(1-U_1)}cos(2\pi U_2)
\end{align}
\]</span></p>
<ul>
<li>Note that <span class="math inline">\(U_1\)</span> and <span class="math inline">\(1 − U_1\)</span> have the same distributions. Therefore
<span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> can be generated from <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> by</li>
</ul>
<p><span class="math display">\[
Z_1=\sqrt{-2ln(U_1)}sin(2\pi U_2) \text{ and } Z_2=\sqrt{-2ln(U_1)}cos(2\pi U_2)
\]</span></p>
<ul>
<li><p>Linear Functions of Normal Random Variables: Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are
independently distributed random variables with distributions
<span class="math inline">\(X\sim N(\mu_x,\sigma^2_x)\)</span> and <span class="math inline">\(Y\sim N(\mu_Y,\sigma^2_Y)\)</span>.</p></li>
<li><ol style="list-style-type: lower-alpha">
<li>The distribution of <span class="math inline">\(aX + b\)</span> is <span class="math inline">\(N(a\mu x+b,a^2\sigma^2_X)\)</span>.</li>
</ol>
<ul>
<li>Proof: The moment generating function of <span class="math inline">\(aX + b\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\psi_{aX+b}(t)=E(e^{t(aX+b)})=e^{tb}E(e^{taX})=e^{tb}e^{ta\mu+t^2a^2\sigma^2/2}=e^{a\mu +b)+t^2(a\sigma)^2/2}
\]</span></p>
<p>and this is the moment generating function of a random variable with
distribution <span class="math inline">\(N(a\mu +b,a^2\sigma^2)\)</span>.</p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>The distribution of
<span class="math inline">\(aX + bY\)</span> is <span class="math inline">\(N(a\mu_X+b\mu y,a^2\sigma^2_X+b^2\sigma^2_Y)\)</span>.</li>
</ol>
<ul>
<li>Proof: The moment generating function of <span class="math inline">\(aX+bY\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}\psi_{aX+bY}(t)&amp;=E(e^{t(aX+bY)})=e^{tb}E(e^{taX})E(e^{tbY}) \text{ by independence }
\\&amp;=e^{ta\mu_X+t^2a^2\sigma^2_X/2}e^{tb\mu _Y+t^2b^2\sigma^2_Y/2}=e^{t(a\mu _X+b\mu_Y)+t^2(a^2\sigma^2_X+b^2\sigma^2_Y)/2}.
\end{align}\
\]</span></p>
<p>and this is the moment generating function of a random variable with
distribution <span class="math inline">\(N(a\mu_X+b\mu_Y,a^2\sigma^2_X+b^2\sigma^2_Y).\)</span><br />
</p>
<ul>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>The above result is readily generalized. Suppose that
<span class="math inline">\(X_i\)</span> for <span class="math inline">\(i=1,\dots,n\)</span> are independently distributed as <span class="math inline">\(X_i \sim (\mu_i,\sigma^2_i)\)</span>.</li>
</ol>
<ul>
<li>If <span class="math inline">\(\Sigma^n_{i=1}a_iX_i\)</span>, then <span class="math inline">\(T \sim N(\mu_T,\sigma^2_T)\)</span>, where <span class="math inline">\(\mu_T=\Sigma^n_{i=1}a_i\mu_i\)</span> and <span class="math inline">\(\sigma^2_T=\Sigma^n_{i=1}a^2_i\sigma^2_i\)</span>.</li>
</ul></li>
</ul>
</div>
<div id="probabilities-and-percentiles" class="section level4 unnumbered hasAnchor">
<h4>Probabilities and Percentiles<a href="chapter6.html#probabilities-and-percentiles" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(X\sim N(\mu_X,\sigma^2_X)\)</span>, then the probability of an interval
is</li>
</ol></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(a\le X\le b)&amp;=P\Bigg[\frac{a-\mu_X}{\sigma_X}\le Z\le\frac{b-\mu_X}{\sigma_X}\Bigg]\\
&amp;=\phi\Bigg(\frac{b-\mu_X}{\sigma_X}\Bigg)-\phi\Bigg(\frac{a-\mu_X}{\sigma_X}\Bigg)
.
\end{align}\
\]</span></p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(X \sim N(\mu_X,\sigma^2_X),\)</span> then the <span class="math inline">\(100p^{th}\)</span> percentile of
<span class="math inline">\(X\)</span> is
<span class="math display">\[x_p=\mu X+\sigma_Xz_p\]</span></li>
</ol></li>
</ul>
<p>where <span class="math inline">\(z_p\)</span> is the <span class="math inline">\(100p^{th}\)</span> percentile of the standard normal distribution.</p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[P(X\le\mu_X+\sigma_Xz_p)=P\Bigg(\frac{X-\mu_X}{\sigma_X}\le z_p\Bigg)=P(Z\le z_p)=p\]</span></p>
<p>because <span class="math inline">\(Z\sim N(0,1)\)</span>.</p>
</div>
<div id="log-normal-distribution" class="section level4 unnumbered hasAnchor">
<h4>Log Normal Distribution<a href="chapter6.html#log-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Definition: If <span class="math inline">\(ln(X) \sim N(\mu,\sigma^2)\)</span>, then <span class="math inline">\(X\)</span> is is said to
have a log normal distribution.</li>
</ul>
<p><span class="math display">\[ln(X)\sim N(\mu,\sigma^2) \Leftrightarrow X \sim LogN(\mu,\sigma^2). \]</span></p>
<ul>
<li>Note: <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are the mean and variance of <span class="math inline">\(ln(X)\)</span>, not of <span class="math inline">\(X\)</span>.</li>
</ul>
<p>-PDF: Let <span class="math inline">\(Y = ln(X)\)</span>, and assume that <span class="math inline">\(Y \sim N(\mu,\sigma^2)\)</span>.
- Note that <span class="math inline">\(x=g(y)\)</span> and <span class="math inline">\(y=g^{-1}(X)\)</span>, where <span class="math inline">\(g(y)=e^y\)</span> and
<span class="math inline">\(g^{-1}(x)=ln(x)\)</span>.
- The Jacobian of the transformation is</p>
<p><span class="math display">\[
|J|=\Bigg|\frac{dy}{dx}\Bigg|=\frac{1}{x}
\]</span></p>
<ul>
<li>Accordingly, the pdf of <span class="math inline">\(X\)</span> is</li>
</ul>
<p><span class="math display">\[f_X(x)=f_Y[g^{-1}(x)]\frac{1}{x}=\frac{e^{-\frac{1}{2\sigma^2}[ln(x)-\mu]^2}}{x\sqrt{2\pi\sigma^2}}I_{(0,\infty)}(x).\]</span></p>
<p>-CDF: If <span class="math inline">\(Y \sim LogN(\mu,\sigma^2)\)</span>, then</p>
<p><span class="math display">\[P(Y \le y)=P[ln(Y)\le ln(y)]=\phi \Bigg(\frac{1n(y)-\mu}{\sigma}\Bigg).\]</span></p>
<ul>
<li>Moments of a log normal random variable.
<ul>
<li>Suppose that <span class="math inline">\(X \sim LogN(\mu,\sigma^2)\)</span>.</li>
<li>Then <span class="math inline">\(E(x^r)=e^{\mu r+r^2\sigma^2/2}\)</span>.</li>
<li>Proof: Let <span class="math inline">\(Y = ln(X)\)</span>. Then <span class="math inline">\(Y \sim N(\mu,\sigma^2)\)</span> and</li>
</ul></li>
</ul>
<p><span class="math display">\[
E(X^r)=E(e^{rY})=e^{r\mu +r^2\sigma^2/2}
\]</span></p>
<p>where the final result is obtained by using the MGF of a normal random variable. To obtain the mean and variance, set r to 1 and 2:</p>
<p><span class="math display">\[
E(X)=e^{\mu+\sigma^2} \text{ and } Var(X)=e^{2\mu +2\sigma^2}-e^{2\mu+\sigma^2}=e^{2\mu+\sigma^2}\bigg[e^{\sigma^2}-1\bigg]
\]</span></p>
<ul>
<li>Displays of various log normal distributions. The figure below displays four log normal distributions. The parameters of the distribution are summarized in the following table:</li>
</ul>
<center>
<img src="fig6/fig6_1.jpg" />
</center>
<ul>
<li>Note that each distribution has mean equal to <span class="math inline">\(100\)</span>. The distributions differ in terms of <span class="math inline">\(θ\)</span>, which is the coefficient of variation.</li>
</ul>
<center>
<img src="fig6/fig6_2.jpg" />
</center>
<ul>
<li>If the coefficient of variation is small, then the log normal distribution
resembles an exponential distribution, As the coefficient of variation
increases, the log normal distribution converges to a normal distribution.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="exponential-distributions" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Exponential Distributions<a href="chapter6.html#exponential-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>PDF: <span class="math inline">\(f_X(x)=\lambda e^{-\lambda x}I_{[0,\infty)}(x)\)</span> where λ is a positive parameter</p></li>
<li><p>CDF: <span class="math inline">\(F_X(x)= 1-e^{-\lambda x}\)</span> provided that <span class="math inline">\(x \ge 0\)</span>.</p></li>
<li><p>We will use the notation <span class="math inline">\(X\sim Exp(\lambda)\)</span> to mean that <span class="math inline">\(X\)</span>
has an exponential distribution with parameter <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>Note that the <span class="math inline">\(100p^{th}\)</span> percentile is <span class="math inline">\(x_p = − ln(1 − p)/λ\)</span>.</p></li>
<li><p>The median, for example, is <span class="math inline">\(x_{0.5} = ln(2)/λ\)</span>.</p></li>
<li><p>Moment Generating Function. If <span class="math inline">\(X\sim Exp(\lambda)\)</span>, then <span class="math inline">\(\psi_X(t) = \lambda/(\lambda − t)\)</span> for <span class="math inline">\(t &lt;\lambda\)</span>.</p>
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
\psi_X(t)&amp;=E(e^{tx})=\int_0^\infty e^{tx}\lambda e^{-\lambda x}dx \\
&amp;=\int_0^\infty \lambda e^{(\lambda -t)x}dx=\frac{\lambda}{\lambda-t}\int_0^\infty (\lambda -t)e^{(\lambda -t)x}dx=\frac{\lambda}{\lambda-t}
\end{align}
\]</span></p>
<ul>
<li><p>because the last integral is the integral of the pdf of a random variable with
distribution <span class="math inline">\(X\sim Exp(\lambda)\)</span>, provided that <span class="math inline">\(\lambda − t &gt; 0\)</span>.</p></li>
<li><p>Moments: If <span class="math inline">\(X \sim Exp(\lambda)\)</span>, then <span class="math inline">\(E(X^r)=r!/\lambda^r\)</span>,</p>
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
\psi_X (t)&amp;=\frac{\lambda}{\lambda-t}=\frac{1}{1-t/\lambda}=\Sigma^\infty_{r=0}\bigg(\frac{t}{\lambda}\bigg)^r \\
&amp;=\Sigma^\infty_{r=0}\bigg(\frac{t^r}{r!}\bigg)\bigg(\frac{r!}{\lambda^r}\bigg)
\end{align}
\]</span></p>
<p>provided that <span class="math inline">\(-\lambda &lt;t &lt; \lambda\)</span>.</p>
<ul>
<li><p>Note that <span class="math inline">\(E(X) = 1/\lambda\)</span>, <span class="math inline">\(E(X^2) = 2/\lambda^2\)</span> and <span class="math inline">\(Var(X) = 1/\lambda^2\)</span>.</p></li>
<li><p>Displays of exponential distributions. Below are plots of four exponential
distributions.</p>
<ul>
<li>Note that the shapes of the distributions are identical.</li>
</ul></li>
</ul>
<center>
<img src="fig6/fig6_3.jpg" />
</center>
<div id="memoryless-property" class="section level4 unnumbered hasAnchor">
<h4>Memoryless Property<a href="chapter6.html#memoryless-property" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Suppose that <span class="math inline">\(X\sim Exp(\lambda)\)</span>.
<ul>
<li>The random variable can be thought of as the waiting time for an event to occur.</li>
<li>Given that an event has not occurred in the interval [0, w), find the probability that the additional waiting time is at least <span class="math inline">\(t\)</span>.</li>
<li>That is, find <span class="math inline">\(P(X &gt; t + w|X &gt; w)\)</span>.</li>
<li>Note: <span class="math inline">\(P(X &gt; t)\)</span> is sometimes called the reliability function.</li>
<li>It is denoted as <span class="math inline">\(R(t)\)</span> and is related to <span class="math inline">\(F_X (t)\)</span> by</li>
</ul></li>
</ul>
<p><span class="math display">\[
R(t)=P(X \gt t)=1-P(X \le t)=1-F_X(t)
\]</span></p>
<ul>
<li><p>The reliability function represents the probability that the lifetime of a
product (i.e., waiting for failure) is at least <span class="math inline">\(t\)</span> units.</p></li>
<li><p>For the exponential distribution, the reliability function is <span class="math inline">\(R(t)=e^{-\lambda t}\)</span>.</p>
<ul>
<li>We are interested in the conditional reliability function <span class="math inline">\(R(t + w|X &gt; w)\)</span>.</li>
<li>Solution:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
R(t+\omega|X \gt \omega)&amp;=P(X &gt; t + w|X &gt; w)=\frac{P(X &gt; t + w)}{P(X &gt; w)} \\
&amp;=\frac{e^{-\lambda (t+\omega)}}{e^{-\lambda\omega}}=e^{-\lambda t}.
\end{align}
\]</span></p>
<ul>
<li><p>Also, <span class="math inline">\(R(t + w|X &gt; w) = 1 − F_X(t + w|X &gt; w)\Longrightarrow  F_X (t + w|X &gt; w) = 1 − e^{-\lambda t}\)</span>.</p></li>
<li><p>That is, no matter how long one has been waiting, the conditional distribution
of the remaining life time is still <span class="math inline">\(Exp(\lambda)\)</span>.</p>
<ul>
<li>It is as though the distribution does not remember that we have already been waiting w time units.</li>
</ul></li>
<li><p>Poison Inter-arrival Times: Suppose that events occur according to a Poisson
process with rate parameter <span class="math inline">\(\lambda\)</span>.</p>
<ul>
<li>Assume that the process begins at time <span class="math inline">\(0\)</span>.</li>
<li>Let <span class="math inline">\(T_1\)</span> be the arrival time of the first event and let <span class="math inline">\(T_r\)</span> be the time interval from the <span class="math inline">\((r-1)^{st}\)</span> arrival to the <span class="math inline">\(r^{th}\)</span> arrival.</li>
<li>That is <span class="math inline">\(T_1,\dots,T_n\)</span> are inter-arrival times.</li>
<li>We will find the joint distribution of <span class="math inline">\(T_1,T_2,\dots,T_n\)</span>.</li>
<li>Consider the joint pdf:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
f_{T_1,T_2,\dots,T_n}(t_1,t_2,\dots,t_n)&amp;=f_{T_1}(t_1)×f_{T_2|T_1}(t_2|t_1)×f_{T_3|T_1,T_2}(t_3|t_1,t_2)
×\dots×f_{T_n|T_1,\dots,T_n}(t)_n|t_1,\dots,t_{n-1})
\end{align}
\]</span></p>
<p>by the multiplication rule.</p>
<ul>
<li>To obtain the first term, first find the cdf of <span class="math inline">\(T_1\)</span>:</li>
</ul>
<p><span class="math display">\[
\begin{align}
f_{T_1}(t_1)&amp;= P(T_1 \le t_1)=P[one\ or\ more\ events\ in\ (0,t_1)]\\
&amp;=1-P[no \ events \ in \ (0,t_1)]=1-\frac{e^{-\lambda t_1}(\lambda t_1)^0}{0!}=1-e^{-\lambda t_1}.
\end{align}
\]</span></p>
<ul>
<li>Differentiating the CDF yields</li>
</ul>
<p><span class="math display">\[
f_{T_1}(t_1)\frac{d}{dt_1}(1-e^{-\lambda t_1})=\lambda e^{-\lambda t_2}
\]</span></p>
<ul>
<li>Each of the remaining conditional pdfs also is just an exponential pdf. Therefore,</li>
</ul>
<p><span class="math display">\[
f_{T_1,T_2,\dots,T_n}(t_1,t_2,\dots,t_n)=\prod _{i=1}^n \lambda e^{-\Lambda t_i}I_{[0,\infty)}(t_i)
\]</span></p>
<p>This joint pdf is the product of n marginal exponential pdfs. Therefore, the
inter-arrival times are iid exponential random variables. That is, <span class="math inline">\(T_i \sim^{iid} \ Exp(\lambda)\)</span>.</p>
<p><br></p>
</div>
</div>
<div id="gamma-distributions" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Gamma Distributions<a href="chapter6.html#gamma-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="erlang-distributions" class="section level4 unnumbered hasAnchor">
<h4>Erlang Distributions<a href="chapter6.html#erlang-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Consider a Poisson process with rate parameter <span class="math inline">\(\lambda\)</span>. Assume that the process begins at time <span class="math inline">\(0\)</span>. Let <span class="math inline">\(Y\)</span> be the time of the <span class="math inline">\(r^{th}\)</span> arrival. Using the differential method, the pdf of <span class="math inline">\(Y\)</span> can be obtained as follows:</li>
</ul>
<p><span class="math display">\[
\begin{align}
P(t&lt;Yt+dt) &amp;\approx P(r-1\ arrivals \ before \ time \ t) × P[one\  arrival \ in (t,t + dt)] \\
&amp;=\frac{e^{-\lambda t}(\lambda t)^{r-1}}{(r-1)!}× λdt
\end{align}
\]</span></p>
<p>Accordingly,</p>
<p><span class="math display">\[
f_Y(y)=\frac{e^{-\lambda y}\lambda^r y^{r-1}}{(r-1)!}I_{[0, \infty )}(y)
\]</span></p>
<ul>
<li><p>The above pdf is called the Erlang pdf.</p></li>
<li><p>Note that <span class="math inline">\(Y\)</span> is the sum of <span class="math inline">\(r\)</span> iid <span class="math inline">\(Exp(\lambda)\)</span> random variables. Accordingly, <span class="math inline">\(E(Y) = r/\lambda\)</span> and <span class="math inline">\(Var(Y) = r/\lambda^2\)</span></p></li>
<li><p>CDF of an Erlang random variable: <span class="math inline">\(F_Y(y) = 1 − P(Y &gt; y)\)</span> and <span class="math inline">\(P(Y &gt; y)\)</span>
is the probability that fewer that r events occur in <span class="math inline">\([0, y)\)</span>. Accordingly,</p></li>
</ul>
<p><span class="math display">\[
F_Y (y) = 1 − P(Y &gt; y)=1-\Sigma_{i=0}^{r-1}\frac{e^{-\lambda y}(\lambda y)^i}{i!}
\]</span></p>
</div>
<div id="gamma-function" class="section level4 unnumbered hasAnchor">
<h4>Gamma Function<a href="chapter6.html#gamma-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Definition:</li>
</ul>
<p><span class="math display">\[
\Gamma(\alpha)=\int_0^\infty u^{\alpha-1}e^{-u}du, where\ \alpha&gt;0
\]</span></p>
<ul>
<li>Alternative expression: Let <span class="math inline">\(z=\sqrt{2u}\)</span> so that <span class="math inline">\(u=z^2/2; du=z \ dz;\)</span> and</li>
</ul>
<p><span class="math display">\[
\Gamma(\alpha)=\int_0^\infty\frac{z^{2α−1}e^{-z^2/2}}{2^{α-1}}dz
\]</span></p>
<div id="properties-of-gammaalpha" class="section level5 unnumbered hasAnchor">
<h5>Properties of <span class="math inline">\(\Gamma(\alpha)\)</span><a href="chapter6.html#properties-of-gammaalpha" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><span class="math inline">\(Γ(1) = 1\)</span>
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\Gamma(1) =\int_0^\infty e^{-w}dw=-e^{-w}\Bigg|_0^\infty=-0+1=1
\]</span></p>
<ul>
<li><span class="math inline">\(\Gamma(\alpha + 1) = \alpha \Gamma(\alpha)\)</span>
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\Gamma(\alpha + 1)=\int_0^\infty w^{\alpha}e^{-w}dw\
\]</span></p>
<ul>
<li>Let <span class="math inline">\(u = w^{\alpha}\)</span>, let <span class="math inline">\(dv = e^{−w}dw\)</span> and use integration by parts to obtain
<span class="math inline">\(du = \alpha w^{\alpha−1}, v = −e^{−w}\)</span> and</li>
</ul>
<p><span class="math display">\[
\begin{align}
\Gamma(\alpha + 1)&amp;=-w^{\alpha}e^{-w}\Bigg|_0^\infty+ \alpha \int_0^\infty w^{\alpha-1}e^{-w}dw \\
&amp;=0+\alpha\Gamma(\alpha).
\end{align}
\]</span></p>
<ul>
<li>If <span class="math inline">\(n\)</span> is a positive integer, then <span class="math inline">\(\Gamma(n) = (n − 1)!\)</span>.
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\Gamma(n) = (n − 1)\Gamma(n − 1) = (n − 1)(n − 2)\Gamma(n − 2)
\]</span></p>
<ul>
<li><span class="math inline">\(\Gamma(\frac{1}{2}) =\sqrt{\pi}\)</span>.
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\Gamma(\frac{1}{2})=\int_0^\infty\frac{e^{-z^2/2}}{2^{-\frac{1}{2}}}dz=\sqrt{\pi}
\]</span></p>
<p>because the integral of the standard normal distribution is one.</p>
</div>
</div>
<div id="gamma-distribution" class="section level4 unnumbered hasAnchor">
<h4>Gamma Distribution<a href="chapter6.html#gamma-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>PDF and CDF: <span class="math inline">\(If \ Y \sim Gam(\alpha, \lambda)\)</span>, then</li>
</ul>
<p><span class="math display">\[
f_Y(y)=\frac{y^{\alpha-1}\lambda^α e^{-\lambda y}}{\Gamma(\alpha)}I_{[0, \infty )}(y) \ and \ F_Y(y)=\int_0^\infty\frac{u^{\alpha-1}\lambda^{\alpha} e^{-\lambda u}}{\Gamma(\alpha)}du
\]</span></p>
<ul>
<li><p>Note: <span class="math inline">\(\alpha\)</span> is called the shape parameter and <span class="math inline">\(\lambda\)</span> is called the scale parameter.</p></li>
<li><p>Moment Generating Function: If <span class="math inline">\(Y \sim Gam(\alpha, \lambda)\)</span>, then</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
\psi_Y(t)&amp;=\int_0^\infty e^{ty}\frac{y^{\alpha-1}\lambda^{\alpha} e^{-\lambda y}}{\Gamma(\alpha)}dy \\
&amp;=\int_0^\infty\frac{y^{\alpha-1}\lambda^α e^{-(\lambda-t) y}}{\Gamma(\alpha)}dy \\
&amp;=\frac{\lambda^{\alpha}}{(\lambda-t)^{\alpha}}\int_0^\infty\frac{y^{\alpha-1}(\lambda-t)^{\alpha} e^{-(\lambda-t) y}}{\Gamma(\alpha)}dy \\
&amp;=\frac{\lambda^{\alpha}}{(\lambda-t)^{\alpha}}
\end{align}
\]</span></p>
<ul>
<li><p>because the last integral is the integral of a random variable with
distribution <span class="math inline">\(Gam(\alpha, \lambda − t)\)</span> provided that <span class="math inline">\(\lambda − t &gt; 0\)</span>.</p></li>
<li><p>Moments: If <span class="math inline">\(Y \sim Gam(\alpha, \lambda)\)</span>, then</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
E(Y)&amp;=\frac{d}{dt}\psi_Y(t)\Bigg|_{t=0} =\frac{\lambda^{\alpha}\alpha}{(\lambda-t)^{\alpha+1}}\Bigg|_{t=0}=\frac{\alpha}{\lambda}; \\
E(Y^2)&amp;=\frac{d^2}{dt^2}\psi_Y(t)\Bigg|_{t=0} = \frac{\lambda^{\alpha}\alpha(\alpha +1)}{(\lambda-t)^{\alpha+2}}\Bigg|_{t=0} \\
&amp;=\frac{\alpha (\alpha+1)}{\lambda^2}; and \\
VAR(Y)&amp;=E(Y^2)-[E(Y)]^2=\frac{\alpha (\alpha+1)}{\lambda^2}-\frac{\alpha^2}{\lambda^2} =\frac{\alpha}{\lambda^2}
\end{align}
\]</span></p>
<ul>
<li>General expression for moments (including fractional moments). If
<span class="math inline">\(Y \sim Gam(\alpha, \lambda)\)</span>, then</li>
</ul>
<p><span class="math display">\[
E(Y^r)=\frac{\Gamma(\alpha + r)}{\lambda^r\Gamma(\alpha)} \,\,\,  \alpha+r &gt;0
\]</span></p>
<ul>
<li>Proof</li>
</ul>
<p><span class="math display">\[
\begin{align}
E(Y^r)&amp;=\int_0^\infty\frac{y^ry^{\alpha-1}\lambda^\alpha e^{-\lambda y}}{\Gamma(\alpha)}dy =\int_0^\infty\frac{y^{\alpha+r-1}\lambda^{\alpha} e^{-\lambda y}}{\Gamma(\alpha)}dy \\
&amp;=\frac{\Gamma(\alpha + r)}{\lambda^r\Gamma(\alpha)}\int_0^\infty\frac{y^{\alpha+r-1}\lambda^{\alpha+r} e^{-\lambda y}}{\Gamma(\alpha+r)}dy=
\frac{\Gamma(\alpha + r)}{\lambda^r\Gamma(\alpha)}
\end{align}
\]</span></p>
<ul>
<li><p>because the last integral is the integral of a random variable with
distribution <span class="math inline">\(Gam(\alpha + r, \lambda)\)</span>, provided that <span class="math inline">\(\alpha + r &gt; 0\)</span>.</p></li>
<li><p>Distribution of the sum of iid exponential random variables. Suppose
that <span class="math inline">\(Y_1, Y_2, \ldots , Y_k\)</span> are iid <span class="math inline">\(Exp(\lambda)\)</span> random variables. Then <span class="math inline">\(T=\Sigma_{i=1}^k=Y_i \sim Gam(k,\lambda)\)</span>.</p>
<ul>
<li>Proof:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\psi_{Y_i}(t)=\lambda/(\lambda-t)\Rightarrow \psi_T(t)=\lambda^k/(\lambda-t)^k
\]</span></p>
<ul>
<li>Note that the Erlang distribution is a gamma distribution with shape
parameter <span class="math inline">\(\alpha\)</span> equal to an integer.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="chi-squared-distributions" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Chi Squared Distributions<a href="chapter6.html#chi-squared-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Definition: Let <span class="math inline">\(Z_i\)</span> for <span class="math inline">\(i = 1, \ldots, k\)</span> be iid <span class="math inline">\(N(0, 1)\)</span> random variables. Then <span class="math inline">\(Y=\Sigma_{i=1}^kZ^2_i\)</span> is said to have a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(k\)</span> degrees of freedom. That is <span class="math inline">\(Y \sim \chi^2_k\)</span>.</p></li>
<li><p>MGF: <span class="math inline">\(\psi_Y (t) = (1 − 2t)^{-\frac{k}{2}}\)</span> for <span class="math inline">\(t&lt;0.5\)</span>.</p>
<ul>
<li>Proof: First find the MGF of <span class="math inline">\(Z_i^2\)</span>:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
\psi_{Z^2_i}(t)&amp;=E(e^{tZ^2})=\int_{-\infty}^\infty e^{tZ^2}\frac{e^{-\frac{1}{2}z^2}}{\sqrt{2\pi}}dz \\
&amp;=\int_{-\infty}^\infty \frac{e^{-\frac{1}{2(1-2t)^{-1}}z^2}}{\sqrt{2\pi}}dz=(1-2t)^{-\frac{1}{2}}\int_{-\infty}^\infty \frac{e^{-\frac{1}{2(1-2t)^{-1}}z^2}}{(1-2t)^{-\frac{1}{2}}\sqrt{2\pi}}dz \\
&amp;=(1-2t)^{-\frac{1}{2}}
\end{align}
\]</span></p>
<ul>
<li>because the last integral is the integral of a <span class="math inline">\(N[0,(1-2t)^{-1}]\)</span> random variable.
It follows that the MGF of Y is <span class="math inline">\((1-2t)^{-\frac{k}{2}}\)</span>. Note that this is the MGF of a
Gamma random variable with parameters <span class="math inline">\(\lambda = 0.5\)</span> and <span class="math inline">\(\alpha = k/2\)</span>. Accordingly</li>
</ul>
<p><span class="math display">\[
f_Y(y)=\frac{y^{\frac{k}{2}-1}e^{-\frac{1}{2}y}}{\Gamma\Big(\frac{k}{2}\Big)2^{\frac{k}{2}}}I_{(0,\infty)}(y)
\]</span></p>
<div id="properties-of-chi2-random-variables" class="section level4 unnumbered hasAnchor">
<h4>Properties of <span class="math inline">\(\chi^2\)</span> Random variables<a href="chapter6.html#properties-of-chi2-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>If <span class="math inline">\(Y \sim \chi^2_k,\)</span> then <span class="math inline">\(E(Y^r)=\frac{2^r\Gamma(k/2 + r)}{\Gamma(k/2)}\)</span> provided that <span class="math inline">\(k/2 + r &gt; 0\)</span>.</p></li>
<li><p>Proof: Use the moment result for Gamma random variables.</p></li>
<li><p>Using <span class="math inline">\(g \Gamma(\alpha + 1) = \alpha \Gamma(\alpha),\)</span> it is easy to show that <span class="math inline">\(E(Y) = k\)</span>, <span class="math inline">\(E(Y^2)==k(k + 2)\)</span>, and <span class="math inline">\(Var(X) = 2k\)</span>.</p></li>
<li><p><span class="math inline">\(Y \sim N(k, 2k)\)</span> for large <span class="math inline">\(k\)</span>. This is an application of the central limit</p></li>
</ul>
<p><span class="math display">\[
\sqrt{2Y}-\sqrt{2k-1}\sim N(0,1)
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(Y_1,Y_2,\dots,Y_n\)</span> are independently distributed as <span class="math inline">\(Y_I \sim χ^2_{k_i},\)</span> then <span class="math inline">\(\Sigma_{i=1}^n\ Y_i \sim χ^2_k,\)</span> where <span class="math inline">\(k=\Sigma^n_{i=1}k_i\)</span>.</p></li>
<li><p>Proof: use the MGF.</p></li>
<li><p><span class="math inline">\(X\sim  \chi^2_k\)</span>, <span class="math inline">\(X+ Y \sim \chi^2_n\)</span> and <span class="math inline">\(X \Perp Y\)</span> then <span class="math inline">\(Y \sim χ^2_{n-k}\)</span>.</p>
<ul>
<li>Proof: Note that by independence</li>
</ul></li>
</ul>
<p><span class="math display">\[
\psi_{X+Y}(t)=\psi_X(t)\psi_Y(t)
\]</span></p>
<p><br></p>
</div>
</div>
<div id="distributions-for-reliability" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Distributions for Reliability<a href="chapter6.html#distributions-for-reliability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: Suppose that <span class="math inline">\(L\)</span> is a nonnegative continuous rv. In particular,
suppose that <span class="math inline">\(L\)</span> is the lifetime (time to failure) of a component. The
reliability function is the probability that the lifetime exceeds <span class="math inline">\(x\)</span>. That is</li>
</ul>
<p><span class="math display">\[
\text{Reliability Function of } L = R_L(x)\overset{def}{=}P(L &gt; x) = 1 −F_L(x)
\]</span></p>
<ul>
<li>Result: If <span class="math inline">\(L\)</span> is a nonnegative continuous rv whose expectation exists, then</li>
</ul>
<p><span class="math display">\[
E(L)=\int_0^\infty R_L(x)dx=\int_0^\infty[1-F_L(x)]\ dx
\]</span></p>
<ul>
<li>Proof: Use integration by parts with <span class="math inline">\(u = RL(x) \Longrightarrow du = −f(x)\)</span> and <span class="math inline">\(dv = dx \Longrightarrow v = x\)</span>. Making these substitutions,</li>
</ul>
<p><span class="math display">\[
\begin{align}
\int_0^\infty R_L(u)du&amp;=\int_0^\infty u \ du= uv\Bigg|_0^\infty-\int_0^\infty u \ du \\
&amp;=x[1-F_L(x)]\Bigg|_0^\infty + \int_0^\infty xf_L(x)dx=E(L), \,\,\, \lim_{x-&gt;\infty}[1-F_L(x)]=0
\end{align}
\]</span></p>
<ul>
<li>Definition: the hazard function is the instantaneous rate of failure at time <span class="math inline">\(x\)</span>,
given that the component lifetime is at least <span class="math inline">\(x\)</span>. That is,</li>
</ul>
<p><span class="math display">\[
\begin{align}
\text{Harazd Function of } L &amp;= h_L(x)\overset{def}{=}\lim_{dx-&gt;0}\frac{P(x&lt;L&lt;x+dx|L&gt;x)}{dx} \\
&amp;=\lim_{dx-&gt;0} \Bigg[\frac{F_L(x+dx)-F_L(x)}{dx}  \Bigg]\frac{1}{R_L(x)}=\frac{f_L(x)}{R_L(x)}
\end{align}
\]</span></p>
<ul>
<li>Result:</li>
</ul>
<p><span class="math display">\[
\begin{align}
h_L(x)&amp;=\frac{d}{dx}ln[R_L(x)]=-\frac{1}{R_L(x)}×\frac{d}{dx}R_L(x) \\
&amp;=-\frac{1}{R_L(x)}× −f_L(x)=\frac{f_L(x)}{R_L(x)}
\end{align}
\]</span></p>
<ul>
<li>Result: If <span class="math inline">\(R_L(0)=1\)</span>, then</li>
</ul>
<p><span class="math display">\[
R_L(x)=exp\left \{-\int_0^\infty h_L(u)du\right \}
\]</span></p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp;h_L(x)=-\frac{d}{dx}\left \{ln[R_L(x)]-ln[R_L(0)]\right \}\\
&amp;\Rightarrow -h_L(x)= \left \{ln[R_L(u)]\Bigg|_0^x\right \} \Rightarrow-\int_0^xh_L(u)du=ln[R_L(x)] \\
&amp;\Rightarrow exp \left \{-\int_0^xh_L(u)du\right \}=R_L(x)
\end{align}
\]</span></p>
<ul>
<li>Result: the hazard function is constant if and only if time to failure has an exponential distribution.
<ul>
<li>Proof: First, suppose that time to failure has an exponential distribution. Then,</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_L(x)=\lambda e^{-\lambda x}I_{(0,\infty)}(x) \Rightarrow R_L(x)=e^{-\lambda x}\Rightarrow h_L(x)=\frac{\lambda e^{-\lambda x}}{e^{-\lambda x}}=\lambda
\]</span></p>
<ul>
<li>Second, suppose that the hazard function is a constant, <span class="math inline">\(\lambda\)</span>. Then,</li>
</ul>
<p><span class="math display">\[
\begin{align}
h_L(x)&amp;=\lambda \Rightarrow R_L(x)exp \Rightarrow exp \left \{-\int_0^x \lambda du\right \} \\
&amp;=e^{-\lambda x} \Rightarrow f_L(x)=\frac{d}{dx}[1-e^{-\lambda x}]=\lambda e^{-\lambda x}
\end{align}
\]</span></p>
<ul>
<li>Weibull Distribution: Increasing hazard function. The hazard function for the
Weibull distribution is</li>
</ul>
<p><span class="math display">\[
h_L(x)=\frac{\alpha x^{\alpha-1}}{\beta^{\alpha}},
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are positive constants.</p>
<ul>
<li>The corresponding reliability function is</li>
</ul>
<p><span class="math display">\[
R_L(x)=exp
\left \{-\int_0^\infty h_L(u)du\right \}=exp
\left \{-\int_0^\infty \bigg(-\frac{x}{\beta}\bigg)^{\alpha}\right \}
\]</span></p>
<p>and the pdf is</p>
<p><span class="math display">\[ f_L(x)=\frac{d}{dx}F_L(x)=\frac{\alpha x^{\alpha-1}}{\beta^{\alpha}}\left \{-\bigg(\frac{x}{\beta}\bigg)^{\alpha}\right \}I_{(0,\infty)}(x).\]</span></p>
<ul>
<li>Gompertz Distribution: exponential hazzard function. The hazzard function
for the Gompertz distribution is</li>
</ul>
<p><span class="math display">\[
h_L(x)=αe^{\beta x}
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are positive constants.</p>
<ul>
<li>The corresponding reliability function is</li>
</ul>
<p><span class="math display">\[
R_L(x)=exp
\left \{-\frac{\alpha}{\beta}[e^{\beta x}-1]\right \}
\]</span></p>
<p>and the pdf is</p>
<p><span class="math display">\[
f_L(x)=\frac{d}{dx}F_L(x)=\alpha e^{\beta x}exp\left \{-\frac{\alpha}{\beta}[e^{\beta x}-1]\right \}I_{(0,\infty)}(x)
\]</span></p>
<ul>
<li>Series Combinations: If a system fails whenever any single component fails,
then the components are said to be in series.
<ul>
<li>The time to failure of the system is the minimum time to failure of the components.</li>
<li>If the failure times of the components are statistically independent, then the reliability function of the system is</li>
</ul></li>
</ul>
<p><span class="math display">\[
R(x) = P(system life &gt; x) = P(all\ components\ survive\ to\ x)
=\prod R_i(x)
\]</span></p>
<p>where <span class="math inline">\(R_i(x)\)</span> is the reliability function of the <span class="math inline">\(i^{th}\)</span> component.</p>
<ul>
<li>Parallel Combinations: If a system fails only if all components fail, then the
components are said to be in parallel.
<ul>
<li>The time to failure of the system is the maximum time to failure of the components.</li>
<li>If the failure times of the components are statistically independent, then the reliability function of the system is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
R(x) &amp;= P(all\ components\ fail\ by\ time\ x) \\
&amp;= 1 − P(no\ component\ fails\ by\ time\ x) \\
&amp;=1-\prod F_i(x)=1-\prod[1-R_i(x)]
\end{align}
\]</span></p>
<p>where <span class="math inline">\(F_i(x)\)</span> is the cdf of the <span class="math inline">\(i^{th}\)</span> component</p>
</div>
<div id="t-f-and-beta-distributions" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and Beta Distributions<a href="chapter6.html#t-f-and-beta-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="t-distributions" class="section level4 unnumbered hasAnchor">
<h4><span class="math inline">\(t\)</span> distributions<a href="chapter6.html#t-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Let <span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> be independently distributed as <span class="math inline">\(Z \sim N(0, 1)\)</span> and <span class="math inline">\(X\sim \chi_k^2\)</span> Then</li>
</ul>
<p><span class="math display">\[
T=\frac{Z}{\sqrt{X/k}}
\]</span></p>
<p>has a central <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(k\)</span> degrees of freedom.</p>
<ul>
<li>The pdf is</li>
</ul>
<p><span class="math display">\[
f_T(t)=\frac{Γ(\frac{k+1}{2})}{Γ(\frac{k}{2})\sqrt{k\pi}(1+\frac{t^2}{k})^{(k+1/2)}}
\]</span></p>
<ul>
<li>If <span class="math inline">\(k = 1\)</span>, then the pdf of <span class="math inline">\(T\)</span> is</li>
</ul>
<p><span class="math display">\[
f_T(t)=\frac{1}{\pi(1+t^2)}
\]</span></p>
<p>which is the pdf of a standard Cauchy random variable.</p>
<ul>
<li>Moments of a <span class="math inline">\(t\)</span> random variable.
<ul>
<li>Suppose that <span class="math inline">\(T \sim t_k\)</span>. Then</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp;E(T^r)=E\Bigg(\frac{k^{r/2}Z^r}{X^{r/2}}\Bigg)=k^{r/2}E(Z^r)E(X^{-r/2}), \,\,\, where \\
&amp; Z \sim N(0,1), \,\,\,  X \sim \chi_k^2, \,\,\, and \,\,\, Z \Perp X
\end{align}
\]</span></p>
<ul>
<li>Recall that odd moments of <span class="math inline">\(Z\)</span> are zero, the <span class="math inline">\(2i^{th}\)</span> moment of <span class="math inline">\(Z\)</span> is <span class="math inline">\((2i)!/[i!2^i]\)</span>, and <span class="math inline">\(E(X^a)=2^a\Gamma(k/2+a)/\Gamma(k/2)\)</span> provided that <span class="math inline">\(a &lt; k/2\)</span>.
<ul>
<li>Accordingly, if <span class="math inline">\(r\)</span> is a non-negative integer, then</li>
</ul></li>
</ul>
<p><span class="math display">\[
E(Z^r)=\begin{cases}does\ not\ exist &amp; if\ r&gt;k;\\
0 &amp; if\ r\ is\ odd\ and\ r&lt;k; \\
k^{r/2}\frac{r!Γ(\frac{k}{2}-r)}{(r/2)!2^rΓ\frac{k}{2}} &amp; if \ r \ is \ even\ and\ r&lt;k.\end{cases}
\]</span></p>
<ul>
<li>Using the above expression, it is easy to show that <span class="math inline">\(E(T) = 0\)</span> if <span class="math inline">\(k &gt; 1\)</span> and
that <span class="math inline">\(Var(T) = k/(k − 2)\)</span> if <span class="math inline">\(k &gt; 2\)</span>.</li>
</ul>
</div>
<div id="f-distributions" class="section level4 unnumbered hasAnchor">
<h4><span class="math inline">\(F\)</span> Distributions<a href="chapter6.html#f-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Let <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> be independent <span class="math inline">\(\chi^2\)</span> random variables with degrees of freedom <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>, respectively. Then</li>
</ul>
<p><span class="math display">\[
Y=\frac{U_1/k_1}{U_2/k_2}
\]</span></p>
<p>has a central <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> degrees of freedom. That is,
<span class="math inline">\(Y \sim F_{k_1,k_2}\)</span>.</p>
<ul>
<li>The pdf is</li>
</ul>
<p><span class="math display">\[
f_Y(y)=\Bigg(\frac{k_1}{k_2}\Bigg)^{k_1/2}\frac{\Gamma\Bigg(\frac{k_1+k_2}{2}\Bigg)y^{(k_1-2/2)}}{\Gamma\Bigg(\frac{k_1}{2}\Bigg){\Gamma\Bigg(\frac{k_2}{2}\Bigg)}{\Bigg(1+\frac{yk_1}{k_2}\Bigg)}^{(k_1+k_2/2)}}I_{(0,\infty)}(y).
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(T \sim t_k\)</span>, then <span class="math inline">\(T^2 \sim F_{1,k}\)</span>.</p></li>
<li><p>Moments of an <span class="math inline">\(F\)</span> random variable. Suppose that <span class="math inline">\(Y \sim F_{k_1,k_2}\)</span>. Then</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp;E(Y^r)=E\Bigg(\frac{(k_2U_1)^r}{(k_1U_2)^r}\Bigg)=\Bigg(\frac{k_2}{k_1}\Bigg)^r\ E(U_1^r)E(U_2^{-r}),where \\
&amp;U_1 \sim χ^2_{k_1},U_2 \sim χ^2_{k_2}, and\ U_1 \Perp U_2
\end{align}
\]</span></p>
<ul>
<li>Using the general expression for the moments of a <span class="math inline">\(\chi^2\)</span> random variable, it can be shown that for any real valued <span class="math inline">\(r\)</span>,</li>
</ul>
<p><span class="math display">\[
E(Y^r)=\begin{cases}
deos\ not\ exist &amp; \text{ if } r&gt;k_2/2; \\
\frac{k^r_2\Gamma (\frac{k_1}{2}+r)\Gamma (\frac{k_2}{2}-r)}{k_1^r\Gamma (\frac{k_1}{2})\Gamma (\frac{k_2}{2})} &amp; \text{ if } r=&lt;k_2/2.
\end{cases}
\]</span></p>
<ul>
<li>Using the above expression, it is easy to show that <span class="math inline">\(E(Y)=\frac{k_2}{K_2-2}\)</span> if <span class="math inline">\(k&gt;2\)</span> and that <span class="math inline">\(Var(Y)=\frac{2k_2^2(k_1+k_2-2)}{k_1(k_2-2)^2(k_2-4)}\)</span> if <span class="math inline">\(k_2 &gt;4\)</span>.</li>
</ul>
</div>
<div id="beta-distributions" class="section level4 unnumbered hasAnchor">
<h4>Beta Distributions<a href="chapter6.html#beta-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Let <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> be independent <span class="math inline">\(\chi^2\)</span> random variables with degrees of freedom<span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>, respectively. Then</li>
</ul>
<p><span class="math display">\[
Y=\frac{U_1}{U_1+U_2} \sim Beta \Bigg(\frac{k_1}{2},\frac{k_2}{2}\Bigg)
\]</span></p>
<ul>
<li><p>That is, <span class="math inline">\(Y\)</span> has a beta distribution with parameters <span class="math inline">\(k_1/2\)</span> and <span class="math inline">\(k_2/2\)</span>.</p></li>
<li><p>More generally, if <span class="math inline">\(U_1 \sim Gam(\alpha_1, \lambda), \ U_2 \sim Gam(\alpha_2, \lambda)\)</span> and <span class="math inline">\(U_1 \Perp U_2\)</span>, then</p></li>
</ul>
<p><span class="math display">\[
Y=\frac{U_1}{U_1+ U_2} \sim Beta (\alpha_1,\alpha_2)
\]</span></p>
<ul>
<li>If <span class="math inline">\(Y\sim Beta (\alpha_1,\alpha_2)\)</span>, then the pdf of <span class="math inline">\(Y\)</span> is</li>
</ul>
<p><span class="math display">\[
f_Y(y)=\frac{y^{\alpha_1-1}(1-y)^{\alpha_2-1}}{\beta(\alpha_1,\alpha_2)}I_{(0,1)}(y)
\]</span></p>
<p>where <span class="math inline">\(\beta(\alpha_1, \alpha_2)\)</span> is the beta function and is defined as</p>
<p><span class="math display">\[
\beta(\alpha_1, \alpha_2)=\frac{\Gamma(\alpha_1)\Gamma(\alpha_2)}{\Gamma(\alpha_1+\alpha_2)}
\]</span></p>
<ul>
<li>If <span class="math inline">\(B \sim Beta(\alpha_1, \alpha_2)\)</span>, then</li>
</ul>
<p><span class="math display">\[
\frac{\alpha_2B}{\alpha_1(1-B)} \sim F_{2\alpha_1,2\alpha_2}
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(B \sim β(\alpha_1, \alpha_2)\)</span> where <span class="math inline">\(\alpha_1=\alpha_2=1\)</span>, then <span class="math inline">\(B \sim Unif(0,1)\)</span>.</p></li>
<li><p>If <span class="math inline">\(X \sim \beta(\alpha_1, \alpha_2)\)</span>, then</p></li>
</ul>
<p><span class="math display">\[
E(X^r)=\frac{Γ(α_1+r)Γ(α_1+α_2)}{Γ(α_1+α_2+r)Γ(α_1)}provided\ that\ α_1 + r &gt; 0
\]</span></p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[
\begin{align}
E(X^r)&amp;=\int_0^1 \frac{x^rx^{\alpha_1-1}(1-x)^{\alpha-1}}{\beta(\alpha_1,\alpha_2)}dx \\
&amp;=\frac{\beta(\alpha_1+r,\alpha_2)}{\beta(\alpha_1,\alpha_2)}\int_0^1 \frac{x^{\alpha_1+r-1}(1-x)^{\alpha-1}}{\beta(\alpha_1+r,\alpha_2)}dx\\
&amp;= \frac{\beta(\alpha_1+r,\alpha_2)}{\beta(\alpha_1,\alpha_2)}= \frac{\Gamma(\alpha_1+r)\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1+\alpha_2+r)\Gamma(\alpha_1)},
\end{align}
\]</span></p>
<p>provided that <span class="math inline">\(\alpha_1 + r &gt; 0\)</span>, because the last integral is the integral of the
pdf of a random variable with distribution <span class="math inline">\(Beta(\alpha_1+r,\alpha_2)\)</span>.</p>
<ul>
<li>If <span class="math inline">\(F \sim F_{k_1,k_2}\)</span>, then</li>
</ul>
<p><span class="math display">\[
\frac{k_1F}{k_1F+k_2} \sim Beta \Bigg(\frac{k_1}{2},\frac{k_2}{2}\Bigg)
\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter5.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter7.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Mathematical Statistics.pdf", "Mathematical Statistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
