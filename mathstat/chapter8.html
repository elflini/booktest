<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Samples, Statistics, &amp; Sampling Distributions | Mathematical Statistics</title>
  <meta name="description" content="This is a Mathematical Statistics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Samples, Statistics, &amp; Sampling Distributions | Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Mathematical Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Samples, Statistics, &amp; Sampling Distributions | Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is a Mathematical Statistics" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2024-08-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter7.html"/>
<link rel="next" href="chapter9.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#algebra-of-events"><i class="fa fa-check"></i><b>1.2</b> Algebra of Events</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#experiments-with-symmetries"><i class="fa fa-check"></i><b>1.3</b> Experiments with Symmetries</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#composition-of-experiments-counting-rules"><i class="fa fa-check"></i><b>1.4</b> Composition of Experiments: Counting Rules</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#sampling-at-random"><i class="fa fa-check"></i><b>1.5</b> Sampling at Random</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#binomial-multinomial-coefficients"><i class="fa fa-check"></i><b>1.6</b> Binomial &amp; Multinomial Coefficients</a></li>
<li class="chapter" data-level="1.7" data-path="chapter1.html"><a href="chapter1.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="1.8" data-path="chapter1.html"><a href="chapter1.html#subjective-probability"><i class="fa fa-check"></i><b>1.8</b> Subjective Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#probability-functions"><i class="fa fa-check"></i><b>2.1</b> Probability Functions</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Joint Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#conditional-probability"><i class="fa fa-check"></i><b>2.3</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#bayes-theorem-law-of-inverse-probability"><i class="fa fa-check"></i><b>2.4</b> Bayes Theorem (Law of Inverse Probability)</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#statistical-independence-of-random-variables"><i class="fa fa-check"></i><b>2.5</b> Statistical Independence of Random Variables</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#exchangeability"><i class="fa fa-check"></i><b>2.6</b> Exchangeability</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#application-probability-of-winning-in-craps"><i class="fa fa-check"></i><b>2.7</b> Application: Probability of Winning in Craps</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Expectations of Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#the-mean"><i class="fa fa-check"></i><b>3.1</b> The Mean</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#expectation-of-a-function"><i class="fa fa-check"></i><b>3.2</b> Expectation of a Function</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#variability"><i class="fa fa-check"></i><b>3.3</b> Variability</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#sums-of-random-variables"><i class="fa fa-check"></i><b>3.5</b> Sums of Random Variables</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#probability-generating-functions"><i class="fa fa-check"></i><b>3.6</b> Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Bernoulli and Related Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#sampling-bernoulli-populations"><i class="fa fa-check"></i><b>4.1</b> Sampling Bernoulli Populations</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#binomial-distribution"><i class="fa fa-check"></i><b>4.2</b> Binomial Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>4.3</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#negative-hypergeometric-distribution"><i class="fa fa-check"></i><b>4.6</b> Negative Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#approximating-binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Approximating Binomial Probabilities</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="chapter4.html"><a href="chapter4.html#normal-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.1</b> Normal approximation to the Binomial</a></li>
<li class="chapter" data-level="4.7.2" data-path="chapter4.html"><a href="chapter4.html#poisson-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.2</b> Poisson Approximation to the Binomial</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#poisson-distribution"><i class="fa fa-check"></i><b>4.8</b> Poisson Distribution</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#law-of-large-numbers"><i class="fa fa-check"></i><b>4.9</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="4.10" data-path="chapter4.html"><a href="chapter4.html#multinomial-distributions"><i class="fa fa-check"></i><b>4.10</b> Multinomial Distributions</a></li>
<li class="chapter" data-level="4.11" data-path="chapter4.html"><a href="chapter4.html#using-probability-generating-functions"><i class="fa fa-check"></i><b>4.11</b> Using Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.1</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#density-and-the-probability-element"><i class="fa fa-check"></i><b>5.2</b> Density and the Probability Element</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#the-median-and-other-percentiles"><i class="fa fa-check"></i><b>5.3</b> The Median and Other Percentiles</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#expected-value"><i class="fa fa-check"></i><b>5.4</b> Expected Value</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#expected-value-of-a-function"><i class="fa fa-check"></i><b>5.5</b> Expected Value of a Function</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#average-deviations"><i class="fa fa-check"></i><b>5.6</b> Average Deviations</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#bivariate-distributions"><i class="fa fa-check"></i><b>5.7</b> Bivariate Distributions</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#several-variables"><i class="fa fa-check"></i><b>5.8</b> Several Variables</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#covariance-and-correlation-1"><i class="fa fa-check"></i><b>5.9</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#independence"><i class="fa fa-check"></i><b>5.10</b> Independence</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#conditional-distributions"><i class="fa fa-check"></i><b>5.11</b> Conditional Distributions</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#moment-generating-functions"><i class="fa fa-check"></i><b>5.12</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Families of Continuous Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#normal-distributions"><i class="fa fa-check"></i><b>6.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#exponential-distributions"><i class="fa fa-check"></i><b>6.2</b> Exponential Distributions</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#gamma-distributions"><i class="fa fa-check"></i><b>6.3</b> Gamma Distributions</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#chi-squared-distributions"><i class="fa fa-check"></i><b>6.4</b> Chi Squared Distributions</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#distributions-for-reliability"><i class="fa fa-check"></i><b>6.5</b> Distributions for Reliability</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#t-f-and-beta-distributions"><i class="fa fa-check"></i><b>6.6</b> <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and Beta Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Organizing &amp; Describing Data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#frequency-distributions"><i class="fa fa-check"></i><b>7.1</b> Frequency Distributions</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#data-on-continuous-variables"><i class="fa fa-check"></i><b>7.2</b> Data on Continuous Variables</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#order-statistics"><i class="fa fa-check"></i><b>7.3</b> Order Statistics</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#data-analysis"><i class="fa fa-check"></i><b>7.4</b> Data Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#the-sample-mean"><i class="fa fa-check"></i><b>7.5</b> The Sample Mean</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#measures-of-dispersion"><i class="fa fa-check"></i><b>7.6</b> Measures of Dispersion</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#correlation"><i class="fa fa-check"></i><b>7.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Samples, Statistics, &amp; Sampling Distributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#random-sampling"><i class="fa fa-check"></i><b>8.1</b> Random Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#likelihood"><i class="fa fa-check"></i><b>8.2</b> Likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#sufficient-statistics"><i class="fa fa-check"></i><b>8.3</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#sampling-distributions"><i class="fa fa-check"></i><b>8.4</b> Sampling Distributions</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>8.5</b> Simulating Sampling Distributions</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#order-statistics-1"><i class="fa fa-check"></i><b>8.6</b> Order Statistics</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#moments-of-sample-means-and-proportionssp"><i class="fa fa-check"></i><b>8.7</b> Moments of Sample Means and Proportionssp</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>8.8</b> The Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#using-the-moment-generating-function"><i class="fa fa-check"></i><b>8.9</b> Using the Moment Generating Function</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#normal-populations"><i class="fa fa-check"></i><b>8.10</b> Normal Populations</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#updating-prior-probabilities-via-likelihood"><i class="fa fa-check"></i><b>8.11</b> Updating Prior Probabilities Via Likelihood</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#some-conjudate-families"><i class="fa fa-check"></i><b>8.12</b> Some conjudate Families</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#predictive-distributions"><i class="fa fa-check"></i><b>8.13</b> Predictive Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Estimation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#point-estimation"><i class="fa fa-check"></i><b>9.1</b> Point Estimation</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#errors-in-estimation"><i class="fa fa-check"></i><b>9.2</b> Errors in Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#consistency"><i class="fa fa-check"></i><b>9.3</b> Consistency</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#large-sample-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Large Sample Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#determining-sample-size"><i class="fa fa-check"></i><b>9.5</b> Determining Sample Size</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#small-sample-confidence-intervals-for-mu_x"><i class="fa fa-check"></i><b>9.6</b> Small Sample Confidence Intervals for <span class="math inline">\(\mu_X\)</span></a></li>
<li class="chapter" data-level="9.7" data-path="chapter9.html"><a href="chapter9.html#the-distribution-of-t"><i class="fa fa-check"></i><b>9.7</b> The Distribution of <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="9.8" data-path="chapter9.html"><a href="chapter9.html#pivotal-quantities"><i class="fa fa-check"></i><b>9.8</b> Pivotal Quantities</a></li>
<li class="chapter" data-level="9.9" data-path="chapter9.html"><a href="chapter9.html#estimating-a-mean-difference"><i class="fa fa-check"></i><b>9.9</b> Estimating a Mean Difference</a></li>
<li class="chapter" data-level="9.10" data-path="chapter9.html"><a href="chapter9.html#umvue"><i class="fa fa-check"></i><b>9.10</b> UMVUE</a></li>
<li class="chapter" data-level="9.11" data-path="chapter9.html"><a href="chapter9.html#bayes-estimators"><i class="fa fa-check"></i><b>9.11</b> Bayes Estimators</a></li>
<li class="chapter" data-level="9.12" data-path="chapter9.html"><a href="chapter9.html#efficiency"><i class="fa fa-check"></i><b>9.12</b> Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Significance Testing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chapter10.html"><a href="chapter10.html#hypotheses"><i class="fa fa-check"></i><b>10.1</b> Hypotheses</a></li>
<li class="chapter" data-level="10.2" data-path="chapter10.html"><a href="chapter10.html#assessing-the-evidence"><i class="fa fa-check"></i><b>10.2</b> Assessing the Evidence</a></li>
<li class="chapter" data-level="10.3" data-path="chapter10.html"><a href="chapter10.html#one-sample-z-tests"><i class="fa fa-check"></i><b>10.3</b> One Sample <span class="math inline">\(Z\)</span> Tests</a></li>
<li class="chapter" data-level="10.4" data-path="chapter10.html"><a href="chapter10.html#one-sample-t-tests"><i class="fa fa-check"></i><b>10.4</b> One Sample <span class="math inline">\(t\)</span> Tests</a></li>
<li class="chapter" data-level="10.5" data-path="chapter10.html"><a href="chapter10.html#some-nonparametric-tests"><i class="fa fa-check"></i><b>10.5</b> Some Nonparametric Tests</a></li>
<li class="chapter" data-level="10.6" data-path="chapter10.html"><a href="chapter10.html#probability-of-the-null-hypothesis"><i class="fa fa-check"></i><b>10.6</b> Probability of the Null Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Tests as Decision Rules</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#rejection-regions-and-errors"><i class="fa fa-check"></i><b>11.1</b> Rejection Regions and Errors</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#the-power-function"><i class="fa fa-check"></i><b>11.2</b> The Power function</a></li>
<li class="chapter" data-level="11.3" data-path="chapter11.html"><a href="chapter11.html#choosing-a-sample-size"><i class="fa fa-check"></i><b>11.3</b> Choosing a Sample Size</a></li>
<li class="chapter" data-level="11.4" data-path="chapter11.html"><a href="chapter11.html#most-powerful-tests"><i class="fa fa-check"></i><b>11.4</b> Most Powerful Tests</a></li>
<li class="chapter" data-level="11.5" data-path="chapter11.html"><a href="chapter11.html#uniformly-most-powerful-tests"><i class="fa fa-check"></i><b>11.5</b> Uniformly Most Powerful Tests</a></li>
<li class="chapter" data-level="11.6" data-path="chapter11.html"><a href="chapter11.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>11.6</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="11.7" data-path="chapter11.html"><a href="chapter11.html#bayesian-testing"><i class="fa fa-check"></i><b>11.7</b> Bayesian Testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chapter12.html"><a href="chapter12.html"><i class="fa fa-check"></i><b>12</b> Appendix</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chapter12.html"><a href="chapter12.html#greek-alphabet"><i class="fa fa-check"></i><b>12.1</b> Greek Alphabet</a></li>
<li class="chapter" data-level="12.2" data-path="chapter12.html"><a href="chapter12.html#abbreviations"><i class="fa fa-check"></i><b>12.2</b> Abbreviations</a></li>
<li class="chapter" data-level="12.3" data-path="chapter12.html"><a href="chapter12.html#practice-exams"><i class="fa fa-check"></i><b>12.3</b> PRACTICE EXAMS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter8" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Samples, Statistics, &amp; Sampling Distributions<a href="chapter8.html#chapter8" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li><p>Definition: Parameter — A characteristic of the population.</p></li>
<li><p>Definition: Statistic — A characteristic of the sample. Specifically, a statistic is a function of the sample;</p></li>
</ul>
<p><span class="math display">\[
T = g(X_1, X_2,\cdots,X_n)
\]</span></p>
<p>      and</p>
<p><span class="math display">\[
t =g(x_1, x_2,\cdots,x_3)
\]</span></p>
<ul>
<li><p>The function <span class="math inline">\(T\)</span> is a random variable and the function <span class="math inline">\(t\)</span> is a realization of the random variable. For example, <span class="math inline">\(T_1 =\bar{X}\)</span> and <span class="math inline">\(T_2 = S^2_X\)</span> are statistics.</p></li>
<li><p>Definition: Sampling Distribution — A sampling distribution is the distribution
of a statistic. For example, the sampling distribution of <span class="math inline">\(\bar{X}\)</span> is the distribution of <span class="math inline">\(\bar{X}\)</span></p></li>
</ul>
<p><br></p>
<div id="random-sampling" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Random Sampling<a href="chapter8.html#random-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Some non-random samples
<ul>
<li>Voluntary response sample: the respondent controls whether or not s/he is in the sample.</li>
<li>Sample of convenience: the investigator obtains a set of units from the population by using units that are available or can be obtained inexpensively.</li>
</ul></li>
<li>Random sampling from a finite population
<ul>
<li>Procedure: select units from the population at random, one at a time.</li>
<li>Sampling can be done with or without replacement.</li>
</ul></li>
</ul>
<div id="properties-of-random-sampling" class="section level4 unnumbered hasAnchor">
<h4>Properties of random sampling<a href="chapter8.html#properties-of-random-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The distribution of the sample is exchangeable.</p></li>
<li><p>All possible samples of size <span class="math inline">\(n\)</span> are equally likely (this is the definition of a simple random sample).</p></li>
<li><p>Each unit in the population has an equal chance of being selected.</p></li>
<li><p>Definition: Population Distribution—the marginal distribution of <span class="math inline">\(X_i\)</span>, where <span class="math inline">\(X_i\)</span> is the value of the <span class="math inline">\(i^{th}\)</span> unit in the sample.</p>
<ul>
<li>Note, the marginal distribution of all <span class="math inline">\(X_i\)</span>s are identical by exchangeability.</li>
</ul></li>
<li><p>Random sample of size <span class="math inline">\(n\)</span></p>
<ul>
<li>In general, a random sample of size <span class="math inline">\(n\)</span> has many possible meanings (e.g., with replacement, without replacement, stratified, etc.).</li>
</ul></li>
<li><p>We will say “random sample of size <span class="math inline">\(n\)</span>” when we mean a sequence of independent and identically distributed (iid) random variables.</p>
<ul>
<li>This can occur if one randomly samples from a finite population with replacement, or randomly samples from an infinite population.</li>
<li>Unless it is qualified, the phrase “random sample of size <span class="math inline">\(n\)</span>” refers to iid random variables and does not refer to sampling without replacement from a finite population.</li>
</ul></li>
<li><p>The joint pdf or pmf of a random sample of size <span class="math inline">\(n\)</span> is denoted by</p></li>
</ul>
<p><span class="math display">\[
f_X(X) \overset{def}= f_{X_1, X_2,\cdots,X_n}(x_1,x_2,\cdots,x_3)
\]</span></p>
<p>where <span class="math inline">\(X\)</span> and <span class="math inline">\(X\)</span> are vectors of random variables and realizations, respectively. That is</p>
<p><span class="math display">\[
X = \begin{pmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \\ \end{pmatrix} and \ x=\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \\ \end{pmatrix}
\]</span></p>
<ul>
<li>The transpose of a column vector <span class="math inline">\(U\)</span> is denoted by <span class="math inline">\(U&#39;\)</span>. For example,</li>
</ul>
<p><span class="math display">\[
X&#39; = (X_1 \ X_2 \cdots X_n)
\]</span></p>
<ul>
<li>Using independence,</li>
</ul>
<p><span class="math display">\[
f_X(X) = \prod f_X{(x_i)}
\]</span></p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X_1, X_2,\cdots, X_n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from an <span class="math inline">\(Exp(\lambda)\)</span> distribution. Then the joint pdf of the sample is</li>
</ul>
<p><span class="math display">\[
f_X(X)= \prod^n_{i=1}\lambda e^{-\lambda x_i} I_{(0, \infty)}(x_i) = \lambda^n \exp \Bigg\{ -\lambda \sum^n_{i=1} x_i \Bigg\}I_{(0,x_{(n)}]}(x_{(1)})I_{[x_{(1)},\infty)}(x_{(n)})
\]</span></p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Unif[a, b]\)</span>. Then the joint pdf of the sample is</li>
</ul>
<p><span class="math display">\[
f_X(X)= \prod^n_{i=1}(b-a)^{-1} I_{[a,b]}(x_i) = (b-a)^n I_{[a,x_{(n)}]}(x_{(1)})I_{[x_{(1)},b]}(x_{(n)})
\]</span></p>
<ul>
<li>PMF of a random sample taken without replacement from a finite population. Consider a population of size <span class="math inline">\(N\)</span> having <span class="math inline">\(k \le N\)</span> distinct values.
<ul>
<li>Denote the values as <span class="math inline">\(v_1, v_2, \cdots, v_k\)</span>.</li>
<li>Suppose that the population contains <span class="math inline">\(M_1\)</span> units with value <span class="math inline">\(v_1\)</span>, <span class="math inline">\(M_2\)</span> units with value <span class="math inline">\(v_2\)</span>, etc.</li>
<li>Note that <span class="math inline">\(N = \sum^k_{j=1}= M_j\)</span>.</li>
<li>Select <span class="math inline">\(n\)</span> units at random without replacement from the population.</li>
<li>Let <span class="math inline">\(X_i\)</span> be the value of the <span class="math inline">\(i^{th}\)</span> unit in the sample and denote the <span class="math inline">\(n \times 1\)</span> vector of <span class="math inline">\(X\)</span>s by <span class="math inline">\(X\)</span>.</li>
<li>Let <span class="math inline">\(x\)</span> be a realization of <span class="math inline">\(X\)</span>.</li>
<li>That is, <span class="math inline">\(x\)</span> is an <span class="math inline">\(n \times 1\)</span> vector whose elements are chosen from <span class="math inline">\(v_1, v_2, \cdots, v_k\)</span>.</li>
<li>Then the pmf of the sample is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_X(X) = P(X=x) = \frac{\displaystyle\prod^k_{j=1} \begin{pmatrix} M_j \\ y_j \end{pmatrix}} {\begin{pmatrix} N \\ n \end{pmatrix} \begin{pmatrix} n \\ y_1, y_2,\cdots,y_n \end{pmatrix}}
\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is the frequency of <span class="math inline">\(v_j\)</span> in <span class="math inline">\(X\)</span>.</p>
<ul>
<li>Proof: Let <span class="math inline">\(Y_j\)</span> for <span class="math inline">\(j=1,\cdots,k\)</span> be the frequency of <span class="math inline">\(v_j\)</span> in <span class="math inline">\(X\)</span>.
<ul>
<li>Note that <span class="math inline">\(\sum^k_{j=1}Y_j=n\)</span>.</li>
<li>Denote the vector of Ys by <span class="math inline">\(Y\)</span> and the vector of <span class="math inline">\(y\)</span>s by <span class="math inline">\(y\)</span>.</li>
<li>Also, denote the number of distinct <span class="math inline">\(X\)</span> sequences that yield <span class="math inline">\(y\)</span> by <span class="math inline">\(n_y\)</span>.</li>
<li>Then</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_Y(y) = Pr(Y=y) = f_x(x) \times n_y
\]</span></p>
<p>where <span class="math inline">\(f_X(X)\)</span> is the probability of any specific sequence of <span class="math inline">\(x\)</span>’s the contains <span class="math inline">\(y_1\)</span> units with value <span class="math inline">\(v_1, y_2\)</span> units with <span class="math inline">\(v_2\)</span>, etc.</p>
<ul>
<li>Multiplication of <span class="math inline">\(f_X(X)\)</span> by <span class="math inline">\(n_y\)</span> is correct because each permutation of <span class="math inline">\(X\)</span> has the same probability (by exchangeability). We can show that</li>
</ul>
<p><span class="math display">\[
f_Y(y) = \frac{\displaystyle\prod^k_{j=1} \begin{pmatrix} M_j \\ y_j \end{pmatrix}} {\begin{pmatrix} N \\ n \end{pmatrix}}\]</span> and <span class="math display">\[ n_y =\begin{pmatrix} n \\ y_i , y_2,\cdots,y_n \end{pmatrix}
\]</span></p>
<ul>
<li>Accordingly, the pmf of the sample is</li>
</ul>
<p><span class="math display">\[
f_X(X) =  \frac{\displaystyle\prod^k_{j=1} \begin{pmatrix} M_j \\ y_j \end{pmatrix}} {\begin{pmatrix} N \\ n \end{pmatrix} \begin{pmatrix} n \\ y_1, y_2,\cdots,y_n \end{pmatrix}}
\]</span></p>
<ul>
<li>Example: Consider the population consisting of 12 voles, <span class="math inline">\(M_j\)</span> voles of species <span class="math inline">\(j\)</span>
for <span class="math inline">\(j = 1, 2, 3\)</span>.
<ul>
<li>Suppose that <span class="math inline">\(X_1, X_2, X_3, X_4\)</span> is a random sample taken without
replacement from the population.</li>
<li>Furthermore, suppose that</li>
</ul></li>
</ul>
<p><span class="math display">\[
X= (s_3 \ s_1 \ s_1 \ s_2)&#39;
\]</span></p>
<p>where <span class="math inline">\(s_j\)</span> denotes species <span class="math inline">\(j\)</span>.</p>
<ul>
<li>The joint pdf of the sample <span class="math inline">\(i\)</span></li>
</ul>
<p><span class="math display">\[
\begin{align}
f_X(s_3, s_1, s_1, s_2) &amp;= \frac{\begin{pmatrix} M_1 \\ 2 \end{pmatrix} \begin{pmatrix} M_2 \\ 1 \end{pmatrix} \begin{pmatrix} M_3 \\ 2 \end{pmatrix}}{\begin{pmatrix} 12 \\ 4 \end{pmatrix}\begin{pmatrix} 4 \\ 2,1,1 \end{pmatrix}} \\
&amp;= \frac{\frac{1}{2}M_1(M_1 -1)M_2M_3}{495 \times 12} \\
&amp;=\frac{M_1(M_1 -1)M_2(12-M_1-M_2)}{11,880}
\end{align}
\]</span></p>
<p><br></p>
</div>
</div>
<div id="likelihood" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Likelihood<a href="chapter8.html#likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Family of probability distributions or models: If the joint pdf or pmf of the
sample depends on the value of unknown parameters, then the joint pdf or
pmf is written as</li>
</ul>
<p><span class="math display">\[
f_X(x|\theta) \text{ where } \theta =(\theta_1 \ \theta_2 \ \cdots \theta_k)&#39;
\]</span></p>
<p>is a vector of unknown parameters.</p>
<ul>
<li>For example, if <span class="math inline">\(X_1,\cdots, X_n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are unknown, then the joint pdf is</li>
</ul>
<p><span class="math display">\[
f_X(x|\theta) = f_X(x|/mu, \sigma^2) = \frac{exp \Bigg\{\frac{1}{2\sigma^2}\displaystyle\sum^n_{i=1}(x_i -\mu)^2\Bigg\}}{(2\pi\sigma^2)^{n/2}}, \ where \ \theta =\begin{pmatrix}\mu  \\ \sigma^2 \end{pmatrix}
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(\theta\)</span> contains only one parameter, then is will be denoted as <span class="math inline">\(\theta\)</span> (i.e., no bold face).</p></li>
<li><p>Likelihood Function: The likelihood function is a measure of how likely a
particular value of <span class="math inline">\(\theta\)</span> is, given that x has been observed.</p>
<ul>
<li>Caution: the likelihood function is not a probability.</li>
<li>The likelihood function is denoted by <span class="math inline">\(L(\theta)\)</span> and is obtained by interchanging the roles of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(x\)</span> in the joint pdf or pmf of <span class="math inline">\(x\)</span>, and dropping all terms that do not depend on <span class="math inline">\(\theta\)</span>. That is,</li>
</ul></li>
</ul>
<p><span class="math display">\[
L(\theta) = L(\theta|x) \propto f_x(x|\theta)
\]</span></p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from an <span class="math inline">\(Exp(\lambda)\)</span> distribution. Then the likelihood function is</li>
</ul>
<p><span class="math display">\[
L(\lambda) = \lambda^n\exp\Bigg\{-\lambda \displaystyle\sum^n_{i=1}x_i\Bigg\}\]</span></p>
<p>provided that all <span class="math inline">\(x\)</span>s are in <span class="math inline">\((0, \infty)\)</span>.</p>
<ul>
<li>Note that the likelihood function and the joint pdf are identical in this example. Suppose that <span class="math inline">\(n = 10\)</span> and that</li>
</ul>
<p><span class="math display">\[
x = (0.4393 \ 0.5937 \ 0.0671 \ 2.0995 \ 0.1320 \ 0.0148 \ 0.0050 \ 0.1186 \ 0.4120 \  0.3483)&#39;
\]</span></p>
<p>has been observed.</p>
<ul>
<li><p>The sample mean is <span class="math inline">\(\bar x = 4.2303/10 = 0.42303\)</span>.</p></li>
<li><p>The likelihood function is plotted below.</p>
<ul>
<li>Ratios are used to compare likelihoods.</li>
<li>For example, the likelihood that <span class="math inline">\(\lambda = 2.5\)</span> is <span class="math inline">\(1.34\)</span> times as large as the likelihood that <span class="math inline">\(\lambda = 3\)</span>;</li>
</ul></li>
</ul>
<p><span class="math display">\[
\frac{L(2.5)}{L(3)} = 1.3390
\]</span></p>
<ul>
<li>Note: the <span class="math inline">\(x\)</span> values actually were sampled from <span class="math inline">\(Exp(2)\)</span>.</li>
</ul>
<p><img src="fig8/ch8_1.png" /></p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from
<span class="math inline">\(Unif[\pi, b]\)</span>. Then the likelihood function is</li>
</ul>
<p><span class="math display">\[
L(b) = (b-\pi)^{-n}I_{[x_{(n)},\infty](b)}
\]</span></p>
<p>provided that <span class="math inline">\(x_{(1)} \gt \pi\)</span>.</p>
<ul>
<li>Suppose that <span class="math inline">\(n = 10\)</span> and that</li>
</ul>
<p><span class="math display">\[
x = (5.9841 \ 4.9298 \ 3.7507 \ 5.1264 \ 3.8780 \ 4.8656 \ 6.0682 \ 4.1946 \ 5.2010 \ 4.3728)&#39;
\]</span></p>
<p>has been observed.</p>
<ul>
<li>For this sample, <span class="math inline">\(x_{(1)} = 3.7507\)</span> and <span class="math inline">\(x_{(n)} = 6.0682\)</span>.
<ul>
<li>The likelihood function is plotted below.</li>
<li>Note, the <span class="math inline">\(x\)</span> values actually were sampled from <span class="math inline">\(Unif(\pi, 2\pi)\)</span>.</li>
</ul></li>
</ul>
<p><img src="fig8/ch8_2.png" /></p>
<ul>
<li>Example: Consider the population consisting consisting of <span class="math inline">\(12\)</span> voles, <span class="math inline">\(M_j\)</span> voles
of species <span class="math inline">\(j\)</span> for <span class="math inline">\(j = 1, 2, 3\)</span>.
<ul>
<li>Suppose that <span class="math inline">\(X_1, X_2, X_3, X_4\)</span> is a random sample taken without replacement from the population.</li>
<li>Furthermore, suppose that</li>
</ul></li>
</ul>
<p><span class="math display">\[
X = (s_3 \ s_1 \ s_1 \ s_2)&#39;
\]</span></p>
<p>where <span class="math inline">\(s_j\)</span> denotes species <span class="math inline">\(j\)</span>.</p>
<ul>
<li>The likelihood function is</li>
</ul>
<p><span class="math display">\[
L(M_1,M_2) = M_1(M_1 -1)M_2(12-M_1-M_2)
\]</span></p>
<ul>
<li><p>Note, there are only two parameters, not three, because <span class="math inline">\(M_1 + M_2 + M_3 = 12\)</span>.</p></li>
<li><p>The likelihood function is displayed in the table below.</p>
<ul>
<li>Note: the <span class="math inline">\(x\)</span> values actually were sampled from a population in which <span class="math inline">\(M_1 = 5, M_2 = 3\)</span>, and <span class="math inline">\(M_3 = 4\)</span>.</li>
</ul></li>
</ul>
<p><img src="fig8/ch8_3.png" /></p>
<div id="likelihood-principle" class="section level4 unnumbered hasAnchor">
<h4>Likelihood Principle<a href="chapter8.html#likelihood-principle" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>All the information which the data provide concerning the relative
merits of two hypotheses is contained in the likelihood ratio of those
hypotheses on the data (Edwards, 1992).</p></li>
<li><p>Another way of stating the likelihood principal is that if two experiments, each
based on a model for <span class="math inline">\(\theta\)</span>, give the same likelihood, then the inference about <span class="math inline">\(\theta\)</span> should be the same in the two experiments.</p></li>
</ul>
</div>
<div id="example-1" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter8.html#example-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Experiment 1: Toss a 35 cent coin <span class="math inline">\(n\)</span> independent times. Let <span class="math inline">\(\theta\)</span> be the
probability of a head and let <span class="math inline">\(X\)</span> be the number of heads observed. Then <span class="math inline">\(X\)</span> has a binomial pmf:</li>
</ul>
<p><span class="math display">\[
f_X(x|\theta) = \begin{pmatrix} n \\x \end{pmatrix}\theta^x (1-\theta)^{n-x}I_{\{0,1,\cdots,n\}}(x)
\]</span></p>
<p>where <span class="math inline">\(n = 20\)</span>.</p>
<ul>
<li>Suppose that <span class="math inline">\(x = 6\)</span> heads are observed. Then the likelihood function is</li>
</ul>
<p><span class="math display">\[
L(\theta|x=6) = \theta^6(1-\theta)^{14}
\]</span></p>
<ul>
<li>Experiment 2: The 35 cent coin was tossed on independent trials until <span class="math inline">\(r = 6\)</span> heads were observed. Let <span class="math inline">\(Y\)</span> be the number of tosses required to obtain 6 heads. Then <span class="math inline">\(Y\)</span> has a negative binomial pmf:</li>
</ul>
<p><span class="math display">\[
f_Y(y|\theta,r) = \begin{pmatrix} y-1 \\ r-1 \end{pmatrix}\theta^y(1-\theta)^{y-r}I_{\{r,r+1,\cdots\}}(y)
\]</span></p>
<p>where <span class="math inline">\(r=6\)</span>.</p>
<ul>
<li>Suppose that the <span class="math inline">\(6^{th}\)</span> head occurred on the <span class="math inline">\(20^{th}\)</span> trial. Then,
the likelihood function is</li>
</ul>
<p><span class="math display">\[
L(\theta|y=20) =\theta^6(1-\theta)^{14}
\]</span></p>
<ul>
<li><p>The likelihood principal requires that any inference about <span class="math inline">\(\theta\)</span> be the same from the two experiments.</p></li>
<li><p>Suppose that we would like to test <span class="math inline">\(H_0 : \theta = 0.5\)</span> against <span class="math inline">\(H_{\alpha} : \theta &lt; 0.5\)</span>.</p></li>
<li><p>Based on the above two experiments, the <span class="math inline">\(p\)</span>-values are</p></li>
</ul>
<p><span class="math display">\[
P(X \le 6|n=20, \theta=0.5 = \displaystyle\sum^6_{x=0} \begin{pmatrix} 20 \\x \end{pmatrix}(1/2)^x(1-1/2)^{20-x}=0.0577
\]</span></p>
<p>in the binomial experiment and</p>
<p><span class="math display">\[
P(X \ge 20|r=6, \theta=0.5 = \displaystyle\sum^{\infty}_{y=20} \begin{pmatrix} y-1 \\6-1 \end{pmatrix}(1/2)^6(1-1/2)^{y-6}=0.0318
\]</span></p>
<p>in the negative binomial experiment.</p>
<ul>
<li>If we fail to reject <span class="math inline">\(H_0\)</span> in the first experiment, but reject <span class="math inline">\(H_0\)</span> in the second experiment, then we have violated the likelihood principle.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="sufficient-statistics" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Sufficient Statistics<a href="chapter8.html#sufficient-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Some definition: A statistic, <span class="math inline">\(T = t(X)\)</span>, is sufficient for a family of
distributions, <span class="math inline">\(f_X(X|\theta)\)</span>, if and only if the likelihood function depends on <span class="math inline">\(X\)</span>
only through <span class="math inline">\(T\)</span>:</li>
</ul>
<p><span class="math display">\[
L(\theta) = h[t(X), \theta]
\]</span></p>
<ul>
<li>Usual definition: A statistic, <span class="math inline">\(T = t(X)\)</span>, is sufficient for a family of
distributions, <span class="math inline">\(f_X(x|\theta)\)</span>, if and only if the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(T\)</span> does not depend on <span class="math inline">\(\theta\)</span>:</li>
</ul>
<p><span class="math display">\[
f_{X|T}(x|t,\theta) = h(x)
\]</span></p>
<ul>
<li><p>This definition says that after observing <span class="math inline">\(T\)</span>, no additional functions of the data
provide information about <span class="math inline">\(\theta\)</span>. It can be shown that the two definitions are
equivalent.</p></li>
<li><p>Sample Space and Partitions: The sample space is the set of all possible values
of <span class="math inline">\(X\)</span>.</p></li>
<li><p>It is the same as the support for the joint pdf (or pmf) of <span class="math inline">\(X\)</span>.</p></li>
<li><p>A statistic partitions the sample space. Each partition corresponds to a different value of of the statistic.</p></li>
<li><p>A specific partition contains all possible values of <span class="math inline">\(X\)</span> that yield the specific value of the statistic that indexes the partition.</p></li>
<li><p>If the statistic is sufficient, then the only characteristic of the data that we need to examine is which partition the sample belongs to.</p></li>
<li><p>Non-uniqueness of the sufficient statistic: If <span class="math inline">\(T\)</span> is a sufficient statistic, then any one-to-one transformation of <span class="math inline">\(T\)</span> also is sufficient.</p></li>
<li><p>Note that any transformation of <span class="math inline">\(T\)</span> induces the same partitioning of the sample space. Accordingly, the sufficient statistic is not unique, but the partitioning that corresponds to <span class="math inline">\(T\)</span> is unique.</p></li>
<li><p>Factorization Criterion (Neyman): A statistic, <span class="math inline">\(T = t(X)\)</span> is sufficient if and
only if the joint pdf (pmf) factors a</p></li>
</ul>
<p><span class="math display">\[
f_X(X|\theta) = g[t(X)|\theta]h(X)
\]</span></p>
<ul>
<li>In some cases, <span class="math inline">\(h(X)\)</span> is a trivial function of <span class="math inline">\(X\)</span>.
<ul>
<li>For example, <span class="math inline">\(h(X) = c\)</span>, where <span class="math inline">\(c\)</span> is a constant not depending on <span class="math inline">\(X\)</span>.</li>
</ul></li>
<li>Example: Bernoulli trials
— Let <span class="math inline">\(X_i\)</span> for <span class="math inline">\(i = 1, \cdots, n\)</span> be iid <span class="math inline">\(Bern(p)\)</span> random variables.
<ul>
<li>Note, <span class="math inline">\(\theta = p\)</span>.</li>
<li>The joint pmf is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_X(X|p) = \prod^n_{i=1}p^{x_i}(1-p)^{1-x_i}I_{\{0,1\}}(x_i)=p^y(1-p)^{n-y}\prod^n_{i=1}I_{\{0,1\}}(x_i)
\]</span></p>
<p>where <span class="math inline">\(y = \sum^n_{i=1}x_i\)</span>.</p>
<ul>
<li><p>Accordingly, <span class="math inline">\(Y = \sum^n_{i=1}X_i\)</span> is sufficient.</p></li>
<li><p>For this example, it is not too hard to verify that the conditional distribution
of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> does not depend on <span class="math inline">\(p\)</span>.</p>
<ul>
<li>The conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(X=x|Y=y) &amp;= \frac{P(X=x)I_{\{y\}}(\sum x_i)}{p(Y=y)} \\
&amp;=
\frac{\displaystyle\prod^n_{i=1}p^{x_i}(1-p)^{1-x_i}I_{\{0,1\}}(x_i)I_{\{y\}}(\sum x_i)}{\begin{pmatrix} n \\ y \end{pmatrix}p^y(1-p)^{n-y}I_{\{0,1,2,\cdots,n\}}(y)} \\ &amp;= \frac{\displaystyle\prod^n_{i=1}I_{\{0,1\}}(x_i)I_{\{y\}}(\sum x_i)}{\begin{pmatrix} n \\ y \end{pmatrix}I_{\{0,1,2,\cdots,n\}}(y)}
\end{align}
\]</span></p>
<p>which does not depend on <span class="math inline">\(p\)</span>.</p>
<ul>
<li><p>That is, the conditional distribution of <span class="math inline">\(X\)</span> given a sufficient statistic does not depend on <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Example: Sampling from <span class="math inline">\(Poi(\lambda)\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(X_1, \cdots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from Poi(<span class="math inline">\(\lambda\)</span>).</li>
<li>The joint pmf is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_X(X|\lambda) = \frac{e^{-n}\lambda^t}{\displaystyle\prod^n_{i=1}x_i!}\prod^n_{i=1}I_{\{0,1,\cdots,\infty\}}(x_i)
\]</span></p>
<p>where <span class="math inline">\(t = \sum^n_{i=1}x_i\)</span>.</p>
<ul>
<li>Accordingly, the likelihood function is</li>
</ul>
<p><span class="math display">\[
L(\lambda) = e^{-n\lambda}\lambda^t
\]</span></p>
<p>and <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient.</p>
<ul>
<li>Recall that <span class="math inline">\(T \sim Poi(\lambda)\)</span>. Therefore, the distribution of <span class="math inline">\(X\)</span> conditional on <span class="math inline">\(T = t\)</span> is</li>
</ul>
<p><span class="math display">\[
\begin{align}
P(X=x|T=t) &amp;= \frac{P(X=x, \ T=t)}{P(T=t)} \\
&amp;= \frac{e^{-n\lambda}\lambda^tI_{\{t\}}(\sum x_i)t! \displaystyle\prod^n_{i=1}I_{\{0,1,\cdots,\infty\}}(x_i)}{\bigg(\displaystyle\prod^n_{i=1}x_i!\bigg)e^{-n\lambda}(n\lambda)^t} \\
&amp;= \begin{pmatrix} t \\ x_1, x_2,\cdots,x_n\end{pmatrix}\bigg(\frac{1}{n}\bigg)^{x_1}\bigg(\frac{1}{n}\bigg)^{x_2}\cdots\bigg(\frac{1}{n}\bigg)^{x_n} \\
&amp;\Longrightarrow (X|T=t) \sim multinom \bigg(t, \frac{1}{n}, \frac{1}{n}, \cdots, \frac{1}{n} \bigg)
\end{align}
\]</span></p>
<ul>
<li><p>Note that the distribution of <span class="math inline">\(X\)</span>, conditional on the sufficient statistic does not
depend on <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>Example: Suppose that <span class="math inline">\(X_i\sim iid N(\mu, 1)\)</span>, for <span class="math inline">\(i = 1, \cdots, n\)</span>. The joint pdf is</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
f_X(X|\mu) &amp;= \frac{\exp\bigg\{-\frac{1}{2}\displaystyle\sum^n_{i=1}(x_i - \mu)^2\bigg\}}{(2\pi)^{\frac{n}{2}}} \\
&amp;= \frac{\exp\bigg\{-\frac{1}{2}\displaystyle\sum^n_{i=1}(x_i - \bar x + \bar x - \mu)^2\bigg\}}{(2\pi)^{\frac{n}{2}}} \\
&amp;= \frac{\exp\bigg\{-\frac{1}{2}\displaystyle\sum^n_{i=1}[(x_i - \mu)^2 +2(x_i - \bar x)(\bar x - \mu)+(\bar x - \mu)^2]\bigg\}}{(2\pi)^{\frac{n}{2}}} \\
&amp;= \frac{\exp\bigg\{-\frac{1}{2}\displaystyle\sum^n_{i=1}(x_i - \bar x)^2 + n(\bar x - \mu)^2\bigg\}}{(2\pi)^{\frac{n}{2}}}
\end{align}
\]</span></p>
<p>because <span class="math inline">\(\displaystyle\sum^n_{i=1}(x_i - \bar x) = 0\)</span> and <span class="math inline">\(\exp \bigg\{-\frac{n}{2}(\bar x- \mu)^2 \bigg\} \frac{\exp \bigg\{-\frac{1}{2}\displaystyle\sum^n_{i=1}(x_i - \bar{x})^2\bigg\}}{(2\pi)^{\frac{n}{2}}}\)</span>.</p>
<ul>
<li>Accordingly, the likelihood function is</li>
</ul>
<p><span class="math display">\[
L(\theta) = \exp \bigg\{-\frac{n}{2}(\bar x - \mu)^2 \bigg\}
\]</span></p>
<p>and <span class="math inline">\(\bar X\)</span> is sufficient for the family of distributions.</p>
<ul>
<li>This means that <span class="math inline">\(\bar X\)</span> contains all of the information about <span class="math inline">\(\mu\)</span> that is contained in the data.
<ul>
<li>That is, if we want to use the sample to learn about <span class="math inline">\(\mu\)</span>, we should examine <span class="math inline">\(\bar X\)</span> and we need not examine any other function of the data.</li>
</ul></li>
<li>Order Statistics are sufficient: If <span class="math inline">\(X_1, \cdots, X_n\)</span> is a random sample (with or
without replacement), then the order statistics are sufficient.
<ul>
<li>Proof: By exchangeability,</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_X(X|\theta) = f_X(X_{(1)}, X_{(2)},\cdots,X_{(n)}|\theta)
\]</span></p>
<ul>
<li>The likelihood function is proportional to the joint pdf or pmf.
<ul>
<li>Therefore, the likelihood function is a function of the order statistics and, by definition, the order statistics are sufficient.</li>
</ul></li>
<li>If the random sample is taken from a continuous distribution, then it can be
shown that</li>
</ul>
<p><span class="math display">\[
P(X=X|x_{(1)},\cdots,x_{(n)}) = \frac{1}{n!}
\]</span></p>
<p>and this distribution does not depend on <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><p>Therefore, by definition the order statistics are sufficient.</p></li>
<li><p>The One Parameter Exponential Family: The random variable <span class="math inline">\(X\)</span> is said to
have a distribution within the one parameter regular exponential family if</p></li>
</ul>
<p><span class="math display">\[
f_X(x|\theta) = B(\theta)h(x)\exp\{Q(\theta)R(x)\}
\]</span></p>
<ul>
<li><span class="math inline">\(Q(\theta)\)</span> is a nontrivial continuous function of <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(R(x)\)</span> is a nontrivial function of <span class="math inline">\(x\)</span>.
<ul>
<li>Note that if the support of <span class="math inline">\(X\)</span> is represented as an indicator variable, then the indicator variable is part of <span class="math inline">\(h(x)\)</span>.</li>
<li>That is, the support cannot depend on <span class="math inline">\(\theta\)</span>.</li>
<li>Either or both of the functions <span class="math inline">\(B(θ)\)</span> and <span class="math inline">\(h(x)\)</span> could be trivial.</li>
</ul></li>
<li>A random sample of size <span class="math inline">\(n\)</span> from an exponential family has pdf (or pmf)</li>
</ul>
<p><span class="math display">\[f_X(X|\theta) = B(\theta)^n \exp \bigg\{ Q(\theta)\displaystyle\sum^n_{i=1}R(x_i)\bigg\}\prod^n_{i=1}h(x_i)
\]</span></p>
<ul>
<li>By the factorization criterion, <span class="math inline">\(T = \sum^n_{i=1}R(X_i)\)</span> is sufficient for <span class="math inline">\(\theta\)</span>.</li>
</ul>
<div id="examples-of-one-parameter-exponential-families" class="section level4 unnumbered hasAnchor">
<h4>Examples of one parameter exponential families<a href="chapter8.html#examples-of-one-parameter-exponential-families" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known. Then <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\mu\)</span> is known. Then <span class="math inline">\(T = \sum^n_{i=1}(X_i-\mu)^2\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Bern(p)\)</span>. Then <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(k\)</span> from <span class="math inline">\(Bin(n,p)\)</span>. Then <span class="math inline">\(T = \sum^n_{i=1}Y_i\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Geom(n,p)\)</span>. Then <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(NegBin(r,p)\)</span>, where r is known. Then <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Poi(\lambda)\)</span>. Then <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Expon(\lambda)\)</span>. Then <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Gam(\alpha, \lambda)\)</span>, where <span class="math inline">\(\alpha\)</span> is known. Then <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Gam(\alpha, \lambda)\)</span>, where <span class="math inline">\(\lambda\)</span> is known. Then <span class="math inline">\(T = \sum^n_{i=1}ln(X_i)\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Beta(\alpha_1, \alpha_2)\)</span>, where <span class="math inline">\(\alpha_1\)</span> is known. Then <span class="math inline">\(T = \sum^n_{i=1}ln(1-X_i)\)</span> is sufficient.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Beta(\alpha_1, \alpha_2)\)</span>, where <span class="math inline">\(\alpha_2\)</span> is known. Then <span class="math inline">\(T = \sum^n_{i=1}ln(X_i)\)</span> is sufficient.</p></li>
</ul>
</div>
<div id="examples-of-distributions-that-do-not-belong-to-the-exponential-family" class="section level4 unnumbered hasAnchor">
<h4>Examples of distributions that do not belong to the exponential family<a href="chapter8.html#examples-of-distributions-that-do-not-belong-to-the-exponential-family" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Unif(a,b)\)</span>, where <span class="math inline">\(a\)</span> is known. Then <span class="math inline">\(T = X_{(n)}\)</span> is sufficient by the factorization criterion.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Unif(a,b)\)</span>, where <span class="math inline">\(b\)</span> is known. Then <span class="math inline">\(T = X_{(1)}\)</span> is sufficient by the factorization criterion.</p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Unif(a,b)\)</span>, where neither <span class="math inline">\(a\)</span> nor <span class="math inline">\(b\)</span> is known.Then</p></li>
</ul>
<p><span class="math display">\[
T = \begin{pmatrix} X_{(1)} \\ X_{(n)}\end{pmatrix}
\]</span></p>
<p>is sufficient by the factorization criterion.</p>
<ul>
<li>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Unif(\theta, \theta+1)\)</span>. Then</li>
</ul>
<p><span class="math display">\[
T = \begin{pmatrix} X_{(1)} \\ X_{(n)}\end{pmatrix}
\]</span></p>
<p>is sufficient by the factorization criterion.</p>
</div>
<div id="example-2" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter8.html#example-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Consider a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(N(\mu, \sigma^2)\)</span>, where neither parameter is known. Write <span class="math inline">\((X_i − \mu)\)</span> as</li>
</ul>
<p><span class="math display">\[
\begin{align}
(X_i -\mu)^2 &amp;= [(X_i - \bar X)+(\bar X - \mu)]^2 \\
&amp;= (X_i - \bar X)^2 +2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2
\end{align}
\]</span></p>
<ul>
<li>The likelihood function can be written as</li>
</ul>
<p><span class="math display">\[
\begin{align}
L(\mu, \sigma^2|X) &amp;= \frac{\exp\bigg\{ \frac{1}{2\sigma^2}\displaystyle\sum^n_{i=1}(X_i - \mu)^2\bigg\}}{(2\pi\sigma^2)^{\frac{n}{2}}} \\
&amp;= \frac{\exp\bigg\{ \frac{1}{2\sigma^2}\displaystyle\sum^n_{i=1}[(X_i - \bar X)^2 +2(X_i -\bar X)(\bar X - \mu) +(\bar X- \mu)^2]\bigg\}}{(2\pi\sigma^2)^{\frac{n}{2}}} \\
&amp;= \frac{\exp\bigg\{\frac{1}{2\sigma^2}\bigg[\displaystyle\sum^n_{i=1}(X_i - \bar X)^2 +n(\bar X -\mu)^2\bigg]\bigg\}}{(2\pi\sigma^2)^{\frac{n}{2}}}
\end{align}
\]</span></p>
<p>By the factorization criterion,</p>
<p><span class="math display">\[
T = \begin{pmatrix} S^2_X \\ \bar X\end{pmatrix}
\]</span></p>
<p>is sufficient.</p>
<p><br></p>
</div>
</div>
<div id="sampling-distributions" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Sampling Distributions<a href="chapter8.html#sampling-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Recall that a statistic is a random variable.
<ul>
<li>The distribution of a statistic is called a sampling distribution.</li>
<li>This section describes some sampling distributions that can be obtained analytically.</li>
</ul></li>
<li>Sampling without replacement from a finite population.
<ul>
<li>Consider a finite population consisting of <span class="math inline">\(N\)</span> units, where each unit has one of just <span class="math inline">\(k\)</span> values, <span class="math inline">\(v_1, \cdots, v_k\)</span>.</li>
<li>Of the <span class="math inline">\(N\)</span> units, <span class="math inline">\(M_j\)</span> have value <span class="math inline">\(v_j\)</span> for <span class="math inline">\(j = 1, \cdots, k\)</span>.</li>
<li>Note that <span class="math inline">\(\sum^k_{j=1}M_j = N\)</span>.</li>
<li>Take a sample of size <span class="math inline">\(n\)</span>, one at a time at random and without replacement.</li>
<li>Let <span class="math inline">\(X_i\)</span> be the value of the <span class="math inline">\(i^{th}\)</span> unit in the sample.</li>
<li>Also, let <span class="math inline">\(Y_j\)</span> be the number of <span class="math inline">\(X\)</span>s in the sample that have value <span class="math inline">\(v_j\)</span>.</li>
<li>If <span class="math inline">\(\theta&#39; = (M_1 \ M_2 \ \cdots M_k)\)</span> is the vector of unknown parameters, then the joint pmf of <span class="math inline">\(X_1, \cdots, X_n\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_X(X|\theta) = \frac{\displaystyle\prod^k_{j=1}\begin{pmatrix} M_j \\ y_j\end{pmatrix}}{\begin{pmatrix} N \\ n\end{pmatrix} \begin{pmatrix} N \\ y_i, y_2,\cdots,y_n\end{pmatrix}} \times l_{\{n\}}(\displaystyle\sum^{k}_{j=1}y_i)\displaystyle\prod^n_{i=1}I_{\{v_1,\cdots,v_k\}}(x_i)\displaystyle\prod^k_{j=1}I_{\{0,1,\cdots,M_j\}}(y_i)
\]</span></p>
<ul>
<li><p>By the factorization theorem, <span class="math inline">\(Y=(Y_1, Y_2 \cdots Y_k)&#39;\)</span> is a sufficient statistic.</p></li>
<li><p>The sampling distribution of <span class="math inline">\(Y\)</span> is</p></li>
</ul>
<p><span class="math display">\[
f_Y(y|\theta) = \frac{\displaystyle\prod^k_{j=1} \begin{pmatrix} M_j \\ y_j \end{pmatrix}}{\begin{pmatrix} N \\ n \end{pmatrix}}I_{\{n\}} \Bigg( \displaystyle\sum^k_{j=1}y_i\Bigg)\displaystyle\prod^k_{j=1}I_{\{0,1,\cdots,M_j\}}(y_j)
\]</span></p>
<ul>
<li><p>Note that <span class="math inline">\(T =(Y_1, Y_2 \cdots Y_{k−1})&#39;\)</span> also is sufficient because
<span class="math inline">\(Y_k = N-\sum^{k-1}_{j=1}Y_j\)</span> and therefore <span class="math inline">\(Y\)</span> is a one-to-one function <span class="math inline">\(T\)</span> If <span class="math inline">\(k=2\)</span>, then the sampling distribution simplifies to the hypergeometric distribution.</p></li>
<li><p>Sampling with replacement from a finite population that has <span class="math inline">\(k\)</span> distinct values
or sampling without replacement from an infinite population that has <span class="math inline">\(k\)</span> distinct values.</p>
<ul>
<li>Consider a population for which the proportion of units having value <span class="math inline">\(v_j\)</span> is <span class="math inline">\(p_j\)</span>, for <span class="math inline">\(j=1,\cdots,k\)</span>.</li>
<li>Note then <span class="math inline">\(\sum^{k}_{j=1}p_j=1\)</span>.</li>
<li>Take a sample of size <span class="math inline">\(n\)</span>, one at a time at random and with replacement if the population is finite.</li>
<li>Let <span class="math inline">\(X_i\)</span> be the value of the <span class="math inline">\(i^{th}\)</span> unit in the sample.</li>
<li>Also, let <span class="math inline">\(Y_j\)</span> be the number of <span class="math inline">\(X\)</span>s in the sample that have value <span class="math inline">\(v_j\)</span>.</li>
<li>Let <span class="math inline">\(\theta&#39; = (p_1 \ p_2 \ \cdots \ p_k)\)</span> be the vector of unknown parameters.</li>
<li>The <span class="math inline">\(X\)</span>s are iid and the joint pmf of the sample is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
f_X(X|\theta) &amp;= \prod^n_{i=1}\prod^k_{j=1}p_j^{I_{\{v_j\}}(x_i)}I_{\{v_1,\cdots,v_k\}}(x_i) \\
&amp;= \prod^{k}_{j=1}p^{y_j}_j I_{\{n\}}\Bigg( \displaystyle\sum^k_{j=1}y_j \Bigg)\prod^k_{j=1}I_{\{0,1,\cdots,n\}}(y_j)
\end{align}
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(k=2\)</span>, then the sampling distribution simplifies to the binomial distribution.</p></li>
<li><p>Accordingly, <span class="math inline">\(Y&#39;=(Y_1, \ Y_2 \ \cdots \ Y_k)\)</span> is a sufficient statistic. The sampling distribution of <span class="math inline">\(Y\)</span> is multinomial:</p></li>
</ul>
<p><span class="math display">\[
f_Y(y|\theta) = \begin{pmatrix} n \\ y_1, y_2, \cdots,y_k \end{pmatrix} \prod^k_{j=1}p^{y_j}_jI_{\{n\}}\bigg( \sum^k_{j-1}y_j\bigg)\prod^k_{j=1}I_{\{0,1,\cdots,n\}}(y_j)
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(k = 2\)</span>, then the sampling distribution simplifies to the binomial distribution.</p></li>
<li><p>Sampling from a Poisson distribution.</p></li>
<li><p>Suppose that litter size in coyotes follows a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1, \cdots, X_n\)</span> be a random sample of litter sizes from <span class="math inline">\(n\)</span> dens.</p></li>
<li><p>The <span class="math inline">\(X\)</span>s are iid and the joint pmf of the sample is</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(X=x) &amp;= \displaystyle\prod^n_{i=1}\frac{e^{-\lambda}\lambda^{x_i}}{x_i!}I_{\{0,1,\cdots\}}(x_i) \\
&amp;=\frac{e^{-n\lambda}\lambda^{x_i}}{\displaystyle\prod^n_{i=1}x_i!}\displaystyle\prod^n_{i=1}I_{\{0,1,\cdots\}}(x_i)
\end{align}
\]</span></p>
<p>where <span class="math inline">\(y=\sum x_i\)</span>.</p>
<ul>
<li>Accordingly, <span class="math inline">\(Y = \sum Xi\)</span> is sufficient. The sampling distribution of <span class="math inline">\(Y\)</span> is Poisson:</li>
</ul>
<p><span class="math display">\[
f_Y(y|\theta) = \frac{e^{-n\lambda}(n\lambda)^y}{y!}I_{\{0,1,\cdots\}}(y)
\]</span></p>
<ul>
<li>Minimum of exponential random variables.
<ul>
<li>Let <span class="math inline">\(T_i\)</span>~iid <span class="math inline">\(Expon(\lambda)\)</span> for <span class="math inline">\(i =1,\cdots,n\)</span> and let <span class="math inline">\(T_{(1)}\)</span> be the smallest order statistic.</li>
<li>Then the sampling distribution of <span class="math inline">\(T_{(1)}\)</span> is <span class="math inline">\(T_{(1)} \sim Exp(\lambda)\)</span>.</li>
</ul></li>
<li>Maximum of exponential random variables.
<ul>
<li>Let <span class="math inline">\(t_i\)</span> be the failure time for the <span class="math inline">\(i^{th}\)</span> bus.</li>
<li>Suppose that <span class="math inline">\(T_i \sim iid Exp(\lambda)\)</span> for <span class="math inline">\(i =1,\cdots,n\)</span> and let <span class="math inline">\(T_{(1)}\)</span> be the largest order statistic.</li>
<li>The cdf of <span class="math inline">\(T_{(n)}\)</span> is (all buses fail before time <span class="math inline">\(t\)</span>)</li>
</ul></li>
</ul>
<p><span class="math display">\[
P(T_{(n)}\le t) = F_{T_{(n)}}(t) = P= \prod^n_{i=1}P(T_i \lt t)
\]</span></p>
<ul>
<li>because the failure times are</li>
</ul>
<p><span class="math display">\[
\prod^n_{i=1}(1-e^{-\lambda t}) = (1-e^{-\lambda t})^nI_{(0,\infty)}(t)
\]</span></p>
<ul>
<li>The pdf of <span class="math inline">\(T_{(n)}\)</span> can be found by differentiation:</li>
</ul>
<p><span class="math display">\[
f_{T_{(n)}}(t) = \frac{d}{dt}F_{T_{(n)}}(t)=(1-e^{-\lambda t)^{n-1}}n\lambda e^{-\lambda t}I_{(0,\infty)}(t)
\]</span></p>
<ul>
<li>Maximum of uniform random variables. Suppose that <span class="math inline">\(X_i\sim iid Unif(0, \theta)\)</span>. The <span class="math inline">\(X\)</span>s are iid and the joint pdf is</li>
</ul>
<p><span class="math display">\[
f_X(X|\theta) = \displaystyle\prod^n_{i=1}\frac{1}{\theta}I_{(0,\theta)}(x_i) = \frac{1}{\theta^n}I_{(0,\theta)}(x_{(n)})\prod^n_{i=1}I_{(0,x_{(n)})}(x_i)
\]</span></p>
<ul>
<li>Accordingly, <span class="math inline">\(X_{(n)}\)</span> is sufficient. The cdf of <span class="math inline">\(X_{(n)}\)</span> is (all <span class="math inline">\(X\)</span>s <span class="math inline">\(\le x\)</span>)</li>
</ul>
<p><span class="math display">\[
P(X_{(n)} \le x) = F_{X_{(n)}}(x)=p= \prod^n_{i=1}P(X_i \lt x)
\]</span></p>
<ul>
<li>because the <span class="math inline">\(X\)</span>s are</li>
</ul>
<p><span class="math display">\[
\prod^{n}_{i=1}\frac{x}{\theta} = \Big(\frac{x}{\theta}\Big)^n I_{(0,\theta)}(x)
\]</span></p>
<ul>
<li>The pdf of <span class="math inline">\(X_{(n)}\)</span> can be found by differentiation:</li>
</ul>
<p><span class="math display">\[
f_{X_{(n)}}(x) = \frac{d}{dx}F_{T_{(n)}}(x)=\frac{nx^{n-1}}{\theta^n}I_{(0,\theta)}(x)
\]</span></p>
<p><br></p>
</div>
<div id="simulating-sampling-distributions" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Simulating Sampling Distributions<a href="chapter8.html#simulating-sampling-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="how-to-simulate-a-sampling-distribution" class="section level4 unnumbered hasAnchor">
<h4>How to simulate a sampling distribution<a href="chapter8.html#how-to-simulate-a-sampling-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>Choose a population distribution of interest: example Cauchy with <span class="math inline">\(\mu = 100\)</span> and <span class="math inline">\(\sigma = 10\)</span></li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Choose a statistic or statistics of interest: example sample median and sample mean</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Choose a sample size: example <span class="math inline">\(n = 25\)</span></li>
</ol></li>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Generate a random sample of size <span class="math inline">\(n\)</span> from the specified distribution. The inverse cdf method is very useful here. For the <span class="math inline">\(Cauchy(\mu, \sigma^2)\)</span> distribution, the cdf is</li>
</ol></li>
</ul>
<p><span class="math display">\[
F_X(x|\mu, \sigma) = \frac{\arctan\Big(\frac{x-\mu}{\sigma}\Big)}{\pi}+\frac{1}{2}
\]</span></p>
<ul>
<li>Accordingly, if <span class="math inline">\(U \sim Unif(0, 1)\)</span>, then</li>
</ul>
<p><span class="math display">\[
X =\tan\bigg[(u-\frac{1}{2})\pi\bigg]\sigma+\mu \sim Cauchy(\mu,\sigma^2)
\]</span></p>
<ul>
<li><ol start="5" style="list-style-type: lower-alpha">
<li>Compute the statistic or statistics.</li>
</ol></li>
<li><ol start="6" style="list-style-type: lower-alpha">
<li>Repeat the previous two steps a large number of times.</li>
</ol></li>
<li><ol start="7" style="list-style-type: lower-alpha">
<li>Plot, tabulate, or summarize the resulting distribution of the statistic.</li>
</ol></li>
</ul>
</div>
<div id="example-3" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter8.html#example-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Sampling distribution of the mean; <span class="math inline">\(n = 25\)</span>, from Cauchy with <span class="math inline">\(\mu = 100\)</span> and <span class="math inline">\(\sigma = 10\)</span>;</p></li>
<li><ol style="list-style-type: lower-alpha">
<li>Number of samples generated: 50,000</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Mean of the statistic: 85.44</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Standard deviation of the statistic: 4,647.55</li>
</ol></li>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Plot of the statistic.</li>
</ol></li>
</ul>
<p><img src="fig8/ch8_4.png" /></p>
<ul>
<li><ol start="5" style="list-style-type: lower-alpha">
<li>Most of the distribution is centered near <span class="math inline">\(\mu\)</span>, but the tails are very fat. It can be shown that the sample mean also has a Cauchy distribution with <span class="math inline">\(\mu = 100\)</span> and <span class="math inline">\(\sigma = 10\)</span>.</li>
</ol></li>
</ul>
</div>
<div id="example-4" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter8.html#example-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Sampling distribution of the median; <span class="math inline">\(n = 25\)</span>, from Cauchy with <span class="math inline">\(\mu = 100\)</span> and <span class="math inline">\(\sigma = 10\)</span>;</p></li>
<li><ol style="list-style-type: lower-alpha">
<li>Number of samples generated: 50,000</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>Mean of the statistic: 100.01</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>Standard deviation of the statistic: 3.35</li>
</ol></li>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Plot of the statistic.</li>
</ol></li>
</ul>
<p><img src="fig8/ch8_5.png" /></p>
<ul>
<li><ol start="5" style="list-style-type: lower-alpha">
<li>Let <span class="math inline">\(M_n\)</span> be the sample median from a sample of size <span class="math inline">\(n\)</span> from the Cauchy distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</li>
</ol>
<ul>
<li>It can be shown that as <span class="math inline">\(n\)</span> goes to infinity, the distribution of the statistic</li>
</ul></li>
</ul>
<p><span class="math display">\[
Z_n = \frac{\sqrt{n}(M_n - \mu)}{\frac{1}{2}\sigma\pi}
\]</span></p>
<p>converges to <span class="math inline">\(N(0, 1)\)</span>.</p>
<ul>
<li>That is, for large <span class="math inline">\(n\)</span>,</li>
</ul>
<p><span class="math display">\[
M_n \sim N \bigg[\mu, \frac{\sigma^2\pi^2}{4n}\bigg]
\]</span></p>
<p>Note, for <span class="math inline">\(n = 25\)</span> and <span class="math inline">\(\sigma = 10\)</span>, <span class="math inline">\(Var(M) \approx \pi^2\)</span>.</p>
<ul>
<li>To generate normal random variables, the Box-Muller method can be used.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="order-statistics-1" class="section level2 hasAnchor" number="8.6">
<h2><span class="header-section-number">8.6</span> Order Statistics<a href="chapter8.html#order-statistics-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="marginal-distributions-of-order-statistics" class="section level4 unnumbered hasAnchor">
<h4>Marginal Distributions of Order Statistics<a href="chapter8.html#marginal-distributions-of-order-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Suppose that <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i= 1, \cdots, n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from a population with pdf <span class="math inline">\(f_X(x)\)</span> and cdf <span class="math inline">\(F_X(x)\)</span>.
<ul>
<li>Consider <span class="math inline">\(X_{(k)}\)</span>, the <span class="math inline">\(k^th\)</span> order staistic.</li>
<li>To find the pdf, <span class="math inline">\(f_{X_{(k)}}(x)\)</span> first partition the real line into three pieces:</li>
</ul></li>
</ul>
<p><span class="math display">\[
I_1 =(\infty ,x], \ \ I_2 = (x,x+dx], \text{ and } I_3 = (x+dx, \infty)
\]</span></p>
<ul>
<li>The pdf of <span class="math inline">\(f_{X_{(k)}}(x)\)</span> is (approximately) the probability of observing <span class="math inline">\(k − 1\)</span> <span class="math inline">\(X\)</span>s in <span class="math inline">\(I_1\)</span>, exactly one <span class="math inline">\(X\)</span> in <span class="math inline">\(I_2\)</span> and the remaining <span class="math inline">\(n − k\)</span> <span class="math inline">\(X\)</span>s in <span class="math inline">\(I_3\)</span>.</li>
<li>This probability is</li>
</ul>
<p><span class="math display">\[
f_{X_{(k)}}(x) \approx \begin{pmatrix} n \\ k-1, 1, n-k\end{pmatrix}[F_X(x)]^{k-1}[f_X(x)dx]^1[1-F_X(x)]^{n-k}
\]</span></p>
<ul>
<li>Accordingly (by the differential method), the pdf of <span class="math inline">\(X_{(k)}\)</span> is</li>
</ul>
<p><span class="math display">\[
f_{X_{(k)}}(x) = \begin{pmatrix} n \\ k-1, 1, n-k\end{pmatrix}[F_X(x)]^{k-1}[1-F_X(x)]^{n-k}f_X(x)
\]</span></p>
<ul>
<li>Example — Smallest order statistic:</li>
</ul>
<p><span class="math display">\[
f_{X_{(k)}}(x) = \begin{pmatrix} n \\ 0,1,n-1\end{pmatrix}[F_X(x)]^0[1-F_X(x)]^{n-1}f_X(x)
=n[1-F_X(x)]^{n-1}f_X(x)
\]</span></p>
<ul>
<li>Example — Largest order statistic:</li>
</ul>
<p><span class="math display">\[
f_{X_{(n)}}(x) = \begin{pmatrix} n \\ n-1,1,0\end{pmatrix}[F_X(x)]^{n-1}[1-F_X(x)]^{0}f_X(x)  =n[F_X(x)]^{n-1}f_X(x)
\]</span>.</p>
<ul>
<li>Example — <span class="math inline">\(Unif(0, 1)\)</span> distribution. The cdf is <span class="math inline">\(F_{X}(x) = x\)</span> and the pdf of the <span class="math inline">\(k^{th}\)</span> order statistic is</li>
</ul>
<p><span class="math display">\[
f_{X_{(k)}}(x) = \begin{pmatrix} n \\ k-1,1,n-k \end{pmatrix}x^{k-1}(1-x)^{n-k}I_{(0,1)}(x)  =\frac{x^{k-1}(1-x)^{n-k}}{B(k,n-k+1)}I_{(0,1)}(x)
\]</span></p>
<p>where <span class="math inline">\(B\)</span> is the beta function. That is, <span class="math inline">\(X_{(k)} \sim Beta(k, n − k + 1)\)</span>.</p>
<ul>
<li>Example: Find the exact pdf of the median from an odd size sample. In this case, <span class="math inline">\(k = (n + 1)/2\)</span> and the pdf is</li>
</ul>
<p><span class="math display">\[
\begin{align}
F_{X_{(n+1)/2}}(x) &amp;= \begin{pmatrix} n \\ \frac{n-1}{2},1,\frac{n-1}{2} \end{pmatrix}[F_X(x)^{(n-1)/2}][1-F_X(x)]^{(n-1)/2}f_X(x) \\
&amp;= \frac{[F_X(x)]^{(n-1)/2}[1-F_X(x)]^{(n-1)/2}f_X(x)}{B\bigg(\frac{n-1}{2}, \frac{n-1}{2} \bigg)}
\end{align}
\]</span></p>
<ul>
<li>For example, if <span class="math inline">\(X\)</span> has a a Cauchy distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, then the cdf is</li>
</ul>
<p><span class="math display">\[
F_x(x) = \frac{\arctan\big(\frac{x-\mu}{\sigma}\big)}{\pi} + \frac{1}{2}
\]</span></p>
<p>and the pdf of the median, <span class="math inline">\(M = X_{\big(\frac{n-1}{2}\big)}\)</span>, is</p>
<p><span class="math display">\[
f_M(m) = \frac{\bigg[\frac{\arctan\big(\frac{m-\mu}{\sigma}\big)}{\pi}+\frac{1}{2}\bigg]^{(n-1)/2} \bigg[ \frac{1}{2} - \frac{\arctan\big(\frac{m-\mu}{\sigma}\big)}{\pi}\bigg]^{(n-1)/2}}{B \bigg(\frac{n-1}{2}, \frac{n-1}{2} \bigg)} \times\frac{1}{\sigma\pi}\bigg[1+\Big(\frac{m-\mu}{\sigma} \Big)^2 \bigg]^{-1}
\]</span>.</p>
</div>
<div id="joint-distributions-of-order-statistics" class="section level4 unnumbered hasAnchor">
<h4>Joint Distributions of Order Statistics<a href="chapter8.html#joint-distributions-of-order-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Suppose that <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i = 1, \cdots, n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from a population with pdf <span class="math inline">\(f_X(x)\)</span> and cdf <span class="math inline">\(F_X(x)\)</span>.
<ul>
<li>Consider (<span class="math inline">\(X_{(k)}\)</span>, <span class="math inline">\(X_{(m)}\)</span>) the <span class="math inline">\(k^{th}\)</span> and <span class="math inline">\(m^{th}\)</span> order statistics, where <span class="math inline">\(k &lt; m\)</span>.</li>
<li>To find the joint pdf <span class="math inline">\(f_{X_{(k)}X_{(m)}}(v, w)\)</span>, first partition the real line into five pieces:</li>
</ul></li>
</ul>
<p><span class="math display">\[
I_4 = (\infty, v], \ I_2 =(v,v+dv], \ I_3=(v+dv,w], \ I_4=(w, w+dw], \text{ and } I_5=(w+dw,\infty)
\]</span></p>
<ul>
<li>The joint pdf of <span class="math inline">\(f_{X_{(k)}X_{(m)}}(v, w)\)</span> is (approximately) the probability of observing <span class="math inline">\(k − 1\)</span> <span class="math inline">\(X\)</span>s in <span class="math inline">\(I_1\)</span>, exactly one <span class="math inline">\(X\)</span> in <span class="math inline">\(I_2\)</span>, <span class="math inline">\(m − k − 1\)</span> <span class="math inline">\(X\)</span>s in <span class="math inline">\(I_3\)</span>, exactly one <span class="math inline">\(X\)</span> in <span class="math inline">\(I_4\)</span> and the remaining <span class="math inline">\(n − m\)</span> <span class="math inline">\(X\)</span>s in <span class="math inline">\(I_5\)</span>.
<ul>
<li>This probability is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
f_{X_{(k)}X_{(m)}}(v, w) &amp;\approx \begin{pmatrix} n \\ k-1,1,m-k-1,1,n-m\end{pmatrix}[F_X(v)]^{k-1} \\
&amp;\times [f_X(v)dv]^1[F_X(w)-F_X(v)]^{m-k-1}[f_X(w)dw]^1 \times [1-F_X(w)]^{n-m}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(v &lt; w\)</span>.</p>
<ul>
<li>Accordingly (by the differential method), the joint pdf of <span class="math inline">\(X_{(k)}\)</span> and <span class="math inline">\(X_{(m)}\)</span> is</li>
</ul>
<p><span class="math display">\[
\begin{align}
f_{X_{(k)}X_{(m)}}(v, w) &amp;= \frac{n!}{(k-1)!(m-k-1)!(n-m)!}[F_X(v)]^{k-1} \\
&amp;\times [F_X(W)-F_X(v)]^{m-k-1}[1-F_X(w)dw]^{n-m} \\
&amp;\times f_X(v)f_x(w)I_{(v,\infty)}(w)
\end{align}
\]</span></p>
<ul>
<li>Example—joint distribution of smallest and largest order statistic.
<ul>
<li>Let <span class="math inline">\(k = 1\)</span> and <span class="math inline">\(m = n\)</span> to obtain</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_{X_{(1)}X_{(n)}}(v, w) = n(n-1)[F_X(w)-F_X(v)]^{n-2} \times f_X(v)f_x(w)I_{(v,\infty)}(w)
\]</span></p>
<ul>
<li>Example—joint distribution of smallest and largest order statistics from <span class="math inline">\(Unif(0, 1)\)</span>.
<ul>
<li>The cdf is <span class="math inline">\(F_X(x) = x\)</span> and the joint distribution of <span class="math inline">\(X_{(1)}\)</span> and <span class="math inline">\(X_{(n)}\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_{X_{(1)}X_{(n)}}(v, w) = n(n-1)(w-v)^{n-2}I_{(v,\infty)}(w)
\]</span></p>
</div>
<div id="distribution-of-sample-range" class="section level4 unnumbered hasAnchor">
<h4>Distribution of Sample Range<a href="chapter8.html#distribution-of-sample-range" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Let <span class="math inline">\(R = X_{(n)}-X_{(1)}\)</span>. The distribution of this random variable is needed to construct <span class="math inline">\(R\)</span> charts in quality control applications and to compute percentiles of Tukey’s studentized range statistic (useful when making comparisons among means in ANOVA).
<ul>
<li>To find the pdf of <span class="math inline">\(R\)</span>, we will first find an expression for the cdf of <span class="math inline">\(R\)</span>:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(R \le r) = F_R(r) &amp;= P[X_{(n)}-X_{(1)} \le r] \\
&amp;= P[X_{(n)} \le r + X_{(1)}] \\
&amp;= P[X_{(1)} \le X_{(n)} \le r +X_{(1)}]
\end{align}
\]</span></p>
<p>because <span class="math inline">\(X_{(1)} \le X_{(n)}\)</span> must be satisfied</p>
<p><span class="math display">\[
= \int^\infty_{-\infty} \int^{v+r}_{v}f_{X_{(k)}X_{(m)}}(v, w)dw \ dv
\]</span></p>
<ul>
<li>To obtain <span class="math inline">\(f_R(r)\)</span>, take the derivative with respect to <span class="math inline">\(r\)</span>.
<ul>
<li>Leibnitz’s rule can be used.</li>
<li>Leibnitz’s Rule: Suppose that <span class="math inline">\(a(\theta)\)</span>, <span class="math inline">\(b(\theta)\)</span>, and <span class="math inline">\(g(x, \theta)\)</span> are differentiable functions of <span class="math inline">\(\theta\)</span>. Then</li>
</ul></li>
</ul>
<p><span class="math display">\[
\frac{d}{d\theta} \int^{b(\theta)}_{a(\theta)}g(x,\theta)dx = g[b(\theta),\theta] \frac{d}{d\theta} b(\theta) - g[a(\theta),\theta]\frac{d}{d\theta}a(\theta) + \int^{b(\theta)}_{a(\theta)}\frac{d}{d\theta}g(x,\theta)dx
\]</span></p>
<ul>
<li>Accordingly,</li>
</ul>
<p><span class="math display">\[
\begin{align}
f_R(r)&amp;= \frac{d}{dr}F_R(r) = \frac{d}{dr} \int^{\infty}_{-\infty}\int^{v+r}_{v}f_{X_{(1)},X_{(n)}}(v,w)dwdv \\
&amp;= \int^{\infty}_{-\infty} \frac{d}{dr}\int^{v+r}_{v}f_{X_{(1)},X_{(n)}}(v,w)dwdv \\ &amp;= \int^{\infty}_{-\infty}\bigg[f_{X_{(1)},X_{(n)}}(v,v+r)\frac{d}{dr}(v+r)-f_{X_{(1)},X_{(n)}}(v,v)\frac{d}{dr}v\bigg]dv \\
&amp;+ \int^{\infty}_{-\infty}\int^{v+r}_{v}\frac{d}{dr}f_{X_{(1)},X_{(n)}}(v,w)dwdv \\ &amp;=\int^{\infty}_{-\infty}f_{X_{(1)},X_{(n)}}(v,v+r)dv
\end{align}
\]</span></p>
<ul>
<li>Example—Distribution of sample range from <span class="math inline">\(Unif(0, 1)\)</span>.
<ul>
<li>In this case, the support for <span class="math inline">\(X_{(1)}, X_{(n)}\)</span> is <span class="math inline">\(0 &lt; v &lt; w &lt; 1\)</span>.</li>
<li>Accordingly, <span class="math inline">\(f_{X_{(1)},X_{(n)}}(v,v+r)\)</span> is zero only if <span class="math inline">\(0 &lt; v &lt; v + r &lt; 1\)</span>. - This implies that <span class="math inline">\(0 &lt; v &lt; 1 − r\)</span> and that <span class="math inline">\(r \in (0, 1)\)</span>.</li>
<li>The pdf of <span class="math inline">\(R\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_R(r) = \int^{1-r}_{0}n(n-1)(v+r-v)^{n-2}dv = n(n-1)r^{n-2}(1-r)I_{(0,1)}(r)  = \frac{r^{n-2}(1-r)}{B(n-1,2)}I_{(0,1)}(r)
\]</span></p>
<ul>
<li><p>That is, <span class="math inline">\(R \sim Beta(n − 1, 2)\)</span>.</p></li>
<li><p>Joint distribution of All Order Statistics.</p>
<ul>
<li>Employing the same procedure as for a pair if order statistics, it can be shown that the joint distribution of <span class="math inline">\(X_{(1)},\cdots, X_(n)\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_{X_{(1)},\cdots,X_{(n)}}=n!\prod^n_{i=1}f_X(x_i),  \text{ where } x_1&lt;x_2&lt;\cdots&lt;x_n
\]</span></p>
<p><br></p>
</div>
</div>
<div id="moments-of-sample-means-and-proportionssp" class="section level2 hasAnchor" number="8.7">
<h2><span class="header-section-number">8.7</span> Moments of Sample Means and Proportionssp<a href="chapter8.html#moments-of-sample-means-and-proportionssp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Let <span class="math inline">\(X_1, \cdots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> taken either with or without replacement from a population having mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma^2_X\)</span>.</p></li>
<li><p>Denote the support of the random variable <span class="math inline">\(X\)</span> by <span class="math inline">\(S_X\)</span>.</p></li>
<li><p>The following definitions are used:</p></li>
</ul>
<p><span class="math display">\[
\bar X = \frac{1}{n}\sum^n_{i=1}X_i, \,\,\, \hat p = \frac{1}{n}\sum^n_{i=1}X_i \,\,\, \text{if} \,\,\, S_X={0,1}
\]</span></p>
<p><span class="math display">\[
S^2_X = \frac{1}{n-1}\sum^n_{i=1}(X_i-\bar X)^2 = \frac{1}{n-1}\bigg[\sum^n_{i=1}X^2_i - n\bar x^2 \bigg]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
S^2_X = \frac{1}{n-1}\sum^n_{i=1}(X_i-\bar X)^2 = \frac{n \hat p (1- \hat p)}{n-1} \,\,\, \text{if} \,\,\, S_X = {0,1}
\]</span></p>
<ul>
<li>This section examines the expectation and variance of <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(\hat p\)</span>; the expectation of <span class="math inline">\(S^2_X\)</span> ; and unbiased estimators of <span class="math inline">\(Var(\bar X)\)</span>.
<ul>
<li>The following preliminary results are important and could be asked for on exams:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
E(X_i) &amp;= \mu_X; \\
Var(X_i) &amp;= \sigma^2_X; \\
E(X^2_i) &amp;= \mu^2_X + \sigma^2_X;\\
Var(\bar X) &amp;= n^{-2}\bigg[\sum^n_{i=1}Var(X_i)+\sum_{i \ne j}Cov(X_i,X_j); \\
Cov(X_i,X_j) &amp;= \begin{cases} 0 \quad if \ sampling \ with \ replacement, \\ -\frac{\sigma^2}{N-1} \quad if \ sampling \ without \ replacemen; and\end{cases}\\
E(\bar X^2) &amp;= \mu^2_{\bar x}+var(\bar X)
\end{align}
\]</span></p>
<ul>
<li>The result in equation is true because <span class="math inline">\(X_1, \cdots, X_n\)</span> are iid if sampling with replacement and</li>
</ul>
<p><span class="math display">\[
Var\Bigg(\sum^N_{i=1}X_i\Bigg)=0=N  \ Var(X_i)+N(N-1)Cov(X_i,X_j)
\]</span></p>
<p>if sampling without replacement.</p>
<ul>
<li><p>The remaining results follow from exchangeability and from the definition of the variance of a random variable.</p>
<ul>
<li>Be able to use the preliminary results to prove any of the following results.</li>
</ul></li>
<li><p>Case I: Random Sample of size <span class="math inline">\(n \Rightarrow X_1, X_2, \cdots, X_n\)</span> are iid.</p></li>
<li><p>Case Ia: Random Variable has Arbitrary Support</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
E(X_i) &amp;=\mu_X \\
Cov(X_i,X_j) &amp;= 0 \ for \ i\ne j \\
Var(X) &amp;= E(X_i^2) - [E(X_i)]^2 = \sigma^2_X \\
E(\bar x) &amp;= \mu_X \\
Var(\bar X) &amp;= \frac{\sigma^2_X}{n}\\
E(S^2_X) &amp;= \sigma^2_X \\
E\bigg(\frac{S^2_X}{n}\bigg)&amp;=\frac{\sigma^2_X}{n}=Var(\bar X)
\end{align}
\]</span></p>
<ul>
<li>Case Ib: Random Variable has Support <span class="math inline">\(S_X = \{0, 1\}\)</span></li>
</ul>
<p><span class="math display">\[
\begin{align}
E(X_i) &amp;= p \\
Cov(X_i, X_j) &amp;= 0 \ for \ i \ne j \\
Var(X) &amp;= E(X^2_i)-[E(X_i)]^2 = \sigma^2_X = p(1-p)\\
E(\hat p) &amp;= p \\
Var(\hat p) &amp;= \frac{\sigma^2_X}{n}=\frac{p(1-p)}{n} \\
E(S^2_X) &amp;= p(1-p)
\end{align}
\]</span></p>
<ul>
<li>When taking large samples from a binary population, <span class="math inline">\(\sigma^2= p(1 − p)\)</span> is usually estimated by <span class="math inline">\(\hat\sigma^2 = \hat p(1 − \hat p)\)</span> rather than <span class="math inline">\(S^2_X = \hat p(1 − \hat p)\)</span>.
<ul>
<li>Note that <span class="math inline">\(\hat \sigma^2\)</span> has bias <span class="math inline">\(−p(1 − p)/n\)</span>.</li>
</ul></li>
</ul>
<p><span class="math display">\[
E\bigg(\frac{S^2_X}{n}\bigg) = \frac{p(1-p)}{n}= Var(\hat p)
\]</span></p>
<ul>
<li><p>Case II: Random Sample of size n without replacement</p></li>
<li><p>Case IIa: Random Variable has Arbitrary Support</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
E(X_i) &amp;= \mu_X \\
Cov(X_i, X_j) &amp;= -\frac{\sigma^2_X}{N} \ for\ i \ne j \\
Var(X) &amp;= E(X^2_i)-[E(X_i)]^2 = \sigma^2_X \\
E(\bar X) &amp;= \mu_X \\
Var(\bar X) &amp;= \frac{\sigma^2_X}{n}(=\frac{p(1-p)}{n}\bigg(1-\frac{n-1}{N-1}\bigg) \\
E(S^2_X) &amp;= \sigma^2_X \frac{N}{N-1} \\
E\Bigg[ \frac{S^2_X}{n}\bigg(1-\frac{n}{N}\bigg) \Bigg]&amp;=\frac{\sigma^2_X}{n}\bigg(1-\frac{n-1}{N-1}\bigg) = Var(\bar X)
\end{align}
\]</span></p>
<ul>
<li>Case IIb: Random Variable has Support <span class="math inline">\(S_X = \{0, 1\}\)</span></li>
</ul>
<p><span class="math display">\[
\begin{align}
E(X_i) &amp;= p \\
Cov(X_i, X_j) &amp;= -\frac{\sigma^2_X}{N}=\frac{p(1-p)}{N} \ for \ i \ne j \\
Var(X) &amp;= E(X^2_i)-[E(X_i)]^2 = \sigma^2_X = p(1-p)\\
E(\hat p) &amp;= p \\
Var(\hat p) &amp;= \frac{\sigma^2_X}{n}\bigg( 1-\frac{n-1}{N-1}\bigg)=\frac{p(1-p)}{n}\bigg( \frac{n-1}{N-1}\bigg) \\
E(S^2_X) &amp;=E\bigg( \frac{n \hat p(1-\hat p)}{n-1}\bigg)=\sigma^2_X \bigg(\frac{N}{N-1} \bigg)= p(1-p)\bigg( \frac{N}{N-1} \bigg) \\
E\Bigg[ \frac{S^2_X}{n}\bigg( (1-\frac{n}{N}\bigg)\Bigg] &amp;= E \Bigg[ \frac{n \hat p (1-\hat  p)}{n(n-1)}\bigg(1-\frac{n}{N} \bigg)\Bigg]= \frac{p(1-p)}{n}\bigg(1-\frac{n-1}{N-1}\bigg) = Var(\hat p)
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="the-central-limit-theorem-clt" class="section level2 hasAnchor" number="8.8">
<h2><span class="header-section-number">8.8</span> The Central Limit Theorem (CLT)<a href="chapter8.html#the-central-limit-theorem-clt" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Theorem: Let <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from a population with mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma^2_X\)</span>.</p></li>
<li><p>Then, the distribution of</p></li>
</ul>
<p><span class="math display">\[
Z_n = \frac{\bar X -\mu_X}{\sigma_X/ \sqrt n}
\]</span></p>
<p>converges to <span class="math inline">\(N(0, 1)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<ul>
<li><p>The importance of the CLT is that the convergence of <span class="math inline">\(Z_n\)</span> to a normal
distribution occurs regardless of the shape of the distribution of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Transforming from <span class="math inline">\(Z_n\)</span> to <span class="math inline">\(\bar X\)</span> reveals that</p></li>
</ul>
<p><span class="math display">\[
\bar X \sim N\Bigg(\mu_X, \frac{\sigma^2_X}{n}\Bigg)
\]</span></p>
<p>if <span class="math inline">\(n\)</span> i large.</p>
<ul>
<li><p>The asymptotic distribution of <span class="math inline">\(\bar X\)</span> is said to be <span class="math inline">\(N(\mu_X, \sigma^2_X/n)\)</span>.</p>
<ul>
<li>The limiting distribution of <span class="math inline">\(\bar X\)</span> is degenerate <span class="math inline">\(\displaystyle\lim_{n \rightarrow \infty}\Pr(\bar X = \mu_X) =1\)</span></li>
</ul></li>
<li><p>Another way to express the CLT is
<span class="math display">\[
\displaystyle\lim_{n \rightarrow \infty}\Pr\Bigg(\frac{\sqrt n (\bar X - \mu_X)}{\sigma_X}\le c\Bigg)  =\Phi(c)
\]</span></p></li>
<li><p>Note, equation should be</p></li>
</ul>
<p><span class="math display">\[
\displaystyle\lim_{n \rightarrow \infty}\Pr(\bar X \le  c) = \begin{cases} 0 \quad if \ c&lt;\mu_x \\ 1 \quad if \ c \ge \mu_X\end{cases}
\]</span></p>
<ul>
<li>Application to Sums of iid random variables: If <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> are iid from a population with mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma^2_X\)</span>, then</li>
</ul>
<p><span class="math display">\[
E \Bigg(\sum^n_{i=1}X_i \Bigg) = n\mu_X, \,\,\, Var \Bigg(\sum^n_{i=1}X_i \Bigg) = n\sigma^2_X,
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} \Pr\Bigg(\frac{\displaystyle\sum^n_{i=1}X_i-n\mu_x}{\sqrt n \sigma_X} \le c \Bigg) = \Phi(c)
\]</span></p>
<ul>
<li>How large must n be before <span class="math inline">\(\bar X\)</span> is approximately normal?
<ul>
<li>The closer the parent distribution is to a normal distribution, the smaller is the required sample size.</li>
<li>When sampling from a normal distribution, a sample size of <span class="math inline">\(n = 1\)</span> is sufficient.</li>
<li>Larger sample sizes are required from parent distributions with strong skewness and/or strong kurtosis.</li>
<li>For example, suppose that <span class="math inline">\(X \sim Exp(\lambda)\)</span>. This distribution has skewness and kurtosis</li>
</ul></li>
</ul>
<p><span class="math display">\[
\kappa_3 = \frac{E(X-\mu_X)^3}{\sigma_X^{\frac{3}{2}}}=2\,\,\, \text{ and } \,\,\, \kappa_4 = \frac{E(X-\mu_X)^4}{\sigma_X^4}-3=6
\]</span></p>
<p>where <span class="math inline">\(\mu_X = 1/\lambda\)</span> and <span class="math inline">\(\sigma^2 = 1/\lambda^2\)</span>.</p>
<ul>
<li>The sample mean, <span class="math inline">\(\bar X\)</span> has distribution <span class="math inline">\(Gam(n, n\lambda)\)</span>.
<ul>
<li>The skewness and kurtosis of <span class="math inline">\(\bar X\)</span> are</li>
</ul></li>
</ul>
<p><span class="math display">\[
\kappa_3 = \frac{E(\bar X-\mu_{\bar X})^3}{\sigma_{\bar X}^{\frac{3}{2}}}=\frac{2}{\sqrt n} \,\,\, \text{and} \,\,\, \kappa_4 = \frac{E(\bar X-\mu_{\bar X})^4}{\sigma_{\bar X}^4}-3=\frac{6}{n}
\]</span></p>
<p>where <span class="math inline">\(\mu_X = 1/\lambda\)</span> and <span class="math inline">\(\sigma^2_{\bar X}  = 1/(n\lambda^2)\)</span>.</p>
<ul>
<li>Below are plots of the pdf of <span class="math inline">\(Z_n\)</span> for <span class="math inline">\(n = 1, 2, 5, 10, 25, 100\)</span>.</li>
</ul>
<p><img src="fig8/ch8_6.png" /></p>
<ul>
<li>Application to Binomial Distribution: Suppose that <span class="math inline">\(X \sim    Bin(n, p)\)</span>.
<ul>
<li>Recall that <span class="math inline">\(X\)</span> has the same distribution as the sum of <span class="math inline">\(n\)</span> iid <span class="math inline">\(Bern(p)\)</span> random variables.</li>
<li>Accordingly, for large <span class="math inline">\(n\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[
\hat p  \sim N \bigg(p, \frac{p(1-p)}{n} \bigg) \,\,\, \text{and} \,\,\, \Pr(\hat p \le c) \approx \Phi \bigg(\frac{\sqrt n (c-p)}{\sqrt p (1-p)} \bigg)
\]</span></p>
<ul>
<li>Continuity Correction. If <span class="math inline">\(X \sim Bin(n, p)\)</span>, then for large <span class="math inline">\(n\)</span></li>
</ul>
<p><span class="math display">\[
X \sim N[np,np(1-p)]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align}
\Pr(X=x) &amp;=\Pr\bigg(x-\frac{1}{2} \le X \le x+\frac{1}{2}\bigg) \,\,\, \text{for} \,\,\, x=0,1,\cdots,n \\
&amp;\approx \Phi\bigg(\frac{x+0.5-np}{\sqrt{np(1-p)}}\bigg) - \approx \Phi\bigg(\frac{x-0.5-np}{\sqrt{np(1-p)}}\bigg)
\end{align}
\]</span></p>
<ul>
<li>Adding or subtracting <span class="math inline">\(0.5\)</span> is called the continuity correction.
<ul>
<li>The continuity corrected normal approximations to the cdfs of <span class="math inline">\(X\)</span> and <span class="math inline">\(\hat p\)</span> are</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
\Pr(X \le x) &amp;=\Pr\bigg(X \le x +\frac{1}{2}\bigg)\,\,\, for \,\,\, x=0,1,\cdots,n \\
&amp;\approx \Phi\bigg(\frac{(x+0.5-np)}{\sqrt{np(1-p)}}\bigg)
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\Pr(\hat p \le c) &amp;=\Pr\bigg(\hat p \le c +\frac{1}{2n}\bigg) \,\,\, for \,\,\, c=\frac{0}{n},\frac{1}{n},\frac{3}{n},\cdots,\frac{n}{n},\\
&amp;\approx \Phi\bigg(\frac{(\sqrt n(c +\frac{1}{2n}-p)}{\sqrt{p(1-p)}}\bigg)
\end{align}
\]</span></p>
<p><br></p>
</div>
<div id="using-the-moment-generating-function" class="section level2 hasAnchor" number="8.9">
<h2><span class="header-section-number">8.9</span> Using the Moment Generating Function<a href="chapter8.html#using-the-moment-generating-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Let <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span>.
<ul>
<li>We wish to find the distribution of <span class="math inline">\(\bar X\)</span>.</li>
<li>One approach is to find the mgf of <span class="math inline">\(\bar X\)</span> and (hopefully) to identify the corresponding pdf or pmf.</li>
<li>Let <span class="math inline">\(\psi_X(t)\)</span> be the mgf of <span class="math inline">\(X\)</span>.</li>
<li>The mgf of <span class="math inline">\(\bar X\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
\psi_{\bar X}(t) &amp;=E \bigg(\exp \bigg\{\frac{t}{n}\sum^n_{i=1}X_i \bigg\}\bigg) \\
&amp;= E\bigg(\prod^n_{i=1}\exp \bigg\{\frac{t}{n} X_i\bigg\} \\
&amp;= \prod^n_{i=1}E\bigg(\exp \bigg\{\frac{t}{n}X_i\bigg\}\bigg) \text{  by independenc  } \\
&amp;= \prod^n_{i=1}\psi_{X_i}\bigg(\frac{t}{n}\bigg) \\
&amp;= \bigg[\psi_{X}\bigg(\frac{t}{n}\bigg)\bigg]^n
\end{align}
\]</span></p>
<p>because the <span class="math inline">\(X\)</span>s are identically distributed.</p>
<ul>
<li>Example: Exponential distribution.
<ul>
<li>If <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Expon(\lambda)\)</span>, then</li>
</ul></li>
</ul>
<p><span class="math display">\[
\psi_X(t) = \frac{\lambda}{\lambda-t}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\psi_X(t)=\bigg( \frac{\lambda}{\lambda-\frac{t}{n}} \bigg)^n = \bigg( \frac{n\lambda}{n\lambda-t}\bigg)^n
\]</span></p>
<p>which is the mgf of <span class="math inline">\(Gam(n, n\lambda)\)</span>.</p>
<ul>
<li>Example: Normal Distribution.
<ul>
<li>If <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from N(<span class="math inline">\(\mu_X, \sigma^2_X\)</span>), then</li>
</ul></li>
</ul>
<p><span class="math display">\[
\psi_X(t) = \exp \bigg\{tu_X + \frac{t^2\sigma^2_X}{2}\bigg\}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\psi_X(t) = \bigg(\exp \bigg\{\frac{t}{n}\mu_X + \frac{t^2 \sigma^2_X}{2n^2} \bigg\}\bigg)^n =  \exp\bigg\{tu_X + \frac{t^2\sigma^2_X}{2n}\bigg\}
\]</span></p>
<p>which is the mgf of <span class="math inline">\(N(\mu_X, \sigma^2_X/n)\)</span>.</p>
<ul>
<li>Example: Poisson Distribution.
<ul>
<li>If <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> is a random sample from <span class="math inline">\(Poi(\lambda)\)</span>, then</li>
</ul></li>
</ul>
<p><span class="math display">\[
\psi_X(t) = e^{\lambda(e^t-1)} \text{  and  } \psi_Y(t) = e^{n\lambda(e^t-1)}
\]</span></p>
<p>where <span class="math inline">\(Y = \sum^n_{i=1}X_i = n\bar X\)</span>.</p>
<ul>
<li>Accordingly, <span class="math inline">\(n \bar X \sim Poi(n\lambda)\)</span> and</li>
</ul>
<p><span class="math display">\[
P(\bar X = x) = P(n \bar X = nx) = \begin{cases} \frac{e^{-n\lambda}\lambda^{nx}}{(nx)!} \ for \ x =\frac{0}{n},\frac{1}{n},\frac{2}{n},\cdots;\\ 0  \quad otherwise \end{cases}
\]</span></p>
<ul>
<li>A useful limit result.
<ul>
<li>Let <span class="math inline">\(a\)</span> be a constant and let <span class="math inline">\(o(n^{—1})\)</span> be a term that goes to zero faster than does <span class="math inline">\(n^{—1}\)</span>.</li>
<li>That is,</li>
</ul></li>
</ul>
<p><span class="math display">\[
\lim_{n \rightarrow \infty}\frac{o(n^{—1})}{1/n} = \lim_{n \rightarrow \infty}no(n^{-1}) = 0
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\lim_{n \rightarrow \infty}\bigg[1+ \frac{a}{n}+o(n^{—1}) \bigg]^n = e^a
\]</span></p>
<ul>
<li>Proof:</li>
</ul>
<p><span class="math display">\[
\begin{align}
\lim_{n \rightarrow \infty}\bigg[1+ \frac{a}{n}+o(n^{—1}) \bigg]^n &amp;= \lim_{n \rightarrow \infty}\exp\bigg\{n\ln\bigg[1+ \frac{a}{n}+o(n^{—1}) \bigg]\bigg\} \\
&amp;=exp \bigg\{\lim_{n \rightarrow \infty}\exp\bigg\{n\ln\bigg[1+ \frac{a}{n}+o(n^{—1}) \bigg]\bigg\}
\end{align}
\]</span></p>
<ul>
<li>The Taylor series expansion of <span class="math inline">\(\ln(1 + \epsilon)\)</span> around <span class="math inline">\(\epsilon = 0\)</span> is</li>
</ul>
<p><span class="math display">\[
\ln (1+\epsilon) = \sum^{\infty}_{i=1}\frac{(-1)^{i+1}\epsilon^i}{i} = \epsilon-\frac{\epsilon^2}{2}+\frac{\epsilon^3}{3}-\frac{\epsilon^4}{4}+\cdots
\]</span></p>
<p>provided that <span class="math inline">\(|\epsilon|&lt;1\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(\epsilon = a/n + o (n^{—1})\)</span>.
<ul>
<li>If <span class="math inline">\(n\)</span> is large enough to satisfy <span class="math inline">\(|a/n + o(n^{-1})|&lt;1\)</span>, then</li>
</ul></li>
</ul>
<p><span class="math display">\[
\ln \bigg[1+ \frac{a}{n}+o(n^{—1}) \bigg] = \frac{a}{n}+o(n^{-1}) - \frac{1}{2}\bigg[ \frac{a}{n}+o(n^{—1})\bigg]^2-\bigg[ \frac{a}{n}+o(n^{—1})\bigg]^3 - \cdots = \frac{a}{n}+o(n^{-1})
\]</span></p>
<p>because terms such as <span class="math inline">\(a^2/n^2\)</span> and <span class="math inline">\(ao(n^{—1})/n\)</span> go to zero faster than does 1/<span class="math inline">\(n\)</span>.</p>
<ul>
<li>Accordingly,</li>
</ul>
<p><span class="math display">\[
\begin{align}
\lim_{n \rightarrow \infty}\bigg[1+ \frac{a}{n}+o(n^{—1}) \bigg]^n &amp;= \exp \bigg\{\lim_{n \rightarrow \infty} n \ln \bigg[1+ \frac{a}{n}+o(n^{—1}) \bigg]\bigg\}\\
&amp;= \exp \bigg\{ \lim_{n \rightarrow \infty} a +no(n^{-1})\bigg\}  \\
&amp;= \exp \{a+0\}=e^a
\end{align}
\]</span></p>
<ul>
<li>Heuristic Proof of CLT using MGF: Write <span class="math inline">\(Z_n\)</span> as</li>
</ul>
<p><span class="math display">\[
\begin{align}
Z_n &amp;= \frac{\bar X -\mu_X}{\sigma_X/\sqrt n} \\
&amp;= \frac{\frac{1}{n}\displaystyle\sum^n_{i=1}X_i-\mu_X}{\sigma_X/\sqrt n} \\  
&amp;= \frac{\displaystyle\sum^n_{i=1}\frac{1}{n}(X_i -\mu_X)}{\sigma_X/\sqrt n} \\
&amp;=\sum^n_{i=1} \frac{Z^*_i}{\sqrt n}
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
Z^*_i= \frac{X_i -\mu_X}{\sigma_X} =
\sum^n_{i=1}U_i,\,\,\, \text{where} \,\,\, U_i = \frac{Z^*_i}{\sqrt n}
\]</span></p>
<ul>
<li>Note that <span class="math inline">\(Z_1, Z_2, \cdots, Z_n\)</span> are iid with <span class="math inline">\(E(Z^*_i) = 0\)</span> and <span class="math inline">\(Var(Z^*_i) = 1\)</span>.
<ul>
<li>Also, <span class="math inline">\(U_1, U_2, \cdots, U_n\)</span> are iid with <span class="math inline">\(E(U_i) = 0\)</span> and <span class="math inline">\(Var(U_i) = 1/n\)</span>.</li>
<li>If <span class="math inline">\(U_i\)</span> has a moment generating function, then it can be written in expanded form as</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
\psi_{U_i}(t) &amp;= E(e^{tU_i}) = \sum^{\infty}_{j=0}\frac{t^j}{j!}E(U^j_i) \\
&amp;= 1 +tE(u_i) + \frac{t^2}{2}E(U^2_i)+\frac{t^3}{3!}E(U^3_i)+\frac{t^4}{4!}E(U^4_i)+\cdots \\
&amp;= 1+t\frac{E(Z^*_i)}{\sqrt n}+\frac{t^2}{2}\frac{E(Z^{*2}_i)}{n}+\frac{t^3}{3!}\frac{E(Z^{*3}_i)}{n^{\frac{3}{2}}}+\frac{t^4}{4!}\frac{E(Z^{*4}_i)}{n^2} + \cdots \\
&amp;= 1+\frac{t^2}{2n}+o(n^{-1})
\end{align}
\]</span></p>
<ul>
<li>Therefore, the mgf of <span class="math inline">\(Z_n\)</span> is</li>
</ul>
<p><span class="math display">\[
\psi_{U_i}(t) = E( \exp\{tZ_n\}) = E \bigg( \exp \bigg\{t\sum^n_{i=1}U_i\bigg\}\bigg)  = [\psi_{u_i}(t)]^n
\]</span></p>
<p>because <span class="math inline">\(U_1, U_2,\cdots,U_n\)</span> are iid</p>
<p><span class="math display">\[
= \bigg[ 1+\frac{t^2}{2n} + o(n^{-1})\bigg]^n
\]</span></p>
<ul>
<li>Now use the limit result to take the limit of <span class="math inline">\(\psi Z_n (t)\)</span> as <span class="math inline">\(n\)</span> goes to <span class="math inline">\(\infty\)</span>:</li>
</ul>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} \psi_{Z_n}(t)= \lim_{n \rightarrow \infty}\bigg[ 1+\frac{t^2}{2n} + o(n^{-1})\bigg]^n = \exp\bigg\{ \frac{t^2}{2}\bigg\}
\]</span></p>
<p>which is the mgf of <span class="math inline">\(N(0, 1)\)</span>.</p>
<ul>
<li>Accordingly, the distribution of <span class="math inline">\(Z_n\)</span> converges to <span class="math inline">\(N(0, 1)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</li>
</ul>
<p><br></p>
</div>
<div id="normal-populations" class="section level2 hasAnchor" number="8.10">
<h2><span class="header-section-number">8.10</span> Normal Populations<a href="chapter8.html#normal-populations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>This section discusses three distributional results concerning normal distributions.</p></li>
<li><p>Let <span class="math inline">\(X_1, \cdots, X_n\)</span> be a random sample from <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p></li>
<li><p>Define, as usual, the sample mean and variance as <span class="math inline">\(\bar X = n^{-1}\sum^n_{i=1}X_i\)</span> and <span class="math inline">\(S^2_X = (n-1)^{-1}\displaystyle\sum^n_{i=1}(X_i-\bar X)^2\)</span>.</p></li>
<li><p>Recall, that <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(S^2_X\)</span> are jointly sufficient for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p></li>
<li><p>The three distributional results are the following.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\bar X \sim N\bigg(\mu, \frac{\sigma^2}{n}\bigg)\)</span></li>
<li><span class="math inline">\(\frac{(n-1)S^2_X}{\sigma^2} \sim \chi^2_{n-1}\)</span></li>
<li><span class="math inline">\(\bar X \Perp S^2_X\)</span></li>
</ol>
<ul>
<li>The argument relies on another result; one that we already have verified:</li>
</ul>
<p><span class="math display">\[
\sum^n_{i=1}(X_i - \mu)^2 = \sum^n_{i=1}(X_i-\bar X)^2 + n(\bar X- \mu)^2
\]</span></p>
<p>Divide both sides by <span class="math inline">\(\sigma^2\)</span> to obtain</p>
<p><span class="math display">\[
\frac{\displaystyle\sum^n_{i=1}(X_i -\mu)^2}{\sigma^2} =\frac{\displaystyle\sum^n_{i=1}(X_i -\bar X)^2}{\sigma^2}+\frac{n(\bar X - \mu)^2}{\sigma^2}
\]</span></p>
<ul>
<li><p>Let <span class="math inline">\(Z_i = \frac{X_i - \mu}{\sigma}\)</span> and <span class="math inline">\(\bar Z = \frac{\bar X - \mu}{\sigma/\sqrt n}\)</span>.</p></li>
<li><p>Note that <span class="math inline">\(Z_i \sim iid N(0, 1)\)</span> and that <span class="math inline">\(\bar Z \sim N(0, 1)\)</span>.</p></li>
<li><p>The equality can be written as</p></li>
</ul>
<p><span class="math display">\[
\sum^n_{i=1}Z^2_i = \frac{(n-1)S^2_X}{\sigma^2}+\bar Z^2
\]</span></p>
<ul>
<li><p>The left-hand-side above is distributed as <span class="math inline">\(\chi^2\)</span> and the second term of the
right-hand-side is distributed as <span class="math inline">\(\chi^2\)</span>.</p></li>
<li><p>If <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(S^2_X\)</span> are independently distributed, then the two right-hand-side terms are independently distributed.</p>
<ul>
<li>It can be concluded that <span class="math inline">\((n − 1)S^2_X /\sigma^2 \sim \chi^2_{n-1}\)</span>.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="updating-prior-probabilities-via-likelihood" class="section level2 hasAnchor" number="8.11">
<h2><span class="header-section-number">8.11</span> Updating Prior Probabilities Via Likelihood<a href="chapter8.html#updating-prior-probabilities-via-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Overview: This section introduces the use of Bayes rule to update probabilities.</p></li>
<li><p>Let <span class="math inline">\(H\)</span> represent a hypothesis about a numerical parameter <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>In the frequentist tradition, the hypothesis must be either true or false because the value of <span class="math inline">\(\theta\)</span> is a fixed number.</p>
<ul>
<li>That is <span class="math inline">\(P(H) = 0\)</span> or <span class="math inline">\(P(H) = 1\)</span>.</li>
</ul></li>
<li><p>In Bayesian analyses, prior beliefs and information are incorporated by conceptualizing <span class="math inline">\(\theta\)</span> as a realization of a random variable <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>In this case, <span class="math inline">\(P(H)\)</span> can take on any value in <span class="math inline">\([0, 1]\)</span>.</li>
<li>The quantity, <span class="math inline">\(P(H)\)</span> is called the prior probability.</li>
<li>It represents the belief of the investigator prior to collecting new data.</li>
<li>One goal of Bayesian analyses is to compute the posterior probability <span class="math inline">\(P(H|X = x)\)</span>, where <span class="math inline">\(X\)</span> represents new data.</li>
</ul></li>
<li><p>By Bayes rule,</p></li>
</ul>
<p><span class="math display">\[
P(H|X=x) = \frac{P(H,X=x)}{P(X=x)} =\frac{P(X=x|H)P(H)}{P(X=x|H)P(H)+P(X=x|H^c)P(H^c)}
\]</span></p>
<ul>
<li>The quantity <span class="math inline">\(P (X = x|H)\)</span> is the likelihood function.
<ul>
<li>The quantity <span class="math inline">\(P (X = x)\)</span> does not depend on <span class="math inline">\(H\)</span> and therefore is considered a constant (conditioning on <span class="math inline">\(X\)</span> makes <span class="math inline">\(X\)</span> a constant rather than a random variable).</li>
<li>Accordingly, Bayes rule can be written as</li>
</ul></li>
</ul>
<p><span class="math display">\[
P(H|X=x) \propto L(H|x)P(H)
\]</span></p>
<ul>
<li><p>That is, the posterior is proportional to the prior times the likelihood function. - Note, the functions<span class="math inline">\(P (X = x|H)\)</span> and <span class="math inline">\(P (X = x)\)</span> are either pmfs or pdfs depending on whether <span class="math inline">\(X\)</span> is discrete or continuous.</p></li>
<li><p>Example: The pap smear is a screening test for cervical cancer. The test is not <span class="math inline">\(100\%\)</span> accurate.</p>
<ul>
<li>Let <span class="math inline">\(X\)</span> be the outcome of a pap smear:</li>
</ul></li>
</ul>
<p><span class="math display">\[
X = \begin{cases} 0 \quad if \ the \ test \ is \ negative, \ and \\
1 \quad if \ the \ test \ is \ positive \end{cases}
\]</span></p>
<ul>
<li>Studies have shown that the false negative rate of the pap smear is approximately <span class="math inline">\(0.1625\)</span> and the false positive rate is approximately <span class="math inline">\(0.1864\)</span>.
<ul>
<li>That is, <span class="math inline">\(16.25\%\)</span> of women without cervical cancer test positive on the pap smear and <span class="math inline">\(18.64\%\)</span> of women with cervical cancer test negative on the pap smear.</li>
<li>Suppose a specific woman, say Gloria, plans to have a pap smear test.</li>
<li>Define the random variable (parameter) <span class="math inline">\(\Theta\)</span> as</li>
</ul></li>
</ul>
<p><span class="math display">\[
\Theta = \begin{cases} 0 \quad if \ Gloria \ dose \ not \ have \ cervical \ cancer, \ and \\
1 \quad if \ Gloria \ dose \ have \ cervical \ cancer \end{cases}
\]</span></p>
<ul>
<li>The likelihood function is</li>
</ul>
<p><span class="math inline">\(P(X = 0|\Theta =1) = 0.1625\)</span>; <span class="math inline">\(P(X = 1|\Theta =1) = 1- 0.1625 =0.8375\)</span>;
<span class="math inline">\(P(X = 0|\Theta =1) = 1 - 0.1864 = 0.8136\)</span>; and <span class="math inline">\(P(X = 1|\Theta =0) = 0.1864\)</span>;</p>
<ul>
<li>Suppose that the prevalence rate of cervical cancer is <span class="math inline">\(31.2\)</span> per <span class="math inline">\(100,000\)</span> women.
<ul>
<li>A Bayesian might use this information to specify a prior probability for Gloria, namely <span class="math inline">\(P(\Theta = 1) = 0.000312\)</span>.</li>
<li>Suppose that Gloria takes the pap smear test and the test is positive.</li>
<li>The posterior probability is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(\Theta=1|X=1) &amp;= \frac{P(X=1|\Theta=1)P(\Theta=1)}{P(X=1)} \\
&amp;= \frac{P(X=1|\Theta=1)P(\Theta=1)}{P(X=1|\Theta+1)P(\Theta=1)+\ P(X=1|\Theta=0)P(\Theta=0)}\\
&amp;=\frac{(0.8375)(0.000312)}{(0.1864)(0.999688)+(0.8375)(0.000312)}=0.0014
\end{align}
\]</span></p>
<ul>
<li>Note that</li>
</ul>
<p><span class="math display">\[
\frac{P(\Theta=1|X=1)}{P(\Theta=1)}=\frac{0.0014}{0.000312}=4.488
\]</span></p>
<p>so that Gloria is approximately four and a half times more likely to have cervical cancer given the positive test than she did before the test, even though the probability that she has cervical cancer is still low.</p>
<ul>
<li>The posterior probability, like the prior probability, is interpreted as a subjective probability rather than a relative frequency probability.
<ul>
<li>A relative frequency interpretation makes no sense here because the experiment can not be repeated (there is only one Gloria).</li>
</ul></li>
<li>Bayes Factor (BF): One way of summarizing the evidence about the hypothesis <span class="math inline">\(H\)</span> is to compute the posterior odds <span class="math inline">\(H\)</span> divided by the prior odds <span class="math inline">\(H\)</span>.
<ul>
<li>This odds ratio is called the Bayes Factor (BF) and it is equivalent to the ratio of likelihood functions. Denote the sufficient statistic by <span class="math inline">\(T\)</span>.</li>
<li>In the pap smear example, <span class="math inline">\(T = X\)</span> because there is just one observation.</li>
<li>Denote the pdfs or pmfs of T given <span class="math inline">\(H\)</span> or <span class="math inline">\(H^c\)</span> by <span class="math inline">\(f_{T|H}(t|H)\)</span> and <span class="math inline">\(f_{T|Hc}(t|H^c)\)</span>, respectively.</li>
<li>The marginal distribution of <span class="math inline">\(T\)</span> is obtained by summing the joint distribution of <span class="math inline">\(T\)</span> and the hypothesis over <span class="math inline">\(H\)</span> and <span class="math inline">\(H^c\)</span>:</li>
</ul></li>
</ul>
<p><span class="math display">\[
m_T(t) = f_{T|H}(t|H)+f_{T|H^c}P(H^c)
\]</span></p>
<ul>
<li>The Posterior odds of <span class="math inline">\(H\)</span> are
<span class="math display">\[
\begin{align}
\text{posterior Odds of } H &amp;= \frac{P(H|T=t)}{1-P(H|T=t)} = \frac{P(H|T=t)}{P(H^c|T=t)} \\
&amp;=\frac{f_{T|H}(t|H)P(H)}{m_T(t)} \div \frac{f_{T|H^c}(t|H^c)P(H^c)}{m_T(t)} \\
&amp;=\frac{f_{T|H}(t|H)P(H)}{m_T(t)}{f_{T|H^c}(t|H^c)P(H^c)}
\end{align}
\]</span></li>
</ul>
<p>The prior odds of <span class="math inline">\(H\)</span> are</p>
<p><span class="math display">\[
\text{Prior Odds of } H = \frac{P(H)}{1-P(H)}=\frac{P(H)}{P(H^c)}
\]</span></p>
<ul>
<li>Result: The Bayes Factor is equivalent to the ratio of likelihood functions,</li>
</ul>
<p><span class="math display">\[
BF = \frac{f_{T|H}(t|H)}{f_{T|H^c}(t|H^c)}
\]</span></p>
<ul>
<li>proof:</li>
</ul>
<p><span class="math display">\[
BF = \frac{Posterior \ odds \ of \ H}{Prior \ odds \ of \ H} = \frac{P(H|T=t)/P(H^c|T=t)}{P(H)/P(H^c)} = \frac{f_{T|H}(t|H)P(H)}{f_{T|H^c}(t|H^c)P(H^c)} \div \frac{P(H)}{P(H^c)} = \frac{f_{T|H}(t|H)}{f_{T|H^c}(t|H^c)}
\]</span></p>
<p>which is the ratio of likelihood functions.</p>
<ul>
<li>Frequentist statisticians refer to this ratio as the likelihood ratio.
<ul>
<li>A Bayes factor greater than 1 means that the data provide evidence for <span class="math inline">\(H\)</span> relative to <span class="math inline">\(H^c\)</span> and a Bayes factor less than 1 means that the data provide evidence for <span class="math inline">\(H^c\)</span> relative to <span class="math inline">\(H\)</span>.</li>
<li>For the cervical cancer example, the hypothesis is <span class="math inline">\(\Theta = 1\)</span> and the Bayes factor is</li>
</ul></li>
</ul>
<p><span class="math display">\[
BF = \frac{P(X=1|\Theta=1)}{P(X=1|\Theta=0)}=\frac{0.8375}{0.1864}=4.493
\]</span></p>
<ul>
<li>The above Bayes factor is nearly the same as the ratio of the posterior probability to the prior probability of <span class="math inline">\(H\)</span> because the prior probability is nearly zero.
<ul>
<li>In general, these ratios will not be equal.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="some-conjudate-families" class="section level2 hasAnchor" number="8.12">
<h2><span class="header-section-number">8.12</span> Some conjudate Families<a href="chapter8.html#some-conjudate-families" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Overview: Let <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> be a random sample (with or without replacement) from a population having pdf or pmf <span class="math inline">\(f_X(x|\theta)\)</span>.
<ul>
<li>A first step in making inferences about <span class="math inline">\(\theta\)</span> is to reduce the data by finding a sufficient statistic.</li>
<li>Let <span class="math inline">\(T\)</span> be the sufficient statistic and denote the pdf or pmf of <span class="math inline">\(T\)</span> by <span class="math inline">\(f_{T|\Theta}(t|\theta)\)</span>.</li>
<li>Suppose that prior beliefs about <span class="math inline">\(\theta\)</span> can be represented as the prior distribution <span class="math inline">\(g_{\Theta}(\theta)\)</span>.</li>
<li>By the definition of conditional probability, the posterior distribution of <span class="math inline">\(\Theta\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
g_{\Theta|T}(\theta|t) = \frac{f_{\Theta, T}(\theta,t)}{m_T(t)} = \frac{f_{T|\Theta}(t|\theta)g_{\Theta}(\theta)}{m_T(t)}
\]</span></p>
<p>where <span class="math inline">\(m_T(t)\)</span> is the marginal distribution of <span class="math inline">\(T\)</span> which can be obtained as</p>
<p><span class="math display">\[
m_T(t) = \int f_{T,\Theta}(t,\theta)d\theta  = \int f_{T|\Theta}(t|\theta)g_{\Theta}(\theta)d\theta
\]</span></p>
<ul>
<li>Integration should be replaced by summation if the prior distribution is discrete.
<ul>
<li>In practice, obtaining an expression for the marginal distribution of <span class="math inline">\(T\)</span> may be unnecessary.</li>
<li>Note that <span class="math inline">\(m_T(t)\)</span> is just a constant in the posterior distribution.</li>
<li>Accordingly, the posterior distribution is</li>
</ul></li>
</ul>
<p><span class="math display">\[
g_{\Theta|T}(\theta|t) \propto L(\theta|t)g_{\Theta}(\theta)
\]</span></p>
<p>becasue <span class="math inline">\(L(\theta|t) \propto f_{T|\Theta}(t|\theta)\)</span>.</p>
<ul>
<li><p>The kernel of a pdf or pmf is proportional to the pmf or pdf and is the part of the function that depends on the random variable.</p>
<ul>
<li>That is, the kernel is obtained by deleting any multiplicative terms that do not depend on the random variable.</li>
<li>The right-hand-side of equation contains the kernel of the posterior.</li>
<li>If the kernel can be recognized, then the posterior distribution can be obtained without first finding the marginal distribution of <span class="math inline">\(T\)</span>.</li>
</ul></li>
<li><p>The kernels of some well-known distributions are given below.</p></li>
<li><p>If <span class="math inline">\(\Theta \sim Unif(a,b)\)</span>, then the kernel is <span class="math inline">\(I_{(a,b)(\theta)}\)</span></p></li>
<li><p>If <span class="math inline">\(\Theta \sim Exp(\lambda)\)</span>, then the kernel is <span class="math inline">\(e^{-\lambda\theta}I_{(0,\infty)}\)</span></p></li>
<li><p>If <span class="math inline">\(\Theta \sim Gamma(a,\lambda)\)</span>, then the kernel is <span class="math inline">\(\theta^{a-1}e^{-\lambda\theta}I_{(0,\infty)}\)</span></p></li>
<li><p>If <span class="math inline">\(\Theta \sim Poi(\lambda)\)</span>, then the kernel is <span class="math inline">\(\frac{\lambda^{\theta}}{\theta!}I_{0,1,2,\cdots}(\theta)\)</span></p></li>
<li><p>If <span class="math inline">\(\Theta \sim Beta(\alpha,\beta)\)</span>, then the kernel is <span class="math inline">\(\theta^{a-1}(1-\theta)^{\beta-1}I_{(0,1)}(\theta)\)</span></p></li>
<li><p>If <span class="math inline">\(\Theta \sim N(\mu,\sigma^2)\)</span>, then the kernel is <span class="math inline">\(e^{-\frac{1}{2}\sigma^2}(\theta^2-2\theta\mu)\)</span></p></li>
<li><p>Conjugate Families: A family of distributions is conjugate for a likelihood function if the prior and posterior distributions both are in the family.</p></li>
<li><p>Example 1. Consider the problem of making inferences about a population proportion, <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>A random sample <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> will be obtained from a Bernoulli(<span class="math inline">\(\theta\)</span>) population. By sufficiency, the data can be reduced to <span class="math inline">\(Y = \sum X_i\)</span>, and conditional on <span class="math inline">\(\Theta= \theta\)</span>, the distribution of <span class="math inline">\(Y\)</span> is <span class="math inline">\(Y \sim Bin(n, \theta)\)</span></li>
<li>One natural prior distribution for <span class="math inline">\(\Theta\)</span> is <span class="math inline">\(Beta(\alpha, \beta)\)</span>.</li>
<li>The beta parameters must be greater than zero.</li>
<li>The limiting distribution as <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> go to zero is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\lim_{\alpha \rightarrow 0, \beta \rightarrow 0} \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)}= \begin{cases} \frac{1}{2} \quad \theta  \in \{0,1\} \\ 0 \quad otherwise \end{cases}
\]</span></p>
<ul>
<li><p>The parameter <span class="math inline">\(\alpha − 1\)</span> can be conceptualized as the number of prior successes and <span class="math inline">\(\beta − 1\)</span> can be conceptualized as the number of prior failures.</p></li>
<li><p>Prior</p></li>
</ul>
<p><span class="math display">\[
g_{\Theta}(\theta|\alpha, \beta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)}I_{(0,1)}(\theta)
\]</span></p>
<ul>
<li>Likelihood Function</li>
</ul>
<p><span class="math display">\[
f_{Y|\Theta}(y|\theta) = \begin{pmatrix} n \\ y \end{pmatrix} \theta^y(1-\theta)^{n-y}I_{0,1,\cdots,n}(y)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
L(\theta|y)=\theta^y(1-\theta)^{n-y}
\]</span></p>
<ul>
<li>Posterior:</li>
</ul>
<p><span class="math display">\[
g_{\Theta|Y}(\theta|\alpha,\beta,y) \propto \theta^y(1-\theta)^{n-y}\frac{\theta^{\alpha -1}(1-\theta)^{\beta -1}}{B(\alpha, \beta)}I_{(0,1)}(\theta)
\]</span></p>
<ul>
<li>The kernel of the posterior is <span class="math inline">\(\theta^{y+\alpha-1}(1-\theta)^{n-y+\beta-1}\)</span>.
<ul>
<li>Accordingly, the posterior distribution is <span class="math inline">\(Beta(y + \alpha, n − y + \beta)\)</span>.</li>
<li>Note that the posterior mean (a point estimator) is</li>
</ul></li>
</ul>
<p><span class="math display">\[
E(\theta|Y=y) = \frac{y+\alpha}{n+\alpha+\beta}
\]</span></p>
<ul>
<li>Note that both the prior and posterior are beta distributions.
<ul>
<li>Accordingly, the beta family is conjugate for the binomial likelihood.</li>
</ul></li>
<li>Example 2. Consider the problem of making inferences about a population mean, <span class="math inline">\(\theta\)</span>, when sampling from a normal distribution having known variance, <span class="math inline">\(\sigma^2\)</span>.
<ul>
<li>By sufficiency, the data can be reduced to <span class="math inline">\(X\)</span>.</li>
<li>One prior for <span class="math inline">\(\Theta\)</span> is <span class="math inline">\(N(\nu, \tau^2)\)</span>.</li>
</ul></li>
<li>Prior</li>
</ul>
<p><span class="math display">\[
g_{\Theta}(\theta|\nu, \tau) = \frac{e^{-\frac{1}{2\tau^2}(\theta-\nu)^2}}{\sqrt{2\pi\tau^2}}
\]</span></p>
<ul>
<li>Likelihood Function</li>
</ul>
<p><span class="math display">\[
f_{\bar X|\Theta}(\bar x|\theta, \sigma^2) = \frac{\exp\{-\frac{n}{2\sigma^2}(\bar x - \theta)^2\} }{\sqrt{2\pi\frac{\sigma^2}{n}}}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
L(\theta|x)=e^{-\frac{n}{2\sigma^2}(\theta^2-2\bar x \theta)}
\]</span></p>
<ul>
<li>Posterior:</li>
</ul>
<p><span class="math display">\[
g_{\Theta|\bar X}(\theta|\sigma, \nu, \tau, \bar x) \propto  \frac{e^{-\frac{1}{2\tau^2}(\theta-\nu)^2}}{\sqrt{2\pi\tau^2}}e^{-\frac{1}{2\tau^2}(\theta^2-2\bar x\theta}
\]</span>
- The combined exponent, after dropping multiplicative terms that do not depend on <span class="math inline">\(\theta\)</span> is</p>
<p><span class="math display">\[
\begin{align}
&amp;-\frac{1}{2}\bigg\{\frac{n}{\sigma^2}(\theta^2-2\theta\bar x)+\frac{1}{\tau^2}(\theta^2-2\theta\tau)\bigg\} \\
&amp;= -\frac{1}{2}\bigg( \frac{n}{\sigma^2} + frac{1}{\tau^2}\bigg) \bigg\{\theta^2 -2\theta\bigg(\frac{n}{\sigma^2} + \frac{1}{\tau^2}\bigg)^{-1}\bigg(\frac{\bar xn}{\sigma^2}+\frac{\nu}{\tau^2}\bigg)+C \bigg\} \\
&amp;= -\frac{1}{2}\bigg( \frac{n}{\sigma^2} + frac{1}{\tau^2}\bigg) \bigg\{\theta -\bigg(\frac{n}{\sigma^2} + \frac{1}{\tau^2}\bigg)^{-1}\bigg(\frac{\bar xn}{\sigma^2}+\frac{\nu}{\tau^2}\bigg)\bigg\}+C^*
\end{align}
\]</span></p>
<p>where <span class="math inline">\(C\)</span> and <span class="math inline">\(C^∗\)</span> are terms that do not depend on <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>Note, we have “completed the square.” This is the kernel of a normal distribution with mean and variance</li>
</ul>
<p><span class="math display">\[
E(\Theta|\bar x) = \bigg(\frac{n}{\sigma^2} + \frac{1}{\tau^2}\bigg)^{-1}\bigg(\frac{\bar xn}{\sigma^2}+\frac{\nu}{\tau^2}\bigg)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
Var(\Theta|\bar x) = \bigg( \frac{n}{\sigma^2}+\frac{1}{\tau^2}\bigg)^{-1}
\]</span></p>
<ul>
<li>Note that both the prior and the posterior are normal distributions.
<ul>
<li>Accordingly, the normal family is conjugate for the normal likelihood when <span class="math inline">\(\sigma^2\)</span> is known.</li>
</ul></li>
<li>Precision. An alternative expression for the posterior mean and variance uses what is called the precision of a random variable.
<ul>
<li>Precision is defined as the reciprocal of the variance.</li>
<li>Thus, as the variance increases, the precision decreases.</li>
<li>For this problem</li>
</ul></li>
</ul>
<p><span class="math display">\[
\pi_{\bar X}= \frac{n}{\sigma^2}, \pi_{\Theta}= \frac{1}{\tau^2}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\pi_{\Theta|\bar X} = \frac{n}{\sigma^2} + \frac{1}{\tau^2} = \pi_{\bar X}+\pi_{\Theta}
\]</span></p>
<ul>
<li>That is, the precision of the posterior is the sum of the precision of the prior plus the precision of the data. Using this notation, the posterior mean and variance are</li>
</ul>
<p><span class="math display">\[
E(\Theta|\bar x) = \frac{\pi_{\bar X}}{\pi_{\bar X}+ \pi_{\theta}}\bar x + \frac{\pi_{\theta}}{\pi_{\bar X}+\pi_{\theta}}\nu
\]</span></p>
<p>and</p>
<p><span class="math display">\[
Var(\Theta|\bar x) = (\pi_\bar X + \pi_{\theta})^{-1}
\]</span></p>
<ul>
<li>Note that the posterior mean is a weighted average of the prior mean and that data mean.</li>
</ul>
<p><br></p>
</div>
<div id="predictive-distributions" class="section level2 hasAnchor" number="8.13">
<h2><span class="header-section-number">8.13</span> Predictive Distributions<a href="chapter8.html#predictive-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>The goal in this section is to make predictions about future observations,
<span class="math inline">\(Y_1, Y_1, \cdots, Y_k\)</span>.
<ul>
<li>We may have current observations <span class="math inline">\(X_1, X_1, \cdots, X_n\)</span> to aid us.</li>
</ul></li>
<li>Case I: No data available.
<ul>
<li>If the value of <span class="math inline">\(\Theta\)</span> is known, then the predictive distribution is simply the pdf (or pmf), <span class="math inline">\(f_{Y|\theta}(y|\theta)\)</span>.</li>
<li>In most applications <span class="math inline">\(\theta\)</span> is not known.</li>
<li>The Bayesian solution is to integrate <span class="math inline">\(\theta\)</span> out of the joint distribution of (<span class="math inline">\(\Theta\)</span>, Y).</li>
<li>That is, the Bayesian predictive distribution is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_Y(y) = E_{\Theta}[f_{y|\Theta}(y|\theta)]  = \int f_{y|\Theta}(y|\theta)g_{\Theta}(\theta)d\theta
\]</span></p>
<p>where <span class="math inline">\(g_{\Theta}(\theta)\)</span> is the prior distribution of <span class="math inline">\(\Theta\)</span>.</p>
<ul>
<li><p>Replace integration by summation if the distribution of <span class="math inline">\(\Theta\)</span> is discrete.</p></li>
<li><p>Case II: Data available.</p>
<ul>
<li>Suppose that <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> has been observed from <span class="math inline">\(f_{X|\Theta}(x|\theta)\)</span>.</li>
<li>Denote the sufficient statistic by <span class="math inline">\(T\)</span> and denote the pdf (pmf) of <span class="math inline">\(T\)</span> given <span class="math inline">\(\Theta\)</span> by <span class="math inline">\(f_{T|\Theta}(t|\theta)\)</span>.</li>
<li>The Bayesian posterior predictive distribution is given by</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_{Y|T}(y|t) = E_{\Theta|T}[f_{y|\Theta}(y|\theta)]  = \int f_{y|\Theta}(y|\theta)g_{\theta|T}(\theta|t)d\theta
\]</span></p>
<p>where <span class="math inline">\(g_{\Theta|T}(\theta|t)\)</span> is the posterior distribution of <span class="math inline">\(\Theta\)</span>.</p>
<ul>
<li>Replace integration by summation if the distribution of <span class="math inline">\(\Theta\)</span> is discrete.
<ul>
<li>The posterior distribution of <span class="math inline">\(\Theta\)</span> is found by Bayes rule</li>
</ul></li>
</ul>
<p><span class="math display">\[
g_{\Theta|T}(\Theta|t) \propto L(\theta|t)g_{\Theta}(\theta)
\]</span></p>
<ul>
<li>Example of case I. Consider the problem of predicting the number of successes in <span class="math inline">\(k\)</span> Bernoulli trials.
<ul>
<li>Thus, conditional on <span class="math inline">\(\Theta = \theta\)</span>, the distribution of the sum of the Bernoulli random variables is <span class="math inline">\(Y \sim Bin(k, \theta)\)</span>.</li>
<li>The probability of success, <span class="math inline">\(\theta\)</span> is not known, but suppose that the prior belief function can be represented by a beta distribution.</li>
<li>Then the Bayesian predictive distribution is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
f_{Y}(y) &amp;= \inf^1_0 \begin{pmatrix} k \\ y \end{pmatrix}\theta^y(1-\theta)^{k-y}\frac{\theta^{\alpha-1(1-\theta)^{\beta-1}}}{B(\alpha, \beta)}d\theta \\
&amp;= \begin{pmatrix} k \\ y \end{pmatrix} \frac{B(\alpha+y, \beta+k-y)}{B(\alpha, \beta)}I_{\{0,1,\cdots,k\}}(y)
\end{align}
\]</span></p>
<ul>
<li>This predictive pmf is known as the beta-binomial pmf. It has expectation</li>
</ul>
<p><span class="math display">\[
E(Y) = E_{\Theta}[E(Y|\Theta)]=E_{\Theta}(k\Theta) = k \frac{\alpha}{\alpha + \beta}
\]</span></p>
<ul>
<li>For example, suppose that the investigator has no prior knowledge and believes that <span class="math inline">\(\Theta\)</span> is equally likely to be anywhere in the <span class="math inline">\((0, 1)\)</span> interval.
<ul>
<li>Then an appropriate prior is <span class="math inline">\(Beta(1, 1)\)</span>, the uniform distribution.</li>
<li>The Bayesian predictive distribution is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_Y(y) = \begin{pmatrix} k \\ y \end{pmatrix} \frac{B(\alpha+y, \beta+k-y)}{B(1,1)}I_{0,1,\cdots,k}(y) = \frac{1}{k+1}I_{0,1,\cdots,k}(y)
\]</span></p>
<p>which is a discrete uniform distribution with support {0, 1,…, <span class="math inline">\(k\)</span>}.</p>
<ul>
<li><p>The expectation of <span class="math inline">\(Y\)</span> is <span class="math inline">\(E(Y) = k/2\)</span>.</p></li>
<li><p>Example of case II.</p>
<ul>
<li>Consider the problem of predicting the number of successes in <span class="math inline">\(k\)</span> Bernoulli trials.</li>
<li>Thus, conditional on <span class="math inline">\(\Theta = \theta\)</span>, the distribution of the sum of the Bernoulli random variables is <span class="math inline">\(Y \sim Bin(k, \theta)\)</span>.</li>
<li>A random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Bern(\theta)\)</span> has been obtained.</li>
<li>The sufficient statistic is <span class="math inline">\(T=\sum X_i\)</span> and <span class="math inline">\(T \sim Bin(n,\theta)\)</span>.</li>
<li>The probability of success, <span class="math inline">\(\theta\)</span> is not known, but suppose that the prior belief function can be represented by a beta distribution.</li>
<li>Then the posterior distribution of <span class="math inline">\(\Theta\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
g_{\Theta|T}(\theta|t) \propto \begin{pmatrix} n \\ t \end{pmatrix}\theta^t(1-\theta)^{n-t}\frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)}
\]</span></p>
<p>By recognizing the kernel, it is clear that the posterior distribution of <span class="math inline">\(\Theta\)</span> is
<span class="math inline">\(beta(t + \alpha, n − t + \beta)\)</span>.</p>
<ul>
<li>The Bayesian posterior predictive distribution is</li>
</ul>
<p><span class="math display">\[
\begin{align}
f_{Y|T}(y|t) &amp;= \int^1_0 \begin{pmatrix} k \\ y \end{pmatrix} \theta^y (1-\theta)^{k-y} \frac{\theta^{t+\alpha-1}(1-\theta)^{n-t+\beta-1}}{B(t+\alpha,n-t+\beta)}d\theta \\
&amp;= \begin{pmatrix} k \\ y \end{pmatrix}\frac{B(\alpha+t+y, \beta+n-t+k-y)}{B(t+\alpha,n-t+\beta)}I_{\{0,1,\cdots,k\}}(y)
\end{align}
\]</span></p>
<p>This is another beta-binomial pmf. It has expectation</p>
<p><span class="math display">\[
E(Y) = E_{\Theta}[E(Y|\Theta)]=E_{\Theta}(k\Theta) = k\frac{\alpha+t}{n+\alpha+\beta}
\]</span></p>
<ul>
<li>For example, suppose that the investigator has no prior knowledge and
believes that <span class="math inline">\(\Theta\)</span> is equally likely to be anywhere in the <span class="math inline">\((0, 1)\)</span> interval.
<ul>
<li>Then an appropriate prior is <span class="math inline">\(Beta(1, 1)\)</span>, the uniform distribution.</li>
<li>One Bernoulli random variable has been observed and its value is <span class="math inline">\(x = 0\)</span>.</li>
<li>That is, the data consist of just one failure; <span class="math inline">\(n = 1,t = 0\)</span>.</li>
<li>The posterior distribution of <span class="math inline">\(\Theta\)</span> is <span class="math inline">\(Beta(1, 2)\)</span> and the Bayesian posterior predictive distribution is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_{Y|T}(y|t) = \begin{pmatrix} k \\ y \end{pmatrix}\frac{B(1+y,2+k-y)}{B(1,1)}I_{0,1,\cdots,k}(y) = \frac{2(k+1+y)}{(k+1)(k+2)}I_{0,1,\cdots,k}(y)
\]</span></p>
<ul>
<li>The expectation of <span class="math inline">\(Y\)</span> is <span class="math inline">\(E(Y ) = k/3\)</span></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter7.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter9.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Mathematical Statistics.pdf", "Mathematical Statistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
