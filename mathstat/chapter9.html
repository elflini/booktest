<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Estimation | Mathematical Statistics</title>
  <meta name="description" content="This is a Mathematical Statistics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Estimation | Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Mathematical Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Estimation | Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is a Mathematical Statistics" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2024-08-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter8.html"/>
<link rel="next" href="chapter10.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#algebra-of-events"><i class="fa fa-check"></i><b>1.2</b> Algebra of Events</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#experiments-with-symmetries"><i class="fa fa-check"></i><b>1.3</b> Experiments with Symmetries</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#composition-of-experiments-counting-rules"><i class="fa fa-check"></i><b>1.4</b> Composition of Experiments: Counting Rules</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#sampling-at-random"><i class="fa fa-check"></i><b>1.5</b> Sampling at Random</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#binomial-multinomial-coefficients"><i class="fa fa-check"></i><b>1.6</b> Binomial &amp; Multinomial Coefficients</a></li>
<li class="chapter" data-level="1.7" data-path="chapter1.html"><a href="chapter1.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="1.8" data-path="chapter1.html"><a href="chapter1.html#subjective-probability"><i class="fa fa-check"></i><b>1.8</b> Subjective Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#probability-functions"><i class="fa fa-check"></i><b>2.1</b> Probability Functions</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#joint-distributions"><i class="fa fa-check"></i><b>2.2</b> Joint Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#conditional-probability"><i class="fa fa-check"></i><b>2.3</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#bayes-theorem-law-of-inverse-probability"><i class="fa fa-check"></i><b>2.4</b> Bayes Theorem (Law of Inverse Probability)</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#statistical-independence-of-random-variables"><i class="fa fa-check"></i><b>2.5</b> Statistical Independence of Random Variables</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#exchangeability"><i class="fa fa-check"></i><b>2.6</b> Exchangeability</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#application-probability-of-winning-in-craps"><i class="fa fa-check"></i><b>2.7</b> Application: Probability of Winning in Craps</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Expectations of Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#the-mean"><i class="fa fa-check"></i><b>3.1</b> The Mean</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#expectation-of-a-function"><i class="fa fa-check"></i><b>3.2</b> Expectation of a Function</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#variability"><i class="fa fa-check"></i><b>3.3</b> Variability</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#sums-of-random-variables"><i class="fa fa-check"></i><b>3.5</b> Sums of Random Variables</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#probability-generating-functions"><i class="fa fa-check"></i><b>3.6</b> Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Bernoulli and Related Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#sampling-bernoulli-populations"><i class="fa fa-check"></i><b>4.1</b> Sampling Bernoulli Populations</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#binomial-distribution"><i class="fa fa-check"></i><b>4.2</b> Binomial Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>4.3</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>4.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#negative-hypergeometric-distribution"><i class="fa fa-check"></i><b>4.6</b> Negative Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#approximating-binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Approximating Binomial Probabilities</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="chapter4.html"><a href="chapter4.html#normal-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.1</b> Normal approximation to the Binomial</a></li>
<li class="chapter" data-level="4.7.2" data-path="chapter4.html"><a href="chapter4.html#poisson-approximation-to-the-binomial"><i class="fa fa-check"></i><b>4.7.2</b> Poisson Approximation to the Binomial</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#poisson-distribution"><i class="fa fa-check"></i><b>4.8</b> Poisson Distribution</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#law-of-large-numbers"><i class="fa fa-check"></i><b>4.9</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="4.10" data-path="chapter4.html"><a href="chapter4.html#multinomial-distributions"><i class="fa fa-check"></i><b>4.10</b> Multinomial Distributions</a></li>
<li class="chapter" data-level="4.11" data-path="chapter4.html"><a href="chapter4.html#using-probability-generating-functions"><i class="fa fa-check"></i><b>4.11</b> Using Probability Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.1</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#density-and-the-probability-element"><i class="fa fa-check"></i><b>5.2</b> Density and the Probability Element</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#the-median-and-other-percentiles"><i class="fa fa-check"></i><b>5.3</b> The Median and Other Percentiles</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#expected-value"><i class="fa fa-check"></i><b>5.4</b> Expected Value</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#expected-value-of-a-function"><i class="fa fa-check"></i><b>5.5</b> Expected Value of a Function</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#average-deviations"><i class="fa fa-check"></i><b>5.6</b> Average Deviations</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#bivariate-distributions"><i class="fa fa-check"></i><b>5.7</b> Bivariate Distributions</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#several-variables"><i class="fa fa-check"></i><b>5.8</b> Several Variables</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#covariance-and-correlation-1"><i class="fa fa-check"></i><b>5.9</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#independence"><i class="fa fa-check"></i><b>5.10</b> Independence</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#conditional-distributions"><i class="fa fa-check"></i><b>5.11</b> Conditional Distributions</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#moment-generating-functions"><i class="fa fa-check"></i><b>5.12</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Families of Continuous Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#normal-distributions"><i class="fa fa-check"></i><b>6.1</b> Normal Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#exponential-distributions"><i class="fa fa-check"></i><b>6.2</b> Exponential Distributions</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#gamma-distributions"><i class="fa fa-check"></i><b>6.3</b> Gamma Distributions</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#chi-squared-distributions"><i class="fa fa-check"></i><b>6.4</b> Chi Squared Distributions</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#distributions-for-reliability"><i class="fa fa-check"></i><b>6.5</b> Distributions for Reliability</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#t-f-and-beta-distributions"><i class="fa fa-check"></i><b>6.6</b> <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span>, and Beta Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Organizing &amp; Describing Data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#frequency-distributions"><i class="fa fa-check"></i><b>7.1</b> Frequency Distributions</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#data-on-continuous-variables"><i class="fa fa-check"></i><b>7.2</b> Data on Continuous Variables</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#order-statistics"><i class="fa fa-check"></i><b>7.3</b> Order Statistics</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#data-analysis"><i class="fa fa-check"></i><b>7.4</b> Data Analysis</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#the-sample-mean"><i class="fa fa-check"></i><b>7.5</b> The Sample Mean</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#measures-of-dispersion"><i class="fa fa-check"></i><b>7.6</b> Measures of Dispersion</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#correlation"><i class="fa fa-check"></i><b>7.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Samples, Statistics, &amp; Sampling Distributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#random-sampling"><i class="fa fa-check"></i><b>8.1</b> Random Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#likelihood"><i class="fa fa-check"></i><b>8.2</b> Likelihood</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#sufficient-statistics"><i class="fa fa-check"></i><b>8.3</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#sampling-distributions"><i class="fa fa-check"></i><b>8.4</b> Sampling Distributions</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>8.5</b> Simulating Sampling Distributions</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#order-statistics-1"><i class="fa fa-check"></i><b>8.6</b> Order Statistics</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#moments-of-sample-means-and-proportionssp"><i class="fa fa-check"></i><b>8.7</b> Moments of Sample Means and Proportionssp</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>8.8</b> The Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#using-the-moment-generating-function"><i class="fa fa-check"></i><b>8.9</b> Using the Moment Generating Function</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#normal-populations"><i class="fa fa-check"></i><b>8.10</b> Normal Populations</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#updating-prior-probabilities-via-likelihood"><i class="fa fa-check"></i><b>8.11</b> Updating Prior Probabilities Via Likelihood</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#some-conjudate-families"><i class="fa fa-check"></i><b>8.12</b> Some conjudate Families</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#predictive-distributions"><i class="fa fa-check"></i><b>8.13</b> Predictive Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Estimation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#point-estimation"><i class="fa fa-check"></i><b>9.1</b> Point Estimation</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#errors-in-estimation"><i class="fa fa-check"></i><b>9.2</b> Errors in Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#consistency"><i class="fa fa-check"></i><b>9.3</b> Consistency</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#large-sample-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Large Sample Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#determining-sample-size"><i class="fa fa-check"></i><b>9.5</b> Determining Sample Size</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#small-sample-confidence-intervals-for-mu_x"><i class="fa fa-check"></i><b>9.6</b> Small Sample Confidence Intervals for <span class="math inline">\(\mu_X\)</span></a></li>
<li class="chapter" data-level="9.7" data-path="chapter9.html"><a href="chapter9.html#the-distribution-of-t"><i class="fa fa-check"></i><b>9.7</b> The Distribution of <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="9.8" data-path="chapter9.html"><a href="chapter9.html#pivotal-quantities"><i class="fa fa-check"></i><b>9.8</b> Pivotal Quantities</a></li>
<li class="chapter" data-level="9.9" data-path="chapter9.html"><a href="chapter9.html#estimating-a-mean-difference"><i class="fa fa-check"></i><b>9.9</b> Estimating a Mean Difference</a></li>
<li class="chapter" data-level="9.10" data-path="chapter9.html"><a href="chapter9.html#umvue"><i class="fa fa-check"></i><b>9.10</b> UMVUE</a></li>
<li class="chapter" data-level="9.11" data-path="chapter9.html"><a href="chapter9.html#bayes-estimators"><i class="fa fa-check"></i><b>9.11</b> Bayes Estimators</a></li>
<li class="chapter" data-level="9.12" data-path="chapter9.html"><a href="chapter9.html#efficiency"><i class="fa fa-check"></i><b>9.12</b> Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Significance Testing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chapter10.html"><a href="chapter10.html#hypotheses"><i class="fa fa-check"></i><b>10.1</b> Hypotheses</a></li>
<li class="chapter" data-level="10.2" data-path="chapter10.html"><a href="chapter10.html#assessing-the-evidence"><i class="fa fa-check"></i><b>10.2</b> Assessing the Evidence</a></li>
<li class="chapter" data-level="10.3" data-path="chapter10.html"><a href="chapter10.html#one-sample-z-tests"><i class="fa fa-check"></i><b>10.3</b> One Sample <span class="math inline">\(Z\)</span> Tests</a></li>
<li class="chapter" data-level="10.4" data-path="chapter10.html"><a href="chapter10.html#one-sample-t-tests"><i class="fa fa-check"></i><b>10.4</b> One Sample <span class="math inline">\(t\)</span> Tests</a></li>
<li class="chapter" data-level="10.5" data-path="chapter10.html"><a href="chapter10.html#some-nonparametric-tests"><i class="fa fa-check"></i><b>10.5</b> Some Nonparametric Tests</a></li>
<li class="chapter" data-level="10.6" data-path="chapter10.html"><a href="chapter10.html#probability-of-the-null-hypothesis"><i class="fa fa-check"></i><b>10.6</b> Probability of the Null Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Tests as Decision Rules</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#rejection-regions-and-errors"><i class="fa fa-check"></i><b>11.1</b> Rejection Regions and Errors</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#the-power-function"><i class="fa fa-check"></i><b>11.2</b> The Power function</a></li>
<li class="chapter" data-level="11.3" data-path="chapter11.html"><a href="chapter11.html#choosing-a-sample-size"><i class="fa fa-check"></i><b>11.3</b> Choosing a Sample Size</a></li>
<li class="chapter" data-level="11.4" data-path="chapter11.html"><a href="chapter11.html#most-powerful-tests"><i class="fa fa-check"></i><b>11.4</b> Most Powerful Tests</a></li>
<li class="chapter" data-level="11.5" data-path="chapter11.html"><a href="chapter11.html#uniformly-most-powerful-tests"><i class="fa fa-check"></i><b>11.5</b> Uniformly Most Powerful Tests</a></li>
<li class="chapter" data-level="11.6" data-path="chapter11.html"><a href="chapter11.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>11.6</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="11.7" data-path="chapter11.html"><a href="chapter11.html#bayesian-testing"><i class="fa fa-check"></i><b>11.7</b> Bayesian Testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chapter12.html"><a href="chapter12.html"><i class="fa fa-check"></i><b>12</b> Appendix</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chapter12.html"><a href="chapter12.html#greek-alphabet"><i class="fa fa-check"></i><b>12.1</b> Greek Alphabet</a></li>
<li class="chapter" data-level="12.2" data-path="chapter12.html"><a href="chapter12.html#abbreviations"><i class="fa fa-check"></i><b>12.2</b> Abbreviations</a></li>
<li class="chapter" data-level="12.3" data-path="chapter12.html"><a href="chapter12.html#practice-exams"><i class="fa fa-check"></i><b>12.3</b> PRACTICE EXAMS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter9" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Estimation<a href="chapter9.html#chapter9" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="point-estimation" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Point Estimation<a href="chapter9.html#point-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="method-of-moments" class="section level4 unnumbered hasAnchor">
<h4>Method of Moments<a href="chapter9.html#method-of-moments" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Setting: Suppose that <span class="math inline">\(X_i, X_2, . . . , X_n\)</span> is a random sample from <span class="math inline">\(f_X(x|\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is a <span class="math inline">\(k \times 1\)</span> vector of parameters.</p></li>
<li><p>The goal is to derive an estimator of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Let <span class="math inline">\(M_j&#39;\)</span> be the <span class="math inline">\(j^{th}\)</span> sample moment about the origin and let <span class="math inline">\(\mu_j&#39;\)</span> be the
corresponding population moment. That is</p></li>
</ul>
<p><span class="math display">\[
M_j&#39; = \frac{1}{n}\sum^{n}_{i=1}X^{j}_i \,\,\, \text{and} \,\,\, \mu_j&#39; = \mu_j&#39;(\theta)=E(X^j)
\]</span></p>
<p>for <span class="math inline">\(j = 1, 2, \ldots\)</span>.</p>
<ul>
<li><p>The population moments are denoted by <span class="math inline">\(\mu_j&#39;(\theta)\)</span> because
they are functions of the components of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>The method of moments estimator consists of equating sample moments
to population moments and solving for <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>That is, solve</li>
</ul></li>
</ul>
<p><span class="math display">\[
M_j&#39; = \mu_j&#39;(\hat \theta),\,\,\, j=1,...,k, \,\,\, for \,\,\, \hat \theta
\]</span></p>
<ul>
<li>Central Moments: It sometimes is more convenient to use central
moments.
<ul>
<li>For the special case of <span class="math inline">\(k = 2\)</span>, solve</li>
</ul></li>
</ul>
<p><span class="math display">\[
M_j = \mu_j(\hat \theta), \,\,\, j=1,2
\]</span></p>
<p>where</p>
<p><span class="math display">\[
M_1 = M_1&#39;= \frac{1}{n}\sum^n_{i=1}X_i
\]</span></p>
<p><span class="math display">\[
M_2 = S^2_X = \frac{1}{n-1}\sum^n_{i=1}(X_i-\bar X)^2
\]</span></p>
<p><span class="math display">\[
\mu_1 = \mu_1&#39; =E(X)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mu_2=\sigma^2_X =E(X-\mu_1)^2
\]</span></p>
<ul>
<li>Example: Gamma Distribution.
<ul>
<li>Suppose <span class="math inline">\(X_1, . . . , X_n\)</span> is a random sample from <span class="math inline">\(\Gamma(\alpha, \lambda)\)</span>.</li>
<li>The moments about the origin are</li>
</ul></li>
</ul>
<p><span class="math display">\[
\mu_j&#39;(\theta)=E(X^j)=\frac{\Gamma(\alpha+j)}{\Gamma(\alpha)\lambda^j}
\]</span></p>
<ul>
<li>The first two central moments are</li>
</ul>
<p><span class="math display">\[
\mu_1(\theta) = E(X) = \frac{\alpha}{\lambda} \,\,\, \text{and} \,\,\, \mu_2(\theta)=Var(X)=\frac{\alpha}{\lambda^2}
\]</span></p>
<ul>
<li>The method of moments estimators of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> are obtained by solving</li>
</ul>
<p><span class="math display">\[
\bar X = \frac{\hat \alpha}{\hat \lambda} \,\,\, \text{and} \,\,\, S^2_X = \frac{\hat \alpha}{\hat {\lambda^2}}
\]</span></p>
<p>for <span class="math inline">\(\hat \alpha\)</span> and <span class="math inline">\(\hat \lambda\)</span>.</p>
<ul>
<li>The solutions are</li>
</ul>
<p><span class="math display">\[
\hat \alpha = \frac{\bar X^2}{S^2_X}
\,\,\, \text{and} \,\,\,
\hat \lambda = \frac{\bar X}{S^2_X}
\]</span></p>
<ul>
<li>Example: Beta Distribution. Suppose <span class="math inline">\(X_1, . . . , X_n\)</span> is a random sample from <span class="math inline">\(Beta(\alpha_1, \alpha_2)\)</span>.
<ul>
<li>The moments about the origin are</li>
</ul></li>
</ul>
<p><span class="math display">\[
\mu_j&#39; = E(X^j) = \frac{B(\alpha_1+j,\alpha_2)}{B(\alpha_1,\alpha_2)}=\frac{\Gamma(\alpha_1+j,\alpha_2)\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_1+\alpha_2+j)}
\]</span></p>
<ul>
<li>The first two central moments are</li>
</ul>
<p><span class="math display">\[
\mu_1(\theta) = E(X)=\frac{\alpha_1}{\alpha_1+\alpha_2} \,\,\, \text{and} \,\,\, \frac{\alpha_1\alpha_2}{(\alpha_1+\alpha_2)^2(\alpha_1+\alpha_2+1)}
\]</span></p>
<ul>
<li>The method of moments estimators of <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span> are obtained by solving</li>
</ul>
<p><span class="math display">\[
\bar X = \frac{\hat{\alpha_1}}{\hat{\alpha_1}+\hat{\alpha_2}} \,\,\, \text{and} \,\,\, S^2_X = \frac{\hat{\alpha_1}\hat{\alpha_2}}{(\hat{\alpha_1}+\hat{\alpha_2})^2(\hat{\alpha_1}+\hat{\alpha_2}+1)}
\]</span></p>
<p>for <span class="math inline">\(\hat{\alpha_1}\)</span> and <span class="math inline">\(\hat{\alpha_2}\)</span>.</p>
<ul>
<li>The solutions are</li>
</ul>
<p><span class="math display">\[
\hat{\alpha_1}=\bar X \bigg[\frac{\bar X(1-\bar X)}{S^2_X}-1\bigg] \,\,\, \text{and} \,\,\, \hat{\alpha_2} = (1-\bar X)\bigg[\frac{\bar X(1-\bar X)}{S^2_X}-1\bigg]
\]</span></p>
</div>
<div id="maximum-likelihood-estimators-mles" class="section level4 unnumbered hasAnchor">
<h4>Maximum Likelihood Estimators (MLEs)<a href="chapter9.html#maximum-likelihood-estimators-mles" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Setting: Suppose that <span class="math inline">\(X_i, X_2, . . . , X_n\)</span> is a random sample from <span class="math inline">\(f_X(x|\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is a <span class="math inline">\(k \times 1\)</span> vector of parameters.</p></li>
<li><p>The goal is to derive an estimator of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Definition: A maximum likelihood estimator (MLE) of <span class="math inline">\(\theta\)</span> is any value <span class="math inline">\(\hat θ\)</span> that maximizes the likelihood function and is a point in the parameter space or on the boundary of the parameter space.</p></li>
<li><p>If the likelihood function is a differentiable function of <span class="math inline">\(\theta\)</span>, then the maximum likelihood estimator is a solution to</p></li>
</ul>
<p><span class="math display">\[
\frac{\partial L(\theta|X)}{\partial \theta}\Bigg|_{\theta=\hat{\theta}}=0
\]</span></p>
<ul>
<li>Note, any maximizer of <span class="math inline">\(L(\theta|X)\)</span> also is a maximizer of <span class="math inline">\(\ln[L(\theta|X)]\)</span>.
<ul>
<li>Accordingly, one can maximize the log likelihood function rather than
the likelihood function.</li>
</ul></li>
<li>Example: Suppose that <span class="math inline">\(X_1, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(Exp(\lambda)\)</span>.
<ul>
<li>The log likelihood function is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\ln[L(\lambda|X)]=n\ln(\lambda)-\lambda\sum^n_{i=1}X_i
\]</span></p>
<ul>
<li>Taking the derivative with respect to <span class="math inline">\(\lambda\)</span>; setting it to zero; and solving for <span class="math inline">\(\hat{\lambda}\)</span> yields</li>
</ul>
<p><span class="math display">\[
\hat \lambda = \frac{1}{\bar X}
\]</span></p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X_1, \ldots, X_n\)</span> is a random sample from <span class="math inline">\(Unif(\theta)\)</span>.
<ul>
<li>The likelihood function is</li>
</ul></li>
</ul>
<p><span class="math display">\[
L(\theta|X_{(n)})=\frac{1}{\theta^n}I_{(X_{(n)},\infty)}(\theta)
\]</span></p>
<ul>
<li><p>Plotting the likelihood function reveals that <span class="math inline">\(\hat\theta = X_{(n)}\)</span> is the MLE.</p></li>
<li><p>Note, taking derivatives does not work in this case.</p></li>
<li><p>Example: Suppose that <span class="math inline">\(X_1, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<ul>
<li>The log likelihood function is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\ln[L(\mu,\sigma^2|\bar X,S^2_X)] = -\frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\sum^n_{i=1}(X_i-\bar X)^2 -\frac{n}{2\sigma^2}(\bar X - \mu)^2
\]</span></p>
<ul>
<li>Taking the derivatives with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> and setting them to zero yields two equations to solve:</li>
</ul>
<p><span class="math display">\[
\frac{n}{\hat {\sigma^2}}(\bar X - \hat \mu)=0
\,\,\, and \,\,\, -\frac{n}{2\hat{\sigma^2}}+\frac{1}{2\hat{\sigma^4}}\sum^n_{i=1}(X_i-\bar X)^2 + \frac{n}{2\hat {\sigma^4}}(\bar X-\hat \mu)^2 =0\]</span></p>
<ul>
<li><p>Solving the first equation for <span class="math inline">\(\hat \mu\)</span> yields <span class="math inline">\(\hat \mu = \bar X\)</span>.</p></li>
<li><p>Substituting <span class="math inline">\(\hat \mu = \bar X\)</span> into the second equation and solving for <span class="math inline">\(\hat {\sigma^2}\)</span> yields.</p></li>
</ul>
<p><span class="math display">\[
\hat {\sigma^2} = \frac{1}{n}\sum^n_{i=1}(X_i -\bar X)^2
\]</span></p>
</div>
<div id="invariance-property-of-mles" class="section level4 unnumbered hasAnchor">
<h4>Invariance property of MLEs<a href="chapter9.html#invariance-property-of-mles" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Let <span class="math inline">\(g(\theta)\)</span> be a function of <span class="math inline">\(\theta\)</span>.
<ul>
<li>Then, the MLE of <span class="math inline">\(g(\theta)\)</span> is <span class="math inline">\(g(\hat \theta)\)</span>, where <span class="math inline">\(\hat \theta\)</span> is the MLE of <span class="math inline">\(\theta\)</span>.</li>
</ul></li>
<li>Proof: We will prove the invariance property only for the special case in which <span class="math inline">\(g(\theta)\)</span> is a one-to-one function.
<ul>
<li>Note, if the dimension of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(k\)</span>, then the dimension of <span class="math inline">\(g(\theta)\)</span> also must be <span class="math inline">\(k\)</span>.</li>
<li>Let <span class="math inline">\(\eta = g(\theta)\)</span>.</li>
<li>Then <span class="math inline">\(\theta = g^{-1}(\eta)\)</span> because <span class="math inline">\(g\)</span> is one-to-one.</li>
<li>Define <span class="math inline">\(L∗(\eta|X)\)</span> as the likelihood function when <span class="math inline">\(g(\theta) = \eta\)</span>.</li>
<li>That is</li>
</ul></li>
</ul>
<p><span class="math display">\[
L*(\eta|X) = f_X[X|g^{-1}(\eta)]=L[g^{-1}(\eta)|X]
\]</span></p>
<p>where <span class="math inline">\(g^{-1}(\eta) =\theta\)</span>.</p>
<ul>
<li>Note That,</li>
</ul>
<p><span class="math display">\[
\max_\eta L*(\eta|X)=\max_\eta L[g^{-1}(\eta)|X]=\max_\theta L[\theta|X]
\]</span></p>
<ul>
<li><p>That is, the maximized likelihood is the same whether one maximizes <span class="math inline">\(L∗\)</span> with respect to <span class="math inline">\(\eta\)</span> or maximizes <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Accordingly, if <span class="math inline">\(\hat \theta\)</span> maximizes the likelihood function <span class="math inline">\(L(\theta|X)\)</span>, then <span class="math inline">\(\eta b = g(\hat \theta)\)</span> maximizes the
likelihood function <span class="math inline">\(L∗(\eta|X)\)</span>.</p></li>
<li><p>Example Suppose that <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p>
<ul>
<li>Find the MLE of the coefficient of variation <span class="math inline">\(g(\mu, \sigma^2)= 100\sigma/\mu\)</span>. Solution:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\hat g = 100\frac{\hat \sigma}{\bar X}
\,\,\, \text{where} \,\,\,
\hat {\sigma^2} = \frac{1}{n}\sum^n_{i=1}(X_i-\bar X)^2
\]</span></p>
<ul>
<li>Properties of MLEs: Under certain regularity conditions, it can be shown that</li>
</ul>
<ol style="list-style-type: decimal">
<li>MLEs are consistent,</li>
<li>MLEs have asymptotic normal distributions, and</li>
<li>MLEs are functions of sufficient statistics.</li>
</ol>
<ul>
<li>This property is easy to prove because the likelihood function depends on the data solely through a sufficient statistic.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="errors-in-estimation" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Errors in Estimation<a href="chapter9.html#errors-in-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Estimator versus Estimate: An estimator of a population parameter, say <span class="math inline">\(\theta\)</span>, is a function of the data and is a random variable. An estimate is a realization of the random variable.</li>
</ul>
<div id="variance-and-bias" class="section level4 unnumbered hasAnchor">
<h4>Variance and Bias<a href="chapter9.html#variance-and-bias" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Let <span class="math inline">\(T = T(X)\)</span> be an estimator of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>The bias of <span class="math inline">\(T\)</span> is</p></li>
</ul>
<p><span class="math display">\[
bias(T) = E(T − θ) = E(T) − θ
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(b_T = 0\)</span>, then <span class="math inline">\(T\)</span> is unbiased for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>The variance of <span class="math inline">\(T\)</span> is</p></li>
</ul>
<p><span class="math display">\[
\sigma^2_T = E(T − \mu_T )^2, \,\,\, where \,\,\, \mu_T = E(T)
\]</span></p>
</div>
<div id="mean-square-error-mse" class="section level4 unnumbered hasAnchor">
<h4>Mean Square Error (MSE)<a href="chapter9.html#mean-square-error-mse" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The mean square error of <span class="math inline">\(T\)</span> is</li>
</ul>
<p><span class="math display">\[
MSE_T(\theta)=E(T-\theta)^2=E[(T-\theta)^2]
\]</span></p>
<ul>
<li>Result</li>
</ul>
<p><span class="math display">\[
MES_T(\theta) = Var(T) +bias^2_T
\]</span></p>
<ul>
<li>proof:</li>
</ul>
<p><span class="math display">\[
\begin{align}
MSE_T(\theta) &amp;= E[(T-\mu_T)+(\mu_T - \theta)]^2 \\
&amp;= E(T-\mu_T)^2+2(\mu_T - \theta)E(T-\mu_T)+E(\mu_T-\theta)^2 \\
&amp;= Var(T) + bias^2_T
\end{align}
\]</span></p>
<ul>
<li>Root Mean Square Error (RMSE):</li>
</ul>
<p><span class="math display">\[
RMSE_T(\theta) = \sqrt{MSE_T(\theta)}
\]</span></p>
<ul>
<li>Example: Sample Variance.
<ul>
<li>Let <span class="math inline">\(X_1, \ldots , X_n\)</span> be a random sample from <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</li>
<li>Compare two estimators of <span class="math inline">\(\sigma^2\)</span>:</li>
</ul></li>
</ul>
<p><span class="math display">\[
S^2 = \frac{1}{n-1}\sum^n_{i=1}(X_i-\bar X)^2 \,\,\, \text{and} \,\,\, V= \frac{1}{n}\sum^n_{i=1}(X_i-\bar X)
\]</span></p>
<ul>
<li>We know that <span class="math inline">\(S^2\)</span> is unbiased for <span class="math inline">\(\sigma^2\)</span> (this result does not depend on normality). Therefore</li>
</ul>
<p><span class="math display">\[
bias_{S^2}=0 \,\,\, \text{and} \,\,\, bias_V = E(\frac{n-1}{n})S^2-\sigma^2= \frac{-\sigma^2}{n}
\]</span></p>
<ul>
<li>Recall that <span class="math inline">\((n-1)S^2/\sigma^2 \sim \chi^2_{n-1}\)</span></li>
</ul>
<p><span class="math display">\[Var \bigg[ \sum^n_{i=1}(X_i-\bar X)^2\bigg] = Var(\sigma^2\chi^2_{n-1})=\sigma^42(n-1)\]</span></p>
<p>because <span class="math inline">\(Var(\chi^2_{n-1})=2(n-1)\)</span>.</p>
<ul>
<li>The MSEs of <span class="math inline">\(S^2\)</span> and <span class="math inline">\(V\)</span> are</li>
</ul>
<p><span class="math display">\[
MSE_{S^2}(\sigma^2)=Var(S^2)=\frac{2(n-1)\sigma^4}{(n-1)^2}=\frac{2\sigma^4}{n-1}
\]</span></p>
<p><span class="math display">\[
MSE_V(\sigma^2)=Var(V)=\frac{2(n-1)\sigma^4}{n^2}+\frac{\sigma^4}{n^2}=\frac{(2n-1)\sigma^4}{n^2}=\frac{2\sigma^4}{n-1}(1-\frac{3n-1}{2n^2})
\]</span></p>
<ul>
<li><p>Note that <span class="math inline">\(MSE_{S^2}(\sigma^2)&gt;MSE_V(\sigma^2)\)</span> even though <span class="math inline">\(S^2\)</span> is unbiased and <span class="math inline">\(V\)</span> is biased.</p></li>
<li><p>Standard Error: An estimator (or estimate) of the standard deviation of an estimator is called the standard error of the estimator.</p></li>
<li><p>Example of standard errors: In the following table, <span class="math inline">\(f = \frac{n-1}{N-1}\)</span></p></li>
</ul>
<table>
<colgroup>
<col width="16%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Parent Distribution</th>
<th align="center">Parameter</th>
<th align="center">Estimator(T)</th>
<th align="center">Var(T)</th>
<th align="center">SE(T)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Any, <br> Smapling w <br> replacement</td>
<td align="center"><span class="math inline">\(\mu\)</span></td>
<td align="center"><span class="math inline">\(\bar X\)</span></td>
<td align="center"><span class="math inline">\(\frac{\sigma^2}{n}\)</span></td>
<td align="center"><span class="math inline">\(\frac{S}{\sqrt{n}}\)</span></td>
</tr>
<tr class="even">
<td align="left">Bernoulli, <br> Sampling w <br> /replacement</td>
<td align="center"><span class="math inline">\(p\)</span></td>
<td align="center"><span class="math inline">\(\hat p\)</span></td>
<td align="center"><span class="math inline">\(\frac{p(1-p)}{n}\)</span></td>
<td align="center"><span class="math inline">\(\sqrt{\frac{\hat p(1-\hat p)}{n-1}}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Any finite pop., <br> Sampling w/o<br> replacement</td>
<td align="center"><span class="math inline">\(\mu\)</span></td>
<td align="center"><span class="math inline">\(\bar X\)</span></td>
<td align="center"><span class="math inline">\(\frac{\sigma^2}{n}(1-f)\)</span></td>
<td align="center"><span class="math inline">\(\sqrt{\frac{S^2}{n}(1-f)}\)</span></td>
</tr>
<tr class="even">
<td align="left">Finite Bern., <br> Sampling w/o <br> replacement</td>
<td align="center"><span class="math inline">\(p\)</span></td>
<td align="center"><span class="math inline">\(\hat p\)</span></td>
<td align="center"><span class="math inline">\(\frac{p(1-p)}{n}(1-f)\)</span></td>
<td align="center"><span class="math inline">\(\sqrt{\frac{\hat p(1-\hat p)}{n-1}\bigg(1-\frac{n}{N}\bigg)}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Normal</td>
<td align="center"><span class="math inline">\(\sigma^2\)</span></td>
<td align="center"><span class="math inline">\(S^2\)</span></td>
<td align="center"><span class="math inline">\(\frac{2\sigma^4}{n-1}\)</span></td>
<td align="center"><span class="math inline">\(S^2\sqrt{\frac{2}{n-1}}\)</span></td>
</tr>
</tbody>
</table>
<p><br></p>
</div>
</div>
<div id="consistency" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Consistency<a href="chapter9.html#consistency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Chebyshev’s Inequality: Suppose that <span class="math inline">\(X\)</span> is a random variable with pdf or pmf <span class="math inline">\(f_X(x)\)</span>.</li>
<li>Let <span class="math inline">\(h(X)\)</span> be a non-negative function of <span class="math inline">\(X\)</span> whose expectation exists
and let <span class="math inline">\(k\)</span> be any positive constant.</li>
<li>Then</li>
</ul>
<p><span class="math display">\[
P[h(x)\ge k] \le \frac{E[h(X)]}{k}
\]</span></p>
<ul>
<li>Proof: Suppose that <span class="math inline">\(X\)</span> is a continuous rv. Let <span class="math inline">\(R\)</span> be the set <span class="math inline">\(R=\{x;x \in S_X; \ h(x) \ge k\}\)</span>. Then</li>
</ul>
<p><span class="math display">\[
\begin{align}
E[h(X)]&amp;=\int_{S_X}h(x)f_X(x)dx \ge \int_R h(x)f_X(x)dx \\
&amp;\ge \int_R f_X(x)dx = kP[h(X) \ge k ] \\
&amp;\Rightarrow \frac{E[h(X)]}{k} \ge P[h(X) \ge k]
\end{align}
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(X\)</span> is discrete, then replace integration by summation.</p></li>
<li><p>Application 1: Suppose that <span class="math inline">\(E(X) = \mu_X\)</span> and <span class="math inline">\(Var(X) = \sigma^2_X \lt \infty\)</span>. Then</p></li>
</ul>
<p><span class="math display">\[
P\bigg[ \frac{|X-\mu_X|^2}{\sigma^2_X} \ge k^2\bigg] \le \frac{1}{k^2}
\]</span></p>
<ul>
<li>Proof: Choose <span class="math inline">\(h(X)\)</span> to be</li>
</ul>
<p><span class="math display">\[
h(X) = \frac{(X-\mu_X)^2}{\sigma^2_X}
\]</span></p>
<ul>
<li>By the definition of <span class="math inline">\(Var(X)\)</span>, it follows that <span class="math inline">\(E[h(X)] = 1\)</span>. Also,</li>
</ul>
<p><span class="math display">\[
\begin{align}
P\bigg[ \frac{|X-\mu_X|}{\sigma_X} \ge k\bigg]&amp;=P[|X-\mu_X| \ge k\sigma_X ] \\
&amp;= P\bigg[ \frac{|X-\mu_X|^2}{\sigma^2_X} \ge k^2\bigg] \le \frac{1}{k^2}
\end{align}
\]</span></p>
<p>by Chebyshev</p>
<p><span class="math display">\[
\Rightarrow P[|X-\mu_X| \ge k\sigma_X ] \ge 1-\frac{1}{k^2}
\]</span></p>
<ul>
<li>Application 2: Suppose that <span class="math inline">\(T\)</span> is a random variable (estimator of the
unknown parameter <span class="math inline">\(\theta\)</span>) with <span class="math inline">\(E(T) = \mu_T\)</span> and <span class="math inline">\(Var(T) = σ^2_T \lt \infty\)</span>. Then</li>
</ul>
<p><span class="math display">\[
P[|X-\theta| &lt; \epsilon] \ge 1 - \frac{MSE_X(\theta)}{\epsilon^2}
\]</span></p>
<ul>
<li>Proof: Choose <span class="math inline">\(h(X)\)</span> to be</li>
</ul>
<p><span class="math display">\[
h(X) = (X-\theta)^2
\]</span></p>
<ul>
<li>Then <span class="math inline">\(E[h(T)] = MSE_T(\theta)\)</span> and</li>
</ul>
<p><span class="math display">\[
\begin{align}
P[|T-\theta| \ge \epsilon] &amp;= P[|T-\theta|^2 \ge \epsilon^2] \\
&amp;\le \frac{MSE_T(\theta)}{\epsilon^2} \\
&amp;=\frac{\sigma^2_T+[E(T)-\theta]^2}{\epsilon^2} \\
&amp;\Rightarrow P[|T-\theta| &lt; \epsilon] \ge 1- \frac{MSE_T(\theta)}{\epsilon^2}
\end{align}
\]</span></p>
<ul>
<li>Consistency Definition: A sequence of estimators, <span class="math inline">\(\{T_n\}\)</span>, is consistent for <span class="math inline">\(\theta\)</span> if</li>
</ul>
<p><span class="math display">\[
\lim_{n \rightarrow \infty}P[|T_n-\theta|&lt;\epsilon]=1
\]</span></p>
<p>for every <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
<ul>
<li><p>Converge in Probability Definition: A sequence of estimators, <span class="math inline">\(\{T_n\}\)</span>, converges in probability to <span class="math inline">\(\theta\)</span> if the sequence is consistent for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Convergence in probability is usually written as</p></li>
</ul>
<p><span class="math display">\[
T_n \overset{prob}\rightarrow \theta
\]</span></p>
<ul>
<li>Law of Large Numbers: If <span class="math inline">\(\bar X\)</span> is the sample mean based on a random sample of size <span class="math inline">\(n\)</span> from a population having mean <span class="math inline">\(\mu_X\)</span>, then</li>
</ul>
<p><span class="math display">\[
\bar X \overset{prob}\rightarrow \mu_X
\]</span></p>
<ul>
<li>We will prove the law of large numbers for the special case in which the population variance is finite.
<ul>
<li>The more general result when the population variance is infinite is sometimes called Khintchine’s Theorem.</li>
</ul></li>
<li>Mean Square Consistent Definition: An estimator of <span class="math inline">\(\theta\)</span> is mean square consistent if</li>
</ul>
<p><span class="math display">\[
\lim_{n \rightarrow \infty}MSE_T(\theta) = 0
\]</span></p>
<ul>
<li>Result: If an estimator is mean square consistent, then it is consistent.
<ul>
<li>Proof: Let Tn be an estimator of <span class="math inline">\(\theta\)</span>.</li>
<li>Assume that Tn has finite mean and variance.</li>
<li>Then it follows from Chebyshev’s Theorem that</li>
</ul></li>
</ul>
<p><span class="math display">\[
P[|T_n-\theta|&lt;\epsilon] \ge 1 - \frac{MSE_{T_n}(\theta)}{\epsilon^2}
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is any positive constant.</p>
<ul>
<li>In <span class="math inline">\(T_n\)</span>, is mean square consistent for <span class="math inline">\(\theta\)</span>, then</li>
</ul>
<p><span class="math display">\[
\lim_{n \rightarrow \infty}P[|T_n-\theta|&lt;\epsilon] \ge \lim_{n \rightarrow \infty}1-\frac{MSE_{T_n}(\theta)}{\epsilon^2}=1
\]</span></p>
<p>because</p>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} \frac{MSE_{T_n}(\theta)}{\epsilon^2}=0
\]</span></p>
<p>for any <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
<ul>
<li>Application: The sample mean based on a random sample of size <span class="math inline">\(n\)</span> from a population with finite mean and variance has mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma^2_X/n\)</span>.
<ul>
<li>Accordingly,</li>
</ul></li>
</ul>
<p><span class="math display">\[
MSE_{\bar X}(\mu_X)=\frac{\sigma^2_X}{n} \,\,\, \text{and} \,\,\, \lim_{n \rightarrow \infty}MSE_{\bar X}(\mu_X)=0
\]</span></p>
<p>which reveals that <span class="math inline">\(\bar X\)</span> is mean square consistent.</p>
<ul>
<li>It follows from the result that <span class="math inline">\(\bar X \overset{prob}\rightarrow \mu_X\)</span>.</li>
</ul>
<p><br></p>
</div>
<div id="large-sample-confidence-intervals" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Large Sample Confidence Intervals<a href="chapter9.html#large-sample-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>General setting: Suppose that <span class="math inline">\(T_n\)</span> is an estimator of <span class="math inline">\(\theta\)</span> and that</li>
</ul>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} P[\frac{T_n-\theta}{\sigma_{T_n}} \le c] = \Phi(c)
\]</span></p>
<ul>
<li><p>That is, <span class="math inline">\(T_n \sim N(\theta,\sigma^2_{T_n})\)</span> provided that sample size is sufficiently large.</p></li>
<li><p>Suppose that <span class="math inline">\(\sigma^2_{T_n} = w^2/n\)</span> and that <span class="math inline">\(W^2_n\)</span> is a consistent estimator of <span class="math inline">\(\omega^2\)</span>.</p>
<ul>
<li>That is, <span class="math inline">\(S_{T_n}=SE(T_n) = W_n/\sqrt{n}\)</span> and <span class="math inline">\(W^2_n \overset{prob}\rightarrow \omega^2\)</span>. Then, it can be shown that</li>
</ul></li>
</ul>
<p><span class="math display">\[
\lim_{n \rightarrow \infty}P \bigg[ \frac{T_n - \theta}{S_{T_n}} \le c \bigg] = \Phi(c)
\]</span></p>
<ul>
<li>Constructing a confidence interval: Denote the <span class="math inline">\(100(1 − \alpha /2)\)</span> percentile of the standard normal distribution by <span class="math inline">\(z_{\alpha/2}\)</span>.</li>
</ul>
<p><span class="math display">\[
\Phi^{-1}(1-\alpha/2)=z_{\alpha/2}
\]</span></p>
<ul>
<li>Then, using the large sample distribution of <span class="math inline">\(T_n\)</span>, it follows that</li>
</ul>
<p><span class="math display">\[
P \bigg[-z_{\alpha/2} \le \frac{T_n-\theta}{S_{T_n}} \le z_{\alpha/2} \bigg] \approx 1-\alpha
\]</span></p>
<ul>
<li>Using simple algebra to manipulate the three sides of the above equation yields</li>
</ul>
<p><span class="math display">\[
P[T_n - z_{\alpha/2}S_{T_n} \le \theta \le T_n+z_{\alpha/2}S_{T_n}] \approx 1-\alpha
\]</span></p>
<ul>
<li>The above random interval is a large sample <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence interval for <span class="math inline">\(\theta\)</span>.
<ul>
<li>The interval is random because <span class="math inline">\(T_n\)</span> and <span class="math inline">\(S_{T_n}\)</span> are random variables.</li>
</ul></li>
<li>Interpretation of the interval: Let <span class="math inline">\(t_n\)</span> and <span class="math inline">\(s_{T_n}\)</span> be realizations of <span class="math inline">\(T_n\)</span> and <span class="math inline">\(S_{T_n}\)</span>. Then</li>
</ul>
<p><span class="math display">\[
(t_n - z_{\alpha/2}s_{t_n}, t_n+z_{\alpha/2}s_{t_n})
\]</span></p>
<p>is a realization of the random interval.</p>
<ul>
<li><p>We say that we are <span class="math inline">\(100(1 − \alpha)\%\)</span> confident that the realization captures the parameter <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>The <span class="math inline">\(1 − \alpha\)</span> probability statement applies to the interval estimator, but not to the interval estimate (i.e., a realization of the interval).</p></li>
<li><p>Example 1: Confidence interval for <span class="math inline">\(\mu_X\)</span>. Let <span class="math inline">\(X_1, \ldots , X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from a population having mean <span class="math inline">\(\mu_X\)</span> a <span class="math inline">\(\sigma^2_X\)</span></p></li>
<li><p>If sample size is large, then <span class="math inline">\(\bar X \sim N(\mu_X, \sigma^2_X/n)\)</span> by the CLT.</p></li>
<li><p>The estimated variance of <span class="math inline">\(\bar X\)</span> is <span class="math inline">\(S^2_{\bar X} = S^2_X /n\)</span>. It can be shown that</p></li>
</ul>
<p><span class="math display">\[
Var(S^2_X) = \frac{2\sigma^4_X}{n-1}\bigg[1+\frac{(n-1)\kappa_4}{2n}\bigg]
\]</span></p>
<p>where <span class="math inline">\(\kappa_4\)</span> is the standardized kurtosis of the parent distribution. Recall, that if <span class="math inline">\(X\)</span> is normal, then <span class="math inline">\(\kappa_4 = 0\)</span>.</p>
<ul>
<li><p>If <span class="math inline">\(\kappa_4\)</span> is finite, then Chebyshev’s inequality reveals
that <span class="math inline">\(S^2_X \overset{prob}\rightarrow \sigma^2\)</span>.</p></li>
<li><p>It follows that</p></li>
</ul>
<p><span class="math display">\[
S_X\overset{prob}\rightarrow \sigma
\]</span></p>
<ul>
<li>Accordingly</li>
</ul>
<p><span class="math display">\[
\lim_{n \rightarrow \infty}P \bigg[\frac{\bar X - \mu_X}{S_X/\sqrt{n}} \le c\bigg]=\Phi(c)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
P[\bar X - z_{\alpha/2}\frac{S_X}{\sqrt n} \le \mu_X \le \bar X + z_{\alpha/2}\frac{S_X}{\sqrt n}] \approx 1- \alpha
\]</span></p>
<ul>
<li>Example 2: Confidence interval for a population proportion, <span class="math inline">\(p\)</span>.
<ul>
<li>Let <span class="math inline">\(X_1,...,X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Bern(p)\)</span>.</li>
<li>If sample size is large, then <span class="math inline">\(\hat p \sim N(p,p(1-p)/n)\)</span> by the CLT.</li>
<li>The usual estimator of <span class="math inline">\(\sigma^2_X\)</span> is <span class="math inline">\(V_X = \hat p(1-\hat p)\)</span>.</li>
<li>We know that <span class="math inline">\(\hat p \overset{prob}\rightarrow p\)</span> (by the law of large numbers).</li>
<li>It follows that <span class="math inline">\(\hat p(1-\hat p) \overset{prob}\rightarrow p\)</span> and, therefore, <span class="math inline">\(V_X = \hat p(1-\hat p)\)</span> is consistent for <span class="math inline">\(\sigma^2_X\)</span>.</li>
<li>Accordingly</li>
</ul></li>
</ul>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} P \bigg[ \frac{\hat p - p}{\sqrt{\hat p(1-\hat p)/n}}
\le c \bigg] = \Phi(c)
\]</span></p>
<p>and</p>
<p><span class="math display">\[P\Bigg[ \hat p - z_{\alpha/2}\sqrt{\frac{\hat p(1-\hat p)}{n}} \le p \le p + z_{\alpha/2}\sqrt{\frac{\hat p(1-\hat p)}{n}}\Bigg] \approx 1-\alpha
\]</span></p>
<p><br></p>
</div>
<div id="determining-sample-size" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Determining Sample Size<a href="chapter9.html#determining-sample-size" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Margin of Error Suppose that, for large <span class="math inline">\(n\)</span>, <span class="math inline">\(T_n \sim˙ N(\theta, \omega^2/n)\)</span> and that <span class="math inline">\(\omega^2\)</span> is known.
<ul>
<li>The large sample <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence interval for <span class="math inline">\(\theta\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
T_n \pm M_\alpha
\]</span></p>
<p>where <span class="math inline">\(M_\alpha = z_{\alpha/2}\frac{\omega}{\sqrt n}\)</span>.</p>
<ul>
<li><p>The quantity <span class="math inline">\(M_\alpha\)</span> is <span class="math inline">\(\frac{1}{2}\)</span> of the confidence interval width and is called the margin of error.</p></li>
<li><p>Choosing <span class="math inline">\(n\)</span>: Suppose that the investigator would like to estimate <span class="math inline">\(\theta\)</span> to within <span class="math inline">\(\pm m\)</span> with confidence <span class="math inline">\(100(1 − \alpha)\%\)</span>.</p>
<ul>
<li>The required sample size is obtained by equating <span class="math inline">\(m\)</span> to <span class="math inline">\(M_\alpha\)</span> and solving for <span class="math inline">\(n\)</span>.</li>
<li>The solution is</li>
</ul></li>
</ul>
<p><span class="math display">\[
n= \bigg( \frac{z_{\alpha/2^\omega}}{m}\bigg)^2
\]</span></p>
<ul>
<li><p>If the solution is not an integer, then round up.</p></li>
<li><p>Application 1: <span class="math inline">\(Let X_1, \ldots , X_n\)</span> be a random sample from a distribution with unknown mean <span class="math inline">\(\mu_X\)</span> and known variance <span class="math inline">\(\sigma^2_X\)</span>.</p></li>
<li><p>If <span class="math inline">\(n\)</span> is large, then <span class="math inline">\(\bar X \sim N(\mu_X, \sigma^2_X/n)\)</span> by the CLT.</p></li>
<li><p>To estimate <span class="math inline">\(\mu_X\)</span> to within <span class="math inline">\(\pm m\)</span> with <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence, use sample size</p></li>
</ul>
<p><span class="math display">\[
n=\bigg( \frac{z_{\alpha/2}\sigma_X}{m}\bigg)^2
\]</span></p>
<ul>
<li><p>Application 2: Let <span class="math inline">\(X_1, \ldots , X_n\)</span> be a random sample from a distribution with unknown mean <span class="math inline">\(\mu_X\)</span> and unknown variance <span class="math inline">\(\sigma^2_X\)</span>.</p></li>
<li><p>If <span class="math inline">\(n\)</span> is large, then <span class="math inline">\(\bar X \sim N(\mu_X, \sigma^2_X/n)\)</span>.</p>
<ul>
<li>The investigator desires to estimate <span class="math inline">\(\mu_X\)</span> to within <span class="math inline">\(\pm m\)</span> with
<span class="math inline">\(100(1 − \alpha)\%\)</span> confidence.</li>
<li>To make any progress, something must be known about <span class="math inline">\(\sigma_X\)</span>.</li>
<li>If the likely range of the data is known, then a rough estimate of <span class="math inline">\(\sigma_X\)</span> is the range divided by four.</li>
<li>Another approach is to begin data collection and then use <span class="math inline">\(S_X\)</span> to estimate <span class="math inline">\(\sigma_X\)</span> after obtaining several observations.</li>
<li>The sample size formula can be used to estimate the number of additional observations that must be taken.</li>
<li>The sample size estimate can be updated after collecting more data are re-estimating <span class="math inline">\(\sigma_X\)</span>.</li>
</ul></li>
<li><p>Application 3: Let <span class="math inline">\(X1, \ldots , Xn\)</span> be a random sample from a <span class="math inline">\(Bern(p)\)</span> distribution.</p></li>
<li><p>If <span class="math inline">\(n\)</span> is large, then <span class="math inline">\(\hat p \sim N(p, p(1 − p)/n)\)</span> by the CLT. To estimate <span class="math inline">\(p\)</span> to within <span class="math inline">\(\pm m\)</span> with <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence, it would appear that we should use sample size</p></li>
</ul>
<p><span class="math display">\[
n = \bigg( \frac{z_{\alpha/2 \sqrt{p(1-p)}}}{m}\bigg)^2
\]</span></p>
<ul>
<li>The right-hand-side above, however, cannot be computed because <span class="math inline">\(p\)</span> is
unknown and, therefore, <span class="math inline">\(p(1 − p)\)</span> also is unknown.
<ul>
<li>Note that <span class="math inline">\(p(1 − p)\)</span> is a quadratic function that varies between <span class="math inline">\(0\)</span> (when <span class="math inline">\(p = 0\)</span> or <span class="math inline">\(p = 1\)</span>) and <span class="math inline">\(0.25\)</span> (when <span class="math inline">\(p = 0.5\)</span>).</li>
<li>A conservative approach is to use <span class="math inline">\(p(1 − p) = 0.25\)</span> in the sample size formula.</li>
<li>This approach ensures that the computed sample size is sufficiently
large, but in most cases it will be larger than necessary.</li>
<li>If it is known that <span class="math inline">\(p &gt; p_0\)</span> or that <span class="math inline">\(p &lt; p_0\)</span>, then <span class="math inline">\(p_0\)</span> may be substituted in the sample size formula.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="small-sample-confidence-intervals-for-mu_x" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Small Sample Confidence Intervals for <span class="math inline">\(\mu_X\)</span><a href="chapter9.html#small-sample-confidence-intervals-for-mu_x" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Setting: Let <span class="math inline">\(X_1, \ldots , X_n\)</span> be a random sample from <span class="math inline">\(N(\mu_X, \sigma^2_X)\)</span>, where neither the mean nor the variance is known.
<ul>
<li>It is desired to construct a <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence interval for <span class="math inline">\(\mu_X\)</span>.</li>
<li>If <span class="math inline">\(n\)</span> is small, then the large sample procedure will not work well because <span class="math inline">\(S_X\)</span> may differ substantially from <span class="math inline">\(\sigma_X\)</span>.</li>
</ul></li>
<li>Solution: Consider the random variable</li>
</ul>
<p><span class="math display">\[
\begin{align}
T &amp;= \frac{\bar X - \mu_X}{s_X/\sqrt n}=\frac{\bar X - \mu_X}{\sigma_X/\sqrt n} \times \frac{\sigma}{S_X} \\
&amp;=  \frac{\bar X - \mu_X}{\sigma_X/\sqrt n} \div \bigg(\frac{(n-1)S^2_X}{\sigma^2(n-1)}\bigg)
\end{align}
\]</span></p>
<ul>
<li>Recall that</li>
</ul>
<p><span class="math display">\[
\frac{\bar X - \mu_X}{\sigma_X/\sqrt n} \sim N(0,1)
\]</span></p>
<p><span class="math display">\[
\frac{(n-1)S^2_X}{\sigma^2} \sim \chi^2_{n-1}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\frac{\bar X - \mu_X}{\sigma_X/\sqrt n} \Perp \frac{(n-1)S^2_X}{\sigma^2}
\]</span></p>
<ul>
<li><p>The independence result follows from <span class="math inline">\(\bar X \Perp S^2_X\)</span>.</p></li>
<li><p>Accordingly, the random variable <span class="math inline">\(T\)</span> has the same distribution as the ratio <span class="math inline">\(Z/\sqrt{W/(n-1)}\)</span> where <span class="math inline">\(Z \sim N(0,1)\)</span> and <span class="math inline">\(W \sim \chi^2_{n-1}\)</span>.</p></li>
<li><p>This quantity has a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n − 1\)</span> degrees of freedom.</p></li>
<li><p>Solution to the problem.</p>
<ul>
<li>Let <span class="math inline">\(t_{\alpha/2,n−1}\)</span> be the <span class="math inline">\(100(1 − \alpha/2)\)</span> percentile of the <span class="math inline">\(t_{n-1}\)</span> distribution.</li>
<li>That is <span class="math inline">\(F^{-1}_T(1 − \alpha/2) = t_{\alpha/2,n−1}\)</span>, where <span class="math inline">\(F_T(\cdot)\)</span> is the cdf of <span class="math inline">\(T\)</span>.</li>
<li>Then, using the symmetry of the <span class="math inline">\(t\)</span> distribution around 0, it follows that</li>
</ul></li>
</ul>
<p><span class="math display">\[
P\bigg( -t_{\alpha/2,n-1} \le \frac{\bar X - \mu_X}{\sigma_X / \sqrt{n}} \le t_{\alpha/2,n-1}\bigg) = 1- \alpha
\]</span></p>
<ul>
<li>Algebraic manipulation reveals that</li>
</ul>
<p><span class="math display">\[
P\bigg[\bar X -t_{\alpha/2,n-1}\frac{S_X}{\sqrt n} \le \mu_X \le \bar X + t_{\alpha/2,n-1}\frac{S_X}{\sqrt n}\bigg]=1-\alpha
\]</span></p>
<ul>
<li>Accordingly, an exact <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence interval for <span class="math inline">\(\mu_X\)</span> is</li>
</ul>
<p><span class="math display">\[
\bar X \pm t_{\alpha/2,n-1}\frac{S_X}{\sqrt n}
\]</span></p>
<ul>
<li>Caution: The above confidence interval is correct if one is sampling from a normal distribution.
<ul>
<li>If sample size is small and skewness or kurtosis is large, then the true confidence can differ substantially from <span class="math inline">\(100(1 − \alpha)\%\)</span>.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="the-distribution-of-t" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> The Distribution of <span class="math inline">\(T\)</span><a href="chapter9.html#the-distribution-of-t" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Recall that if <span class="math inline">\(Z \sim N(0, 1)\)</span>, <span class="math inline">\(Y \sim \chi^2_k\)</span> and <span class="math inline">\(Z \Perp Y\)</span>, then</li>
</ul>
<p><span class="math display">\[
T = \frac{Z}{\sqrt{\frac{Y}{k}}} \sim t_k
\]</span></p>
<ul>
<li>In this section, we will derive the pdf of <span class="math inline">\(T\)</span>.
<ul>
<li>The strategy that we will use is (a) first find an expression for the cdf of <span class="math inline">\(T\)</span> and then (b) differentiate the cdf to obtain the pdf.</li>
<li>The cdf of <span class="math inline">\(T\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(T \le t) &amp;= F_T(t) = P\Bigg[\frac{Z}{\sqrt{\frac{Y}{k}}}\le t \Bigg] \\ &amp;= P\Bigg[Z \le t{\sqrt{\frac{Y}{k}}}\Bigg] = \int^{\infty}_{0}\int^{t\sqrt{y/k}}_{-\infty}f_Z(z)f_Y(y)dzdy
\end{align}
\]</span></p>
<p>using <span class="math inline">\(f_{Z,Y}(z, y) = f_Z(z)f_Y(y)\)</span> which follows from <span class="math inline">\(Y \Perp Z\)</span>.</p>
<ul>
<li>Using Leibnitz’s rule, the pdf of <span class="math inline">\(T\)</span> is</li>
</ul>
<p><span class="math display">\[
\begin{align}
f_T(t) &amp;= \frac{d}{dt}F_T(t) = \int^{\infty}_{0}\frac{d}{dt}\int^{t\sqrt{y/k}}_{-\infty}f_Z(z)f_Y(y)dzdy \\
&amp;= \int^{\infty}_0 \sqrt{y/k}f_Z(t\sqrt{y/k})f_Y(y)dy
\end{align}
\]</span></p>
<ul>
<li>Substituting the pdfs for <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> yields.</li>
</ul>
<p><span class="math display">\[
\begin{align}
f_T(t) &amp;= \int^{\infty}_{0}\sqrt{y/k}\frac{e^{-\frac{1}{2}\frac{t^2}{k}y}y^{\frac{k}{2}-1}e^{-\frac{1}{2}y}}{\sqrt{2\pi}\Gamma\big(\frac{k}{2}2^{\frac{k}{2}}\big)}dy = \int^{\infty}_{0}\frac{y^{\frac{k+1}{2}-1}e^{-\frac{1}{2}\big(\frac{t^2}{k}+1\big)y}}{\sqrt{k\pi}\Gamma\big(\frac{k}{2}\big)2^{\frac{k+1}{2}}}dy \\
&amp;= \frac{\Gamma\big(\frac{k+1}{2}\big)}{\Gamma\big(\frac{k}{2}\big)\sqrt{k\pi}\big(\frac{t^2}{k}+1\big)^{\frac{k+1}{2}}}I_{(-\infty,\infty)}(t)
\end{align}
\]</span></p>
<ul>
<li>The last integral is evaluated by recognizing the kernel of a gamma distribution. That is,</li>
</ul>
<p><span class="math display">\[
\int^{\infty}_{0}\frac{y^{\alpha-1}e^{-\lambda y}\lambda^{\alpha}}{\Gamma(\alpha)}dy =1 \Longleftrightarrow y^{\alpha-1}e^{-\lambda y}dy = \frac{\Gamma(\alpha)}{\lambda^{\alpha}}
\]</span></p>
<p><br></p>
</div>
<div id="pivotal-quantities" class="section level2 hasAnchor" number="9.8">
<h2><span class="header-section-number">9.8</span> Pivotal Quantities<a href="chapter9.html#pivotal-quantities" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Definition: A pivotal quantity is a function of a statistic and a parameter.
<ul>
<li>The distribution of the pivotal quantity does not depend on any unknown parameters.</li>
</ul></li>
<li>How to construct confidence intervals.
<ul>
<li>Suppose that <span class="math inline">\(Q(T; \theta)\)</span> is a pivotal quantity.</li>
<li>The distribution of <span class="math inline">\(Q\)</span> is known, so percentiles of <span class="math inline">\(Q\)</span> can be
computed.</li>
<li>Let <span class="math inline">\(q_1\)</span> and <span class="math inline">\(q_2\)</span> be percentiles that satisfy</li>
</ul></li>
</ul>
<p><span class="math display">\[
P[q_1 \le Q(T; \theta) \le q_2]=1-\alpha
\]</span></p>
<ul>
<li>If <span class="math inline">\(Q(T; \theta)\)</span> is a monotonic increasing or decreasing function of <span class="math inline">\(\theta\)</span> for each realization of <span class="math inline">\(T\)</span>, then the inverse function <span class="math inline">\(Q^{−1}[Q(T; \theta)] = \theta\)</span> exists, and</li>
</ul>
<p><span class="math display">\[
P[Q^{-1}(q_1) \le \theta \le Q^{-1}(q_2)]=1-\alpha
\]</span></p>
<p>if <span class="math inline">\(Q(T;\theta)\)</span> is and increasing function of <span class="math inline">\(\theta\)</span> and</p>
<p><span class="math display">\[
P[Q^{-1}(q_2) \le \theta \le Q^{-1}(q_1)]=1-\alpha
\]</span></p>
<p>if <span class="math inline">\(Q(T;\theta)\)</span> is and decreasing function of <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><p>Example 1: Suppose that <span class="math inline">\(X_1, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(N(\mu, \sigma^2)\)</span>.</p></li>
<li><p><span class="math inline">\(Q(\bar X, S_X ; \mu) = \frac{\bar X - \mu}{S_X/ \sqrt{n}} \sim t_{n-1}\)</span> which reveals that <span class="math inline">\(Q\)</span> is a pivotal quantity.</p>
<ul>
<li>Note that</li>
</ul></li>
</ul>
<p><span class="math display">\[
T=\begin{pmatrix} \bar X \\ S_X\end{pmatrix}
\]</span></p>
<p>is two-dimensional.</p>
<ul>
<li>Also, <span class="math inline">\(Q\)</span> is a decreasing function of <span class="math inline">\(\mu\)</span>,</li>
</ul>
<p><span class="math display">\[
Q^{-1}(Q) = \bar X - \frac{S_X}{\sqrt{n}}Q = \mu
\]</span></p>
<p>and</p>
<p><span class="math display">\[
P\bigg[\bar X - \frac{S_X}{\sqrt n}q_2 \le \bar X - \frac{S_X}{\sqrt n}q_1\bigg]=1-\alpha
\]</span></p>
<p>where <span class="math inline">\(q_1\)</span> and <span class="math inline">\(q_2\)</span> are appropriate percentiles of the <span class="math inline">\(t_{n−1}\)</span> distribution.</p>
<ul>
<li><span class="math inline">\(Q(S^2_X; \sigma^2) = \frac{(n-1)S^2_X}{\sigma^2} \sim \chi^2_{n-1}\)</span> which reveals that <span class="math inline">\(Q\)</span> is a pivotal quantity.
<ul>
<li>Furthermore, <span class="math inline">\(Q\)</span> is a decreasing function of <span class="math inline">\(\sigma\)</span>,</li>
</ul></li>
</ul>
<p><span class="math display">\[
Q^{-1}(Q) = \sqrt{\frac{(n-1)S^2_X}{Q}}=Q
\]</span></p>
<p>and</p>
<p><span class="math display">\[
P\bigg[\sqrt{\frac{(n-1)S^2_X}{q_2}} \le \sigma \le \sqrt{\frac{(n-1)S^2_X}{q_1}}\bigg]=1-\alpha
\]</span></p>
<p>where <span class="math inline">\(q_1\)</span> and <span class="math inline">\(q_2\)</span> are appropriate percentiles of the <span class="math inline">\(\chi^2_{n-1}\)</span> distribution.</p>
<ul>
<li>Example 2: Suppose that <span class="math inline">\(X_1, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(Unif(0, \theta)\)</span>.
<ul>
<li>It is easy to show that <span class="math inline">\(X_{(n)}\)</span> is sufficient statistic.</li>
<li>Note that <span class="math inline">\(X_i/\theta ∼ Unif(0, 1)\)</span>.</li>
<li>Accordingly, <span class="math inline">\(X_{(n)}/\theta\)</span> is distributed as the largest order statistic from a <span class="math inline">\(Unif(0, 1)\)</span> distribution.</li>
<li>That is, <span class="math inline">\(Q(X_{(n)}; \theta) = X_{(n)}/\theta \sim Beta(n 1)\)</span> which
reveals that <span class="math inline">\(Q\)</span> is a pivotal quantity. Furthermore, <span class="math inline">\(Q\)</span> is a decreasing function of <span class="math inline">\(\theta\)</span>,</li>
</ul></li>
</ul>
<p><span class="math display">\[
Q^{-1}(Q) = \frac{X_{(n)}}{Q}=\theta
\]</span></p>
<p>and</p>
<p><span class="math display">\[
P\bigg[\frac{X_{(n)}}{q_2} \le \theta \le \frac{X_{(n)}}{q_1}\bigg]=1-\alpha
\]</span></p>
<p>where <span class="math inline">\(q_1\)</span> and <span class="math inline">\(q_2\)</span> are appropriate percentiles of the <span class="math inline">\(Beta(n, 1)\)</span> distribution.</p>
<ul>
<li>Note, <span class="math inline">\(q_2 = 1\)</span> is the 100th percentile of <span class="math inline">\(Beta(n, 1)\)</span> and <span class="math inline">\(q_1 = \alpha^{1/n}\)</span> is the <span class="math inline">\(100\alpha\)</span> percentile of <span class="math inline">\(Beta(n, 1)\)</span>.
<ul>
<li>Accordingly, a <span class="math inline">\(100(1 − \alpha)\)</span> confidence interval for <span class="math inline">\(\theta\)</span> can be based on</li>
</ul></li>
</ul>
<p><span class="math display">\[
p\bigg[ X_{(n)} \le \theta \le \frac{X_{(n)}}{\alpha^{1/n}} \bigg]=1-\alpha
\]</span></p>
<ul>
<li>Note, <span class="math inline">\(q_1 = 0\)</span> is the <span class="math inline">\(0^{th}\)</span> percentile of <span class="math inline">\(Beta(n, 1)\)</span> and <span class="math inline">\(q2 = (1 − \alpha)^{1/n}\)</span> is the <span class="math inline">\(100(1 − \alpha)\)</span> percentile of <span class="math inline">\(Beta(n, 1)\)</span>.
<ul>
<li>Accordingly, a <span class="math inline">\(100(1 − \alpha)\)</span> one-sided confidence interval for <span class="math inline">\(\theta\)</span> can be based on</li>
</ul></li>
</ul>
<p><span class="math display">\[
p\bigg[ \frac{X_{(n)}}{(1-\alpha)^{1/n}} \le \theta \le \infty \bigg] = 1- \alpha
\]</span></p>
<p><br></p>
</div>
<div id="estimating-a-mean-difference" class="section level2 hasAnchor" number="9.9">
<h2><span class="header-section-number">9.9</span> Estimating a Mean Difference<a href="chapter9.html#estimating-a-mean-difference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Setting: Suppose that <span class="math inline">\(T_{1,n1} \sim  N\big(\theta_1, \omega^2_1/m_1\big)\)</span>; <span class="math inline">\(T_{2,n2} \sim  N\big(\theta_2, \omega^2_2/m_1\big)\)</span> and <span class="math inline">\(T_{1,n_1} \Perp T_{2,n_2}\)</span>.</p></li>
<li><p>The goal is to construct a confidence interval for <span class="math inline">\(\theta_1 − \theta_2\)</span>. Note that</p></li>
</ul>
<p><span class="math display">\[
T_{1,n_1}-T_{2,n_2} \sim N\bigg(\theta_1-\theta_2, \frac{\omega^2_1}{n_1} +\frac{\omega^2_2}{n_2}\bigg)
\]</span></p>
<ul>
<li>If <span class="math inline">\(W^2_1\)</span> and <span class="math inline">\(W^2_2\)</span> are consistent estimators of <span class="math inline">\(\omega^2_1\)</span> and <span class="math inline">\(\omega^2_2\)</span> (i.e., <span class="math inline">\(W^2_1 \overset{prob}\rightarrow \omega^2_1\)</span> and <span class="math inline">\(W^2_2 \overset{prob}\rightarrow \omega^2_2\)</span>), then</li>
</ul>
<p><span class="math display">\[
\frac{(T_{1,n_1}-T_{2,n_2})-(\theta_1 - \theta_2)}{\sqrt{\frac{W^2_1}{n_1}+\frac{W^2_2}{n_2}}}\sim N(0,1)
\]</span></p>
<ul>
<li>A large sample <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence interval for <span class="math inline">\(\theta_1 − \theta_2\)</span> can be based on</li>
</ul>
<p><span class="math display">\[
P[T_{1,n_1}-T_{2,n_2} -Z_{\alpha/2}SE \le \theta_1-\theta_2 \le T_{1,n_1}-T_{2,n_2} +Z_{\alpha/2}SE] \approx 1- \alpha
\]</span></p>
<p>where <span class="math inline">\(SE=SE(T_{1,n_1} -T_{2,n_2})=\sqrt{\frac{W^2_1}{n_1}+\frac{W^2_2}{n_2}}\)</span></p>
<ul>
<li>Application 1: Suppose that <span class="math inline">\(X_{11}, X_{12}, \ldots , X_{1n_1}\)</span> is a random sample from a population having mean <span class="math inline">\(\mu_1\)</span> and variance <span class="math inline">\(\sigma^2_1\)</span> and that <span class="math inline">\(X_{21}, X_{22}, \ldots , X_{2n_1}\)</span> is an independent random sample from a population having mean <span class="math inline">\(\mu_2\)</span> and variance <span class="math inline">\(\sigma^2_2\)</span>. Then</li>
</ul>
<p><span class="math display">\[
\frac{(\bar X_1 - \bar X_2)- (\mu_1 - \mu_2)}{\sqrt{\frac{S^2_1}{n_1}+\frac{S^2_1}{n_1}}}\sim N(0,1)
\]</span></p>
<ul>
<li>A large sample <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence interval for <span class="math inline">\(\mu_1 − \mu_2\)</span> can be based on</li>
</ul>
<p><span class="math display">\[P \bigg[ \bar X_1-\bar X_2 -z_{\alpha/2}\sqrt{\frac{S^2_1}{n_1}+\frac{S^2_2}{n_2}} \le \mu_1 - \mu_2 \le \bar X_1-\bar X_2 +z_{\alpha/2}\sqrt{\frac{S^2_1}{n_1}+\frac{S^2_2}{n_2}} \bigg] \approx 1-\alpha\]</span>.</p>
<ul>
<li>Application 2: Suppose that <span class="math inline">\(X_{11}, X_{12}, \ldots , X_{1n_1}\)</span>
is a random sample from <span class="math inline">\(Bern(p_1)\)</span> and that <span class="math inline">\(X_{21}, X_{22}, \ldots X_{2n_1}\)</span> is an independent random sample from <span class="math inline">\(Bern(p_2)\)</span>. Then</li>
</ul>
<p><span class="math display">\[
\frac{(\hat p_1 - \hat p_2)-(p_1 - p_2)}{\sqrt{\frac{\hat p_1 (1-\hat p_1)}{n_1} \frac{\hat p_2 (1-\hat p_2)}{n_2}}}\sim N(0,1)
\]</span></p>
<ul>
<li>A large sample <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence interval for <span class="math inline">\(p_1 − p_2\)</span> can be based on</li>
</ul>
<p><span class="math display">\[
P[\hat p_1 - \hat p_2 -z_{\alpha/2}SE \le p_1-p_2 \le \hat p_1 - \hat p_2 +z_{\alpha/2}SE] \approx 1-\alpha
\]</span></p>
<p>where</p>
<p><span class="math display">\[
SE=SE(\hat p_1 - \hat p_2) = \sqrt{\frac{\hat p_1 (1-\hat p_1)}{n_1} +\frac{\hat p_2 (1-\hat p_2)}{n_2}}
\]</span></p>
<p><br></p>
</div>
<div id="umvue" class="section level2 hasAnchor" number="9.10">
<h2><span class="header-section-number">9.10</span> UMVUE<a href="chapter9.html#umvue" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="rao-blackwell-theorem" class="section level4 unnumbered hasAnchor">
<h4>Rao-Blackwell Theorem<a href="chapter9.html#rao-blackwell-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>If <span class="math inline">\(U(X)\)</span> is unbiased for <span class="math inline">\(\theta\)</span> (a scalar) and <span class="math inline">\(T(X)\)</span> is sufficient, then <span class="math inline">\(V = V (T) = E(U|T)\)</span> is</li>
</ul>
<ol style="list-style-type: decimal">
<li>a statistic,</li>
<li>unbiased for <span class="math inline">\(\theta\)</span>, and</li>
<li><span class="math inline">\(Var(V) ≤ Var(U)\)</span>, with strict inequality iff and only if <span class="math inline">\(U\)</span> is a function of <span class="math inline">\(T\)</span>.</li>
</ol>
<ul>
<li>Proof</li>
<li><span class="math inline">\(V\)</span> is a statistic because the distribution of <span class="math inline">\(X\)</span> conditional on T does not depend on <span class="math inline">\(\theta\)</span>.
<ul>
<li>Accordingly, the expectation of <span class="math inline">\(V\)</span> with respect to the distribution of <span class="math inline">\(X\)</span> conditional on <span class="math inline">\(T\)</span> does not depend on <span class="math inline">\(\theta\)</span>.</li>
</ul></li>
<li>Note, <span class="math inline">\(E(U) = \theta\)</span> because <span class="math inline">\(U\)</span> is unbiased.
<ul>
<li>Now use iterated expectation:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\theta = E(U) = E_T [E(U|T)] = E_T (V ) = E(V)
\]</span></p>
<ul>
<li>Use iterated variance:</li>
</ul>
<p><span class="math display">\[
Var(U) = E [Var(U|T)] + Var[E(U|T)] = E [Var(U|T)] + Var(V ) \ge  Var(V )
\]</span></p>
<ul>
<li><p>Example. Suppose that <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is a random sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(Poi(\lambda)\)</span>.</p></li>
<li><p>The goal is to find a good unbiased estimator of of <span class="math inline">\(P(X = 0) = e^{−\lambda}\)</span>.</p></li>
<li><p>Recall that <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient and that <span class="math inline">\(T \sim Poi(n\lambda)\)</span>.</p></li>
<li><p>Consider</p></li>
</ul>
<p><span class="math display">\[
U = I_{\{0\}}(X_1) = \begin{cases} 1 \quad if \ X_1 = 0 \\ 0 \quad if \ X_1 = 1  \end{cases}
\]</span></p>
<ul>
<li>The support of <span class="math inline">\(U\)</span> is {0, 1} and the expectation of <span class="math inline">\(U\)</span> is</li>
</ul>
<p><span class="math display">\[
E(U) =1 \times P(U=1) + 0 \times P(U=0) =1 \times P(x_1 = 0) =e^{-\lambda}
\]</span></p>
<ul>
<li><p>Thus, <span class="math inline">\(U\)</span> is unbiased for <span class="math inline">\(e^{-\lambda}\)</span>.</p></li>
<li><p>To find a better estimator, use the Rao-Blackwell theorem.</p>
<ul>
<li>The conditional distribution of <span class="math inline">\(X_1, X_2, ldots , X_k\)</span> given <span class="math inline">\(T = t\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp;P(X_1=x_1, \ X_2=x_2,..., \ X_n=x_n|T=t) \\
&amp;= \begin{pmatrix} t \\ x_1, x_2,x_3,...,x_n \end{pmatrix}\bigg(\frac{1}{n}\bigg)^{x_1}\bigg(\frac{1}{n}\bigg)^{x_2}\cdots\bigg(\frac{1}{n}\bigg)^{x_n}
\end{align}
\]</span></p>
<ul>
<li>That is, given <span class="math inline">\(T = t\)</span>, the <span class="math inline">\(X\)</span>’s have a multinomial distribution with <span class="math inline">\(t\)</span> trials, <span class="math inline">\(n\)</span> categories, and probability <span class="math inline">\(1/n\)</span> for each category.
<ul>
<li>Note that the conditional distribution of the data given <span class="math inline">\(T\)</span> does not depend on <span class="math inline">\(\lambda\)</span>.</li>
<li>This is because <span class="math inline">\(T\)</span> is sufficient.</li>
<li>The conditional distribution of <span class="math inline">\(X_1\)</span> given <span class="math inline">\(T = t\)</span> is binomial:</li>
</ul></li>
</ul>
<p><span class="math display">\[
X_1|(T=t)\sim Bin\bigg[T,\frac{1}{n}\bigg]
\]</span></p>
<ul>
<li>The expectation of <span class="math inline">\(U\)</span> given <span class="math inline">\(T = t\)</span> is</li>
</ul>
<p><span class="math display">\[
\begin{align}
E(U|T=t)&amp;= 1 \times P(U=1|T=1)+0 \times P(U=0|T) \\
&amp;= P(U=1|T=t) =P(X_1=0|T=t) \\
&amp;= \begin{pmatrix} t \\ 0 \end{pmatrix}\begin{pmatrix} 1 \\ n \end{pmatrix}^0\bigg(1-\frac{1}{n}\bigg)^{t-0}=\bigg(1-\frac{1}{n}\bigg)^{t}
\end{align}
\]</span></p>
<ul>
<li>Accordingly, an unbiased estimator of <span class="math inline">\(e^{−\lambda}\)</span> that has smaller variance than <span class="math inline">\(U\)</span> is</li>
</ul>
<p><span class="math display">\[
E(U|T) = \bigg( 1-\frac{1}{n}\bigg)^T
\]</span></p>
</div>
<div id="rao-blackwell-theorem-1" class="section level4 unnumbered hasAnchor">
<h4>Rao-Blackwell theorem<a href="chapter9.html#rao-blackwell-theorem-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Add it</p></li>
<li><p>Add completeness</p></li>
</ul>
<p><br></p>
</div>
</div>
<div id="bayes-estimators" class="section level2 hasAnchor" number="9.11">
<h2><span class="header-section-number">9.11</span> Bayes Estimators<a href="chapter9.html#bayes-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Setting: Suppose that we have (a) data, <span class="math inline">\(X_1, \ldots , X_n\)</span>, (b) the corresponding likelihood function, <span class="math inline">\(L(\theta|T)\)</span>, where <span class="math inline">\(T\)</span> is sufficient, and (c) a prior distribution for <span class="math inline">\(\theta\)</span>, <span class="math inline">\(g_\Theta(\theta)\)</span>.
<ul>
<li>Then, if we are skillful, we can find the posterior <span class="math inline">\(g_{\Theta|T} (\theta|T)\)</span>. We would like to find point and interval estimators of <span class="math inline">\(\theta\)</span>.</li>
</ul></li>
<li>Point Estimators: In general, we can use some characteristic of the posterior distribution as our point estimator.
<ul>
<li>Suitable candidates are the mean, median, or mode.</li>
<li>How do we choose which one to use?</li>
</ul></li>
<li>Loss Function: Suppose that we can specify a loss function that describes the penalty for missing the mark when estimating <span class="math inline">\(\theta\)</span>.
<ul>
<li>Denote our estimator as <span class="math inline">\(a\)</span> or <span class="math inline">\(a(t)\)</span> because it will depend on <span class="math inline">\(t\)</span>.</li>
<li>Two possible loss functions are</li>
</ul></li>
</ul>
<p><span class="math display">\[
\ell_1(\theta, a) = |\theta-a| \,\,\, \text{and} \,\,\, \ell_2(\theta,a)=(\theta-a)^2
\]</span></p>
<ul>
<li>Bayes Estimator: Recall that <span class="math inline">\(\Theta\)</span> is a random variable.
<ul>
<li>A posterior estimator, <span class="math inline">\(a\)</span>, is a Bayes estimator with loss function <span class="math inline">\(\ell\)</span> if <span class="math inline">\(a\)</span> minimizes the posterior expected loss (Bayes loss):</li>
</ul></li>
</ul>
<p><span class="math display">\[  
Bayes Loss = B(a) = E_{\Theta|T}[\ell(\Theta-a)]
\]</span></p>
<ul>
<li>From prior results (see page 24 of these notes), the Bayes estimator for loss <span class="math inline">\(\ell_1\)</span> is known to be the median of the posterior distribution.
<ul>
<li>Recall that <span class="math inline">\(E(X)\)</span> is the minimizer of <span class="math inline">\(E(X − a)^2\)</span> with respect to <span class="math inline">\(a\)</span>.</li>
<li>Accordingly, the Bayes estimator for loss <span class="math inline">\(\ell_2\)</span> is the mean of the posterior distribution</li>
</ul></li>
<li>Example: Suppose that <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(Bern(\theta)\)</span>.
<ul>
<li>Recall that <span class="math inline">\(T = \sum^n_{i=1}X_i\)</span> is sufficient. Furthermore, suppose that the prior on <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\Theta \sim beta(\alpha_1, \alpha_2)\)</span>.</li>
<li>Then the posterior, conditional on <span class="math inline">\(T=t\)</span> is <span class="math inline">\(\Theta \sim beta(\alpha_1+t, \alpha_2+n-t)\)</span>.</li>
<li>The mean of the posterior is</li>
</ul></li>
</ul>
<p><span class="math display">\[
E(\Theta|T=t) = \frac{\alpha_1 +t}{\alpha_1 +\alpha_2 + n}
\]</span></p>
<ul>
<li>If <span class="math inline">\(n = 10\)</span>, <span class="math inline">\(\alpha_1 = \alpha_2 = 1\)</span>, and <span class="math inline">\(t = 6\)</span>, then</li>
</ul>
<p><span class="math display">\[
E(\Theta|T=t) = \frac{7}{12} = 0.5833
\]</span></p>
<p>and the median of the <span class="math inline">\(beta(7, 5)\)</span> distribution is <span class="math inline">\(0.5881\)</span>.</p>
<ul>
<li>Interval Estimator: Use the posterior distribution to find lower and upper limits, <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span> such that</li>
</ul>
<p><span class="math display">\[
P(h_1 \le \Theta \le h_2 |T=t) = 1-\alpha
\]</span></p>
<ul>
<li>The above interval is a Bayesian <span class="math inline">\(100(1 − \alpha)\%\)</span> confidence interval.
<ul>
<li>In the statistical literature, this interval usually is called a credibility interval.</li>
<li>Unlike the frequentist confidence interval, the credibility interval is interpreted as a probability.</li>
<li>That is, we say that <span class="math inline">\(\Theta\)</span> is contained in the interval with
probability <span class="math inline">\(1 − \alpha\)</span>.</li>
<li>This is the interpretation that 216 students often give to
frequentist intervals and we give them no credit when they do so.</li>
</ul></li>
<li>Example 1
<ul>
<li>In the binomial example, the posterior distribution of <span class="math inline">\(\Theta\)</span> is
<span class="math inline">\(beta(7, 5)\)</span>.</li>
<li>A <span class="math inline">\(95\%\)</span> Bayesian confidence interval is</li>
</ul></li>
</ul>
<p><span class="math display">\[
P(0,3079 \le \Theta \le 0.8325) = 0.95
\]</span></p>
<ul>
<li>Example 2
<ul>
<li>Suppose that <span class="math inline">\(X_1, ldots , X_n\)</span> is a random sample from <span class="math inline">\(N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known.</li>
<li>If the prior on <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\mu \sim N(\nu ,\tau^2)\)</span>, then the posterior distribution is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\mu|\bar x \sim N\bigg[\frac{\pi_{\bar x}}{\pi_{\bar x}+\pi_{\mu}} \bar x+ \frac{\pi_{\mu}}{\pi_{\bar x}+\pi_{\mu}}\nu, \ (\pi_{\bar x}+\pi_{\mu}^{-1})\bigg]
\]</span></p>
<p>where the precisions are</p>
<p><span class="math display">\[
\mu_{\bar x} = \frac{n}{\sigma^2} \,\,\, \text{and} \,\,\, \pi_\mu = \frac{1}{\tau^2}
\]</span></p>
<ul>
<li>A <span class="math inline">\(95\%\)</span> Bayesian confidence interval for <span class="math inline">\(\mu\)</span> is</li>
</ul>
<p><span class="math display">\[
P\bigg[\hat\mu -1.96\sqrt{(\pi_{\bar x}+\pi_{\mu})^{-1}} \le \mu \le \hat\mu +1.96\sqrt{(\pi_{\bar x}+\pi_{\mu})^{-1}}\bigg\vert\bar X = \bar x \bigg] =0.95
\]</span></p>
<ul>
<li>This is interpreted as a fixed interval that has probability <span class="math inline">\(0.95\)</span> of capturing the random variable <span class="math inline">\(\mu\)</span>.
<ul>
<li>Note that the above Bayesian credibility interval is identical to the usual 95% frequentist confidence interval for <span class="math inline">\(\mu\)</span> when <span class="math inline">\(\sigma^2\)</span> is known.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="efficiency" class="section level2 hasAnchor" number="9.12">
<h2><span class="header-section-number">9.12</span> Efficiency<a href="chapter9.html#efficiency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>This section is concerned with optimal estimators.
<ul>
<li>For example, suppose that we are interested in estimating <span class="math inline">\(g(\theta)\)</span>, for some function <span class="math inline">\(g\)</span>.</li>
<li>The question to be addressed is — how do we know if we have the best possible estimator?</li>
<li>A partial answer is given by the Cram´er-Rao inequality.</li>
<li>It gives a lower bound on the variance of an unbiased estimator.</li>
<li>If our estimator attains the Cram´er-Rao lower bound, then we know that we have the best unbiased estimator.</li>
</ul></li>
<li>Cram´er-Rao Inequality (Information Inequality).
<ul>
<li>Suppose that the joint pdf (pmf) of <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is <span class="math inline">\(f_X(x|\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is a scalar and the support of <span class="math inline">\(X\)</span> does not depend on <span class="math inline">\(\theta\)</span>.</li>
<li>Furthermore, suppose that the statistic <span class="math inline">\(T(X)\)</span> is an unbiased estimator of a differentiable function of <span class="math inline">\(\theta\)</span>.</li>
<li>That is, <span class="math inline">\(E(T) = g(\theta)\)</span>.</li>
<li>Then, under mild regularity conditions,</li>
</ul></li>
</ul>
<p><span class="math display">\[
Var(T) \ge \frac{\bigg[\frac{\partial g(\theta)}{   \partial \theta}\bigg]^2}{I_\theta}, \,\,\, \text{where} \,\,\, I_\theta = E\bigg[\bigg( \frac{\partial\ln f_X(X|\theta)}{\partial\theta}\bigg)^2\bigg]\]</span></p>
<ul>
<li>The quantity <span class="math inline">\(I_\theta\)</span> is called Fisher’s information and it is an index of the amount of information that <span class="math inline">\(X\)</span> has about <span class="math inline">\(\theta\)</span>.
<ul>
<li>Proof</li>
<li>Define the random variable <span class="math inline">\(S\)</span> as</li>
</ul></li>
</ul>
<p><span class="math display">\[
S=S(X,\theta)=\frac{\partial \ln f_X(X|\theta)}{\partial\theta} = \frac{1}{f_X(X|\theta)}\frac{\partial f_X(X|\theta)}{\partial \theta}
\]</span></p>
<ul>
<li><p>This quantity is called the score function (not in your text). Your text denotes the score function by <span class="math inline">\(W\)</span>.</p></li>
<li><p>Result: The expected value of the score function is zero.</p>
<ul>
<li>Proof: This result can be shown by interchanging integration and
differentiation (justified if the regularity conditions are satisfied):</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
E(x)&amp;= \int S(X,\theta)f_X(X|\theta)dX \\
&amp;= \int \frac{1}{f_X(X|\theta)}\frac{\partial f_X(X|\theta)}{\partial\theta}f_X(X|\theta)dX  \\
&amp;= \int \frac{\partial f_X(X|\theta)}{\partial\theta}dX \\
&amp;= \frac{\partial}{\partial \theta}1=0
\end{align}
\]</span></p>
<p>because the integral of the joint pdf over the entire sample space is <span class="math inline">\(1\)</span>.</p>
<ul>
<li><p>Substitute summation for integration if the random variables are discrete</p></li>
<li><p>Result: The variance of <span class="math inline">\(S\)</span> is <span class="math inline">\(I+\theta\)</span>.</p>
<ul>
<li>Proof: This result follows from the first result and from the definition of <span class="math inline">\(I_\theta\)</span>:</li>
</ul></li>
</ul>
<p><span class="math display">\[
Var(S) = E(S^2) -[E(S)]^2 =E(S^2)= I_\theta
\]</span></p>
<ul>
<li>Result: The covariance between <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> is</li>
</ul>
<p><span class="math display">\[
Cov(S,T)= \frac{\partial g(\theta)}{\partial\theta}
\]</span></p>
<ul>
<li>Proof: To verify this result, again we will interchange integration and
differentiation.
<ul>
<li>First, note that <span class="math inline">\(Cov(S, T) = E(ST) − E(S)E(T) = E(ST)\)</span> because <span class="math inline">\(E(S) = 0\)</span>.</li>
<li>Accordingly,</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
Cov(S,T) &amp;= E(ST) = \int S(X,\theta)T(X)f_X(X|\theta)dX \\
&amp;= \int \frac{1}{f_X(X|\theta)}\frac{\partial f_X(X|\theta)}{\partial \theta}T(X)f_X(X|\theta)dX \\
&amp;= \int \frac{\partial f_X(X|\theta)}{\partial \theta}T(X)dX \\
&amp;= \frac{\partial}{\partial \theta}\int f_X(X|\theta)T(X)dX \\
&amp;= \frac{\partial}{\partial \theta}E(T) = \frac{\partial g(\theta)}{\partial \theta}
\end{align}
\]</span></p>
<ul>
<li>Result: Cram´er-Rao Inequality:</li>
</ul>
<p><span class="math display">\[
Var(T) \ge \frac{\bigg[\frac{\partial g(\theta)}{\partial \theta}\bigg]^2}{I_\theta}
\]</span></p>
<ul>
<li>The right-hand-side of the above equation is called the Cram´er-Rao
Lower Bound (CRLB). That is,</li>
</ul>
<p><span class="math display">\[
CRLB = \frac{\bigg[\frac{\partial g(\theta)}{\partial \theta}\bigg]^2}{I_\theta}
\]</span></p>
<ul>
<li>Proof: If <span class="math inline">\(\rho\)</span> is a correlation coefficient, then from the Cauchy-Schwartz inequality it is known that <span class="math inline">\(0 \le \rho^2 \le 1\)</span>.</li>
</ul>
<p><span class="math display">\[
\rho^2_{S,T} = \frac{[Cov(S,T)]^2}{Var(s)Var(T)}\le 1
\Longrightarrow var(T) \ge \frac{[Cov(S,T)]^2}{Var(s)}=\frac{\bigg[\frac{\partial g(\theta)}{\partial \theta}\bigg]^2}{I_\theta}
\]</span></p>
<ul>
<li>If <span class="math inline">\(g(\theta) = \theta\)</span>, then the inequality simplifies to</li>
</ul>
<p><span class="math display">\[
Var(T) \ge \frac{1}{I_\theta} \,\,\, \text{because} \,\,\, \frac{\partial}{\partial\theta}\theta=1
\]</span></p>
<ul>
<li>Note: if <span class="math inline">\(X_1, X_2,\ldots , X_n\)</span> are iid, then the score function can be written as</li>
</ul>
<p><span class="math display">\[
\begin{align}
S(X|\theta) &amp;= \frac{\partial}{\partial\theta}\sum^n_{i=1}\ln f_X(X_i|\theta) \\
&amp;= \sum^n_{i=1}\frac{\partial \ln f_X(X_i|\theta)}{\partial \theta} \\
&amp;= \sum^n_{i=1}S_i(X_i,\theta)
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
S_i = \frac{\partial \ln f_X(X_i|\theta)}{\partial \theta}
\]</span></p>
<p>is the score function for <span class="math inline">\(X_i\)</span>.</p>
<ul>
<li>The score functions <span class="math inline">\(S_i\)</span> for <span class="math inline">\(i = 1, \ldots , n\)</span> are
iid, each with mean zero.
<ul>
<li>Accordingly,</li>
</ul></li>
</ul>
<p><span class="math display">\[
Var(S) = Var\bigg(\sum^n_{i=1}S_i\bigg) =\sum^n_{i=1}Var(S_i)
\]</span></p>
<p>by independence</p>
<p><span class="math display">\[
=\sum^{n}_{i=1}E(S^2_i)=nE(S^2_i)
\]</span></p>
<p>where <span class="math inline">\(S_1\)</span> is the score function for <span class="math inline">\(X_1\)</span>.</p>
<ul>
<li>Example: Suppose that <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is a random sample from <span class="math inline">\(Poi(\lambda)\)</span>.
<ul>
<li>The score function for a single <span class="math inline">\(X\)</span> is<br />
</li>
</ul></li>
</ul>
<p><span class="math display">\[
S(X_i,\lambda) = \frac{\partial[-\lambda+X_i\ln(\lambda)-\ln(X_i!)]}{\partial\lambda} = -1+\frac{x_i}{\lambda}
\]</span></p>
<ul>
<li>Accordingly, the information is</li>
</ul>
<p><span class="math display">\[
I_\theta = nVar(-1+X_i/\lambda) = n\frac{Var(X_i)}{\lambda^2} = n\frac{\lambda}{\lambda^2} = \frac{n}{\lambda}
\]</span></p>
<ul>
<li>Suppose that the investigator would like to estimate <span class="math inline">\(g(\lambda) = \lambda\)</span>. The MLE of <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(E(\bar X) = \lambda\)</span>, so the MLE is unbiased.
<ul>
<li>The variance of a Poisson random variable is <span class="math inline">\(\lambda\)</span> and therefore <span class="math inline">\(Var(\bar X) = \lambda/n\)</span>.</li>
<li>The CRLB for estimating <span class="math inline">\(\lambda\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
CRLB = \frac{\big[ \frac{\partial}{\partial\lambda}\big]^2}{n/\lambda}=\frac{\lambda}{n}
\]</span></p>
<ul>
<li><p>Therefore, the MLE attains the CRLB.</p></li>
<li><p>Efficiency: The efficiency of an unbiased estimator of <span class="math inline">\(g(\theta)\)</span> is the ratio of the CRLB to the variance of the estimator.</p>
<ul>
<li>That is, suppose that <span class="math inline">\(T\)</span> is an unbiased estimator of <span class="math inline">\(g(\theta)\)</span>.</li>
<li>Then the efficiency of <span class="math inline">\(T\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
Efficiency &amp;= \frac{CRLB}{Var(T)} \\
&amp;=\frac{\bigg( \frac{\partial g(\theta)}{\partial \theta}\bigg)^2}{I_\theta} \div Var(T) \\
&amp;= \frac{\bigg( \frac{\partial g(\theta)}{\partial \theta}\bigg)^2}{I_\theta Var(T)}
\end{align}
\]</span></p>
<ul>
<li>If this ratio is one, then the estimator is said to be efficient.
<ul>
<li>Efficiency always is less than or equal to one.</li>
</ul></li>
<li>Exponential Family Results:
<ul>
<li>Recall, if the distribution of <span class="math inline">\(X\)</span> belongs to the one parameter exponential family and <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is a random sample, then
the joint pdf (pmf) is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_X(X|\theta) = [B(\theta)]^n\bigg[ \prod^n_{i=1}h(X_i)\bigg]\exp\bigg\{ Q(\theta) \sum^n_{i=1}R(X_i)\bigg\}
\]</span></p>
<ul>
<li>The score function is</li>
</ul>
<p><span class="math display">\[
S(T,\theta) = n\frac{\partial\ln B(\theta)}{\partial \theta}+T\frac{\partial Q(\theta)}{\partial \theta}
\]</span></p>
<p>where <span class="math inline">\(T = \sum^n_{i=1}R(X_i)\)</span>.</p>
<ul>
<li>Note that the score function is a linear function of <span class="math inline">\(T\)</span>:</li>
</ul>
<p><span class="math display">\[S=a+bT, \,\,\, \text{where} \,\,\, a=n\frac{\partial \ln B(\theta)}{\partial \theta}\,\,\, \text{and} \,\,\, b= \frac{\partial Q(\theta)}{\partial\theta}
\]</span></p>
<ul>
<li>Recall that <span class="math inline">\(E(S) = 0\)</span>.
<ul>
<li>It follows that</li>
</ul></li>
</ul>
<p><span class="math display">\[
n=\frac{\partial \ln B(\theta)}{\partial \theta}+E(T)\frac{\partial Q(\theta)}{\partial\theta}=0
\,\,\, \text{and} \,\,\,
E(T) = -n\frac{\partial \ln B(\theta)}{\partial \theta}\bigg[\frac{\partial Q(\theta)}{\partial\theta}\bigg]^{-1}
\]</span></p>
<ul>
<li>Result: Suppose that <span class="math inline">\(g(\theta)\)</span> is chosen to be</li>
</ul>
<p><span class="math display">\[
g(\theta) = E(T) = -n\frac{\partial \ln B(\theta)}{\partial \theta}\bigg[\frac{\partial Q(\theta)}{\partial\theta}\bigg]^{-1}
\]</span></p>
<ul>
<li>Then,</li>
</ul>
<p><span class="math display">\[
Var(T) = \frac{\bigg( \frac{\partial g (\theta)}{\partial \theta}\bigg)^2}{I_\theta}=CRLB
\]</span></p>
<p>and <span class="math inline">\(T\)</span> is the minimum variance unbiased estimator of <span class="math inline">\(g(\theta)\)</span>.</p>
<ul>
<li>Proof
<ul>
<li>First, note that <span class="math inline">\(T\)</span> is unbiased for <span class="math inline">\(E(T)\)</span>.</li>
<li>Second, note that</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align}
S &amp;= a+bT \Rightarrow \rho^2_{S,T} =1 \Rightarrow \frac{\bigg[\frac{\partial g (\theta)}{\partial \theta}\bigg]^2}{Var(T)I_\theta}=1 \\
&amp;\Rightarrow Var(T) = \frac{\bigg( \frac{\partial g (\theta)}{\partial \theta}\bigg)^2}{I_\theta}=CRLB
\end{align}
\]</span></p>
<ul>
<li>Example
<ul>
<li>Suppose that <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is a random sample form Geom(<span class="math inline">\(\theta\)</span>).</li>
<li>The pmf of <span class="math inline">\(X_i\)</span> is</li>
</ul></li>
</ul>
<p><span class="math display">\[
f_X(x_i|\theta) = (1-\theta)^{x_i-1}\theta I_{\{1,2,...\}}(x_i) = \frac{\theta}{1-\theta}I_{\{1,2,...\}}(x_i)\exp\{\ln(1-\theta)x_i\}
\]</span></p>
<ul>
<li>Accordingly, the distribution of <span class="math inline">\(X\)</span> belongs to the exponential family with</li>
</ul>
<p><span class="math display">\[
B(\theta) =\frac{\theta}{1-\theta}; \,\,\, h(x_i) = I_{\{1,2,...\}}(x_i);\,\,\, Q(\theta) = \ln(1-\theta); \,\,\, \text{and} \,\,\, R(x_i) =x_i
\]</span></p>
<ul>
<li>The score function for the entire sample is</li>
</ul>
<p><span class="math display">\[
\begin{align}
S(X,\theta) &amp;= n\frac{\partial \ln \frac{\theta}{1-\theta}}{\partial \theta}+T\frac{\partial \ln (1-\theta)}{\partial \theta} \\
&amp;= n \bigg(\frac{1}{\theta}+\frac{1}{1-\theta}\bigg)-\frac{T}{1-\theta}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(T=\sum^n_{i=1}X_i\)</span>.</p>
<ul>
<li>It follows that</li>
</ul>
<p><span class="math display">\[
E(T)=g(\theta) =\frac{n}{\theta};\,\,\, E(\frac{1}{n}T)=E(\bar X) = \frac{1}{\theta}
\]</span></p>
<p>and that <span class="math inline">\(T\)</span> is the minimum variance unbiased estimator of <span class="math inline">\(n/\theta\)</span>.</p>
<ul>
<li>Equivalently, <span class="math inline">\(T/n = \bar X\)</span> is the minimum variance unbiased estimator of <span class="math inline">\(1/\theta\)</span>.
<ul>
<li>The variance of <span class="math inline">\(T\)</span> can be obtained from the moment generating function.</li>
<li>The result is</li>
</ul></li>
</ul>
<p><span class="math display">\[
Var(X_i) = \frac{1-\theta}{\theta^2} \Longrightarrow Var(T/n) = \frac{1-\theta}{n\theta^2}
\]</span></p>
<ul>
<li>Another Exponential Family Result:
<ul>
<li>Suppose that the joint pdf (pmf) of <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> is <span class="math inline">\(f_X(x|\theta)\)</span>; <span class="math inline">\(T(X)\)</span> is a statistic that is unbiased for <span class="math inline">\(g(\theta)\)</span>; and <span class="math inline">\(Var(T)\)</span> attains the Cram´er-Rao lower bound.</li>
<li>Then <span class="math inline">\(T\)</span> is sufficient and the joint pdf (pmf) belongs to the one parameter exponential family.</li>
<li>Proof: If <span class="math inline">\(Var(T)\)</span> attains the Cram´er-Rao lower bound, then it must be true that <span class="math inline">\(\rho^2_{S,T}=1\)</span> and that</li>
</ul></li>
</ul>
<p><span class="math display">\[
S(X,\theta) = a(\theta)+b(\theta)T(X)
\]</span></p>
<p>for some functions <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<ul>
<li>Integrating <span class="math inline">\(S\)</span> with respect to <span class="math inline">\(\theta\)</span> gives</li>
</ul>
<p><span class="math display">\[
\int S(X,\theta)d\theta = \ln f_X(X|\theta) + K_1(X)
\]</span></p>
<p>for some function <span class="math inline">\(K_1(X)\)</span></p>
<p><span class="math display">\[
= \int a(\theta) +b(\theta)T(X)d\theta = A(\theta) +B(\theta)T(X)+K_2(X)
\]</span></p>
<p><span class="math display">\[
\Rightarrow f_X(X|\theta) = \exp\{A(\theta)\}\exp\{[K_2(X)-K_1(X)]\}\exp\{B(\theta)T(X)\}
\]</span></p>
<p>which shows that the distribution belongs to the exponential family and that <span class="math inline">\(T\)</span> is sufficient.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter8.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter10.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Mathematical Statistics.pdf", "Mathematical Statistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
