<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Discrete Random Variables | Mathematical Statistics</title>
  <meta name="description" content="This is a Mathematical Statistics" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Discrete Random Variables | Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Mathematical Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Discrete Random Variables | Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is a Mathematical Statistics" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2023-08-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="1" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1</b> Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability.html"><a href="probability.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>1.1</b> Sample Spaces and Events</a></li>
<li class="chapter" data-level="1.2" data-path="probability.html"><a href="probability.html#algebra-of-events"><i class="fa fa-check"></i><b>1.2</b> Algebra of Events</a></li>
<li class="chapter" data-level="1.3" data-path="probability.html"><a href="probability.html#experiments-with-symmetries"><i class="fa fa-check"></i><b>1.3</b> Experiments with Symmetries</a></li>
<li class="chapter" data-level="1.4" data-path="probability.html"><a href="probability.html#composition-of-experiments-counting-rules"><i class="fa fa-check"></i><b>1.4</b> Composition of Experiments: Counting Rules</a></li>
<li class="chapter" data-level="1.5" data-path="probability.html"><a href="probability.html#sampling-at-random"><i class="fa fa-check"></i><b>1.5</b> Sampling at Random</a></li>
<li class="chapter" data-level="1.6" data-path="probability.html"><a href="probability.html#binomial-multinomial-coefficients"><i class="fa fa-check"></i><b>1.6</b> Binomial &amp; Multinomial coefficients</a></li>
<li class="chapter" data-level="1.7" data-path="probability.html"><a href="probability.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="1.8" data-path="probability.html"><a href="probability.html#subjective-probability"><i class="fa fa-check"></i><b>1.8</b> Subjective Probability</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html"><i class="fa fa-check"></i><b>2</b> Discrete Random Variables</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="discrete-random-variables" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Discrete Random Variables<a href="discrete-random-variables.html#discrete-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ol style="list-style-type: decimal">
<li><p>Definition: A Random Variable is a characteristic of the outcome of an experiment.</p></li>
<li><p>Notation: Use capital letters to denote random variables (rvs). Example: <span class="math inline">\(X(\omega)\)</span> is a rv. Use small letters to denote a realization of the random variable.</p></li>
<li><p>Example: Consider the experiment of choosing a student at random from a classroom. Then <span class="math inline">\(\Omega\)</span> = <span class="math inline">\(\{\)</span>Jack, Dolores, <span class="math inline">\(\dots \}\)</span>. Let <span class="math inline">\(X(\omega)\)</span> be a characteristic of student <span class="math inline">\(\omega\)</span>. Then <span class="math inline">\(X(\omega)\)</span> is a rv.</p></li>
<li><p>Types of random variables</p>
<ol style="list-style-type: lower-alpha">
<li>Categorical versus Numerical<br />
● <span class="math inline">\(X(\omega)\)</span> = gender of selected student is a categorical random variable and <span class="math inline">\(X(\omega_2)\)</span> = <span class="math inline">\(x_2\)</span> = “female” is a realization of the random variable.<br />
● <span class="math inline">\(Y(\omega)\)</span> = age of selected student is a numerical random variable and <span class="math inline">\(Y(\omega_1)\)</span> = <span class="math inline">\(y_1\)</span> = 19.62 is a realization of the random variable.<br />
</li>
<li>Continuous versus Discrete<br />
● If the possible values of a rv are countable, then the rv is discrete.<br />
● If the possible values of a rv are contained in open subsets (or half open subsets) of the real line, then the rv is continuous.</li>
</ol></li>
</ol>
<p>2.1 Probability Functions</p>
<ol style="list-style-type: decimal">
<li><p>Definition: The probability function (p.f.) of a discrete rv assigns a probability to the event <span class="math inline">\(X(\omega)\)</span> = <span class="math inline">\(x\)</span>. The p.f. is denoted by <span class="math inline">\(f_X(x)\)</span> and is defined by<br />
<span class="math display">\[ f_X(x) \overset{\rm def}= \Pr[X(\omega) = x] = \displaystyle\sum_{X(\omega) = x} \Pr(\omega). \]</span><br />
This function also is called a probability mass function (pmf). The terminology pmf appears to be more often used than p.f., so I will use pmf rather than p.f.</p></li>
<li><p>The pmf can be an equation, a table, or a graph that shows how probability is assigned to possible values of the random variable.</p></li>
<li><p>The distribution of probabilities across all possible values is called the probability distribution. A probability distribution may be displayed as (a) a table, (b) a graph, or (c) an equation.</p></li>
<li><p>Examples</p>
<ol style="list-style-type: lower-alpha">
<li>Example: Roll a fair four sided die twice. The face values on the die are 1, 2, 3, and 4. The sample space is <span class="math inline">\(\Omega\)</span> = <span class="math inline">\(\{\)</span>(1, 2),(1, 2), . . . ,(4, 4)<span class="math inline">\(\}\)</span>. Note that <span class="math inline">\(\#(\Omega)\)</span> = 16 and each outcome <span class="math inline">\(\omega = (\omega_1, \omega_2)\)</span> is equally likely. Let <span class="math inline">\(X(\omega) = \max(\omega_1, \omega_2)\)</span>. Find the pmf of <span class="math inline">\(X\)</span>. Solution:</li>
</ol></li>
</ol>
<p align="center">
<img src="fig2/fig2_1.jpg" />
</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Example: Choose a baby name at random from <span class="math inline">\(\Omega\)</span> = <span class="math inline">\(\{\)</span>John, Josh, Thomas, William<span class="math inline">\(\}\)</span>. Let <span class="math inline">\(X(\omega)\)</span> =first letter of name. Then <span class="math inline">\(P(X = J) = 0.5\)</span>.</li>
</ol>
<ol start="5" style="list-style-type: decimal">
<li>Support of the distribution: The set of possible values of <span class="math inline">\(X\)</span> that have non-zero probability is called the support of the distribution. We will denote this set by <span class="math inline">\(S\)</span>. That is,</li>
</ol>
<p><span class="math display">\[ S = {x; f(x) ＞0}. \]</span><br />
The support of a random variable is analogous to the sample space of an experiment. Note, <span class="math inline">\(f_X(x)\)</span> is abbreviated as <span class="math inline">\(f(x)\)</span>. This convention will be followed if it is clear that the pmf <span class="math inline">\(f(x)\)</span> refers to the random variable <span class="math inline">\(X\)</span>.</p>
<ol start="6" style="list-style-type: decimal">
<li><p>Properties of a pmf<br />
● <span class="math inline">\(f(x) ≥ 0\)</span> for all <span class="math inline">\(x\)</span>. This property also can be written as <span class="math inline">\(f(x) ≥ 0 \ \forall x.\)</span><br />
● <span class="math inline">\(\displaystyle\sum_{x \in S} \Pr(X = x) = 1\)</span>.</p></li>
<li><p>Indicator Function:</p></li>
</ol>
<p><span class="math display">\[ I_A(a) = \begin{cases}
1 &amp; \text{ if } a \in A, \\
0 &amp; \text{otherwise.}
\end{cases} \]</span></p>
<ol start="8" style="list-style-type: decimal">
<li>Application of indicator function. Consider, again, the random variable <span class="math inline">\(X(\omega) = \max(\omega_1, \omega_2)\)</span>, where <span class="math inline">\((\omega_1, \omega_2)\)</span> is an outcome when rolling a fair four-sided die twice. The pmf of <span class="math inline">\(X\)</span> is</li>
</ol>
<p><span class="math display">\[ f_x(x) = \begin{cases}
\frac{2x-1}{16} &amp; \text{ if } x = 1, 2, 3, 4 \\
0 &amp; \text{otherwise.}
\end{cases} \\ = \frac{2x-1}{16}I_{\{1,2,3,4\}}(x) \]</span></p>
<p>2.2 Joint Distributions</p>
<ol style="list-style-type: decimal">
<li>Joint Probability Functions: Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be discrete random variables defined on <span class="math inline">\(S_X\)</span> and <span class="math inline">\(S_Y\)</span> , respectively. Then the joint pmf (or p.f.) of <span class="math inline">\((X, Y)\)</span> is defined as</li>
</ol>
<p><span class="math display">\[ f_{X, Y}(x, y) \overset{\rm def}= \Pr[X(\omega) = x, Y(\omega) = y] = \Pr(X = x, Y = y). \]</span><br />
Note, joint distributions can be extended from the bivariate case (above) to the general multivariate case. A joint pmf satisfies</p>
<p><span class="math display">\[ f(x, y) ≥ 0 \ \text{for all pairs} \ (x, y) \ \text{and} \\
  \displaystyle\sum_{(x, y) \in S} f(x, y) = 1, \ \text{where} \\
  S = S_X × S_Y = \{(u, \upsilon); \ u \in S_X, \ \upsilon \in S_Y \}. \]</span><br />
Note: The set <span class="math inline">\(S = S_X × S_Y\)</span> could include <span class="math inline">\((x, y)\)</span> pairs that have probability zero. If so, then the true support is a subset of <span class="math inline">\(S\)</span>.<br />
Example: Two way table for powerball. See problem 1-R11 on page 39. Let <span class="math inline">\(X(\omega)\)</span> = number of matches out of 5 on first drawing and <span class="math inline">\(Y(\omega)\)</span> = number of matches out of 1 on second drawing. Then</p>
<span class="math display">\[ f_{X, Y}(x, y) = P(X = x, Y = y) = \frac{\binom{5}{x}\binom{40}{5-x}\binom{1}{y}\binom{44}{1-y}}{\binom{45}{5}\binom{45}{1}}I_{\{0, 1, \dots, 5\}}(x)I_{\{0, 1\}}(y), \]</span><br />
where <span class="math inline">\(x = 0, . . . , 5\)</span> and <span class="math inline">\(y = 0, 1\)</span>. For other values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the probability is zero. These probabilities, multiplied by 54,979,155, are given below:<br />

<p align="center">
<img src="fig2/fig2_2.jpg" />
</p>
<p><br>
In decimal form, the probabilities are</p>
<p align="center">
<img src="fig2/fig2_3.jpg" />
</p>
<ol start="2" style="list-style-type: decimal">
<li>Marginal pmf: Sum the joint pmf over all other variables to obtain the marginal pmf of one random variable.
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(f_X(x) = \displaystyle\sum_{y \in S_Y} f(x, y)\)</span>.<br />
</li>
<li><span class="math inline">\(f_Y(y) = \displaystyle\sum_{x \in S_x} f(x, y)\)</span>.<br />
</li>
<li><span class="math inline">\(f_Z(z) = \displaystyle\sum_{x \in S_x}\displaystyle\sum_{y \in S_Y} f_{X, Y, Z}(x, y, z)\)</span>.</li>
</ol></li>
<li>Example 1: The marginal pmfs for the powerball problem are</li>
</ol>
<p><span class="math display">\[ f_X(x) = \displaystyle\sum_{y=0}^{1}f_{X, Y}(x, y) = P(X = x) = \frac{\binom{5}{x}\binom{40}{5-x}}{\binom{45}{5}} I_{\{0, 1, \dots, 5\}}(x) \ \text{and} \\
  f_Y(y) = \displaystyle\sum_{x=0}^{5}f_{X, Y}(x, y) = P(Y = y) = \frac{\binom{1}{y}\binom{44}{1-y}}{\binom{45}{1}} I_{\{0, 1\}}(y). \]</span><br />
The numerical values of these pmfs are displayed in the margins of the tables on page 26.</p>
<ol start="4" style="list-style-type: decimal">
<li>Example 2: Suppose that</li>
</ol>
<p><span class="math display">\[ f_{X, Y}(x, y) = \begin{cases}
\frac{2(i + 2j)}{3n(n + 1)^2} &amp; i = 0, 1, \dots, n \ \text{ and } j = 0, 1, \dots, n \\
0 &amp; \text{otherwise.}
\end{cases} \]</span></p>
<p>Use the result</p>
<p><span class="math display">\[ \displaystyle\sum_{i=0}^n i = \displaystyle\sum_{i=1}^n i = \frac{n(n + 1)}{2} \]</span><br />
to obtain</p>
<p><span class="math display">\[ f_{X}(i) = \begin{cases}
\frac{2(n + i)}{3n(n + 1)} &amp; i = 0, 1, \dots, n \\
0 &amp; \text{otherwise.}
\end{cases} \ \text{and} \\
  f_{Y}(j) = \begin{cases}
\frac{n + 4j}{3n(n + 1)} &amp; j = 0, 1, \dots, n \\
0 &amp; \text{otherwise.}
\end{cases} \\ \]</span></p>
<p>2.3 Conditional Probability</p>
<ol style="list-style-type: decimal">
<li>Definitions:</li>
</ol>
<p><span class="math display">\[ P(\omega|B) \overset{\rm def}= \begin{cases}
\frac{P(\omega)}{P(B)} &amp; \text{if} \ \omega \in B \\
0 &amp; \text{otherwise.}
\end{cases} \\
P(A|B) \overset{\rm def}= \frac{P(A \cap B)}{P(B)} \ \text{provided that} \ P(B) ＞ 0. \]</span></p>
<p>These quantities are read as “probability of <span class="math inline">\(\omega\)</span> given <span class="math inline">\(B\)</span>” and “probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>.” Think of <span class="math inline">\(B\)</span> as the new sample space and then re-scale <span class="math inline">\(P(\omega)\)</span> and <span class="math inline">\(P(A \cap B)\)</span> relative to <span class="math inline">\(P(B)\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Examples:<br />
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>I will do <span class="math inline">\(\#2.15\)</span> on page <span class="math inline">\(58\)</span> in class.<br />
</li>
<li>I will do <span class="math inline">\(\#2.17\)</span> on page <span class="math inline">\(58\)</span> in class.<br />
</li>
<li>Lets Make a Deal. Note: this is not the same game that is described on page <span class="math inline">\(79\)</span> of the text. An SUV is randomly placed behind one of three identical doors. Goats are placed behind the other two doors. You choose a door (say door 1) and will win the item behind the door after it is opened. Before your door is opened, however, Monte Hall reveals a goat behind one of the two remaining doors (either door 2 or door 3). If he reveals a goat behind door 2, then he gives you the option of switching from door 1 to door 3. If he reveals a goat behind door 3, then he gives you the option of switching from door 1 to door 2. To maximize the probability of winning the SUV, should you stick with your original choice or should you switch? Assume that Monte knows where the SUV is; he always reveals a goat; and he never reveals the content behind the door that you choose.<br />
Solution: Let <span class="math inline">\(C = i\)</span> (C for choose) be the event that your initial choice is door <span class="math inline">\(i\)</span>. Let <span class="math inline">\(S = i\)</span> (<span class="math inline">\(S\)</span> for SUV) be the event that the SUV is behind door <span class="math inline">\(i\)</span>. Let <span class="math inline">\(R = i\)</span> (<span class="math inline">\(R\)</span> for reveal) be the event that Monte reveals a goat behind door <span class="math inline">\(i\)</span>. Conditional on <span class="math inline">\(C = 1\)</span>, the table of joint probabilities for <span class="math inline">\((R, S)\)</span> is as follows</li>
</ol>
<p align="center">
<img src="fig2/fig2_4.jpg" />
</p>
<p>In the above table, the value of p1 must satisfy <span class="math inline">\(p_1 \in (0,\frac{1}{3})\)</span>. If Monte chooses a door at random when <span class="math inline">\(S = 1\)</span>, then <span class="math inline">\(p_1 = \frac{1}{6}\)</span>. Accordingly,</p>
<p><span class="math display">\[ P(S = 1|C = 1) = \frac{1}{3}, \\ P(S ≠ 1|C = 1) = 1 - P(S = 1|C = 1) = \frac{2}{3}. \]</span><br />
If your strategy is to stay with door 1, then you win the SUV with
probability <span class="math inline">\(\frac{1}{3}\)</span>. If your strategy is to switch, then you will win the SUV if <span class="math inline">\(S ≠ 1\)</span> because you always switch to the correct door. This event has probability <span class="math inline">\(\frac{2}{3}\)</span>. Therefore, the best strategy is to switch. For more information, go to <a href="http://math.rice.edu/∼ddonovan/montyurl.html" class="uri">http://math.rice.edu/∼ddonovan/montyurl.html</a>.</p>
<ol start="3" style="list-style-type: decimal">
<li>Multiplication Rule
<ol style="list-style-type: lower-alpha">
<li>Two events: <span class="math inline">\(P(E \cap F) = P(F|E)P(E) = P(E|F)P(F)\)</span>.<br />
</li>
<li>More than two events: <span class="math inline">\(P(\displaystyle\bigcap_{i=1}^{k} E_i) = P(E_1)\prod_{j=2}^{k}P(E_j|\bigcap_{i=1}^{j-1}E_j)\)</span>. For example, with 4 events,<br />
<span class="math inline">\(P(E_1, E_2, E_3, E_4) = P(E_1) × P(E_2|E_1) × P(E_3|E_1, E_2) × P(E_4|E_1, E_2, E_3)\)</span>.</li>
</ol></li>
<li>Applications of Multiplication rule
<ol style="list-style-type: lower-alpha">
<li>If samples are selected at random one at a time without replacement, then all sequences are equally likely.<br />
Proof: The number of distinct sequences of <span class="math inline">\(n\)</span> objects selected from <span class="math inline">\(N\)</span> objects is <span class="math inline">\((N)_n\)</span> Label the <span class="math inline">\(N\)</span> objects as <span class="math inline">\(o_1, o_2, \dots , o_N\)</span>. Label the first selection as <span class="math inline">\(S_1\)</span>, the second selection as <span class="math inline">\(S_2\)</span>, etc. Then</li>
</ol></li>
</ol>
<p><span class="math display">\[ (S_1 = o_{i_1}, S_2 = o_{i_2}, S_3 = o_{i_3}, \dots , S_n = o_{i_n}) \]</span></p>
<p>is a sequence provided that the subscripts <span class="math inline">\(i_1, i_2, \dots , i_n\)</span> are all distinct. For example, if <span class="math inline">\(N = 100\)</span> and <span class="math inline">\(n = 3\)</span>, then <span class="math inline">\((S_1 = o_{23}, S_2 = o_{14}, S_3 = o_{89})\)</span> is a sequence. Using the multiplication rule, the probability of a sequence can be written as follows:</p>
<p><span class="math display">\[ P(S_1 = o_{i_1}, S_2 = o_{i_2}, S_3 = o_{i_3}, \dots , S_n = o_{i_n}) \\
= P(S_1 = o_{i_1}) × P(S_2 = o_{i_2}|S_1 = o_{i_1}) × P(S_3 = o_{i_3}|S_1 = o_{i_1}|S_2 = o_{i_2}) \\
× \dots × P(S_n = o_{i_n}|S_1 = o_{i_1}, \dots, S_{n-1} = o_{i_{n-1}}) \\
= \frac{1}{N} × \frac{1}{N-1} × \frac{1}{N-2} × \dots × \frac{1}{N-n+1} = \frac{(N-n)!}{N!} = \frac{1}{(N)_n}. \]</span></p>
<p>Accordingly, all sequences are equally likely.<br />
(b) If samples are selected at random without replacement, then all combinations are equally likely.<br />
Proof: The unordered set</p>
<p><span class="math display">\[ (o_{i_1}, o_{i_2}, \dots, o_{i_n}) \]</span></p>
<p>is a combination provided that the subscripts <span class="math inline">\(i_1, i_2, \dots , i_n\)</span> are all distinct. The number of distinct combinations is <span class="math inline">\(\binom{N}{n}\)</span> and the objects in each combination can be ordered in <span class="math inline">\(n!\)</span> ways. Therefore, each combination corresponds to <span class="math inline">\(n!\)</span> sequences and</p>
<p><span class="math display">\[ P(o_{i_1}, o_{i_2}, \dots, o_{i_n}) = n! × (S_1 = o_{i_1}, S_2 = o_{i_2}, \dots , S_n = o_{i_n}) = \frac{n!}{(N)_n} = \frac{1}{\binom{N}{n}}. \]</span></p>
<p>Accordingly, each combination is equally likely.</p>
<ol start="5" style="list-style-type: decimal">
<li>Conditional pmf: Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be discrete random variables. Then,</li>
</ol>
<p><span class="math display">\[ f_{X|Y}(x|y) \overset{\rm def}= P(X = x| Y = y) = \frac{f_{X,Y}(x,y)}{f_Y(y)} = \frac{P(X = x, Y = y)}{P(Y = y)} \]</span></p>
<p>2.4 Bayes Theorem (Law of Inverse Probability)</p>
<p>Bayes Theorem answers the question—How do you express <span class="math inline">\(P(E|F)\)</span> in terms of <span class="math inline">\(P(F|E)\)</span>?</p>
<ol style="list-style-type: decimal">
<li>Bayes Theorem states that</li>
</ol>
<p><span class="math display">\[ P(E|F) = \frac{P(F|E)P(E)}{P(F|E)P(E) + P(F|E^c)P(E^c)}. \]</span></p>
<p>Proof:</p>
<p><span class="math display">\[ P(E|F) = \frac{P(E \cap F)}{P(F)} \ \text{by the definition of conditional probability} \\
= \frac{P(F|E)P(E)}{P(F)} \ \text{by the multiplication rule} \\
= \frac{P(F|E)P(E)}{P(F \cap E) + P(F \cap E^c)} \ \text{by the law of total probability} \\
= = \frac{P(F|E)P(E)}{P(F|E)P(E) + P(F|E^c)P(E^c)} \ \text{by the multiplication rule}. \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>More generally, Bayes Theorem states that if <span class="math inline">\(E_1, E_2, \dots , E_n\)</span> is a partition of
<span class="math inline">\(\Omega\)</span>, then</li>
</ol>
<p><span class="math display">\[ \Pr(E_k|F) = \frac{\Pr(F|E_k)\Pr(E_k)}{\displaystyle\sum_{i=1}^{n} \Pr(F|E_i)\Pr(E_i)}. \]</span></p>
<p>Furthermore, the conditional odds of <span class="math inline">\(E_i\)</span> to <span class="math inline">\(E_j\)</span> is</p>
<p><span class="math display">\[ \text{Odds of } E_i \text{ to } E_j \text{ conditional on } F = \frac{P(E_i|F)}{P(E_j|F)} = \frac{P(F|E_i)}{P(F|E_j)} × \frac{P(E_i)}{P(E_j)}. \]</span></p>
<p>2.5 Statistical Independence of Random Variables</p>
<ol style="list-style-type: decimal">
<li><p>Definition: Two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> , are independent if and only if (iff) <span class="math inline">\(f_{X,Y}(x, y) = f_X(x)f_Y(y)\)</span> for all <span class="math inline">\((x, y) \in S_{X,Y}\)</span>. To denote independence, we write <span class="math inline">\(X ㅛ Y\)</span>.</p></li>
<li><p>Definition: <span class="math inline">\(k\)</span> random variables, <span class="math inline">\(X_1, X_2, \dots , X_k\)</span>, are mutually independent iff <span class="math inline">\(f(x_1, \dots, x_k) = \displaystyle\prod_{i=1}^{k}f_i(x_i)\)</span> for all <span class="math inline">\((x_1, \dots, x_k) \in S_{X_1, \dots, X_k}\)</span>.</p></li>
<li><p>Example: Consider a random variable <span class="math inline">\(X\)</span> with pmf <span class="math inline">\(f_X(x)\)</span>. Let <span class="math inline">\(X_1, X_2, \dots , X_n\)</span> be a sequence of random variables obtained by sampling at random from <span class="math inline">\(f_X\)</span> . Then, <span class="math inline">\(X_1, \dots , X_n\)</span> are independent random variables and their joint distribution is <span class="math inline">\(f_{X_1, \dots, X_n}(x_1, \dots , x_n) = \prod_{i=1}^{n}f_X(x_i)\)</span>.</p></li>
<li><p>Independent Events. Let <span class="math inline">\(E_1, E_2, \dots , E_k\)</span> be events. A set of indicator random variables, <span class="math inline">\(X_1, \dots , X_k\)</span> can be defined as</p></li>
</ol>
<p><span class="math display">\[X_i = \begin{cases}
1 &amp; \text{if } E_i \text{ occurs, and } \\
0 &amp; \text{otherwise.}
\end{cases} \]</span></p>
<p>Then the events <span class="math inline">\(E_1, E_2, \dots , E_k\)</span> are mutually independent if and only if the indicator variables <span class="math inline">\(X_1, \dots , X_k\)</span> are mutually independent.</p>
<ol start="5" style="list-style-type: decimal">
<li>Result: Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be events. Then <span class="math inline">\(A ㅛ B\)</span> if and only if <span class="math inline">\(P(A \cap B) = P(A)P(B)\)</span>.<br />
Proof: Define the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as</li>
</ol>
<p><span class="math display">\[X_i = \begin{cases}
1 &amp; \text{if } A \text{ occurs, } \\
0 &amp; \text{otherwise}
\end{cases} \text{ and } Y = \begin{cases}
1 &amp; \text{if } B \text{ occurs, } \\
0 &amp; \text{otherwise.}
\end{cases}\]</span></p>
<p>First, assume that <span class="math inline">\(A ㅛ B\)</span>. Then</p>
<p><span class="math display">\[ A ㅛ B \Leftrightarrow X ㅛ Y \\
\Rightarrow P(A \cap B) = f_{X, Y}(1, 1) = f_X(1)f_Y(1) = P(A)P(B). \]</span></p>
<p>Second, assume that <span class="math inline">\(P(A \cap B) = P(A)P(B)\)</span>. Then,</p>
<p><span class="math display">\[ P(A \cap B) = P(A)P(B) \Rightarrow f_{X, Y}(1, 1) = f_X(1)f_Y(1). \]</span></p>
<p>Use this result to fill in the two-by-two table of joint and marginal probabilities:</p>
<p align="center">
<img src="fig2/fig2_5.jpg" />
</p>
<p align="center">
<img src="fig2/fig2_6.jpg" />
</p>
<p><span class="math display">\[ \Rightarrow X ㅛ Y \text{ because } f_{X, Y}(x, y) = f_X(x)f_Y(y) \text{ for } x = 0, 1, \ y = 0, 1 \\ \Rightarrow A ㅛ B \]</span></p>
<ol start="6" style="list-style-type: decimal">
<li>Example: : Roll two distinct fair 6-sided dice. Let <span class="math inline">\(E_1\)</span> be the event that the first die is odd, et <span class="math inline">\(E_2\)</span> be the event that the second die is even, and let <span class="math inline">\(E_3\)</span> be the event that there exactly one odd and one even die occur. Are these events mutually independent? Are there any pairs of events that are independent?</li>
</ol>
<p>2.6 Exchangeability</p>
<ol style="list-style-type: decimal">
<li><p>Definition: Two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> , are said to be exchangeable iff <span class="math inline">\(f_{X, Y}(x, y) = f_{X, Y}(y, x)\)</span> for all <span class="math inline">\((x, y) \in S_{X, Y}\)</span>. Note, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are exchangeable, then <span class="math inline">\(S_{X, Y} = S_{Y, X}\)</span>.</p></li>
<li><p>Definition: <span class="math inline">\(n\)</span> random variables, <span class="math inline">\(X_1, \dots , X_n\)</span> are said to be exchangeable iff <span class="math inline">\(f_{X_1, \dots, X_n}(x_1, \dots, x_n) = f_{X_1, \dots, X_n}(x^*_1, \dots, x^*_n)\)</span> for all <span class="math inline">\((x_1, \dots , x_n) \in S_{X_1, \dots ,X_n}\)</span> and for all <span class="math inline">\((x^*_1, \dots, x^*_n)\)</span>, where <span class="math inline">\((x^*_1, \dots, x^*_n)\)</span> is a permutation of <span class="math inline">\((x_1, \dots, x_n)\)</span>. Note, the equality must be satisfied for all <span class="math inline">\(n!\)</span> permutations.</p></li>
<li><p>Result: If <span class="math inline">\(X_1, \dots , X_n\)</span> are exchangeable, then the marginal distributions of each <span class="math inline">\(X_i\)</span> are identical. Also, the joint distributions of any subset of <span class="math inline">\(k \ Xs\)</span> is thesame as the distribution of any other set of <span class="math inline">\(k \ Xs\)</span>, where <span class="math inline">\(k\)</span> can be <span class="math inline">\(1, 2, \dots , n\)</span>.<br />
Proof that bivariate marginals are identical when 3 random variables are exchangeable: Recall, that the joint pmf of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is obtained from the joint pmf of <span class="math inline">\(X_1, X_2\)</span>, and <span class="math inline">\(X_3\)</span> as follows:</p></li>
</ol>
<p><span class="math display">\[ f_{X_1, X_2}(x_1, x_2) = \sum_{x_3 \in S_{X_3}}f_{X_1, X_2, X_3}(x_1, x_2, x_3). \]</span></p>
<p>If <span class="math inline">\(X_1, X_2\)</span>, and <span class="math inline">\(X_3\)</span> are exchangeable, then</p>
<p><span class="math display">\[ f_{X_1, X_2, X_3}(x_1, x_2, x_3) = f_{X_1, X_2, X_3}(x_1, x_3, x_2) and \\
f_{X_1, X_2}(x_1, x_2) = \sum_{x_3 \in S_{X_3}}f_{X_1, X_2, X_3}(x_1, x_3, x_2) = f_{X_1, X_3}(x_1, x_2) \]</span></p>
<p>Also,</p>
<p><span class="math display">\[ f_{X_1, X_2, X_3}(x_1, x_2, x_3) = f_{X_1, X_2, X_3}(x_3, x_1, x_2) and \\
f_{X_1, X_2}(x_1, x_2) = \sum_{x_3 \in S_{X_3}}f_{X_1, X_2, X_3}(x_3, x_1, x_2) = f_{X_2, X_3}(x_1, x_2) \]</span></p>
<p>Accordingly, exchangeability implies that</p>
<p><span class="math display">\[ f_{X_1, X_2}(x_1, x_2) = f_{X_1, X_3}(x_1, x_2) = f_{X_2, X_3}(x_1, x_2) \]</span></p>
<p>In the same manner, it can be shown that exchangeability implies that</p>
<p><span class="math display">\[ f_{X_1}(x_1) = f_{X_2}(x_1) = f_{X_3}(x_1) \]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><p>Example 1: If <span class="math inline">\(X_1, \dots , X_n\)</span> are independently and identically distributed (iid), then <span class="math inline">\(X_1, \dots , X_n\)</span> are exchangeable.</p></li>
<li><p>Example 2: Consider the procedure of sampling at random without replacement from a finite population of size <span class="math inline">\(N\)</span>. Let <span class="math inline">\(X_1, X_2, \dots , X_n\)</span> be the first, second, etc selection and let <span class="math inline">\(x_1, \dots , x_n\)</span> be the population units. The random variables are not independent but the random variables are exchangeable. For example</p></li>
</ol>
<p><span class="math display">\[ f_{X_1, X_2}(x_i, x_j) = f_{X_1}(x_i)f{X_2|X_1}(x_j|X_1 = x_i) \\
= \bigg(\frac{1}{N} \bigg) \bigg(\frac{1}{N-1} \bigg) = \frac{1}{N(N-1)} \text{ and } \\
f_{X_1, X_2}(x_j, x_i) = f_{X_1}(x_j)f{X_2|X_1}(x_i|X_1 = x_j) \\
= \bigg(\frac{1}{N} \bigg) \bigg(\frac{1}{N-1} \bigg) = \frac{1}{N(N-1)}. \]</span></p>
<p>2.7 Application: Probability of Winning in Craps</p>
<ol style="list-style-type: decimal">
<li><p>Recall, the rules of the game are as follows. Roll a pair of dice (i.e., two die). If the sum of the dice is 7 or 11, then the player wins and the game is over. If the sum of the dice is 2, 3, or 12, then the player loses and the game is over. If the sum of the dice is anything else, then the sum is called “the point” and the game continues. In this case, a player repeatedly rolls the pair of dice until either the sum is either 7 or equal to the point. If a 7 occurs first, then the player loses. If the point occurs first, then the player wins.</p></li>
<li><p>The sample space when rolling two dice is</p></li>
</ol>
<p><span class="math display">\[ \Omega = \{(1,1), (1,2), (2,1), \dots, (6,6) \}. \]</span></p>
<p>If the dice are fair, then the 36 outcomes are equally likely. Let <span class="math inline">\(Y(\omega)\)</span> be the sum of the two dice on the first roll. It is easy to show that the pmf for <span class="math inline">\(Y\)</span> is</p>
<p align="center">
<img src="fig2/fig2_7.jpg" />
</p>
<p>Alternatively,</p>
<p><span class="math display">\[ f_Y(y) = \frac{6 - |x - 7|}{36}I_{\{2, 3, \dots, 12\}}(x). \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Let <span class="math inline">\(X(\omega)\)</span> be the sum of the dice on the last roll of the game. Then the joint support for <span class="math inline">\((Y, X)\)</span> is</li>
</ol>
<p><span class="math display">\[ S_{Y, X} = { \{(2, 2),(3, 3),(4, 4),(4, 7),(5, 5),(5, 7),(6, 6),(6, 7),} \\
{(7, 7),(8, 8),(8, 7),(9, 9),(9, 7),(10, 10),(10, 7),(11, 11),(12, 12)\}}. \]</span></p>
<p>The winning <span class="math inline">\((Y, X)\)</span> values are</p>
<p><span class="math display">\[ (4, 4),(5, 5),(6, 6),(7, 7),(8, 8),(9, 9),(10, 10), \text{ and } (11, 11). \]</span></p>
<p>The losing (Y, X) values are</p>
<p><span class="math display">\[ (2, 2),(3, 3),(4, 7),(5, 7),(6, 7),(8, 7),(9, 7),(10, 7), \text{ and } ,(12, 12). \]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Suppose that the first roll yields a 4. Then the game continues until a 7 or another 4 is rolled. Denote a non-4, non-7 by <span class="math inline">\(N\)</span>. Then the game is won if a sequence such as <span class="math inline">\(\{4, 4\}, \{4, N, 4\}, \{4, N, N, 4\}, \{4, N, N, N, 4\}\)</span> etc is observed. Note that <span class="math inline">\(P(N) = 1 − P(4) − P(7) = 27/36\)</span>. In any case, the first roll is a 4 and the last role is a 4. That is, <span class="math inline">\(Y = 4\)</span> and <span class="math inline">\(X = 4\)</span>. The probability that <span class="math inline">\(Y = 4\)</span> and <span class="math inline">\(X = 4\)</span> can be computed as follows:</li>
</ol>
<p><span class="math display">\[ f_{Y, X}(4, 4) = P\{4, 4\} + P\{4, N, 4\} + P\{4, N, N, 4\} + P\{4, N, N, N, 4\} + \cdots \\
= \bigg(\frac{3}{36} \bigg)^2 + \bigg(\frac{3}{36} \bigg)^2\bigg(\frac{27}{36} \bigg) + \bigg(\frac{3}{36} \bigg)^2 \bigg(\frac{27}{36} \bigg)^2 + \bigg(\frac{3}{36} \bigg)^2 \bigg(\frac{27}{36} \bigg)^3 \cdots \\
= \bigg(\frac{3}{36} \bigg)^2 \displaystyle\sum_{i=0}^{\infty} \bigg(\frac{27}{36} \bigg)^i \\
= \bigg(\frac{3}{36} \bigg)^2 \frac{1}{1-\Big(\frac{27}{36} \Big)} \text{ by the geometric series result } \\
= \frac{1}{36}. \]</span></p>
<p>Furthermore, the conditional probability of winning, given that the point is 4 is</p>
<p><span class="math display">\[ f_{X|Y}(4|4) = \frac{f_{Y, X}(4, 4)}{f_Y(4)} = \frac{1/36}{3/36} = \frac{1}{3}. \]</span></p>
<p>Accordingly,</p>
<p><span class="math display">\[ f_{X|Y}(x|4) = \begin{cases}
\frac{1}{3} &amp; \text{if } x = 4 \\
\frac{2}{3} &amp; \text{if } x = 7 \\
0 &amp; \text{otherwise.}
\end{cases} .\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li>The probabilities <span class="math inline">\(f_{Y, X}(y, y)\)</span> are summarized in the following table. The probabilities that correspond to a win are summed. The result is</li>
</ol>
<p><span class="math display">\[ P(\text{Win}) = \frac{244}{495} = 0.5 − \frac{7}{990} ≈ 0.4929. \]</span></p>
<p align="center">
<img src="fig2/fig2_8.jpg" />
</p>
<!-------------------------------------->

</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Mathematical Statistics.pdf", "Mathematical Statistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
