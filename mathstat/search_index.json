[["chapter12.html", "Chapter 12 Appendix 12.1 Greek Alphabet 12.2 Abbreviations 12.3 PRACTICE EXAMS", " Chapter 12 Appendix 12.1 Greek Alphabet 12.2 Abbreviations BF: Bayes Factor. If \\(H\\) is a hypothesis and \\(T\\) is a sufficient statistic, then \\[ BF = \\frac{Posterior \\ odds \\ of \\ H}{Prior \\ odds \\ of \\ H}=\\frac{P(H|T = t)/P(H^c|T = t)}{P(H)/P(H^c)}=\\frac{f_{T|H}(t|H)}{f_{T|H^c} (t|H^c)} \\] CDF or cdf: Cumulative Distribution Function. If \\(X\\) is a random variable, then \\[ F_X(x) = P(X ≤ x) \\] is the cdf of \\(X\\). CLT: Central Limit Theorem. If \\(X_1, X_2, \\ldots , X_n\\) is a random sample of size \\(n\\) from a population with mean \\(\\mu X\\) and variance \\(\\sigma^2_X\\), then, the distribution of \\[ Z_n =\\frac{\\bar X-\\mu_X}{\\sigma_X /\\sqrt{n}} \\] converges to \\(N(0, 1)\\) as \\(n \\rightarrow \\infty\\). CRLB: Cramer-Rao Lower Bound. The CRLB is the lower bound on the variance of an unbiased estimator of \\(g(\\theta)\\). The bound is \\[ CRLB = \\frac{[\\frac{\\partial g(\\theta)}{\\partial \\theta}]^2}{I_\\theta}, \\] where \\(I_\\theta\\) is Fisher’s information. LR: Likelihood Ratio. When testing a simple null against a simple alternative, the LR is \\[ \\lambda =\\frac{f_0(x)}{f_1(x)}. \\] When testing a composite null against a composite alternative, the LR is \\[ \\lambda =\\frac{\\underset{\\theta \\in \\theta_0}{sup}f(x|\\theta)}{\\underset{\\theta \\in \\theta_a}{sup}f(x|\\theta)}, \\] where \\(\\theta_0\\) and \\(\\theta_a\\) are the parameter spaces under \\(H_0\\) and \\(H_a\\), respectively. LRT: Likelihood Ratio Test. The LRT of \\(H_0\\) versus \\(H_a\\) is to reject \\(H_0\\) for small values of the LR. The critical value is chosen so that the size of the test is \\(\\alpha\\). MGF or mgf: Moment Generating Function. If \\(X\\) is a random variable, then \\[ \\psi_X (t) = E(e^{tX}) \\] is the mgf of \\(X\\). MLE: Maximum Likelihood Estimator. Suppose that \\(X_i, X_2, \\ldots , X_n\\) is a random sample from \\(f_X(x|\\theta)\\), where \\(\\theta\\) is a \\(k \\times 1\\) vector of parameters. A maximum likelihood estimator of \\(\\theta\\) is any value \\(\\hat \\theta\\) that maximizes the likelihood function and is a point in the parameter space or on the boundary of the parameter space. MSE: Mean Square Error. If \\(T\\) is an estimator of a parameter, \\(\\theta\\), then \\[ MSE_T (\\theta) = E(T − \\theta)^2 = \\sigma^2_T + bias^2, \\] where bias = \\(E(T − \\theta)\\). PDF or pdf: Probability Density Function. If \\(X\\) is a continuous random variable, then \\[ \\frac{d}{dx} F_X(x)=f_X (x) \\] is the pdf of \\(X\\). PF or pf: Probability Function. If \\(X\\) is a discrete random variable, then \\[ P(X = x) = f_X(x) \\] is the pf of \\(X\\). The terms pf and pmf are interchangeable. PGF or pgf: Probability Generating Function. If \\(X\\) is a random variable, then \\[ \\eta_X(t) = E(t^X) \\] is the pgf of \\(X\\). The pgf is most useful for discrete random variables. PMF or pmf: Probability Mass Function. If \\(X\\) is a discrete random variable, then \\[ P(X = x) = f_X(x) \\] is the pmf of X. The terms pmf and pf are interchangeable. RV or rv: Random Variable. UMP Test: Uniformly Most Powerful Test. A UMP test of \\(H_0\\) against \\(H_a\\) is most powerful regardless of the value of the parameter under \\(H_0\\) and \\(H_a\\) 12.3 PRACTICE EXAMS Series and Limits \\[ \\begin{align} &amp;\\sum_{i=1}^{n}r^i=\\left\\{\\begin{matrix} \\frac{1-r^{n+1}}{1-r} \\ \\ if \\ r \\ne 1 &amp; \\\\ n+1 \\ \\ if \\ r=1 &amp; \\end{matrix}\\right. \\sum_{i=1}^{\\infty}r^i=\\left\\{\\begin{matrix} \\frac{1-r^{n+1}}{1-r} \\ \\ if \\ |r|&lt;1 &amp; &amp; \\\\ \\infty \\ \\ \\ \\ \\ \\ \\ \\ if \\ r&gt;1&amp; &amp; \\\\ undefined \\ \\ if \\ r&lt;-1 &amp; &amp; \\end{matrix}\\right.\\\\ &amp;(a + b)^n=\\sum_{i=0}^{n}\\binom{n}{i}a^ib^{n-i} \\ \\ \\ \\ \\ \\ ln(1 + ε)=-\\sum_{i=1}^{\\infty}\\frac{(−ε)^i}{i}if|ε|&lt;1 \\\\ &amp;ln(1 + ε) = ε + o(ε) \\ \\ if|ε|&lt;1 \\\\ &amp;\\underset{n\\rightarrow\\infty}{lim}(1 +\\frac{a}{n}+o(n^{-1}))^n=e^a \\ \\ \\ \\ \\ e^a=\\sum_{i=1}^{n}\\frac{a^i}{i^!} \\end{align} \\] Distribution of Selected Sums &amp; Expectations \\[ \\begin{align} &amp;X_i \\sim iid \\ Bern(\\theta) =⇒ E(X_i) = \\theta; \\ Var(X_i) = \\theta(1 − \\theta); \\ and \\ \\sum_{i=1}^{n} X_i \\sim Bin(n, \\theta)\\\\ &amp;X_i \\sim iid \\ Geom(\\theta) =⇒ E(X_i) = \\frac{1}{\\theta}; \\ Var(Xi) =\\frac{1 − \\theta}{\\theta^2}; \\ and \\ \\sum_{i=1}^{n} X_i \\sim NegBin(n, \\theta)\\\\ &amp;X_i \\sim iid \\ Poi(\\lambda) =⇒ E(X_i) = \\lambda; \\ Var(X_i) = \\lambda; \\ and \\ \\sum_{i=1}^{n} X_i \\sim Poi(n\\lambda)\\\\ &amp;X_i \\sim iid \\ Expon(\\lambda) =⇒ E(X_i) = \\frac{1}{\\lambda}; \\ Var(X_i) = \\frac{1}{\\lambda^2}; \\ and \\ \\sum_{i=1}^{n} X_i \\sim Gamma(n, \\lambda)\\\\ &amp;X_i \\sim iid \\ NegBin(k, \\theta) =⇒ E(X_i) = \\frac{k}{\\theta};\\ Var(X_i) = \\frac{k(1 − \\theta)}{\\theta^2}; \\ and \\sum_{i=1}^{n} X_i \\sim NegBin(n, \\theta)\\\\ \\end{align} \\] Suppose \\(X \\sim Gam(\\alpha, \\lambda)\\); \\[ f_X(x)=\\frac{x^{\\alpha−1} \\lambda^{\\alpha} e^{-\\lambda x}}{\\Gamma(\\alpha)}I_{(0,\\infty)}(x), \\] where \\(\\alpha &gt; 0\\) and \\(\\lambda &gt; 0\\). Verify that the mgf of \\(X\\) is \\[ \\psi_X (t)=(\\frac{\\lambda}{\\lambda-t})^\\alpha. \\] For what values of t does the mgf exist? Suppose that \\(W_1, \\ldots , W_n\\) is a random sample of size \\(n\\) from \\(Exp(\\lambda)\\); \\[ f_W (w) = \\lambda e^{−\\lambda w}I_{(0,\\infty)}(w), \\] where \\(\\lambda &gt; 0\\). Use mgfs to obtain the distribution of \\(Y =\\sum_{i=1}^{n} W_i\\). Hint: The mgf of \\(W\\) can be obtained from question #1 because the exponential distribution is a special case of the gamma distribution. Suppose that \\(X\\) is a random variable with mgf \\[ \\psi_X (t) = \\frac{1}{1-t}. \\] Give the pdf of \\(X\\). Derive an expression for \\(E(X^r)\\); \\(r = 0, 1, 2, \\ldots.\\) Suppose that \\(X \\sim N(\\mu_X, \\sigma^2_X)\\); \\(Y \\sim N(\\mu_Y, \\sigma^2_Y)\\); and that \\(X \\Perp Y\\). The mgf of \\(X\\) is \\[ \\psi_X(t) = exp\\left \\{t\\mu_X+\\frac{t^2\\sigma^2_X}{2}\\right \\} \\] C.3. EXAM 2 Verify that \\(E(X) = \\mu_X\\) and that \\(Var(X) = \\sigma^2_X\\). Prove that \\(X − Y \\sim N(\\mu_X − \\mu_Y , \\sigma^2_X + \\sigma^2_Y)\\). Suppose that \\(X \\sim LogN(\\mu, \\sigma^2)\\). Compute \\[ Pr(e^\\mu≤ X ≤ e^{\\mu+\\sigma}) \\] Let \\(W_i\\) for \\(i = 1, \\ldots , n\\) and \\(X_i\\) for \\(i = 1, \\ldots , m\\) be iid random variables, each with distribution \\(N(0, \\sigma^2)\\). Give the distribution of \\[ U =\\sum_{i=1}^{n}(\\frac{W_i}{\\sigma})^2 \\] Justify your answer. Hint: First give the distribution of \\(W_i/\\sigma\\). Give the distribution of \\[ V =(\\frac{m}{n})(\\frac{\\sum_{i=1}^{n}W^2_i}{\\sum_{i=1}^{m}X^2_i}) \\] Justify your answer. Suppose that \\(X_i\\) is a random sample of size \\(n\\) from an infinite sized population having mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar X\\) be the sample mean. Verify that \\(E(\\bar X) = \\mu\\) Verify that \\(Var(\\bar X) = \\sigma^2/n\\) Let \\(S^2\\) be the sample variance; \\[ S^2=\\frac{1}{n-1} \\sum_{i=1}^{n}(X_i-\\bar X)^2=\\frac{1}{n-1}[\\sum_{i=1}^{n}X^2_i-n\\bar X^2]. \\] Verify that \\(E(S^2) = \\sigma^2\\). Suppose that \\(X_i\\) is a random sample of size \\(n\\) from an infinite sized population having mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar X\\) be the sample mean. Verify that \\(E(\\bar X) = \\mu\\) Verify that \\(Var(\\bar X) = \\sigma^2/n.\\) Let \\(S^2\\) be the sample variance; \\[ S^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\bar X)^2=\\frac{1}{n-1}[\\sum_{i=1}^{n}X^2_i-n\\bar X^2]. \\] Verify that \\(E(S^2) = \\sigma^2\\). C.3 Exam 2 Suppose that \\(X_1, X_2, \\ldots , X_n\\) is a random sample of size \\(n\\) from \\(f_X (x|\\alpha, \\beta)\\), where \\[ f_X (x|\\alpha, \\beta) =\\frac{\\alpha\\beta^\\alpha}{x^{\\alpha+1}}I_{(\\beta,\\infty)}(x), \\] where \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\) are unknown parameters. This distribution is called the Pareto\\((\\alpha, \\beta)\\) distribution. Find a two dimensional sufficient statistic. Verify that the pdf of \\(X_{(1)}\\) is Pareto\\((n\\alpha, \\beta)\\). That is, \\[ f_{X(1)} (x|\\alpha, \\beta) = \\frac{n\\alpha\\beta^{n\\alpha}}{x^{n\\alpha+1}}I_{(\\beta,\\infty)}(x). \\] The joint sampling distribution of the sufficient statistics can be studied using simulation. Let \\(U_1, U_2, \\ldots , U_n\\) be a random sample from \\(Unif(0, 1)\\). Show how \\(U_i\\) can be transformed into a random variable having a \\(Pareto(\\alpha, \\beta)\\) distribution. Suppose that \\(X \\sim Gamma(\\alpha, \\lambda)\\), where \\(\\lambda\\) is known. Verify that the distribution of X belongs to the exponential family. Let \\(X_1, X_2, \\ldots , X_n\\) be a random sample from the \\(Gamma(\\alpha, \\lambda)\\) distribution, where \\(\\lambda\\) is known. Use the results from part (a) to find a sufficient statistic. Give the likelihood function that corresponds to part (b). \\[ f_{X|\\theta}(x|\\theta) = \\theta(1 − \\theta)^{x−1}I_{1,2,...}(x). \\] Verify that \\(T =\\sum_{i=1}^{n}\\) is a sufficient statistic. Verify that the conditional distribution \\(P(X = x|T = t)\\) does not depend on \\(\\theta\\). Suppose that the investigator’s prior beliefs about \\(\\theta\\) can be summarized as \\(\\theta \\sim Beta(\\alpha, \\beta)\\). Find the posterior distribution of \\(\\theta\\) and find the expectation of \\(\\theta\\) conditional on \\(T = t\\). Let \\(Z_1, Z_2, \\ldots , Z_k\\) be a sequence of future \\(Geom(\\theta)\\) random variables and let \\(Y=\\sum_{i=1}^{k} Z_i\\). Find the posterior predictive distribution of \\(Y\\) given \\(T\\). That is, find \\(f_{Y |T} (y|t)\\). Let \\(X_1, X_2, \\ldots , X_n\\) be a random sample of size \\(n\\) from a distribution having mean \\(\\mu\\), variance \\(\\sigma^2\\). Define \\(Z_n\\) as \\[ Z_n=\\frac{\\bar X − \\mu}{\\sigma/\\sqrt{n}}. \\] State the central limit theorem. Verify that \\[ Z_n =\\sum_{i=1}^{n}U_i,\\ where \\ U_i=\\frac{Z^∗_i}{\\sqrt{n}} \\ and \\ Z^∗_i=\\frac{X_i − \\mu}{\\sigma}. \\] C.4. EXAM 3 Assume that X has a moment generating function. Verify that \\[ \\psi_{Zn}(t) = [\\psi_{Ui}(t)]^n. \\] Verify that the mean and variance of \\(U_i\\) are 0 and \\(n^{−1}\\), respectively. Complete the proof of the central limit theorem. C.4 Exam 3 Let \\(X\\) be a random variable; let \\(h(X)\\) be a non-negative function whose expectation exists; and let k be any positive number. Chebyshev’s inequality reveals that \\[ P [h(X) ≥ k] ≤ \\frac{E [h(X)]}{k} \\] or, equivalently, that \\[ P [h(X) &lt; k] ≥ 1-\\frac{E [h(X)]}{k}. \\] Define what it means for an estimator Tn to be consistent for a parameter \\(\\theta\\). Use Chebyshev’s inequality to verify that \\(\\underset{n\\rightarrow\\infty}{lim} \\ MSE_{T_n}(\\theta)=0 \\Longrightarrow T_n \\xrightarrow[]{prob} \\theta\\). Suppose that \\(X_1, X_2, \\ldots , X_n\\) is a random sample from Bern\\((\\theta)\\). Give the likelihood function. Find a sufficient statistic. Verify that the score function is \\[ S(\\theta|X) =\\frac{\\sum_{i=1}^{n}X_i-n\\theta}{\\theta(1 − \\theta)} \\] Derive the MLE of \\(\\theta\\). Derive the MLE of \\(\\frac{1}{\\theta}\\). Derive Fisher’s information. Verify or refute the claim that the MLE of \\(\\theta\\) is the minimum variance unbiased estimator of \\(\\theta\\). Suppose that \\(X_i \\sim iid\\) \\(Expon(\\lambda)\\) for \\(i = 1, \\ldots , n\\). It can be shown that \\(Y =\\sum_{i=1}^{n}Xi\\) is sufficient and that \\(Y \\sim Gamma(n, \\lambda)\\). Derive the moment generating function of \\(Q = 2\\lambda \\ \\sum_{i=1}^{n} Xi\\) and verify that \\(Q\\) is a pivotal quantity. Use the moment generating function of \\(Q\\) to determine its distribution. Use \\(Q\\) to find a \\(100(1 − \\alpha)\\%\\) confidence interval for \\(\\lambda\\). Suppose that \\(X_1, X_2, \\ldots , X_n\\) is a random sample from \\(f_X (x|\\theta)\\), where \\[ f_X (x|\\theta) = \\theta x^{\\theta−1} I_{(0,1)}(x) \\ and \\ \\theta &gt; 0. \\] Verify or refute the claim that the distribution of \\(X\\) belongs to the exponential class. Find the most powerful test of \\(H_0 : \\theta = \\theta_0\\) versus \\(H_a : \\theta = \\theta_a\\), where \\(\\theta_a &gt; \\theta_0\\). Find the most uniformly powerful test of \\(H_0 : \\theta = \\theta_0\\) versus \\(H_a : \\theta &gt; \\theta_0\\). Suppose that the investigator’s prior beliefs about \\(\\theta\\) can be summarized as \\(\\theta \\sim Gamma(\\alpha, \\lambda)\\). Find the posterior distribution of \\(\\theta\\). Hint: write xi as \\(x_i = e^{ln(x_i)}\\). Find the Bayes estimator of \\(\\theta\\) based on a squared error loss function. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
