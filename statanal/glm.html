<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 일반화 선형모형 | Statistical Analysis</title>
  <meta name="description" content="This is a Statistical Analysis" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 일반화 선형모형 | Statistical Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Statistical Analysis" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 일반화 선형모형 | Statistical Analysis" />
  
  <meta name="twitter:description" content="This is a Statistical Analysis" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2023-02-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression.html"/>
<link rel="next" href="psm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>머리말</a></li>
<li class="chapter" data-level="1" data-path="ttest.html"><a href="ttest.html"><i class="fa fa-check"></i><b>1</b> 평균차이 검정</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ttest.html"><a href="ttest.html#일표본-t-검정"><i class="fa fa-check"></i><b>1.1</b> 일표본 <span class="math inline">\(t\)</span>-검정</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="ttest.html"><a href="ttest.html#with-r"><i class="fa fa-check"></i><b>1.1.1</b> With R</a></li>
<li class="chapter" data-level="1.1.2" data-path="ttest.html"><a href="ttest.html#with-sas"><i class="fa fa-check"></i><b>1.1.2</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ttest.html"><a href="ttest.html#독립표본-t-검정"><i class="fa fa-check"></i><b>1.2</b> 독립표본 <span class="math inline">\(t\)</span>-검정</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ttest.html"><a href="ttest.html#두-모분산-비의-검정"><i class="fa fa-check"></i><b>1.2.1</b> 두 모분산 비의 검정</a></li>
<li class="chapter" data-level="1.2.2" data-path="ttest.html"><a href="ttest.html#with-r-1"><i class="fa fa-check"></i><b>1.2.2</b> With R</a></li>
<li class="chapter" data-level="1.2.3" data-path="ttest.html"><a href="ttest.html#with-sas-1"><i class="fa fa-check"></i><b>1.2.3</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ttest.html"><a href="ttest.html#대응표본-t-검정"><i class="fa fa-check"></i><b>1.3</b> 대응표본 <span class="math inline">\(t\)</span>-검정</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ttest.html"><a href="ttest.html#with-r-2"><i class="fa fa-check"></i><b>1.3.1</b> With R</a></li>
<li class="chapter" data-level="1.3.2" data-path="ttest.html"><a href="ttest.html#with-sas-2"><i class="fa fa-check"></i><b>1.3.2</b> with SAS</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>2</b> 분산분석</a>
<ul>
<li class="chapter" data-level="2.1" data-path="anova.html"><a href="anova.html#분산분석"><i class="fa fa-check"></i><b>2.1</b> 분산분석</a></li>
<li class="chapter" data-level="2.2" data-path="anova.html"><a href="anova.html#일원배치-분산분석"><i class="fa fa-check"></i><b>2.2</b> 일원배치 분산분석</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="anova.html"><a href="anova.html#사후분석다중검정"><i class="fa fa-check"></i><b>2.2.1</b> 사후분석(다중검정)</a></li>
<li class="chapter" data-level="2.2.2" data-path="anova.html"><a href="anova.html#with-r-3"><i class="fa fa-check"></i><b>2.2.2</b> With R</a></li>
<li class="chapter" data-level="2.2.3" data-path="anova.html"><a href="anova.html#with-sas-3"><i class="fa fa-check"></i><b>2.2.3</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="anova.html"><a href="anova.html#이원배치-분산분석"><i class="fa fa-check"></i><b>2.3</b> 이원배치 분산분석</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="anova.html"><a href="anova.html#교호작용"><i class="fa fa-check"></i><b>2.3.1</b> 교호작용</a></li>
<li class="chapter" data-level="2.3.2" data-path="anova.html"><a href="anova.html#with-r-4"><i class="fa fa-check"></i><b>2.3.2</b> With R</a></li>
<li class="chapter" data-level="2.3.3" data-path="anova.html"><a href="anova.html#with-sas-4"><i class="fa fa-check"></i><b>2.3.3</b> With SAS</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>3</b> 비모수 검정</a>
<ul>
<li class="chapter" data-level="3.1" data-path="nonpara.html"><a href="nonpara.html#정규성-검정"><i class="fa fa-check"></i><b>3.1</b> 정규성 검정</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="nonpara.html"><a href="nonpara.html#with-r-5"><i class="fa fa-check"></i><b>3.1.1</b> With R</a></li>
<li class="chapter" data-level="3.1.2" data-path="nonpara.html"><a href="nonpara.html#with-sas-5"><i class="fa fa-check"></i><b>3.1.2</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="nonpara.html"><a href="nonpara.html#윌콕슨-부호순위-검정"><i class="fa fa-check"></i><b>3.2</b> 윌콕슨 부호순위 검정</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="nonpara.html"><a href="nonpara.html#with-r-6"><i class="fa fa-check"></i><b>3.2.1</b> With R</a></li>
<li class="chapter" data-level="3.2.2" data-path="nonpara.html"><a href="nonpara.html#with-sas-6"><i class="fa fa-check"></i><b>3.2.2</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="nonpara.html"><a href="nonpara.html#윌콕슨-순위합-검정"><i class="fa fa-check"></i><b>3.3</b> 윌콕슨 순위합 검정</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="nonpara.html"><a href="nonpara.html#with-r-7"><i class="fa fa-check"></i><b>3.3.1</b> With R</a></li>
<li class="chapter" data-level="3.3.2" data-path="nonpara.html"><a href="nonpara.html#with-sas-7"><i class="fa fa-check"></i><b>3.3.2</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="nonpara.html"><a href="nonpara.html#크루스칼-왈리스-검정"><i class="fa fa-check"></i><b>3.4</b> 크루스칼-왈리스 검정</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="nonpara.html"><a href="nonpara.html#with-r-8"><i class="fa fa-check"></i><b>3.4.1</b> With R</a></li>
<li class="chapter" data-level="3.4.2" data-path="nonpara.html"><a href="nonpara.html#with-sas-8"><i class="fa fa-check"></i><b>3.4.2</b> With SAS</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="category.html"><a href="category.html"><i class="fa fa-check"></i><b>4</b> 분할표 검정</a>
<ul>
<li class="chapter" data-level="4.1" data-path="category.html"><a href="category.html#상대위험률과-오즈비"><i class="fa fa-check"></i><b>4.1</b> 상대위험률과 오즈비</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="category.html"><a href="category.html#상대위험률"><i class="fa fa-check"></i><b>4.1.1</b> 상대위험률</a></li>
<li class="chapter" data-level="4.1.2" data-path="category.html"><a href="category.html#오즈비"><i class="fa fa-check"></i><b>4.1.2</b> 오즈비</a></li>
<li class="chapter" data-level="4.1.3" data-path="category.html"><a href="category.html#with-r-9"><i class="fa fa-check"></i><b>4.1.3</b> With R</a></li>
<li class="chapter" data-level="4.1.4" data-path="category.html"><a href="category.html#with-sas-9"><i class="fa fa-check"></i><b>4.1.4</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="category.html"><a href="category.html#독립성-검정"><i class="fa fa-check"></i><b>4.2</b> 독립성 검정</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="category.html"><a href="category.html#with-r-10"><i class="fa fa-check"></i><b>4.2.1</b> With R</a></li>
<li class="chapter" data-level="4.2.2" data-path="category.html"><a href="category.html#with-sas-10"><i class="fa fa-check"></i><b>4.2.2</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="category.html"><a href="category.html#동질성-검정"><i class="fa fa-check"></i><b>4.3</b> 동질성 검정</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="category.html"><a href="category.html#with-r-11"><i class="fa fa-check"></i><b>4.3.1</b> With R</a></li>
<li class="chapter" data-level="4.3.2" data-path="category.html"><a href="category.html#with-sas-11"><i class="fa fa-check"></i><b>4.3.2</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="category.html"><a href="category.html#fisher의-정확-검정"><i class="fa fa-check"></i><b>4.4</b> Fisher의 정확 검정</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="category.html"><a href="category.html#with-r-12"><i class="fa fa-check"></i><b>4.4.1</b> With R</a></li>
<li class="chapter" data-level="4.4.2" data-path="category.html"><a href="category.html#with-sas-12"><i class="fa fa-check"></i><b>4.4.2</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="category.html"><a href="category.html#맥니마-검정"><i class="fa fa-check"></i><b>4.5</b> 맥니마 검정</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="category.html"><a href="category.html#이론적-배경"><i class="fa fa-check"></i><b>4.5.1</b> 이론적 배경</a></li>
<li class="chapter" data-level="4.5.2" data-path="category.html"><a href="category.html#with-r-13"><i class="fa fa-check"></i><b>4.5.2</b> With R</a></li>
<li class="chapter" data-level="4.5.3" data-path="category.html"><a href="category.html#with-sas-13"><i class="fa fa-check"></i><b>4.5.3</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="category.html"><a href="category.html#cochran-mantel-haenzel-test"><i class="fa fa-check"></i><b>4.6</b> Cochran-Mantel-Haenzel test</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="category.html"><a href="category.html#with-r-14"><i class="fa fa-check"></i><b>4.6.1</b> With R</a></li>
<li class="chapter" data-level="4.6.2" data-path="category.html"><a href="category.html#with-sas-14"><i class="fa fa-check"></i><b>4.6.2</b> With SAS</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="category.html"><a href="category.html#진단법의-평가"><i class="fa fa-check"></i><b>4.7</b> 진단법의 평가</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="category.html"><a href="category.html#민감도와-특이도"><i class="fa fa-check"></i><b>4.7.1</b> 민감도와 특이도</a></li>
<li class="chapter" data-level="4.7.2" data-path="category.html"><a href="category.html#양성예측도와-음성예측도"><i class="fa fa-check"></i><b>4.7.2</b> 양성예측도와 음성예측도</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>5</b> 회귀분석</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression.html"><a href="regression.html#상관분석"><i class="fa fa-check"></i><b>5.1</b> 상관분석</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="regression.html"><a href="regression.html#상관계수"><i class="fa fa-check"></i><b>5.1.1</b> 상관계수</a></li>
<li class="chapter" data-level="5.1.2" data-path="regression.html"><a href="regression.html#스피어만-상관계수"><i class="fa fa-check"></i><b>5.1.2</b> 스피어만 상관계수</a></li>
<li class="chapter" data-level="5.1.3" data-path="regression.html"><a href="regression.html#켄달의-타우"><i class="fa fa-check"></i><b>5.1.3</b> 켄달의 타우</a></li>
<li class="chapter" data-level="5.1.4" data-path="regression.html"><a href="regression.html#편상관계수"><i class="fa fa-check"></i><b>5.1.4</b> 편상관계수</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="regression.html"><a href="regression.html#단순선형회귀분석"><i class="fa fa-check"></i><b>5.2</b> 단순선형회귀분석</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="regression.html"><a href="regression.html#최소제곱법"><i class="fa fa-check"></i><b>5.2.1</b> 최소제곱법</a></li>
<li class="chapter" data-level="5.2.2" data-path="regression.html"><a href="regression.html#회귀계수-추정"><i class="fa fa-check"></i><b>5.2.2</b> 회귀계수 추정</a></li>
<li class="chapter" data-level="5.2.3" data-path="regression.html"><a href="regression.html#회귀계수-가설검정"><i class="fa fa-check"></i><b>5.2.3</b> 회귀계수 가설검정</a></li>
<li class="chapter" data-level="5.2.4" data-path="regression.html"><a href="regression.html#결정계수"><i class="fa fa-check"></i><b>5.2.4</b> 결정계수</a></li>
<li class="chapter" data-level="5.2.5" data-path="regression.html"><a href="regression.html#예제"><i class="fa fa-check"></i><b>5.2.5</b> 예제</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="regression.html"><a href="regression.html#다중선형회귀분석"><i class="fa fa-check"></i><b>5.3</b> 다중선형회귀분석</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="regression.html"><a href="regression.html#변수-선택"><i class="fa fa-check"></i><b>5.3.1</b> 변수 선택</a></li>
<li class="chapter" data-level="5.3.2" data-path="regression.html"><a href="regression.html#결정계수-1"><i class="fa fa-check"></i><b>5.3.2</b> 결정계수</a></li>
<li class="chapter" data-level="5.3.3" data-path="regression.html"><a href="regression.html#표준화-회귀계수"><i class="fa fa-check"></i><b>5.3.3</b> 표준화 회귀계수</a></li>
<li class="chapter" data-level="5.3.4" data-path="regression.html"><a href="regression.html#다중공선성"><i class="fa fa-check"></i><b>5.3.4</b> 다중공선성</a></li>
<li class="chapter" data-level="5.3.5" data-path="regression.html"><a href="regression.html#더미변수가변수"><i class="fa fa-check"></i><b>5.3.5</b> 더미변수(가변수)</a></li>
<li class="chapter" data-level="5.3.6" data-path="regression.html"><a href="regression.html#회귀모형에-대한-가정-검토"><i class="fa fa-check"></i><b>5.3.6</b> 회귀모형에 대한 가정 검토</a></li>
<li class="chapter" data-level="5.3.7" data-path="regression.html"><a href="regression.html#예제-1"><i class="fa fa-check"></i><b>5.3.7</b> 예제</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>6</b> 일반화 선형모형</a>
<ul>
<li class="chapter" data-level="6.1" data-path="glm.html"><a href="glm.html#the-generalized-linear-models"><i class="fa fa-check"></i><b>6.1</b> The Generalized Linear models</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="glm.html"><a href="glm.html#the-general-linear-model"><i class="fa fa-check"></i><b>6.1.1</b> The General Linear Model</a></li>
<li class="chapter" data-level="6.1.2" data-path="glm.html"><a href="glm.html#generalized-linear-models-glms"><i class="fa fa-check"></i><b>6.1.2</b> Generalized Linear Models (GLMs)</a></li>
<li class="chapter" data-level="6.1.3" data-path="glm.html"><a href="glm.html#normal-glms-as-a-special-case"><i class="fa fa-check"></i><b>6.1.3</b> Normal GLMs as a Special Case</a></li>
<li class="chapter" data-level="6.1.4" data-path="glm.html"><a href="glm.html#modelling-binomial-data"><i class="fa fa-check"></i><b>6.1.4</b> Modelling Binomial Data</a></li>
<li class="chapter" data-level="6.1.5" data-path="glm.html"><a href="glm.html#modelling-poisson-data"><i class="fa fa-check"></i><b>6.1.5</b> Modelling Poisson Data</a></li>
<li class="chapter" data-level="6.1.6" data-path="glm.html"><a href="glm.html#transformation-vs.-glms"><i class="fa fa-check"></i><b>6.1.6</b> Transformation vs. GLMs</a></li>
<li class="chapter" data-level="6.1.7" data-path="glm.html"><a href="glm.html#exponential-family"><i class="fa fa-check"></i><b>6.1.7</b> Exponential Family</a></li>
<li class="chapter" data-level="6.1.8" data-path="glm.html"><a href="glm.html#canonical-links"><i class="fa fa-check"></i><b>6.1.8</b> Canonical Links</a></li>
<li class="chapter" data-level="6.1.9" data-path="glm.html"><a href="glm.html#estimation-of-the-model-parameters"><i class="fa fa-check"></i><b>6.1.9</b> Estimation of the Model Parameters</a></li>
<li class="chapter" data-level="6.1.10" data-path="glm.html"><a href="glm.html#distribution-and-link-function"><i class="fa fa-check"></i><b>6.1.10</b> Distribution and Link Function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="glm.html"><a href="glm.html#medical-cost-data-analysis"><i class="fa fa-check"></i><b>6.2</b> Medical Cost Data Analysis</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="glm.html"><a href="glm.html#health-care-cost"><i class="fa fa-check"></i><b>6.2.1</b> health care cost</a></li>
<li class="chapter" data-level="6.2.2" data-path="glm.html"><a href="glm.html#regression-analysis-with-log-transformation"><i class="fa fa-check"></i><b>6.2.2</b> Regression analysis with log transformation</a></li>
<li class="chapter" data-level="6.2.3" data-path="glm.html"><a href="glm.html#gamma-generalized-linear-models"><i class="fa fa-check"></i><b>6.2.3</b> Gamma generalized linear models</a></li>
<li class="chapter" data-level="6.2.4" data-path="glm.html"><a href="glm.html#transformation-vs.-glms-1"><i class="fa fa-check"></i><b>6.2.4</b> Transformation vs. GLMs</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="glm.html"><a href="glm.html#binary-and-categorical-data-analysis"><i class="fa fa-check"></i><b>6.3</b> Binary and Categorical data Analysis</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="glm.html"><a href="glm.html#overview-of-logistic-regression-model"><i class="fa fa-check"></i><b>6.3.1</b> Overview of Logistic Regression Model</a></li>
<li class="chapter" data-level="6.3.2" data-path="glm.html"><a href="glm.html#introduction"><i class="fa fa-check"></i><b>6.3.2</b> Introduction</a></li>
<li class="chapter" data-level="6.3.3" data-path="glm.html"><a href="glm.html#binary-outcomes"><i class="fa fa-check"></i><b>6.3.3</b> Binary Outcomes</a></li>
<li class="chapter" data-level="6.3.4" data-path="glm.html"><a href="glm.html#two-way-tables"><i class="fa fa-check"></i><b>6.3.4</b> Two-Way Tables</a></li>
<li class="chapter" data-level="6.3.5" data-path="glm.html"><a href="glm.html#linear-regression-with-a-binary-outcome"><i class="fa fa-check"></i><b>6.3.5</b> Linear Regression with a Binary Outcome</a></li>
<li class="chapter" data-level="6.3.6" data-path="glm.html"><a href="glm.html#logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3.7" data-path="glm.html"><a href="glm.html#interpretation-of-a-logistic-regression"><i class="fa fa-check"></i><b>6.3.7</b> Interpretation of a Logistic Regression</a></li>
<li class="chapter" data-level="6.3.8" data-path="glm.html"><a href="glm.html#interpretation-on-the-probability-scale"><i class="fa fa-check"></i><b>6.3.8</b> Interpretation on the Probability Scale</a></li>
<li class="chapter" data-level="6.3.9" data-path="glm.html"><a href="glm.html#model-building-and-assessment"><i class="fa fa-check"></i><b>6.3.9</b> Model Building and Assessment</a></li>
<li class="chapter" data-level="6.3.10" data-path="glm.html"><a href="glm.html#multinomial-regression"><i class="fa fa-check"></i><b>6.3.10</b> Multinomial Regression</a></li>
<li class="chapter" data-level="6.3.11" data-path="glm.html"><a href="glm.html#ordered-multinomial-regression"><i class="fa fa-check"></i><b>6.3.11</b> Ordered Multinomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="glm.html"><a href="glm.html#poisson-regression"><i class="fa fa-check"></i><b>6.4</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="glm.html"><a href="glm.html#example-danish-lung-cancer-counts"><i class="fa fa-check"></i><b>6.4.1</b> Example: Danish Lung Cancer Counts</a></li>
<li class="chapter" data-level="6.4.2" data-path="glm.html"><a href="glm.html#what-about-accounting-for-population-size"><i class="fa fa-check"></i><b>6.4.2</b> What About Accounting for Population Size?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="psm.html"><a href="psm.html"><i class="fa fa-check"></i><b>7</b> 성향점수(propensity score)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="psm.html"><a href="psm.html#observational-study"><i class="fa fa-check"></i><b>7.1</b> Observational Study?</a>
<ul>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#randomized-controlled-trials-gold-standard"><i class="fa fa-check"></i>Randomized Controlled Trials ("gold standard")</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#characteristics-of-rcts"><i class="fa fa-check"></i>Characteristics of RCTs</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#what-is-observational-data"><i class="fa fa-check"></i>What is Observational Data?</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#confounding-due-to-selection-bias"><i class="fa fa-check"></i>Confounding due to Selection Bias</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#impact-of-selection-bias-on-analytic-inferences"><i class="fa fa-check"></i>Impact of Selection Bias on Analytic Inferences</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#statistical-adjustment-for-observational-data"><i class="fa fa-check"></i>Statistical Adjustment for Observational Data</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#addressing-selection-bias-with-exact-matching"><i class="fa fa-check"></i>Addressing Selection Bias with Exact Matching</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#matching-on-specific-variables"><i class="fa fa-check"></i>Matching on Specific Variables</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#matching-on-specific-variables-1"><i class="fa fa-check"></i>Matching on Specific Variables</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="psm.html"><a href="psm.html#propensity-score"><i class="fa fa-check"></i><b>7.2</b> Propensity Score?</a>
<ul>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#general-procedure"><i class="fa fa-check"></i>General Procedure</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#choosing-variables-for-propensity-scores"><i class="fa fa-check"></i>Choosing Variables for Propensity Scores</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#calculate-propensity-score"><i class="fa fa-check"></i>Calculate Propensity Score</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#initial-checks-balance-across-groups"><i class="fa fa-check"></i>Initial Checks: Balance across Groups</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#check-balance-of-covariates"><i class="fa fa-check"></i>Check Balance of Covariates</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#assess-balance-with-standardized-differences"><i class="fa fa-check"></i>Assess Balance with Standardized Differences</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#balance-of-covariates-caution"><i class="fa fa-check"></i>Balance of Covariates: Caution</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#matching-and-weighting-strategies"><i class="fa fa-check"></i>Matching and Weighting Strategies</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#choices-when-matching-sample-by-propensity-score"><i class="fa fa-check"></i>Choices When Matching Sample by Propensity Score</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#which-strategy-to-chooose"><i class="fa fa-check"></i>Which Strategy to Chooose?</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#evaluate-balance-in-sample-matched-or-weighted"><i class="fa fa-check"></i>Evaluate Balance in Sample Matched or Weighted</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#visualization-of-standardized-differences"><i class="fa fa-check"></i>Visualization of Standardized Differences</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#plots-of-covariates-between-groups"><i class="fa fa-check"></i>Plots of Covariates between Groups</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="psm.html"><a href="psm.html#propensity-score-methods"><i class="fa fa-check"></i><b>7.3</b> Propensity Score Methods</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="psm.html"><a href="psm.html#propensity-score-matching"><i class="fa fa-check"></i><b>7.3.1</b> Propensity Score Matching</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#general-procedure-for-propensity-score-matching"><i class="fa fa-check"></i>General Procedure for Propensity Score Matching</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#greedy-matching"><i class="fa fa-check"></i>Greedy Matching</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#mahalanobis-metric-matching"><i class="fa fa-check"></i>Mahalanobis Metric Matching</a></li>
<li class="chapter" data-level="7.3.2" data-path="psm.html"><a href="psm.html#stratification-using-propensity-score"><i class="fa fa-check"></i><b>7.3.2</b> Stratification using Propensity Score</a></li>
<li class="chapter" data-level="7.3.3" data-path="psm.html"><a href="psm.html#inverse-probability-of-treatment-weighting"><i class="fa fa-check"></i><b>7.3.3</b> Inverse Probability of Treatment Weighting</a></li>
<li class="chapter" data-level="7.3.4" data-path="psm.html"><a href="psm.html#propensity-score-adjustment-as-covariate"><i class="fa fa-check"></i><b>7.3.4</b> Propensity Score Adjustment as Covariate</a></li>
<li class="chapter" data-level="7.3.5" data-path="psm.html"><a href="psm.html#property-of-propensity-score"><i class="fa fa-check"></i><b>7.3.5</b> Property of Propensity Score</a></li>
<li class="chapter" data-level="" data-path="psm.html"><a href="psm.html#property-of-observational-studies"><i class="fa fa-check"></i>Property of Observational Studies</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="psm.html"><a href="psm.html#practice---sas"><i class="fa fa-check"></i><b>7.4</b> Practice - SAS</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="psm.html"><a href="psm.html#greedy-nearest-neighbor-matching"><i class="fa fa-check"></i><b>7.4.1</b> Greedy Nearest Neighbor Matching</a></li>
<li class="chapter" data-level="7.4.2" data-path="psm.html"><a href="psm.html#propensity-score-stratification"><i class="fa fa-check"></i><b>7.4.2</b> Propensity Score Stratification</a></li>
<li class="chapter" data-level="7.4.3" data-path="psm.html"><a href="psm.html#inverse-probability-treatment-weighting"><i class="fa fa-check"></i><b>7.4.3</b> Inverse Probability Treatment Weighting</a></li>
<li class="chapter" data-level="7.4.4" data-path="psm.html"><a href="psm.html#matching-with-existing-propensity-scores"><i class="fa fa-check"></i><b>7.4.4</b> Matching with Existing propensity scores</a></li>
<li class="chapter" data-level="7.4.5" data-path="psm.html"><a href="psm.html#matching-with-existing-propensity-scores-1"><i class="fa fa-check"></i><b>7.4.5</b> Matching with Existing propensity scores</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="glm" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> 일반화 선형모형<a href="glm.html#glm" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="the-generalized-linear-models" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> The Generalized Linear models<a href="glm.html#the-generalized-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-general-linear-model" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> The General Linear Model<a href="glm.html#the-general-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>general linear model</strong></p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{1i} + \ldots + \beta_p x_{pi} + \epsilon_i
\]</span></p>
<p>the <strong>response</strong> <span class="math inline">\(y_i\)</span>, <span class="math inline">\(i=1,\ldots ,n\)</span> is modelled by a linear function
of <strong>explanatory</strong> variables <span class="math inline">\(x_j\)</span>, <span class="math inline">\(j=1 , \ldots , p\)</span> plus an error
term.</p>
<ul>
<li>Here general refers to the dependence on potentially more than one
explanatory variable, v.s. the simple linear model:</li>
</ul>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]</span></p>
<ul>
<li>The model is linear in the parameters, e.g. </li>
</ul>
<p><span class="math display">\[
\begin{aligned}
                y_i &amp;= \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \epsilon_i \\
                y_i&amp;=\beta_0 + \beta_1 x_1x_2 + \beta_2 exp(x_2) +\epsilon_i
            \end{aligned}
\]</span></p>
<p>but not e.g. </p>
<p><span class="math display">\[
\begin{aligned}
                y_i &amp;= \beta_0 + \beta_1 x_1^{ \beta_2}+ \epsilon_i \\
                y_i&amp;=\beta_0 exp(\beta_1 )x_1 +\epsilon_i
            \end{aligned}
\]</span></p>
<ul>
<li>We assume that the errors <span class="math inline">\(\epsilon_i\)</span> are independent and
identically distributed such that</li>
</ul>
<p><span class="math display">\[
E[\epsilon_i]=0 \,\,\,\,
            \text{and}
            \,\,\,\, var[\epsilon_i]=\sigma^2
\]</span></p>
<ul>
<li>Typically we assume</li>
</ul>
<p><span class="math display">\[
\epsilon_i \sim N(0 , \sigma^2 )
\]</span></p>
<p>as a basis for inference, e.g. t-tests on parameters.</p>
<ul>
<li><p>Although a very useful framework, there are some situations where
general linear models are not appropriate.</p>
<ul>
<li>the range of <span class="math inline">\(Y\)</span> is restricted (e.g. binary, count)</li>
<li>the variance of <span class="math inline">\(Y\)</span> depends on the mean</li>
</ul></li>
<li><p>Generalized linear models (GLMs) extend the range of application of
general linear models by accommodating response variables with
non-normal conditional distribution to address both of these issues.</p></li>
<li><p>Except for the error, the right-hand side of a generalized linear
model is essentially the same as for a general linear model.</p></li>
</ul>
</div>
<div id="generalized-linear-models-glms" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Generalized Linear Models (GLMs)<a href="glm.html#generalized-linear-models-glms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>A generalized linear model consists of three components:</p></li>
<li><p>Random component : specifying the conditional distribution of the
response variable, <span class="math inline">\(Y_i\)</span>, given the explanatory variables.</p></li>
<li><p>Traditionally, the random component is a member of an "exponential
family" - the normal (Gaussian), binomial, Poisson, gamma, or
inverse-Gaussian families of distributions - but generalized linear
models have been extended beyond the exponential families.</p></li>
<li><p>Systematic component : A linear function of the regressors, called
the linear predictor,</p></li>
</ul>
<p><span class="math display">\[
\eta_i = \beta_0 + \beta_1 x_{1i} + \ldots + \beta_p x_{pi}
\]</span></p>
<p>on which the expected value <span class="math inline">\(\mu_i\)</span> of <span class="math inline">\(Y_i\)</span> depends.</p>
<ul>
<li><p>The <span class="math inline">\(X\)</span>’s may include quantitative predictors, but they may also
include transformations of predictors, polynomial terms, contrasts
generated from factors, interaction regressors, etc.</p></li>
<li><p>Link function : an invertible link function which transforms the
expectation of the response to the linear predictor.</p></li>
<li><p>A link function that describes how the mean, <span class="math inline">\(E(Y_i )=\mu_i\)</span>,
depends on the linear predictor <span class="math display">\[g(\mu_i )=\eta_i\]</span></p></li>
<li><p>The inverse of the link function is sometimes called the mean
function : <span class="math inline">\(g^{-1}(\eta_i )=\mu_i\)</span>.</p></li>
<li><p>A variance function that describes how the variance, <span class="math inline">\(var(Y_i )\)</span>
depends on the mean <span class="math inline">\(var(Y_i )=\phi V(\mu_i)\)</span> where the dispersion
parameter <span class="math inline">\(\phi\)</span> is a constant.</p></li>
</ul>
<p>The <strong>generalized linear model</strong></p>
<p><span class="math display">\[
g(\mu_i) = \beta_0 + \beta_1 x_{1i} + \ldots + \beta_p x_{pi}
\]</span></p>
<p>where <span class="math inline">\(g(\cdot)\)</span> is the link function, <span class="math inline">\(\mu_i\)</span> is the expectation of
random component <span class="math inline">\(Y_i\)</span> and explanatory variables <span class="math inline">\(x_j\)</span>,
<span class="math inline">\(j=1 , \ldots , p\)</span> .</p>
</div>
<div id="normal-glms-as-a-special-case" class="section level3 hasAnchor" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Normal GLMs as a Special Case<a href="glm.html#normal-glms-as-a-special-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>For the general linear model with <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span> we
have the linear predictor</li>
</ul>
<p><span class="math display">\[
\eta_i = \beta_0 + \beta_1 x_{1i} + \ldots + \beta_p x_{pi}
\]</span></p>
<p>the
link functions</p>
<p><span class="math display">\[
g(\mu_i ) = \mu_i
\]</span></p>
<p>and the variance function</p>
<p><span class="math display">\[
V(\mu_i )=1
\]</span></p>
<ul>
<li>The linear regression form</li>
</ul>
<p><span class="math display">\[
E(Y_i | X)=\mu_i = \beta_0 + \beta_1 x_{1i} + \ldots + \beta_p x_{pi}
\]</span></p>
</div>
<div id="modelling-binomial-data" class="section level3 hasAnchor" number="6.1.4">
<h3><span class="header-section-number">6.1.4</span> Modelling Binomial Data<a href="glm.html#modelling-binomial-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Suppose</li>
</ul>
<p><span class="math display">\[
Y_i \sim Binomial(n_i , p_i)
\]</span></p>
<p>and we wish to model the
proportions <span class="math inline">\(Y_i / n_i\)</span>. Then</p>
<p><span class="math display">\[
E(Y_i /n_i)=p_i , \qquad var(Y_i /n_i ) = \frac{1}{n_i } p_i (1-p_i )
\]</span></p>
<p>And variance function is</p>
<p><span class="math display">\[
V(\mu_i )=\mu_i ( 1-\mu_i)
\]</span></p>
<ul>
<li>Link function must map from <span class="math inline">\((0,1) \rightarrow (-\infty, \infty)\)</span>. A
coommon choice is</li>
</ul>
<p><span class="math display">\[
g(\mu_i ) = logit(\mu_i )=log(\frac{\mu_i}{1-\mu_i})
\]</span></p>
<ul>
<li>The linear regression form</li>
</ul>
<p><span class="math display">\[
log(\frac{\mu_i}{1-\mu_i})= \beta_0 + \beta_1 x_{1i} + \ldots + \beta_p x_{pi}
\]</span></p>
</div>
<div id="modelling-poisson-data" class="section level3 hasAnchor" number="6.1.5">
<h3><span class="header-section-number">6.1.5</span> Modelling Poisson Data<a href="glm.html#modelling-poisson-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Suppose
<span class="math display">\[
Y_i \sim Poisson(\lambda_i)
\]</span></li>
</ul>
<p>and we wish to model the
counts <span class="math inline">\(Y_i\)</span>. Then</p>
<p><span class="math display">\[
E(Y_i )=\lambda_i , \qquad var(Y_i ) = \lambda_i
\]</span></p>
<p>And variance
function is</p>
<p><span class="math display">\[
V(\mu_i )=\mu_i
\]</span></p>
<ul>
<li>Link function must map from
<span class="math inline">\((0,\infty) \rightarrow (-\infty, \infty)\)</span>. A natural choice is</li>
</ul>
<p><span class="math display">\[
g(\mu_i ) = log(\mu_i )
\]</span></p>
<ul>
<li>The linear regression form</li>
</ul>
<p><span class="math display">\[
log(\mu_i)= \beta_0 + \beta_1 x_{1i} + \ldots + \beta_p x_{pi}
\]</span></p>
</div>
<div id="transformation-vs.-glms" class="section level3 hasAnchor" number="6.1.6">
<h3><span class="header-section-number">6.1.6</span> Transformation vs. GLMs<a href="glm.html#transformation-vs.-glms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>In some situations a response variable can be transformed to improve
linearity and homogeneity of variance so that a general linear model
can be applied.</p></li>
<li><p>This approach has some drawbacks</p>
<ul>
<li>Response variable has changed!</li>
<li>Transformation must simultaneously improve linearity and homogeneity of variance</li>
<li>Transformation may not be defined on the boundaries of the
sample space</li>
</ul></li>
<li><p>For example, a common remedy for the variance increasing with the
mean is to apply the log transform. e.g.</p></li>
</ul>
<p><span class="math display">\[
log(y_i)=\beta_0 +\beta_1 x_1 +\epsilon_i
\]</span></p>
<p><span class="math display">\[
\Rightarrow E(logY_i )=\beta_0 +\beta_1x_1
\]</span></p>
<ul>
<li><p>This is alinear model for the mean of <span class="math inline">\(logY\)</span> which may not always be
appropriate.</p></li>
<li><p>If <span class="math inline">\(Y\)</span> is income perhaps, we are really interested in the mean
income of population subgroups, in which case it would be better to
model <span class="math inline">\(E(Y)\)</span> using a GLMs:</p></li>
</ul>
<p><span class="math display">\[
logE(Y_i)=\beta_0 +\beta_1x_1
\]</span></p>
<p>       with
<span class="math inline">\(V(\mu )=\mu\)</span>. This also avoids difficulties with <span class="math inline">\(y=0\)</span>.</p>
</div>
<div id="exponential-family" class="section level3 hasAnchor" number="6.1.7">
<h3><span class="header-section-number">6.1.7</span> Exponential Family<a href="glm.html#exponential-family" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Most of the commonly used statistical distributions, e.g. Normal,
Binomial and Poisson, are members of the exponential family of
distributions whose densities can be written in the form</li>
</ul>
<p><span class="math display">\[
f(y; \theta, \phi )=exp \left ( \frac{y\theta-b(\theta)}{\phi}+c(y,\phi)  \right)
\]</span></p>
<p>       where <span class="math inline">\(\phi\)</span> is the dispersion parameter and <span class="math inline">\(\theta\)</span> is the
canonical parameter.</p>
<ul>
<li>It can be shown that</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
                E(Y) &amp;= b&#39;(\theta) = \mu \\
                var(Y)&amp;=\phi b&#39;&#39;(\theta)=\phi V(\mu)
            \end{aligned}
\]</span></p>
</div>
<div id="canonical-links" class="section level3 hasAnchor" number="6.1.8">
<h3><span class="header-section-number">6.1.8</span> Canonical Links<a href="glm.html#canonical-links" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>For a GLMs where the response follows an exponential distribution we
have</li>
</ul>
<p><span class="math display">\[
g(\mu_i )=g(b&#39;(\theta_i ))=\beta_0 +\beta_1 x_{1i} + \ldots + \beta_p x_{pi}
\]</span></p>
<ul>
<li>The canonical link is defined as</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
                g&amp;=(b&#39;)^{-1} \\
                \Rightarrow g(\mu_i )&amp;=\theta_i = \beta_o + \beta_1 x_{1i}+ \ldots + \beta_p x_{pi}
            \end{aligned}
\]</span></p>
<ul>
<li><p>Canonical links lead to desirable statistical properties of the GLMs
hence tend to be used by default.</p></li>
<li><p>However there is no a priori reason why the systematic effects in
the model should be additive on the scale given by this link.</p></li>
</ul>
</div>
<div id="estimation-of-the-model-parameters" class="section level3 hasAnchor" number="6.1.9">
<h3><span class="header-section-number">6.1.9</span> Estimation of the Model Parameters<a href="glm.html#estimation-of-the-model-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>A single algorithm can be used to estimate the parameters of an
exponential family GLMs using maximum likelihood.</p></li>
<li><p>The log-likelihood for the sample <span class="math inline">\(y_1, \ldots , y_n\)</span> is</p></li>
</ul>
<p><span class="math display">\[
l=\sum_{i=1}^n \frac{y_i\theta_i - b(\theta_i )}{\phi_i}+c(y_i , \phi_i )
\]</span></p>
<ul>
<li>The maximum likelihood estimates are obtained by solving the score
equations</li>
</ul>
<p><span class="math display">\[
s(\beta_j )=\frac{\partial l}{\partial\beta_j} =\sum_{i=1}^n \frac{a_i(y_i -\mu_i)}{ V(\mu_i )}\times \frac{x_{ij}}{g&#39;(\mu_i )}=0
\]</span></p>
<p>       Here <span class="math inline">\(\phi_i = \phi/a_i\)</span> where <span class="math inline">\(\phi\)</span> is a single dispersion
parameter and <span class="math inline">\(a_i\)</span> are known prior weights.</p>
<ul>
<li>By the Fisher’s scoring method, the solution can be written as</li>
</ul>
<p><span class="math display">\[
\mathbf{\beta}^{(r+1)} = (\mathbf{X}^T \mathbf{W}^{(r)} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W}^{(r)} \mathbf{z}^{(r)}
\]</span></p>
<p>       i.e. the score equations for a weighted least squares regression of
<span class="math inline">\(\mathbf{z}^{(r)}\)</span> on <span class="math inline">\(\mathbf{X}\)</span> with weights
<span class="math inline">\(\mathbf{W}^{(r)}=diag(w_i )\)</span>, where</p>
<p><span class="math display">\[
\begin{aligned}
                z_i^{(r)} &amp;= \eta_i^{(r)} + (y_i - \mu_i^{(r)} )g&#39; (\mu_i^{(r)}) \\
                \text{and} \quad w_i^{(r)}&amp;=\frac{a_i}{V(\mu_i^{(r)})(g&#39;(\mu_i^{(r)}))^2}
            \end{aligned}
\]</span></p>
<ul>
<li><p>The estimates can be found using an Iteratively (Re-)Weighted Least
Squares (IRLS) algorithm</p>
<ol style="list-style-type: decimal">
<li><p>Start with initial estimates <span class="math inline">\(\mu_i^{(r)}\)</span></p></li>
<li><p>Calculate working responses <span class="math inline">\(z_i^{(r)}\)</span> and working weights
<span class="math inline">\(w_i^{(r)}\)</span></p></li>
<li><p>Calculate <span class="math inline">\(\mathbf{\beta}^{(r+1)}\)</span> by weighted least squares</p></li>
<li><p>Repeat 2 and 3 till convergence</p></li>
</ol></li>
<li><p>For models with the canonical link, this is simply the
Newton-Raphson method.</p></li>
</ul>
</div>
<div id="distribution-and-link-function" class="section level3 hasAnchor" number="6.1.10">
<h3><span class="header-section-number">6.1.10</span> Distribution and Link Function<a href="glm.html#distribution-and-link-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="glm/link.jpg" style="width:100.0%" /></p>
</div>
</div>
<div id="medical-cost-data-analysis" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Medical Cost Data Analysis<a href="glm.html#medical-cost-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="health-care-cost" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> health care cost<a href="glm.html#health-care-cost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Health care cost: medical cost, direct medical cost, etc.</li>
<li>There is no absolute concept of health care costs.
<ul>
<li>When describing health care costs,
<ul>
<li>various terms may be used such as charges, payments, expenditures …</li>
<li>It may vary depending on the insurer and the setting</li>
</ul></li>
<li>Depending on perspectives: patient, payer, provider</li>
</ul></li>
<li>The closest approximation to a health care cost
<ul>
<li>Total expenditures associated with the service, for a given patient in a given health care setting and with a given payer</li>
<li>The sum of the insurance payment, the out-of-pocket amount, and any expenses associated with the health plan</li>
</ul></li>
</ul>
<p><br></p>
<div id="health-care-cost-example" class="section level5 unnumbered hasAnchor">
<h5>Health care cost example<a href="glm.html#health-care-cost-example" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><img src="fig3/f1.png" /></p>
<p><br></p>
</div>
<div id="health-care-cost---meps-medical-expenditure-panel-survey" class="section level5 unnumbered hasAnchor">
<h5>Health care cost - MEPS (Medical Expenditure Panel Survey)<a href="glm.html#health-care-cost---meps-medical-expenditure-panel-survey" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><img src="fig3/f2.png" /></p>
<p><br></p>
</div>
<div id="health-care-cost---nhis-claims-data" class="section level5 unnumbered hasAnchor">
<h5>Health care cost - NHIS claims data<a href="glm.html#health-care-cost---nhis-claims-data" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><img src="fig3/f3.png" /></p>
<p><br></p>
<ul>
<li>Almost of health care cost do not follow normal distribution.
<ul>
<li>Not symmetric and bell shape<br />
</li>
<li>Right-skewed</li>
</ul></li>
</ul>
<p><img src="fig3/f4.png" /></p>
<p><br></p>
<ul>
<li>Skewed data
<ul>
<li>The mean is not always the best descriptive measure.</li>
<li>The median might be better than the mean.
<ul>
<li>Median is not sensitive to extreme values.</li>
<li>The log of the median is also the median of the log cost.</li>
</ul></li>
</ul></li>
<li>Given the heterogeneity of health care expenditures, the predictors and correlates of these expenditures are of great interest.
<ul>
<li>Extremely non-normal nature of cost outcomes means that standard linear regression analyses may not be appropriate for inference.</li>
</ul></li>
</ul>
<p><img src="fig3/f5.png" />
Felix, J., Andreozzi, V., Soares, M., Borrego, P., Gervasio, H., Moreira, A., . . . Portuguese Group for the Study of Bone, M. (2011). Hospital resource utilization and treatment cost of skeletal-related events in patients with metastatic breast or prostate cancer: estimation for the Portuguese National Health System. Value Health, 14(4), 499-505. <a href="doi:10.1016/j.jval.2010.11.014" class="uri">doi:10.1016/j.jval.2010.11.014</a></p>
<p><br></p>
<p><img src="fig3/f6.png" />
Leibson et al. (2020). Objective estimates of direct-medical costs among persons aged 3 to 38 years with and without research-defined autism spectrum disorder ascertained during childhood: A population-based birth-cohort study. Value Health, 23(5): 494-605.</p>
<p><br></p>
<p><img src="fig3/f7.png" />
Chaikledkaew et al. (2008). Factors affecting health-care costs and hospitalizations among diabetic patients in Thai public hospitals. International Society for Pharmacoeconomics and Outcomes Research (ISPOR), 1098-3015/08/S69 S69-S74.</p>
<p><br></p>
<p><img src="fig3/f8.png" />
Schneider, P. P., Ramaekers, B. L., Pouwels, X., Geurts, S., Ibragimova, K., de Boer, M., . . . Joore, M. (2021). Direct Medical Costs of Advanced Breast Cancer Treatment: A Real-World Study in the Southeast of The Netherlands. Value Health, 24(5), 668-675. <a href="doi:10.1016/j.jval.2020.12.007" class="uri">doi:10.1016/j.jval.2020.12.007</a></p>
<p><br></p>
</div>
</div>
<div id="regression-analysis-with-log-transformation" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Regression analysis with log transformation<a href="glm.html#regression-analysis-with-log-transformation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="log-normal-distribution" class="section level4 unnumbered hasAnchor">
<h4>Log-normal distribution<a href="glm.html#log-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig3/f11.png" /></p>
<p><br></p>
</div>
<div id="log-cost-regression-model" class="section level4 hasAnchor" number="6.2.2.1">
<h4><span class="header-section-number">6.2.2.1</span> Log cost regression model<a href="glm.html#log-cost-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Classical approach for regression modeling of a right-skewed outcome on the positive real line
<ul>
<li>Standard linear regression with the log-transformed outcome, log(Y)</li>
</ul></li>
<li>Motivation with highly skewed data
<ul>
<li>Log transformation will make it normal.</li>
<li>Or at least normal enough that linear regression is valid.</li>
</ul></li>
<li>The log transformation is a compressive function
<ul>
<li>Applied to a set of positive numbers.</li>
<li>It shrinks those at the higher end of the range more than those at the lower end.</li>
</ul></li>
<li>The inverse of the natural logarithm, the exponential transformation is a magnifying function.
<ul>
<li>It stretches those at the higher end of the range more than those at the lower end.</li>
</ul></li>
<li>Standard linear regression with log of <span class="math inline">\(Y\)</span>
<span class="math display">\[
log(Y) \sim N(\mu, \sigma^2)
\]</span></li>
</ul>
<p>In the case where <span class="math inline">\(Y\)</span> is costs, <span class="math inline">\(log(Y)\)</span> is the log of costs, <span class="math inline">\(\mu\)</span> is the mean of log costs, and <span class="math inline">\(\sigma^2\)</span> is the variance of log costs.</p>
<ul>
<li>In the regression setting with covariates <span class="math inline">\(X_1, \ldots, X_k\)</span> the model is</li>
</ul>
<p><span class="math display">\[
log(Y|X_1,\ldots,X_k) \sim N(\beta_0+\beta_1X_1+\cdots+\beta_kX_k, \sigma^2)
\]</span></p>
<p>where <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_k\)</span> are the regression coefficients the represent an intercept and the effects of a change in the corresponding covariate on the mean of <span class="math inline">\(log(Y)\)</span>.</p>
<ul>
<li>Regression model</li>
</ul>
<p><span class="math display">\[
log(Y_i)=\beta_0+\beta_1X_{i1}+\cdots+\beta_kX_{ik}+\epsilon_i, \,\,\, \epsilon_i \sim N(0, \sigma^2)
\]</span></p>
<ul>
<li><p>Interpretation of <span class="math inline">\(\beta\)</span></p>
<ul>
<li>One-unit change in any of the covariates is associated with a constant change, <span class="math inline">\(\beta\)</span>, in the mean of the outcome, regardless of the actual value of the covariate.</li>
</ul></li>
<li><p>The <span class="math inline">\(\beta\)</span>s quantify the association between the covariates <span class="math inline">\(X\)</span> and the mean of <span class="math inline">\(log(Y)\)</span>.</p></li>
<li><p>However, health care costs are not easily understood in log costs.</p>
<ul>
<li>Ideally we would like estimates of the effects of covariates on the mean of <span class="math inline">\(Y\)</span>.
In short, when out model is for <span class="math inline">\(E[log(Y)]=\mu\)</span>, then what is <span class="math inline">\(E(Y)\)</span>?</li>
<li>It turns out that <span class="math inline">\(E(Y)\)</span> is not simply <span class="math inline">\(exp(\mu)\)</span>, the inverse transformation of <span class="math inline">\(\mu\)</span>.</li>
</ul></li>
<li><p>The mean of the lognormal random variable depends not only on the mean of the original random variable but also on its variance.</p></li>
<li><p>Formally, when <span class="math inline">\(E[log(Y)]=\mu\)</span> and <span class="math inline">\(Var[log(Y)]=\sigma^2\)</span>, then</p></li>
</ul>
<p><span class="math display">\[
E(Y)=exp(\mu+\frac{1}{2}\sigma^2)=exp(\mu)\times exp(\frac{1}{2}\sigma^2)
\]</span></p>
<ul>
<li>The mean of <span class="math inline">\(Y\)</span> is the exponentiated mean of <span class="math inline">\(log(Y)\)</span> multiplied by a term that depends on the variance.</li>
<li>The multiplicative term is always larger than 1, and it increases with the variance of the normally distributed <span class="math inline">\(log(Y)\)</span>.</li>
</ul>
<p><br></p>
<ul>
<li>Example: Fitted linear regression of the log of medical expenditures</li>
</ul>
<p><img src="fig3/f12.png" /></p>
<p><span class="math display">\[
log(Y)=\beta_0 + \beta_1 \times Diabetes + \epsilon
\]</span>
* Fitted model</p>
<p><span class="math display">\[
log(expenditures)=7,386+1.230 \times Diabetes
\]</span></p>
<ul>
<li><p>Interpretation</p>
<ul>
<li>The mean of log expenditures increases by 1.230 (<span class="math inline">\(\beta_1\)</span>) when <span class="math inline">\(X\)</span> changes from 0 (no diabetes) to 1 (diabetes)</li>
<li>In practical, the <strong>average expenditures</strong> with a prior diabetes diagnosis are exp(1.230)=3.42 (<span class="math inline">\(exp(\beta_1)\)</span>) higher than the average medical expenditures without a prior diagnosis.</li>
</ul></li>
<li><p>To obtain the implications of this model for the average medical expenditures, we can use <span class="math inline">\(E(Y)\)</span> above.</p></li>
<li><p>When our model is <span class="math inline">\(log(Y|X) \sim N(\beta_0 +\beta_1X , \sigma^2)\)</span> then</p></li>
</ul>
<p><span class="math display">\[
E(Y|X)=exp(\beta_0+\beta_1X+\frac{1}{2}\sigma^2)=exp(\beta_0)\times exp(\beta_1X) \times exp(\frac{1}{2}\sigma^2)
\]</span></p>
<ul>
<li>The variance is same between with <span class="math inline">\((X=1)\)</span> and without <span class="math inline">\((X=0)\)</span> a prior diabetes diagnosis</li>
</ul>
<p><span class="math display">\[
\frac{E(Y|X=1)}{E(Y|X=0)}=exp(\beta_1)
\]</span></p>
<ul>
<li>The variance is not the same across values of <span class="math inline">\(X\)</span>: heteroskedastic</li>
</ul>
<p><span class="math display">\[
\frac{E(Y|X=1)}{E(Y|X=0)}=exp(\beta_1+\frac{1}{2}(\sigma_1^2-\sigma_0^2))
\]</span></p>
<ul>
<li><p>Although a very useful framework, there are some situations where
general linear models are not appropriate.</p>
<ul>
<li><p>the range of <span class="math inline">\(Y\)</span> is restricted (e.g. binary, count)</p></li>
<li><p>the variance of <span class="math inline">\(Y\)</span> depends on the mean</p></li>
<li><p>Non-normality with continuous variable</p></li>
</ul></li>
<li><p>Generalized linear models (GLMs) extend the range of application of
general linear models by accommodating response variables with
non-normal conditional distribution to address both of these issues.</p></li>
<li><p>Except for the error, the right-hand side of a generalized linear
model is essentially the same as for a general linear model.</p></li>
</ul>
<p><br></p>
</div>
</div>
<div id="gamma-generalized-linear-models" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Gamma generalized linear models<a href="glm.html#gamma-generalized-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>The log cost model has traditionally been a staple for estimating the mean of right-skewed costs.
<ul>
<li>Potential for heteroskedasticity and the resulting retransformation issues limit its practical application.</li>
</ul></li>
<li>One would model a cost outcome directly, in a way that avoids logging the cost.
<ul>
<li>The model should ideally allow for the variance to increase with the mean, as might be expected for health cost outcomes.</li>
<li>And the effects of covariates on the outcome should be readily interpretable.</li>
</ul></li>
<li>The gamma generalized linear model (GLM) with a log link satisfies all these requirements.
<ul>
<li>The gamma distribution is a flexible distribution on the positive real line.</li>
</ul></li>
<li>Gamma distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span></li>
</ul>
<p><img src="fig3/f13.png" /></p>
<ul>
<li>The “log link” means that the model relates the log of the mean to the linear predictors.</li>
</ul>
<p><span class="math display">\[
log[E(Y)]=\beta_0+\beta_1X_1+\cdots+\beta_kX_k
\]</span></p>
<ul>
<li>Or equivalently</li>
</ul>
<p><span class="math display">\[
E(Y)=exp(\beta_0+\beta_1X_1+\cdots+\beta_kX_k)=exp(\beta_0)\times exp(\beta_1X_1)\times \cdots \times exp(\beta_kX_k)
\]</span></p>
<ul>
<li>Interpretation: if <span class="math inline">\(X_j\)</span> is a binary covariate then</li>
</ul>
<p><span class="math display">\[
\frac{E(Y|X_j=1)}{E(Y|X_j=0)}=\exp(\beta_j)
\]</span></p>
<ul>
<li><p>This interpretation applies for any one-unit change in <span class="math inline">\(X_j\)</span> if it is a continuous covariate.</p></li>
<li><p>Example: Fitted gamma GLMs of the expenditures</p></li>
</ul>
<p><img src="fig3/f14.png" /></p>
<p><span class="math display">\[
log(\mu)=\beta_0+\beta_1 \times Age-18
\]</span></p>
<ul>
<li>Fitted model</li>
</ul>
<p><span class="math display">\[
log(mean \,\,\, expenditures)=7.954+0.021\times Age
\]</span></p>
<ul>
<li>Interpretation
<ul>
<li>The <strong>log mean of expenditures</strong> increases by 0.021 (<span class="math inline">\(\beta_1\)</span>) when <span class="math inline">\(X\)</span> changes from 0 (under age 18) to 1 (over age 18)</li>
<li>The <strong>average expenditures</strong> with a over 18 years old are exp(0.021)=1.02 (<span class="math inline">\(exp(\beta_1)\)</span>) higher than the average medical expenditures with under 18 years old.</li>
</ul></li>
<li>Comparison betwen gamma GLM and linear regression of the log of cost</li>
</ul>
<p><img src="fig3/f15.png" /></p>
</div>
<div id="transformation-vs.-glms-1" class="section level3 hasAnchor" number="6.2.4">
<h3><span class="header-section-number">6.2.4</span> Transformation vs. GLMs<a href="glm.html#transformation-vs.-glms-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>In some situations a response variable can be transformed to improve
linearity and homogeneity of variance so that a general linear model
can be applied.</p></li>
<li><p>This approach has some drawbacks</p>
<ul>
<li><p>Response variable has changed!</p></li>
<li><p>Transformation must simultaneously improve linearity and
homogeneity of variance</p></li>
<li><p>Transformation may not be defined on the boundaries of the
sample space</p></li>
</ul></li>
<li><p>For example, a common remedy for the variance increasing with the
mean is to apply the log transform. e.g.
<span class="math display">\[log(y_i)=\beta_0 +\beta_1 x_1 +\epsilon_i\]</span>
<span class="math display">\[\Rightarrow E(logY_i )=\beta_0 +\beta_1x_1\]</span></p></li>
<li><p>This is a linear model for the mean of <span class="math inline">\(logY\)</span> which may not always be
appropriate.</p></li>
<li><p>If <span class="math inline">\(Y\)</span> is income perhaps, we are really interested in the mean
income of population subgroups, in which case it would be better to
model <span class="math inline">\(E(Y)\)</span> using a GLMs: <span class="math display">\[logE(Y_i)=\beta_0 +\beta_1x_1\]</span> with
<span class="math inline">\(V(\mu )=\mu\)</span>. This also avoids difficulties with <span class="math inline">\(y=0\)</span>.</p></li>
</ul>
<p><br></p>
</div>
</div>
<div id="binary-and-categorical-data-analysis" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Binary and Categorical data Analysis<a href="glm.html#binary-and-categorical-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="overview-of-logistic-regression-model" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Overview of Logistic Regression Model<a href="glm.html#overview-of-logistic-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>로지스틱 회귀분석은 특정 질병의 유무에 영향을 미치는 요인을 밝히는
통계적 방법</p></li>
<li><p>종속변수가 범주형 자료인 경우 연속형 자료를 종속변수로 이용하는
회귀분석은 사용 불가능. 이때 로지스틱 회귀분석을 이용</p></li>
<li><p>선형회귀모형
<span class="math display">\[y = \beta_0 + \beta_1 x_{1} + \ldots + \beta_p x_{p} + \epsilon\]</span></p></li>
<li><p>로지스틱 회귀모형
<span class="math display">\[\log(\frac{p}{1-p})= \beta_0 + \beta_1 x_{1} + \ldots + \beta_p x_{p}\]</span></p></li>
<li><p>여기서 <span class="math inline">\(p\)</span>는 성공(예: 질환의 발병) 확률을 의미</p></li>
<li><p>체중(<span class="math inline">\(X\)</span>)과 고혈압 유무(<span class="math inline">\(P\)</span>) 관계 그래프</p>
<p><img src="glm/lo1.jpg" style="width:80.0%" /></p></li>
<li><p>체중이 증가함에 따라 고혈압이 있을 확률이 증가</p></li>
<li><p>고혈압 여부는 연속형 변수가 아닌 <span class="math inline">\(0\)</span>과 <span class="math inline">\(1\)</span>의 두 가지 값을 갖는
이분형 변수이기 때문에 선형회귀 분석 불가</p></li>
<li><p>위험요인(체중)의 각 수준에 따른 질병(고혈압)이 있을 확률 <span class="math inline">\(p\)</span>를
로짓변환(logit transformation)해 주면 <span class="math inline">\(logit(p)\)</span>는
<span class="math inline">\(-\infty \sim \infty\)</span>의 연속형 변수로 변환됨</p></li>
<li><p>로짓변환 <span class="math display">\[logit(p)=\ln \frac{p}{1-p}\]</span></p>
<p><img src="glm/lo2.jpg" style="width:80.0%" /></p></li>
<li><p>로지스틱 회귀분석은 <span class="math inline">\(logit(p)\)</span>를 종속변수로 두고 회귀분석을 적용한
방법</p></li>
<li><p>질병이 있을 확률 <span class="math inline">\(p\)</span> 추정</p>
<p><img src="glm/lo3.jpg" style="width:80.0%" /></p></li>
<li><p>확률 곡선</p>
<p><img src="glm/lo4.jpg" style="width:60.0%" /></p></li>
<li><p>로지스틱 회귀 직선</p>
<p><img src="glm/lo5.jpg" style="width:60.0%" /></p></li>
<li><p>로지스틱 회귀분석은 질병이 있을 확률을 기준으로 질병 여부에 대한
예측 가능</p></li>
<li><p>예측의 정확도는 분류표를 통해 산출 가능</p></li>
<li><p>분류표</p>
<p><img src="glm/lo6.jpg" style="width:90.0%" /></p></li>
<li><p>회귀 계수의 해석</p>
<p><img src="glm/lo7.jpg" style="width:80.0%" /></p></li>
<li><p>로지스틱 회귀분석에서 개별 위험인자의 영향은 <span class="math inline">\(exp(\beta)\)</span>값을 통해
질병 발생에 대한 교차비(odds ratio)로 표현</p></li>
<li><p>교차비가 1인 경우 요인과 질병은 전혀 관계가 없으며, 1보다 크면
요인에 의해 질병의 위험이 증가, 1보다 작으면 감소함을 의미</p></li>
<li><p>위험인자와 질병의 관련성을 표현할때는 교차비, 유의수준, 교차비의
<span class="math inline">\(95\%\)</span> 신뢰구간을 동시에 제시하는 것이 좋음</p></li>
<li><p>교차비의 <span class="math inline">\(95\%\)</span> 신뢰구간과 <span class="math inline">\(p\)</span>-value</p>
<p><img src="glm/lo8.jpg" style="width:90.0%" /></p></li>
<li><p>모형 적합도를 평가하는 방법으로 Hosmer-Lemeshow의 적합도
검정(goodness-of-fit test)을 이용</p></li>
<li><p>가설</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: 로짓에 대한 회귀모형이 적합하다.</li>
<li><span class="math inline">\(H_1\)</span>: 모형이 적합하지 않다.</li>
</ul></li>
<li><p>Hosmer-Lemeshow 검정은 표본크기가 충분히 큰 경우에 적용</p></li>
<li><p>일반적으로 모형계수 전체 검정(model chi-square test)을 이용하여 모형
적합도를 검정</p></li>
<li><p>가설</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: 모든 <span class="math inline">\(\beta_i =0\)</span>이다.</li>
<li><span class="math inline">\(H_1\)</span>: 최소한 하나의 <span class="math inline">\(\beta_i\)</span>는 <span class="math inline">\(0\)</span>이 아니다.</li>
</ul></li>
<li><p>이 외에 분류 정확도를 이용해 모형을 판단</p></li>
<li><p>Cox &amp; Snell의 결정계수(<span class="math inline">\(R_{cs}^2\)</span>)와 Nagelkerke의
결정계수(<span class="math inline">\(R_N^2\)</span>)를 통해 모형의 설명력 제시</p>
<p><img src="glm/lo9.jpg" style="width:70.0%" /></p></li>
</ul>
<p><br></p>
<p>예제) 다음은 1986년 미국의 Baystate Medical Center, Springfield에서 <span class="math inline">\(189\)</span>명의
신생아 출산에 대한 자료이다. 출생 체중 <span class="math inline">\(2.5kg\)</span> 미만을 저체중 출산으로
정의한다. 이 자료를 통해 저체중 출산의 위험요인을 분석하라. 변수명은
차례로 저체중 여부, 산모 나이, 산모 체중, 인종, 흡연력, 조산 여부,
고혈압 유무, 자궁의 불안정성 여부이다.</p>
<p><img src="glm/ex1.jpg" style="width:80.0%" /></p>
<p>SAS Code</p>
<p><img src="glm/ex2.jpg" style="width:100.0%" /></p>
<p><img src="glm/ex3.jpg" style="width:60.0%" /></p>
<p><img src="glm/ex4.jpg" style="width:80.0%" /></p>
<p><img src="glm/ex5.jpg" style="width:50.0%" /></p>
<p><img src="glm/ex6.jpg" style="width:90.0%" /></p>
</div>
<div id="introduction" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Introduction<a href="glm.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Binary and categorical outcomes arise in many health services and health outcome research studies.</p></li>
<li><p>Examples of binary outcomes</p>
<ul>
<li><p>presence or absence of certain behaviors or conditions, such as smoking, wearing a seatbelt, or having diabetes.</p></li>
<li><p>Occurrence or nonoccurrence of disease events, such as hospital re-admission after surgery, in-hospital mortality, or cancer progression after primary treatment.</p></li>
</ul></li>
<li><p>Categorical or multinomial outcomes have more than two choices.</p></li>
<li><p>Example</p>
<ul>
<li>body mass index (BMI) can be categorized as underweight, normal weight, overweight, or obese.</li>
<li>While categorical outcomes like BMI have a logical order, outcomes like race/ethnicity have no natural ordering.</li>
</ul></li>
<li><p>Unlike continuous outcomes, categorical outcomes do not have a default numerical scale.</p></li>
<li><p>Consequently, standard statistical summaries such as the mean, median, or variance are not meaningful.</p></li>
<li><p>Even if the outcome is coded using numbers, these should not be interpreted quantitatively.</p>
<ul>
<li>Example, a survey may ask respondents to report their health status by choosing 1 (excellent), 2 (very good), 3 (good), 4 (fair), or 5 (poor).</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="binary-outcomes" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Binary Outcomes<a href="glm.html#binary-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>A binary outcome, <span class="math inline">\(Y\)</span> , can take one of two possible values that we can generically label positive or negative and denote as 1 or 0, respectively.</p></li>
<li><p>Example, in a study of 10-day re-admissions after hospitalization</p>
<ul>
<li>Readmitted (<span class="math inline">\(Y=1\)</span>) or not (<span class="math inline">\(Y=0\)</span>)</li>
</ul></li>
<li><p>Example, in a study of chronic disease</p>
<ul>
<li>Have diabetes (<span class="math inline">\(Y=1\)</span>) or not (<span class="math inline">\(Y=0\)</span>);</li>
</ul></li>
<li><p>Example, in a study of medical costs in a given year</p>
<ul>
<li>Have positive (<span class="math inline">\(Y=1\)</span>) or zero (<span class="math inline">\(Y=0\)</span>) medical expenditures.</li>
</ul></li>
<li><p>We can think of a binary outcome as a random variable with distribution comprised of the probabilities that <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span>. (This is called a Bernoulli distribution)</p></li>
<li><p>Because these two probabilities must sum to one, the distribution is completely determined by one of the two probabilities, which is conventionally taken to be the probability of a positive outcome, <span class="math inline">\(P(Y=1)\)</span>.</p></li>
<li><p>When considering the association between a covariate and a binary outcome, therefore, we need simply to understand how this probability is associated with the covariate.</p></li>
<li><p>Regardless of the application setting, we can observe that the mean of the binary outcome <span class="math inline">\(Y\)</span> is the same as the proportion of positive outcomes, or <span class="math inline">\(E(Y)=P(Y=1)\)</span>.</p></li>
<li><p>As a practical example, we will examine how the proportion of obese persons changes over time and by age.</p>
<ul>
<li>We will use data from the National Health and Nutrition Examination Survey (NHANES), which is the most authoritative source of data on the body mass index (BMI) in the United States.</li>
</ul></li>
</ul>
</div>
<div id="two-way-tables" class="section level3 hasAnchor" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span> Two-Way Tables<a href="glm.html#two-way-tables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The National Health and Nutrituion Examination Survey (NHANES) dataset include 6864 individuals age 20–59 years: 3046 from the 1999–2000 survey and 3818 from the 2015–2016 survey.</p></li>
<li><p>Mean BMI increased from 28.4 kg/m2 in 1999–2000 to 29.6 kg/m2 in 2015–2016, an increase of 1.2 units. This might not be a public health concern if it were due to changes in BMI within the underweight or normal weight groups.</p>
<ul>
<li>However, it would be concerning if it indicated an increase in overweight and obese persons as this would affect the incidence of chronic diseases, such as heart failure and diabetes.</li>
</ul></li>
</ul>
<p><img src="fig4/table4.1.png" />
*Statistics for Health Data Science, Ruth et al., Springer</p>
<ul>
<li><p>The two-way table shown in Table presents the frequency of obese and non-obese persons for the two calendar years. The table shows an increase in the proportion obese from 32.5% to 40.4% in 15 years.</p></li>
<li><p>The absolute 8% increase has the same meaning regardless of the baseline proportion in 1999–2000. Sometimes the relative increase is reported, in which case the baseline proportion matters.</p></li>
<li><p>For example, when the baseline proportion is 5%, a 10% relative increase translates into an absolute 0.5% increase; when the baseline proportion is 50%, a 10% relative increase translates into an absolute 5% increase.</p></li>
<li><p>The relative increase is often expressed using a risk ratio or relative risk (RR). In the obesity example, the RR for obesity in 2015–2016 relative to 1999–2000 is calculated as:</p></li>
</ul>
<p><span class="math display">\[
RR = \frac{Proportion \,\,\, obese \,\,\, in \,\,\, 2015-2016}{Proportion \,\,\, obese \,\,\, in \,\,\, 1999-2000} = \frac{0.404}{0.325} = 1.24.
\]</span></p>
<ul>
<li><p>An RR close to 1 means there is little relative change; an RR that is substantially smaller or larger than 1 means there is a substantial relative change.</p></li>
<li><p>In this example, the RR of 1.24 means the proportion obese in 2015–2016 represented a 24% increase relative to 1999–2000.</p></li>
<li><p>The change in RR across values of a covariate (calendar year in this example) measures the strength of association between the outcome and that covariate.</p></li>
<li><p>An alternative measure of association is the odds ratio or relative odds (OR).</p></li>
<li><p>The “odds” is a well-known quantity in gambling. It is itself a ratio, namely, the probability of the positive outcome divided by the probability of the negative outcome, i.e., <span class="math inline">\(P(Y=1)/P(Y=0)\)</span>.</p></li>
<li><p>When the covariate is binary, the OR is simply the ratio of the odds of the outcome in each group. In our obesity example, calendar year is a binary covariate, and the OR is defined as:</p></li>
</ul>
<p><span class="math display">\[
OR=\frac{(Proportion\,\,\, obese \,\,\,in \,\,\,2015–2016)/(Proportion\,\,\, not \,\,\, obese \,\,\,in \,\,\,2015–2016)} {(Proportion\,\,\, obese\,\,\, in\,\,\, 1999–2000)/(Proportion\,\,\, not\,\,\, obese\,\,\, in\,\,\, 1999–2000)}.
\]</span></p>
<ul>
<li>Calculation of the OR from the two-way table is simple; it is just the product of the diagonal elements divided by the product of the off-diagonal elements:</li>
</ul>
<p><span class="math display">\[
OR=\frac{(Number\,\,\, not\,\,\, obese\,\,\, in\,\,\, 1999–2000) \times (Number\,\,\, obese\,\,\, in\,\,\, 2015–2016)} {(Number\,\,\, obese\,\,\,\ in\,\,\, 1999–2000) \times (Number\,\,\, not\,\,\, obese\,\,\, in\,\,\, 2015–2016)}
\]</span></p>
<p><span class="math display">\[
=\frac{1899 \times 1469}{914 \times 2165}=1.41
\]</span></p>
<ul>
<li><p>The OR is symmetric, with the roles of the outcome and the covariate interchangeable.</p></li>
<li><p>While the RR is intuitive and readily interpretable, the OR is neither.</p></li>
<li><p>However, as we will see, the OR is what is typically modeled. Consequently, it is helpful to observe a few properties of the OR:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>The OR indicates the same direction of association as the RR. If there is no association between the outcome and covariate, both the OR and RR equal 1. Also, the OR &gt; 1 whenever the RR &gt; 1, and the OR &lt; 1 whenever the RR &lt; 1.</p></li>
<li><p>If the outcome is associated with the covariate, the OR is always farther from 1 than the RR. For example, if OR 1.41, we know that 1 &lt; RR &lt; 1.41.</p></li>
<li><p>For rare positive outcomes, the OR and the RR are similar. Since odds <span class="math inline">\(P(Y=1)/P(Y=0)\)</span> is close to <span class="math inline">\(P(Y= 1)\)</span> when <span class="math inline">\(P(Y=0)\)</span> is close to 1, the ratio of odds is close to the ratio of probabilities.</p></li>
</ol>
<ul>
<li><p>The third property is handy because it means we can interpret the OR as being approximately equal to the RR when positive outcomes are uncommon. Unfortunately, this is not true when positive outcomes are common.</p></li>
<li><p>When <span class="math inline">\(P(Y=1)\)</span> is non-trivial, <span class="math inline">\(P(Y=0)\)</span> is not close to 1, and this can make the OR and RR dissimilar.</p></li>
</ul>
<p><img src="fig4/table4.2.png" /></p>
<p>Table 4.2 shows the RR and the OR for different instances of a setting with a binary covariate <span class="math inline">\(X\)</span>.</p>
<ul>
<li><p>When the probabilities of a positive outcome are very small, the OR is close to the RR.</p></li>
<li><p>But when the probabilities of positive outcomes are not very small, the OR is not close to the RR.</p></li>
</ul>
<p><br></p>
</div>
<div id="linear-regression-with-a-binary-outcome" class="section level3 hasAnchor" number="6.3.5">
<h3><span class="header-section-number">6.3.5</span> Linear Regression with a Binary Outcome<a href="glm.html#linear-regression-with-a-binary-outcome" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Can we use a linear regression to examine the association between a binary outcome and a covariate?</li>
</ul>
<p><img src="fig4/fig4.1.png" /></p>
<ul>
<li><p>Figure 4.1 shows a scatterplot of obesity status versus age in the NHANES sample data.</p>
<ul>
<li>To avoid “overplotting” points with the same age and obesity status, the size of each point is proportional to the number of observations.</li>
</ul></li>
<li><p>While a linear regression line can technically be calculated, it is not immediately clear what it represents.</p></li>
<li><p>Thinking of each age as defining a sub-population, we can try to model the proportion of obese persons across sub-populations.</p></li>
<li><p>Noting that the proportion of obese persons at each age is just the mean of the obesity outcome at that age, this framing of the problem exactly mirrors the idea behind linear regression.</p></li>
</ul>
<p><img src="fig4/fig4.2.png" /></p>
<p>The dots in Fig. 4.2 show the proportion of obese persons at each age. The fitted linear regression is shown as the blue line.</p>
<ul>
<li><p>While the linear model reasonably captures the proportion of obese persons across the age range considered in this example, the linear regression formulation does not constrain the predicted probabilities to be between zero and one.</p></li>
<li><p>When outcomes are rare, the predictions may even be outside these bounds. We would not trust a model that could predict the percent of obese persons to be 12% or 131%!</p></li>
<li><p>Furthermore, we should be concerned about making any inferences when a linear regression that assumes a continuous, normally distributed outcome variable is applied to a discrete, non-normal outcome.</p></li>
<li><p>Binary regression constrains the regression equation to be between zero and one, and it represents a coherent way of addressing the discrete, non-normal nature of the data.</p></li>
<li><p>The orange line in Fig. 4.2 shows the prediction from a logistic regression, the most popular type of binary regression. In our obesity example, the logistic regression is very close to the linear model for the range of data considered, but this is not always the case.</p></li>
</ul>
</div>
<div id="logistic-regression" class="section level3 hasAnchor" number="6.3.6">
<h3><span class="header-section-number">6.3.6</span> Logistic Regression<a href="glm.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Logistic regression is a way to model the association between the probability of a positive outcome <span class="math inline">\(P(Y= 1)\)</span> and one or more covariates <span class="math inline">\(X\)</span> so that the regression line stays within the interval [0, 1].</p></li>
<li><p>How is this done?</p></li>
<li><p>To understand the reasoning behind logistic regression, recall that linear regression of a continuous outcome <span class="math inline">\(Y\)</span> on a set of covariates <span class="math inline">\(X_1,\ldots, X_k\)</span> has two parts:</p></li>
</ul>
<p><span class="math display">\[
Y = \beta_0 + \beta_1X_1 +\cdots    + \beta_kX_k + \epsilon,
\]</span></p>
<p>where <span class="math inline">\(\beta_0 + \beta_1X_1 +\cdots + \beta_kX_k\)</span> is the systematic part, determining the mean of <span class="math inline">\(Y\)</span> in the sub-populations defined by the <span class="math inline">\(X\)</span>s, and <span class="math inline">\(\epsilon\)</span> the error term that reflects random deviation of a subject from the mean in that sub-population.</p>
<ul>
<li>However, with a binary outcome, this approach fails because</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>The systematic part for the probability <span class="math inline">\(P (Y= 1)\)</span> can be smaller than 0 or larger than 1, an undesirable possibility</p></li>
<li><p>The error term has a very limited range of possible values and depends on the systematic part since the sum of the two parts must be either 0 or 1.</p></li>
</ol>
<ul>
<li><p>As the formulation “outcome = systematic part + error part” fails, we must adopt a different formulation to model binary outcomes.</p></li>
<li><p>A natural way is to think about the outcome of each person as being a result of a binary experiment, such as tossing a coin. The systematic part defines the probability of “success” of the experiment, corresponding to <span class="math inline">\(P(Y=1)\)</span>, and the random part of the model is replaced by a random result of the experiment.</p></li>
<li><p>Since the linear formulation for the systematic part does not guarantee that the model will conform to the 0, 1 range, a mathematical “trick” is required. This mathematical trick is simply a transformation of the systematic part from the linear formulation.</p></li>
<li><p>The most commonly used transformation is the logistic function, which yields:</p></li>
</ul>
<p><span class="math display">\[
P(Y_i=1)=\frac{exp(\beta_0 + \beta_1X_{1i} + \cdots + \beta_kX_{ki})} {1 + exp(\beta_0 + \beta_1X_{1i} + \cdots + \beta_kX_{ki})},
\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> is the outcome of subject <span class="math inline">\(i\)</span> and <span class="math inline">\(X_{1i},\ldots, X_{ki}\)</span> are the covariates.</p>
<ul>
<li><p>Let’s think about why this model works.</p></li>
<li><p>Suppose that we have only one continuous covariate, <span class="math inline">\(X_1\)</span>, and suppose also that <span class="math inline">\(\beta_1\)</span> is positive.</p></li>
<li><p>Then when <span class="math inline">\(X_1\)</span> is positive and increases in magnitude, both the numerator and the denominator in Equation increase, and their ratio approaches 1.</p></li>
<li><p>Conversely, when <span class="math inline">\(X_1\)</span> is negative and increases in magnitude, the numerator approaches 0, and the denominator approaches 1, so the ratio in Equation approaches 0.</p></li>
<li><p>More generally, the numerator is always less than the denominator by construction, so the ratio is always between 0 and 1.</p></li>
<li><p>Thus, this function is a perfect candidate to satisfy the requirements of a regression model for probabilities.</p></li>
<li><p>In Equation, the effect of the <span class="math inline">\(X\)</span>s is additive and linear before the transformation, but the transformation generates a complicated, non-linear function for <span class="math inline">\(P(Y=1)\)</span>.</p></li>
<li><p>An equivalent formulation of this model preserves the linear specification for the <span class="math inline">\(X\)</span>s but applies the inverse transformation to <span class="math inline">\(Y\)</span>.</p></li>
<li><p>This transformation is called the logit, and the formulation is as follows:</p></li>
</ul>
<p><span class="math display">\[
logit[P(Y_i=1)]= log[\frac{P(Y_i = 1)}{P(Y_i=0)}]=\beta_0+\beta_1X_{1i}+\cdots +\beta_kX_{ki}.
\]</span></p>
<ul>
<li><p>Note that the logit is simply the natural logarithm of the odds.</p></li>
<li><p>Thus, the logistic regression model assumes that the log of the odds depends linearly on the covariates.</p></li>
<li><p>The transformation that produces logistic regression is only one of several available for binary outcomes.</p></li>
<li><p>In general, to convert the range of the systematic part to be between 0 and 1, any function that converts a number in the range <span class="math inline">\((-\infty ,\infty )\)</span> to a number in 0, 1 in a sensible way will do the trick.</p></li>
</ul>
</div>
<div id="interpretation-of-a-logistic-regression" class="section level3 hasAnchor" number="6.3.7">
<h3><span class="header-section-number">6.3.7</span> Interpretation of a Logistic Regression<a href="glm.html#interpretation-of-a-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="a-single-binary-covariate" class="section level4 hasAnchor" number="6.3.7.1">
<h4><span class="header-section-number">6.3.7.1</span> A Single Binary Covariate<a href="glm.html#a-single-binary-covariate" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Due to the form of the transformed response variable, logistic regression tells us how the log odds of a positive outcome (<span class="math inline">\(Y=1\)</span>) changes when one of the <span class="math inline">\(X\)</span>s change.</li>
</ul>
<p><img src="fig4/table4.3.png" /></p>
<ul>
<li><p>Table 4.3 is similar to that obtained in linear regression, where the standard error (SE) is a measure of uncertainty of the coefficient estimate and a p-value is given for the hypothesis test that each coefficient is equal to 0.</p></li>
<li><p>As in the linear model, the p-value is calculated by dividing the coefficient by its standard error and comparing the result to a standard normal distribution.</p></li>
<li><p>How can we interpret the estimate of 0.343 for the association with year?</p></li>
<li><p>This is not interpreted as the expected increase in Y on the original scale; it must be interpreted on the log odds scale.</p></li>
<li><p>Symbolically we have:</p></li>
</ul>
<p><span class="math display">\[
logit[P (Y = 1 | X = 1)]− logit[P (Y = 1 | X = 0)]= 0.343.
\]</span></p>
<ul>
<li>If we exponentiate both sides, we get:</li>
</ul>
<p><span class="math display">\[
odds[P (Y = 1 | X = 1)]/odds[P (Y = 1 | X = 0)]= exp(0.343) = 1.41.
\]</span></p>
<ul>
<li><p>The exponentiated log odds ratio is simply the odds ratio.</p></li>
<li><p>Because the response variable was log transformed, the logistic model induces a multiplicative effect on the odds of <span class="math inline">\(Y=1\)</span>.</p></li>
<li><p>Thus, while linear regression is an additive model for the mean of the outcome, logistic regression is a multiplicative model for the odds.</p></li>
<li><p>Also notice that the result in this example (<span class="math inline">\(OR=1.41\)</span>) is exactly the result we obtained in Sect. 4.2 using the two-way table.</p></li>
</ul>
<p>Now look at the corresponding probabilities; this can be done with the help of above Equation:</p>
<p><span class="math display">\[
P (Y=   1|   X= 0)= \frac{exp(−0.731)}{1 + exp(−0.731)}=0.325\\
P (Y=   1|   X= 1)= \frac{exp(−0.731 + 0.343)}{1 + exp(−0.731 + 0.343)}=0.404
\]</span></p>
<ul>
<li><p>Again, these are exactly the numbers obtained using the two-way table in Sect. 4.2.</p></li>
<li><p>A logistic regression with a single binary covariate is quite simple: it just replicates the calculations of the proportions and their odds in the comparison of two groups.</p></li>
</ul>
</div>
<div id="the-general-case" class="section level4 hasAnchor" number="6.3.7.2">
<h4><span class="header-section-number">6.3.7.2</span> The General Case<a href="glm.html#the-general-case" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>In this subsection, we consider models with multiple covariates and work through issues with interpreting the results of a logistic regression.</p></li>
<li><p>We fit a logistic regression to the indicator of obesity (OBESE) with binary covariates SEX (with reference level male) and YEAR (with reference level 1999–2000), a five-level categorical covariate RACE (with reference level non-Hispanic white) coded as a set of four dummy variables and continuous covariate AGE (centered at 20 years, the youngest age in our analysis).</p></li>
<li><p>For this setup, the reference group is the subpopulation of non-Hispanic white men age 20 in 1999–2000.</p></li>
</ul>
<p><img src="fig4/table4.4.png" /></p>
<ul>
<li><p>Table 4.4 provides the fraction obese corresponding to each level of each covariate.</p></li>
<li><p>For non-reference levels, the odds of being obese relative to the reference level is also shown.</p></li>
<li><p>The OR for SEX is greater than 1, suggesting the risk of being obese is higher for women than for men.</p></li>
<li><p>This can also be seen by comparing the percentages: about 32% of men are obese compared to 41% of women.</p></li>
<li><p>For RACE, we see that the OR of the Other/mixed group is smaller than 1, indicating that persons in this group tend to be less at risk of obesity than the reference group (non-Hispanic white persons).</p></li>
<li><p>The ORs for the other RACE groups are greater than 1, and their order matches the ordering of the percentages of obese persons.</p></li>
</ul>
<p><img src="fig4/table4.5.png" /></p>
<ul>
<li><p>Table 4.5 provides the results of a logistic regression with multiple covariates.</p></li>
<li><p>The baseline category for YEAR is 1999–2000; the coefficient estimate for YEAR 2015–2016 is 0.44.</p></li>
<li><p>This tells us that, given AGE, SEX, and RACE, persons in 2015–2016 have a 0.44 increased log odds of being obese compared with persons in 1999–2000.</p></li>
<li><p>The corresponding odds ratio is 1.55; thus, in 2015–2016, the odds of being obese are 1.55 times higher than in 1999–2000. The ORs for the multivariate regression have a conditional interpretation in that they are interpretable as the change in the odds of being obese for the sub-population defined by the values of all the other covariates.</p></li>
<li><p>The interpretation of the coefficient in a logistic regression is similar in principle to a linear regression setting: we can think of it as the estimated effect of a one-unit change in the covariate holding the other covariates fixed. (Note: we use the term “effect” here for conciseness and not to imply causality [1].)</p></li>
<li><p>However, in a logistic regression, we are not estimating the effect on E(Y)—we’re estimating the effect on the log odds of <span class="math inline">\(Y=1\)</span>.</p></li>
<li><p>Thus, the coefficient of a categorical covariate is the estimated difference in log odds compared to the reference level.</p></li>
<li><p>For example, being a woman is associated with a 0.40 difference in the log odds of being obese compared to being a man.</p></li>
<li><p>Alternatively, being a woman is associated with 1.48 times the odds of being obese compared to being a man. The estimated association between covariate and log odd or odds of the outcome is the same for any race, persons of any age, and in either survey calendar year.</p></li>
<li><p>In the case of a continuous covariate, interpretation is similar.</p></li>
<li><p>For example, being 1 year older is associated with an increase in the log odds of being obese of 0.02 or, alternatively, an OR of 1.02.</p></li>
<li><p>This estimated association does not depend on the specific value of age, and it does not depend on the specific values of the other covariates.</p></li>
<li><p>To estimate the effect of a 10-year increase in age, we simply multiply the coefficient by 10, obtaining a difference in log odds of 0.2 and a corresponding OR of <span class="math inline">\(exp(0.2)=1.22\)</span>.</p></li>
<li><p>So far we have discussed how changes in covariates are associated with the log odds or odds of a positive response.</p></li>
<li><p>But we often want to know how changes in covariates are associated with the probability of a positive response.</p></li>
<li><p>This is generally easier to understand. As noted above (Sect. 4.2.1), the OR is sometimes interpretable as an approximate relative risk—i.e., a ratio of probabilities rather than a ratio of odds—but this interpretation is only valid when the outcome is rare.</p></li>
<li><p>When positive outcomes are not rare, the OR can be quite different from the relative risk. For example, a 1999 article in The New England Journal of Medicine that compared rates of referral to cardiac catheterization in African American and white patients found an OR of 0.6.</p></li>
<li><p>This result was considered to be evidence of a dramatic racial disparity in the receipt of cardiac catheterization, but the corresponding risk ratio was actually 0.93 [3]—much closer to 1!</p></li>
<li><p>The two measures were so different owing to a relatively high fraction of each group receiving a referral (92% of white patients and 89% of black patients).</p></li>
<li><p>This left rather small denominators for the odds in each group, which led to an odds ratio that completely misrepresented the relatively minor difference between the probabilities.</p></li>
<li><p>It is not infrequent to see results of logistic regression analyses interpreted in this way— the natural language of risk is probability, not odds.</p></li>
<li><p>So, what if we really want to understand our logistic regression results in terms of relative probabilities or differences in probabilities instead of relative odds and differences in log odds?</p></li>
</ul>
</div>
</div>
<div id="interpretation-on-the-probability-scale" class="section level3 hasAnchor" number="6.3.8">
<h3><span class="header-section-number">6.3.8</span> Interpretation on the Probability Scale<a href="glm.html#interpretation-on-the-probability-scale" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="estimating-probabilities" class="section level4 hasAnchor" number="6.3.8.1">
<h4><span class="header-section-number">6.3.8.1</span> Estimating Probabilities<a href="glm.html#estimating-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Equation tells us how to calculate the probability of a positive outcome for any sub-population. For example, the probability of a positive outcome in the reference group of non-Hispanic white (NHW) men age 20 years in 1999–2000 is:</li>
</ul>
<p><span class="math display">\[
P (Y    =1 |  NHW, Male, 20)=   \frac{exp(−1.46)}{1 + exp(−1.46)}=0.188.
\]</span></p>
<ul>
<li>If we want to estimate the probability for an otherwise similar woman, we simply add the coefficient of SEX:</li>
</ul>
<p><span class="math display">\[
P (Y=   1|   NHW, Female, 20)=  \frac{exp(−1.46 + 0.40)}{1 + exp(−1.46 + 0.40)}=0.257,
\]</span></p>
<ul>
<li>and if we want to estimate the probability for an otherwise similar woman age 52 years, we simply add the coefficient of AGE times 32 (= 52 − 20):</li>
</ul>
<p><span class="math display">\[
P (Y=   1 |  NHW, Female, 52)=  \frac{exp(−1.46 + 0.40 + 0.02 × 32)}{1 + exp(−1.46 + 0.40 + 0.02 × 32)}=0.397.
\]</span></p>
<ul>
<li><p>Because interpreting ORs is not straightforward, we would like to define the effect of a covariate on the probability of a positive outcome, <span class="math inline">\(P(Y=1)\)</span>.</p></li>
<li><p>There are two types of effects for a covariate X:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>An additive effect (AE), equal to the absolute change in <span class="math inline">\(P(Y= 1| X)\)</span> for a one-unit change in <span class="math inline">\(X\)</span>:</li>
</ol>
<p><span class="math display">\[
AE = P (Y = 1 | X = 1) − P (Y = 1 | X = 0).
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>A multiplicative effect (ME) equal to the relative change in <span class="math inline">\(P (Y= 1| X)\)</span> for a one-unit change in <span class="math inline">\(X\)</span>:</li>
</ol>
<p><span class="math display">\[
ME = P (Y = 1 | X = 1)/P (Y = 1 | X = 0).
\]</span></p>
<ul>
<li><p>The additive effect is sometimes referred to as a risk difference.</p></li>
<li><p>The multiplicative effect is the previously defined risk ratio. For either effect, we would ideally like an estimate that does not depend on the specific values of the other covariates, just as in linear regression.</p></li>
</ul>
<p><img src="fig4/fig4.3.png" /></p>
<ul>
<li><p>Figure 4.3 shows why this is a tall order.</p></li>
<li><p>The figure shows predicted probabilities of being obese implied by the fitted logistic regression in Table 4.5 for persons in race category Other/mixed and different values of AGE, SEX, and YEAR.</p></li>
<li><p>One can observe that the predicted probabilities are not quite linear in the covariates.</p></li>
<li><p>This is a consequence of the logit transformation that converted the model from a linear to a non-linear model.</p></li>
<li><p>The model is only linear on the log odds scale—not on the original probability scale.</p></li>
<li><p>In fact, not only do the predicted probabilities increase non-linearly with age, but the effect of year differs depending on age.</p></li>
<li><p>This is reminiscent of the normal linear model with an interaction between age and year, which was a non-linear model.</p></li>
<li><p>The regression equation in this case does not explicitly include an interaction, but when converted to the probability scale, it is also non-linear.</p></li>
</ul>
<p><img src="fig4/fig4.4.png" /></p>
<ul>
<li><p>The top panel of Fig. 4.4 shows the difference between the predicted probabilities in years 2015–2016 and 1999–2000 across values of SEX and AGE for RACE Other/mixed.</p></li>
<li><p>If the model were additive and linear on the probability scale, the differences would be similar for men and women and constant across ages.</p></li>
<li><p>However, this is not the case.</p></li>
<li><p>In fact, the additive effect of YEAR depends on AGE and SEX; the absolute change in BMI over time is greater for older people and for women. We refer to this effect as a conditional additive effect—the additive effect depends on the exact values of the other covariates: AGE, SEX, and RACE.</p></li>
<li><p>The conditional multiplicative effect is defined similarly as the conditional ratio of the probabilities of being obese in the later and earlier years.</p></li>
<li><p>The middle panel of Fig. 4.4 shows this ratio across values of SEX and AGE for RACE Other/mixed.</p></li>
<li><p>In contrast to the absolute change, the relative change over time in BMI is lower for older persons and for women.</p></li>
<li><p>In Fig. 4.4, only the odds ratio (bottom panel) is the same across ages and for men and women.</p></li>
<li><p>In logistic regression, the odds ratio is the single measure of association that does not depend on the specific values of other covariates.</p></li>
<li><p>The question that remains, then, is whether we can estimate an additive or multiplicative effect on the probability scale (i.e., “risk differences”).</p></li>
</ul>
</div>
<div id="marginal-effects" class="section level4 hasAnchor" number="6.3.8.2">
<h4><span class="header-section-number">6.3.8.2</span> Marginal Effects<a href="glm.html#marginal-effects" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>One way to estimate the effect of a covariate on the probability scale is to convert the conditional additive or multiplicative effect to a marginal effect.</p></li>
<li><p>Here, we are using the term “marginal” in its technical sense, meaning as an average.</p></li>
<li><p>In the logistic regression setting, the marginal additive effect of YEAR is an average of the conditional additive effects, defined above, over the distribution of all the other covariates: AGE, SEX, and RACE. The result is meaningful as a difference in the probability of being obese in 2015–2016 versus 1999–2000; it is adjusted for all covariates, and it does not depend on their specific values since it averages over them.</p></li>
<li><p>Estimation of a marginal effect consists of a two-step process:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Estimate the conditional effect of interest (e.g., of YEAR) based on the observed data.</p></li>
<li><p>Average the estimates in (1) over the joint distribution of all other covariates.</p></li>
</ol>
<ul>
<li><p>To estimate the marginal effect of YEAR in the obesity example, step (2) averages the conditional effect estimates over the distributions of AGE, SEX, and RACE.</p></li>
<li><p>This may make sense theoretically, but how can we implement it practically in our sample?</p></li>
<li><p>The recycled prediction method implements step (2) by averaging over the empirical distribution of the other covariates (i.e., the set of values actually observed in the data), yielding a sample average version of step (2).</p></li>
<li><p>We explain how the method works using the association between BMI and YEAR as an illustrative example.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>For each person in the data, calculate a person-specific conditional additive effect as the difference between their predicted probabilities:</li>
</ol>
<p><span class="math display">\[
P (Y = 1 | YEAR = 2015–2016) − P (Y = 1 | YEAR = 1999–2000).
\]</span></p>
<ul>
<li><p>The first predicted probability is calculated from the fitted logistic regression by setting YEAR to 1 (2015–2016), and the second is calculated by setting YEAR to 0 (1999–2000) fixing the other covariates at the values observed for that person.</p></li>
<li><p>Thus, two predicted probabilities are calculated for each person.</p></li>
<li><p>The differences are individual-level conditional effects and are presented in the top panel of Fig. 4.4 for persons in the race category Other/mixed.</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Calculate the sample average of the individual-level conditional effects.</li>
</ol>
<ul>
<li><p>This is the same thing as calculating a weighted average of the conditional effects over the empirical (sample) distribution of the other covariates.</p></li>
<li><p>This produces a sample average version of step (2) above.</p></li>
<li><p>Variance estimation for the marginal additive effect is generally performed using a technique called the delta method or by numerical methods such as bootstrapping; the package margins in R can do all the necessary calculations. The variance can be used to construct a confidence interval for the marginal additive effect.</p></li>
<li><p>The marginal additive effect for YEAR is estimated as 0.102 with 95% confidence interval (0.077, 0.128).</p></li>
<li><p>Thus, the fraction of adults that are obese in 2015–2016 is about 10% higher than in 1999–2000.</p></li>
<li><p>Even though this estimate averages over the other covariates and therefore does not explicitly depend on them, it does depend on their distribution in the sample.</p></li>
<li><p>If the sample distribution of the other covariates is not representative of the population distribution, the result may not generalize to the population.</p></li>
<li><p>Finally, although recycled predictions may be reported using terms like “effects” and so may seem to suggest a causal framework, this is simply convenient nomenclature, and results should not be interpreted causally unless causal inference methods are employed.</p></li>
<li><p>If interest instead focuses on multiplicative effects, we can calculate the ratio:</p></li>
</ul>
<p><span class="math display">\[
\frac{P (Y = 1 | YEAR = 2015–2016)}{P (Y = 1 | YEAR = 1999–2000)}
\]</span></p>
<p>rather than the difference in step (1) and average over all the estimates.</p>
<ul>
<li>This will yield an average risk ratio over the sample.</li>
</ul>
</div>
</div>
<div id="model-building-and-assessment" class="section level3 hasAnchor" number="6.3.9">
<h3><span class="header-section-number">6.3.9</span> Model Building and Assessment<a href="glm.html#model-building-and-assessment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The principles of building a logistic regression model and assessing its fit are similar to those used for linear regression models (Chap. 3), but certain additional considerations apply due to the binary nature of the response variable.</p></li>
<li><p>If the goal of the analysis is to conduct inference to test a pre-specified hypothesis, then model adequacy will hinge on whether the model accurately reflects the data-generating mechanism.</p></li>
<li><p>If the goal of the analysis is to deliver a conclusion that points to a causal relationship, then model adequacy will rest on whether the model properly accounts for other potential confounding variables.</p></li>
<li><p>And if the goal is to predict the occurrence of an event or of a condition encoded using a binary response, then predictive accuracy will be key.</p></li>
</ul>
<div id="model-comparison-aic-and-bic" class="section level4 hasAnchor" number="6.3.9.1">
<h4><span class="header-section-number">6.3.9.1</span> Model Comparison: AIC and BIC<a href="glm.html#model-comparison-aic-and-bic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The parameters of logistic regression are estimated by maximum likelihood, where the parameters <span class="math inline">\(\beta_0, \beta_1,\ldots, \beta_k\)</span> are chosen to maximize the likelihood of obtaining the data observed.</p></li>
<li><p>We can test for the adequacy of a model with fewer covariates, say a model that assumes there is no effect of the race variable in the obesity example, by omitting those variables, re-estimating the regression model, and comparing the likelihood of the observed data with and without those variables.</p></li>
<li><p>This is the likelihood ratio test for nested models, described in Chap. 3.</p></li>
<li><p>In Chap. 3, we also introduced the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for comparison of nested and non-nested models.</p></li>
<li><p>These are versions of the maximized log-likelihood <span class="math inline">\(L\)</span> of a model M that include an added penalty for model complexity.</p></li>
<li><p>The goal is to find the model that minimizes the AIC (or BIC). The two statistics are similar, but the BIC invokes a more severe penalty for model complexity that also depends on the sample size n, so it may select simpler, more parsimonious models than the AIC in certain cases.</p></li>
</ul>
</div>
<div id="model-calibration-hosmerlemeshow-test" class="section level4 hasAnchor" number="6.3.9.2">
<h4><span class="header-section-number">6.3.9.2</span> Model Calibration: Hosmer–Lemeshow Test<a href="glm.html#model-calibration-hosmerlemeshow-test" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The maximum likelihood estimates of the parameters have several appealing properties.</p></li>
<li><p>For example, the sum of predicted probabilities of being obese (i.e., the sum of predicted <span class="math inline">\(P(Y=1|X)\)</span> over the sample) exactly equals the observed number of obese persons.</p></li>
<li><p>Moreover, this is true not only for the total number of obese persons in the sample but also within each discrete sub-population defined by SEX, RACE, or YEAR.</p></li>
<li><p>Thus, the predicted number of obese persons in 1999–2000 equals the observed number of obese persons in that survey year.</p></li>
<li><p>But what about sub-populations defined by continuous variables or by more than one discrete variable? We say that a model is well calibrated if the number of obese persons it predicts in each possible sub-population is close to the observed number in that group.</p></li>
<li><p>For example, we would like the predicted number of obese non-Hispanic white men in the age group 30–35 in 1999–2000 to closely match the observed number in that group. Since there are many possible sub-populations of this kind to look at, a systematic way to assess model calibration is needed.</p></li>
<li><p>A now common approach was suggested by Hosmer and Lemeshow.</p></li>
<li><p>The Hosmer-Lemeshow approach compares observed and predicted quantities as follows:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Order the observations according to their predicted <span class="math inline">\(P(Y=1|X)\)</span>, so the person with the lowest predicted probability of being obese is first, and the person with the highest predicted probability of being obese is last.</p></li>
<li><p>Partition the ordered observations into m equal-sized groups. For example, if m 10, partition the ordered observations into ten groups, with each group comprising approximately 10% of the sample.</p></li>
<li><p>Create a two-way contingency table by cross-tabulating counts of obese and non-obese persons across groups.</p></li>
<li><p>Create the corresponding table for the predicted number of persons in each group, where the predicted number is given by the sum of the predicted probabilities in that group.</p></li>
<li><p>Compare the observed and predicted numbers in corresponding cells in the two tables.</p></li>
</ol>
<ul>
<li>For contingency tables, this is known as Pearson’s residual:</li>
</ul>
<p><span class="math display">\[
Res = \frac{Observed - Predicted}{\sqrt{Predicted}}
\]</span></p>
<ul>
<li><p>Pearson’s residual tells us about over-prediction and under-prediction within each cell of the table.</p></li>
<li><p>To summarize across groups, Hosmer and Lemeshow suggest summing the square of the Pearson’s residuals (<span class="math inline">\(Res^2\)</span>) over the table and comparing it to a <span class="math inline">\(\chi^2\)</span> (“chi-squared”) distribution with <span class="math inline">\(m=2\)</span> degrees of freedom.</p></li>
<li><p>The Hosmer–Lemeshow <span class="math inline">\(\chi^2\)</span> test can be problematic for large sample sizes because it can reject models that approximate the data reasonably well but not perfectly.</p></li>
<li><p>A simulation study suggested not using the test when sample size is larger than 10,000 and to use more than <span class="math inline">\(m=100\)</span> groups for sample sizes around 5000.</p></li>
<li><p>Still, partitioning the data into few groups (say <span class="math inline">\(m=10\)</span> or <span class="math inline">\(20\)</span>) and looking for cells with relatively extreme residuals can be a useful informal tool for model checking.</p></li>
<li><p>Any systematic pattern of residuals across the cells can be informative about the validity of linearity assumptions or may suggest a need to include additional covariates.</p></li>
</ul>
<p><img src="fig4/table4.6.png" /></p>
<ul>
<li><p>Table 4.6 shows observed and predicted numbers of obese and non-obese persons in <span class="math inline">\(m=10\)</span> groups.</p></li>
<li><p>The Pearson’s residuals are distributed across groups without patterns or extreme values.</p></li>
<li><p>However, the Hosmer–Lemeshow statistic is calculated to be 21.1, corresponding to a p-value of 0.007 according to a <span class="math inline">\(\chi^2\)</span> with 8 degrees of freedom.</p></li>
<li><p>If we partition the data into m 100 groups, the p-value is 0.02. In both cases, we would conclude that the model was not well calibrated, but, given our sample size of over 6800 individuals, would take this result with a grain of salt.</p></li>
<li><p>What is the Hosmer–Lemeshow test actually evaluating? It is sometimes billed as a check on the linearity assumption of the logistic regression model.</p>
<ul>
<li>But really it is an omnibus test of the ability of the model to replicate the pattern of <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span> across groups of observations given the covariates included and the structure of the model.</li>
</ul></li>
</ul>
</div>
<div id="model-prediction-roc-and-auc" class="section level4 hasAnchor" number="6.3.9.3">
<h4><span class="header-section-number">6.3.9.3</span> Model Prediction: ROC and AUC<a href="glm.html#model-prediction-roc-and-auc" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>If the objective of the analysis is prediction, measures of predictive accuracy are most relevant.</p></li>
<li><p>By prediction, we mean an educated guess about a future outcome based on the values of relevant covariates.</p></li>
<li><p>In the case of binary outcomes, there are several standard measures of predictive accuracy.</p></li>
<li><p>A simple way to think about the predictive accuracy of a binary regression model is to look at the overlap between the distributions of the predicted probabilities for positive and negative observed outcomes.</p></li>
</ul>
<p><img src="fig4/fig4.5.png" /></p>
<ul>
<li><p>Figure 4.5 shows histograms of predicted probabilities for obese and non-obese persons.</p></li>
<li><p>While the predicted probabilities tend to be lower for non-obese persons than for obese persons, there is considerable overlap between the two histograms, illustrating relatively weak discrimination between these two groups.</p></li>
<li><p>Formal measures of predictive accuracy effectively quantify the agreement between the observed binary outcome and the model prediction.</p></li>
<li><p>The model prediction is not binary; it is a probability between zero and one.</p></li>
<li><p>But if one is willing to specify a threshold <span class="math inline">\(T\)</span> above which a prediction is positive (one) and below which a prediction is negative (zero), then one could assess agreement between the observed binary outcome and the binarized predicted outcome.</p></li>
<li><p>A natural threshold is <span class="math inline">\(T=0.5\)</span>, where a person is predicted to be positive (<span class="math inline">\(Y=1\)</span>) if the predicted probability is greater than 0.5 and he/she is predicted to be negative (<span class="math inline">\(Y=0\)</span>) otherwise.</p></li>
</ul>
<p><img src="fig4/table4.7.png" /></p>
<ul>
<li><p>Table 4.7 presents the results of this approach for the multivariate logistic regression in Table 4.5.</p></li>
<li><p>The rows of this two-way table show the observed obesity status, and the columns show predicted obesity status based on the threshold <span class="math inline">\(T=0.5\)</span>.</p></li>
<li><p>We see that 92% of non-obese persons are correctly predicted to be non-obese by this model and this criterion, but only 18% of obese persons are correctly predicted to be obese.</p></li>
<li><p>Correctly classifying obese persons may be more important than correctly classifying non-obese persons, and this can be controlled by changing the threshold.</p></li>
<li><p>In the setting of binary outcomes, two basic measures of predictive accuracy play a central role:</p>
<ul>
<li><p>Sensitivity: Probability that the model prediction is positive when the outcome is positive.</p></li>
<li><p>Sensitivity measures the ability of the model to correctly predict a positive outcome.</p></li>
<li><p>In the obesity example, for the threshold <span class="math inline">\(T=0.5\)</span>, the sensitivity is 18%.</p></li>
<li><p>Specificity: The probability that the model prediction is negative when the outcome is negative.</p></li>
<li><p>Specificity measures the ability of the model to correctly predict a negative outcome.</p></li>
<li><p>In the obesity example, for the threshold <span class="math inline">\(T=0.5\)</span>, the specificity is 92%.</p></li>
</ul></li>
<li><p>The sensitivity is also referred to as the true-positive rate (TPR), while one minus the specificity is the false-positive rate (FPR).</p></li>
<li><p>Good predictive performance is characterized by high TPR and low FPR, but in many settings, the importance of one outweighs that of the other.</p></li>
<li><p>For example, in developing tests to diagnose COVID-19, high sensitivity is critical because infected individuals who test negative (false negative) may continue to spread the infection unwittingly.</p></li>
<li><p>On the other hand, in developing tests to detect COVID-19 antibodies, high specificity is critical because individuals who have not had the virus but who think that they have had it (false positive) may behave as if they are protected when they are not.</p></li>
<li><p>It should be apparent that sensitivity and specificity will vary depending on the threshold <span class="math inline">\(T\)</span> . As the threshold increases, the sensitivity will decrease but specificity will increase.</p></li>
<li><p>Thus, sensitivity and specificity move in opposite directions as the threshold <span class="math inline">\(T\)</span> varies.</p></li>
<li><p>When choosing a specific threshold <span class="math inline">\(T\)</span> to dichotomize predictions from a binary regression model, it is important to consider the consequences of both false negatives and false positives and to weight them appropriately in the selection of <span class="math inline">\(T\)</span>.</p></li>
</ul>
<p>However, generally speaking, we would like to judge predictive accuracy without specifying a threshold. The receiver operating characteristic (ROC) curve does this.</p>
<ul>
<li>The ROC curve plots the TPR (sensitivity) versus the FPR (1-specificity) as <span class="math inline">\(T\)</span> varies.</li>
</ul>
<p><img src="fig4/fig4.6.png" /></p>
<ul>
<li><p>Figure 4.6 considers the problem of predicting obesity based on the model in Table 4.5.</p></li>
<li><p>Naturally, if we really wanted to predict whether a person was obese or would become obese, we would bring many more covariates to bear. But for illustration, we plot the ROC curve for this model.</p></li>
<li><p><span class="math inline">\(T\)</span> increases from zero to one; both the TPR (sensitivity) and FPR (1-specificity) increase from zero to one.</p></li>
<li><p>The ideal ROC curve hugs the upper left corner of the unit square, where the TPR is highest and the FPR is lowest.</p></li>
<li><p>A quantitative measure of the predictive performance of an ROC curve is the area under the curve (AUC), sometimes also called the concordance or C-statistic. The ideal ROC curve has an AUC of 1.</p></li>
<li><p>An ROC curve that sits on the 45-degree line that bisects the unit square reflects predictive accuracy that is no better than flipping a coin; it has an AUC of 0.5.</p></li>
<li><p>For the multivariate regression in Table 4.5, the ROC is only somewhat above the 45 degree line, and the AUC is a relatively unimpressive 0.63.</p></li>
<li><p>Incorporating other variables in the model, such as chronic conditions (e.g., diabetes, hypertension, and cardiovascular disease), behaviors like exercise, and socioeconomic factors like income and education, might improve the predictive performance.</p></li>
</ul>
</div>
</div>
<div id="multinomial-regression" class="section level3 hasAnchor" number="6.3.10">
<h3><span class="header-section-number">6.3.10</span> Multinomial Regression<a href="glm.html#multinomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="an-extension-of-logistic-regression" class="section level4 hasAnchor" number="6.3.10.1">
<h4><span class="header-section-number">6.3.10.1</span> An Extension of Logistic Regression<a href="glm.html#an-extension-of-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Multinomial regression extends logistic regression to settings where the outcome variable has more than two categories.</p></li>
<li><p>To illustrate, we examine how SEX, RACE, AGE, and YEAR are associated with the probability of four BMI categories: underweight (<span class="math inline">\(BMI &lt; 18.5\)</span>), normal (<span class="math inline">\(18.5\le BMI &lt; 25\)</span>), overweight (<span class="math inline">\(25\le BMI &lt; 30\)</span>), and obese (<span class="math inline">\(BMI \ge 30\)</span>).</p></li>
<li><p>Multinomial regression translates this question into how the covariates are associated with observed proportions in these categories.</p></li>
<li><p>In principle, we could repeat the logistic regression analysis in Table 4.5 for each BMI category separately.</p></li>
<li><p>However, such an analysis ignores important aspects of the multi-category setting.</p></li>
<li><p>We will compare the two approaches and discuss the differences between them.</p></li>
<li><p>We start this section with a single binary covariate, YEAR.</p></li>
<li><p>Table 4.8 shows frequencies within the different BMI categories in 1999–2000 and 2015–2016.</p></li>
</ul>
<p><img src="fig4/table4.8.png" /></p>
<ul>
<li><p>The absolute 8% increase in the obese category coincided with an absolute decrease of 5% in the normal category and an absolute decrease of 3% in the overweight category.</p></li>
<li><p>The proportions of underweight persons were small and similar in absolute magnitude in the 2 survey years.</p></li>
<li><p>The risk ratios (RRs) and odds ratios (ORs) summarize relative change between years, but these measures do not take the multi-category structure into account.</p></li>
<li><p>First we define a multi-category OR of category <span class="math inline">\(k\)</span> relative to a reference category
as:</p></li>
</ul>
<p><span class="math display">\[
OR=\frac{P(Y = k | 2015–2016)/P (Y = reference | 2015–2016)}{P (Y = k | 1999–2000)/P (Y = reference | 1999–2000)}
\]</span></p>
<ul>
<li><p>This definition of OR extends the one for binary outcomes for which the relative category was always <span class="math inline">\(Y=0\)</span>.</p></li>
<li><p>The bottom row of Table 4.8 shows the multi-category ORs taking normal weight as the reference category.</p></li>
<li><p>These multi-category ORs could be obtained by calculating two-category ORs from two-way tables of each category with the reference category.</p></li>
<li><p>For example, the multi-category OR of 1.07 for overweight persons is the two-category OR based on the normal and overweight columns in Table 4.8, which can be calculated as <span class="math inline">\((920\times 1095)/(930\times 1013)\)</span>.</p></li>
<li><p>Based on the multi-category OR, we estimate a 6% increase in the odds of being underweight, a 7% increase in the odds of being overweight, and a 46% increase in the odds of being obese relative to the change in normal weight persons in 2015–2016 compared to 1999–2000.</p></li>
<li><p>Thus, the multi-category ORs characterize the relative change within each category relative to the change in the reference category.</p></li>
</ul>
<p><img src="fig4/fig4.7.png" /></p>
<ul>
<li><p>For further comparison with the binary case, Fig. 4.7 shows the proportions of persons in each BMI category by age. The points show the observed proportions, and the lines are separate linear regressions for each category.</p></li>
<li><p>We introduced binary regression out of concerns about predictions that are outside the sensible 0, 1 range and the non-normal distribution of binary outcomes.</p></li>
<li><p>For multinomial outcomes, an additional consideration is that predictions across categories should sum to 1 at each age—and, more generally, at each value of the covariates. Multinomial regression is precisely designed for this purpose.</p></li>
<li><p>For each subject, multinomial regression predicts the probability that they belong to each category in a way that respects the multi-category nature of the outcome.</p></li>
<li><p>In both binary and multinomial regressions, the central question is whether and how the proportions of the outcome variable change with the covariates. Whether the outcome variable has two, three, or more categories, addressing this question involves assessing the shape of the outcome distribution given the covariates.</p></li>
<li><p>In the case of a binary outcome, the overall proportions are <span class="math inline">\(p_0 = P (Y = 0)\)</span> and <span class="math inline">\(p_1 = P (Y = 1)\)</span>.</p></li>
<li><p>In the case of a multinomial outcome with three categories, the overall proportions are <span class="math inline">\(p_0 = P (Y = 0)\)</span>, <span class="math inline">\(p_1 = P (Y = 1)\)</span>, and <span class="math inline">\(p_2 = P (Y = 2)\)</span>.</p></li>
<li><p>In general, for <span class="math inline">\(K\)</span> categories, the overall proportions are <span class="math inline">\(p_0 = P (Y = 0)\)</span>, <span class="math inline">\(p_1 = P (Y = 1)\)</span>, <span class="math inline">\(p_2 = P (Y = 2), \ldots, p_{K−1} = P (Y = K − 1)\)</span>.</p></li>
<li><p>Binary regression addresses the central question by examining how the ratio <span class="math inline">\(p1/p0\)</span>, the odds of a positive outcome, depends on the covariates.</p></li>
<li><p>Testing whether the odds of <span class="math inline">\(Y\)</span> changes as <span class="math inline">\(X\)</span> changes from 0 to 1 is equivalent to testing whether the outcome distribution changes.</p></li>
<li><p>In the case of a binary covariate, the OR is the ratio of the odds (of a positive outcome) when <span class="math inline">\(X = 1\)</span> compared to when <span class="math inline">\(X = 0\)</span>.</p></li>
<li><p>In the case of a <span class="math inline">\(K\)</span>-category outcome, multinomial regression examines <span class="math inline">\(K − 1\)</span> ratios, <span class="math inline">\(p_1/p_0, p_2/p_0, \ldots, p_{K−1}/p_0\)</span>.</p></li>
<li><p>As in binary regression, testing whether any of these ratios changes as X changes from 0 to 1 is equivalent to testing whether the outcome distribution changes.</p></li>
<li><p>Multinomial regression analysis therefore consists of multiple regressions, each one corresponding to one of these ratios.</p></li>
<li><p>In our example of <span class="math inline">\(K=4\)</span> BMI categories, <span class="math inline">\(p_0=P (Y= normal)\)</span>, <span class="math inline">\(p_1=P (Y= underweight)\)</span>, <span class="math inline">\(p_2= P (Y= overweight)\)</span>, and <span class="math inline">\(p_3= P(Y=obese)\)</span>, and there are three regression models to fit, each of which is similar to the logistic regression in Eq. (4.2):</p></li>
</ul>
<p><span class="math display">\[
log(p_1/p_0) = \beta_0 + \beta_1X \\log(p_2/p_0) = \gamma_0 + \gamma_1X \\log(p_3/p_0) = \delta_0 + \delta_1X.
\]</span></p>
<ul>
<li><p>Note that each model has its own set of parameters and, moreover, the covariates can vary across models.</p></li>
<li><p>However, the odds in the three models share the same reference category (<span class="math inline">\(Y=0\)</span>).</p></li>
<li><p>In each model, the coefficient of X can be interpreted as the log of a quantity analogous to an OR.</p></li>
<li><p>In the first model, this quantity is the ratio <span class="math inline">\(p_1/p_0\)</span> when <span class="math inline">\(X=1\)</span> divided by the ratio <span class="math inline">\(p_1/p_0\)</span> when <span class="math inline">\(X =0\)</span>.</p></li>
<li><p>This is sometimes referred to as a relative risk ratio, but for consistency with the binary case, we will continue to refer to it as an OR.</p></li>
<li><p>If the OR is greater than 1, corresponding to <span class="math inline">\(\beta_1 &gt; 0\)</span>, this means that an increase in <span class="math inline">\(X\)</span> from 0 to 1 corresponds to a shift in the proportion of the outcome in category <span class="math inline">\(Y=0\)</span> into category <span class="math inline">\(Y=1\)</span>.</p></li>
<li><p>Similarly, if the OR in the second regression is greater than 1, corresponding to <span class="math inline">\(\gamma_1 &gt; 0\)</span>, this means that an increase in <span class="math inline">\(X\)</span> from 0 to 1 corresponds to a shift in the proportion of the outcome in category <span class="math inline">\(Y=0\)</span> into category <span class="math inline">\(Y=2\)</span>.</p></li>
<li><p>We fit only three regressions for our four-category variable because the fourth category is completely determined by the first three.</p></li>
</ul>
<p><img src="fig4/table4.9.png" /></p>
<ul>
<li><p>Table 4.9 compares separate logistic regressions to multinomial regression.</p></li>
<li><p>For example, the logistic regression for obese persons was fit using only data for obese and normal weight persons after removing underweight and overweight persons.</p></li>
<li><p>It is therefore different from the logistic regression in Table 4.5, which was fit using underweight, normal and overweight as “non-obese” persons.</p></li>
<li><p>The coefficients in the separate logistic regression and the joint multinomial regression are almost identical; the two sets of results give similar insights into the data.</p></li>
<li><p>For example, the coefficients for YEAR 2015–2016 are all positive; equivalently, the ORs are greater than 1, indicating that the odds of each “non-normal” BMI category relative to the normal category were higher in 2015–2016.</p></li>
<li><p>For a more specific interpretation of these results, consider the multi-category ORs from the multinomial regression, which (like the ORs in the logistic regres- sions) are just the exponentiated coefficients. The ORs for <span class="math inline">\(YEAR=2015–2016\)</span> for the underweight and overweight categories are 1.16 and 1.18, which means that from 1999–2000 to 2015–2016, the estimated probability of each of these categories relative to the normal BMI category increased by 16% and 18%.</p></li>
<li><p>The OR for the obese category is much larger (1.69), showing a larger relative shift into that category between the years.</p></li>
<li><p>In summary, interpretation of multinomial regression results is similar to the interpretation of logistic regression results when data are restricted to only two categories. The effects of the covariates may differ from category to category and may be positive for one category and negative for another.</p></li>
<li><p>For example, age has a significant negative effect for underweight persons, meaning that older people have lower odds of being underweight than younger people (in the age range considered).</p></li>
<li><p>However, age has a significant positive effect for overweight and obese persons, meaning that older people have higher odds of being in these groups than younger people (in the age range considered).</p></li>
</ul>
</div>
<div id="marginal-effects-1" class="section level4 hasAnchor" number="6.3.10.2">
<h4><span class="header-section-number">6.3.10.2</span> Marginal Effects<a href="glm.html#marginal-effects-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>If there are <span class="math inline">\(K\)</span> categories in the outcome, multinomial regression runs <span class="math inline">\(K-1\)</span> regressions and yields <span class="math inline">\(K-1\)</span> sets of coefficients to interpret.</p></li>
<li><p>This can get cumbersome quite quickly; moreover, it can be difficult to make inferences about how the overall shape of the outcome distribution changes based on the estimated relative risk ratios.</p></li>
<li><p>A recycled predictions approach can be used to obtain inferences on the original probability scale of the outcome variable.</p></li>
<li><p>For the YEAR variable, this works by calculating the predicted probabilities of each of the four categories of BMI for each person, first setting their YEAR to 1999–2000 and then to 2015–2016.</p></li>
<li><p>This produces two sets of four numbers with predicted probabilities in either set that sum to 1.</p></li>
<li><p>The four numbers are then averaged across individuals in the sample, and then the difference in averages between sets is calculated to produce a marginal additive effect on the probability scale.</p></li>
<li><p>This exactly replicates the method used for logistic regression.</p></li>
<li><p>The marginal additive effect is the difference in average predicted values for each BMI category and reflects the estimated change in the probability of being in that BMI category between years.</p></li>
</ul>
<p><img src="fig4/table4.10.png" /></p>
<ul>
<li><p>Table 4.10 shows the recycled prediction estimates of the marginal additive effect of YEAR on the probability of being in each BMI category.</p></li>
<li><p>The columns show the average predicted probability of being in each BMI category under the estimated model if all persons were in the indicated survey year.</p></li>
<li><p>The last column shows the difference in 2015–2016 relative to 1999–2000.</p></li>
<li><p>The results indicate that the more recent survey year is associated with a nearly 10% increase in the probability of being in the obese category. This increase in the obese category corresponds to decreases of 7% in the normal and of 3% in the overweight categories.</p></li>
<li><p>There is virtually no change in the underweight category.</p></li>
</ul>
</div>
</div>
<div id="ordered-multinomial-regression" class="section level3 hasAnchor" number="6.3.11">
<h3><span class="header-section-number">6.3.11</span> Ordered Multinomial Regression<a href="glm.html#ordered-multinomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Multinomial regression is sometimes referred to as an “unordered” multinomial logit model, because it is agnostic to any implied ordering of the categories. When categories have an ordering, such as BMI categories, or self-reported health status, another multi-category regression model is also available.</p></li>
<li><p>It makes fairly strong assumptions, so we do not generally recommend its use; however, it is widely available, so we briefly summarize its structure and limitations.</p></li>
<li><p>The proportional odds model is derived assuming that the categories reflect intervals partitioning an underlying continuous scale.</p></li>
<li><p>Like the multinomial model, the proportional odds model includes a set of logit-type regression equations.</p></li>
<li><p>For <span class="math inline">\(Y = BMI\)</span> and <span class="math inline">\(X = YEAR\)</span>, the equations can be written:</p></li>
</ul>
<p><span class="math display">\[
logit[P (Y &gt; k)]= \gamma k + \beta X,
\]</span></p>
<p>for <span class="math inline">\(k=1\)</span> to <span class="math inline">\(K-1\)</span>, where <span class="math inline">\(K=4\)</span> is the number of BMI categories.</p>
<ul>
<li><p>Thus, this model describes how the log odds of being in a higher category relative to being in a lower one depend on covariates.</p></li>
<li><p>When <span class="math inline">\(X=0\)</span>, the coefficients <span class="math inline">\(\gamma_k\)</span> describe the multinomial probability distribution of BMI categories in 1999–2000.</p></li>
<li><p>When <span class="math inline">\(X=1\)</span>, the distribution of BMI is allowed to change but only in a highly restrictive, monotone way because there is only one coefficient, <span class="math inline">\(\beta\)</span>, describing this change.</p></li>
<li><p>If <span class="math inline">\(\beta\)</span> is positive, then the log odds of being in a higher BMI category increases in the later year, and this increase is the same regardless of the specific category.</p></li>
<li><p>Thus, the log odds of being overweight or higher and the log odds of being obese both increase by the same amount from 1999–2000 to 2015–2016.</p></li>
<li><p>On the probability scale, we have probability shifting from lower categories to higher categories in a manner prescribed by <span class="math inline">\(\beta\)</span>.</p></li>
<li><p>But this monotone constraint does not accommodate a variety of association patterns.</p></li>
<li><p>For example, if <span class="math inline">\(X\)</span> is a binary version of an extended age variable that includes all ages, we might find that both overweight and underweight increase for elderly individuals.</p></li>
<li><p>This kind of pattern, where multinomial probabilities become weighted toward extreme categories as a covariate changes values, would not be possible to model or detect with a proportional odds model, but it would be identified with an unordered multinomial logit model.</p></li>
<li><p>Therefore, we generally recommend the unordered model unless a specific hypothesis that conforms with the assumptions of proportional odds is of particular interest.</p></li>
</ul>
<p><br></p>
</div>
</div>
<div id="poisson-regression" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Poisson Regression<a href="glm.html#poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Count data, in which there is no upper limit to the number of
counts, usually fall into two types</p>
<ul>
<li><p>Rates : counts per unit of time/area/distance, etc</p></li>
<li><p>Contingency tables : counts cross-classified by categorical
variables</p></li>
</ul></li>
<li><p>We concentrate on rates data can be modelled using Poisson GLMs with
a log link</p></li>
<li><p>Log-linear model for mean rate:
<span class="math display">\[\log(\lambda_i ) = \beta_0 + \beta_1 x_{1i} + \ldots +\beta_p x_{pi}\]</span>
where <span class="math inline">\(p\)</span> is the number of predictors (or covariates) in the model</p></li>
<li><p>Random component: <span class="math display">\[Y_i|\mathbf{X}_i \sim Poisson(\lambda_i )\]</span></p></li>
<li><p>Here, <span class="math inline">\(\lambda_i = E(Y_i|\mathbf{X}_i)=var(Y_i|\mathbf{X}_i)\)</span></p></li>
<li><p>Exponentiating gives us a model for the rate parameter, or expected
counts:
<span class="math display">\[\lambda_i  = e^{\beta_0 + \beta_1 x_{1i} + \ldots +\beta_p x_{pi}}\]</span></p></li>
<li><p>For Poisson random variables, expectation of <span class="math inline">\(Y_i\)</span> is <span class="math inline">\(\lambda_i\)</span>,
so the log-linear model provides a prediction for the expected value
of <span class="math inline">\(Y_i\)</span></p></li>
<li><p><span class="math inline">\(e^{\beta_j}\)</span>=Rate ratio for a <span class="math inline">\(1\)</span> unit increase in <span class="math inline">\(x_j\)</span>, i.e. rate
ratio for <span class="math inline">\(x_j +1\)</span> compared to <span class="math inline">\(x_j\)</span>, with other covariates held
constant.</p></li>
<li><p><span class="math inline">\(e^{\Delta\beta_j}\)</span>=Rate ratio for a <span class="math inline">\(\Delta\)</span> unit increase in
<span class="math inline">\(x_j\)</span>, i.e. rate ratio for <span class="math inline">\(x_j +\Delta\)</span> compared to <span class="math inline">\(x_j\)</span>, with
other covariates held constant.</p></li>
<li><p><span class="math inline">\(e^{\beta_0}\)</span>=Baseline rate value, i.e. rate for an observation with
all <span class="math inline">\(X\)</span>’s equal to zero.</p></li>
<li><p>The systematic portion of the model allows linear combinations of
the covariates:
<span class="math display">\[\beta_0 + \beta_1 x_{1i} + \ldots +\beta_p x_{pi}\]</span></p></li>
<li><p>Since we have no restrictions on the predictors <span class="math inline">\(X_1 , \ldots, X_p\)</span>,
the predicted values can take any values on the real line:
<span class="math inline">\((-\infty, +\infty)\)</span></p></li>
<li><p>But our outcome variable <span class="math inline">\(Y_i\)</span> consistes of counts, so the expected
value of <span class="math inline">\(Y_i\)</span> has the restriction:
<span class="math inline">\(\lambda_i \in \left[0, +\infty\right)\)</span></p></li>
<li><p>After taking a log transform, we get:
<span class="math inline">\(\log(\lambda_i) \in (-\infty, +\infty)\)</span> which is just what we
wanted.</p></li>
</ul>
<div id="example-danish-lung-cancer-counts" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Example: Danish Lung Cancer Counts<a href="glm.html#example-danish-lung-cancer-counts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Cases of lung cancer were counted in four Danish cities between
<span class="math inline">\(1968\)</span> and <span class="math inline">\(1971\)</span> inclusive.</p></li>
<li><p>We have <span class="math inline">\(24\)</span> observations on each of <span class="math inline">\(4\)</span> variables:</p>
<ul>
<li><p>Cases: the number of lung cancer cases</p></li>
<li><p>Pop: the popilation of each age group in each city</p></li>
<li><p>Age: the categorical age group; one of 40-54, 55-59, 60-64,
65-69, 70-74 or $&gt;$74</p></li>
<li><p>city: the city; one of Fredericia, Horsens, Kolding, or Vejle</p></li>
</ul></li>
<li><p>Questions of interest: How does the expected number of lung cancer
counts vary by age?</p></li>
<li><p>Dataset</p>
<p><img src="glm/dataset.jpg" style="width:60.0%" /></p></li>
<li><p>Boxplots of observed counts versus age category</p>
<p><img src="glm/boxplot.jpg" style="width:60.0%" /></p></li>
<li><p>Model</p></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
                &amp;\log(\lambda_i )\\
                &amp;= \beta_0 +\beta_1 I(Age55-59_i)+\beta_2 I(Age60-64_i) \\
                &amp;+\beta_3 I(Age65-69_i)+\beta_4 I(Age70-74_i)+\beta_5 I(Age&gt;74_i)
            \end{aligned}
\]</span></p>
<ul>
<li><p>We are fitting a model with indicators for each of the age
categories.</p></li>
<li><p>Baseline is the group aged <span class="math inline">\(40-54\)</span>.</p></li>
<li><p><span class="math inline">\(I(age55-59)\)</span> is an indicator of having age <span class="math inline">\(55-59\)</span>, it is equal to
<span class="math inline">\(1\)</span> for those of age <span class="math inline">\(55-59\)</span> and <span class="math inline">\(0\)</span> otherwise.</p></li>
<li><p><span class="math inline">\(I(age60-64)\)</span> is an indicator of having age <span class="math inline">\(60-64\)</span>, it is equal to
<span class="math inline">\(1\)</span> for those of age <span class="math inline">\(60-64\)</span> and <span class="math inline">\(0\)</span> otherwise.</p></li>
<li><p>etc...</p></li>
</ul>
<blockquote>
<p>SAS Code for Poisson regression</p>
</blockquote>
<pre><code>proc genmod data=danish;
class age(ref=&#39;age40-54&#39;);
model cases=age/ **dist=poisson link=log** type3;
estimate &#39;age2&#39; age 1 0 0 0 0 -1/exp;
estimate &#39;age3&#39; age 0 1 0 0 0 -1/exp;
estimate &#39;age4&#39; age 0 0 1 0 0 -1/exp;
estimate &#39;age5&#39; age 0 0 0 1 0 -1/exp;
estimate &#39;age6&#39; age 0 0 0 0 1 -1/exp;
run;</code></pre>
<ul>
<li><p>Results</p>
<p><img src="glm/result1.jpg" style="width:90.0%" /></p></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
                \log(\lambda_i ) &amp;= 2.110 -0.031 I(Age55-59_i)+0.265 I(Age60-64_i) \\
                &amp;+0.310 I(Age65-69_i)+0.192 I(Age70-74_i)\\
                &amp;-0.063 I(Age&gt;74_i)
            \end{aligned}
\]</span></p>
<ul>
<li><p>We interpret <span class="math inline">\(\hat{\beta}_0=2.110\)</span> as the log expected count of
cancer cases among individuals aged <span class="math inline">\(40-54\)</span></p></li>
<li><p>We interpret <span class="math inline">\(\hat{\beta}_0+\hat{\beta}_1=2.079\)</span> as the log expected
count of cancer cases among individuals aged <span class="math inline">\(55-59\)</span></p></li>
<li><p>We interpret <span class="math inline">\(\hat{\beta}_1=-0.031\)</span> as the difference in log
expected count of cancer cases comparing the <span class="math inline">\(55-59\)</span> age group to
the <span class="math inline">\(40-54\)</span> age group. We can also interpret <span class="math inline">\(\hat{\beta}_1\)</span> as a
log relative rate</p></li>
<li><p>We interpret <span class="math inline">\(exp(\hat{\beta}_0)=8.24\)</span> as the expected count of
cancer cases among individuals aged <span class="math inline">\(40-54\)</span></p></li>
<li><p>We interpret <span class="math inline">\(exp(\hat{\beta}_0+\hat{\beta}_1)=8.00\)</span> as the expected
count of cancer cases among individuals aged <span class="math inline">\(55-59\)</span></p></li>
<li><p>We interpret <span class="math inline">\(exp(\hat{\beta}_1)=0.97\)</span> as the ratio of expected
counts comparing the <span class="math inline">\(55-59\)</span> age group to the <span class="math inline">\(40-54\)</span> age group. We
can also interpret <span class="math inline">\(exp(\hat{\beta}_1)\)</span> as a relative rate</p></li>
<li><p>Let’s perform a likelihood ratio test to look at the global
hypothesis:
<span class="math display">\[H_0 : \beta_1 = \beta_2 = \beta_3 = \beta_4 =\beta_5 =0\]</span> versus
the alternative hypothesis:
<span class="math display">\[H_a : \text{at least one of the } \beta_i \text{&#39;s is not } 0, \text{for } i\in 1,\ldots,5\]</span></p></li>
<li><p>Test statistic:</p></li>
</ul>
<span class="math display">\[\begin{aligned}

TS &amp;= -2(logLik(intercept \,\,\, only \,\,\, model)-logLik(Age \,\,\, model)) \\

&amp;= 4.95 \sim \chi_5^2
\end{aligned}\]</span>
<ul>
<li><p>Critical value for the hypothesis at level <span class="math inline">\(\alpha=0.05\)</span>:
<span class="math inline">\(\chi_{5,0.05}^2 = 11.07\)</span>. Fail to reject the null hypothesis.</p></li>
<li><p>Conclusions:</p>
<ul>
<li>Based on the Poisson model of cancer case counts as a function of
Age, we noted a generally increasing number of cases with increasing
age.</li>
<li>The trend was not monotonically increasing with age.</li>
<li>Not a statistically significant result.</li>
</ul></li>
</ul>
</div>
<div id="what-about-accounting-for-population-size" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> What About Accounting for Population Size?<a href="glm.html#what-about-accounting-for-population-size" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>So far we modelled the observed counts of cancer cases as Poisson
counts.</p></li>
<li><p>The population size from each of these counts was drawn is also
known.</p></li>
<li><p>Can we improve our analysis?</p>
<ul>
<li>Each city and age group has a different population size.</li>
</ul></li>
<li><p>If we model expected counts without accounting for population size,
we may just be picking up effects of population distribution by age.</p></li>
<li><p>Accounting for population size can refine our analysis.</p></li>
<li><p>The distribution of population sizes appears bimodal.</p></li>
<li><p>The population sizes range from a minimum of <span class="math inline">\(509\)</span> to a miximum of
<span class="math inline">\(3,142\)</span> people.</p></li>
</ul>
<p><img src="glm/dist.jpg" style="width:60.0%" /></p>
<ul>
<li><p>So far, we have modeled expected counts for each population group,
within the <span class="math inline">\(4\)</span> year period of time: <span class="math display">\[Rate=Counts/4years\]</span></p></li>
<li><p>It may be more intersting to know the rate per person, per <span class="math inline">\(4\)</span> year
period of time:
<span class="math display">\[Rate=\frac{Counts/Population\ size}{4\ years} = \frac{Counts}{4\ person-years}\]</span></p></li>
<li><p>Even better, we can get the rate per person year as:
<span class="math display">\[Rate=\frac{Counts/4\ Population\ size}{4\ years} = \frac{Counts}{person-years}\]</span></p></li>
<li><p>Here, <span class="math inline">\(4\cdot Population\ size\)</span> is equal to the number of
person-years that we observed to obtain each count.</p></li>
<li><p>We can think of the person years at the denominator to be used to
calculate the cancer rate per person, per year.</p></li>
<li><p>If we prefer the cancer rate per <span class="math inline">\(100\)</span> person-years, we can just
multiply the rate per person-year by <span class="math inline">\(100\)</span>.</p></li>
<li><p>Suppose we are told that <span class="math inline">\(100,000\)</span> new cases of HIV were reported in
the world, during the past three years.</p></li>
<li><p>What is the incidence rate of HIV?</p></li>
<li><p>We can calculate incidence as:</p></li>
</ul>
<p><span class="math display">\[\begin{aligned}
                &amp; \frac{Number\ of\ new\ cases}{Number\ of\ people\ observed \cdot Amount\ of\ time\ observed} \\
                &amp;= \frac{100,000\ cases}{6,000,000,000\ people\ in\ the\ world \cdot 3\ years\ of\ observation}\\
                &amp;= 5.55 \times 10^{-6} \ cases / person - year \\
                &amp;= 5.55\ cases/ 1,000,000\ person - years
            \end{aligned}
\]</span></p>
<p>Based on this incidence rate, we could say
that each year, there are about <span class="math inline">\(5.55\)</span> new cases of HIV per
<span class="math inline">\(1,000,000\)</span> people per year.</p>
<ul>
<li><p>So far, we have written a model for the expected number of counts
over the <span class="math inline">\(4\)</span> year period of observation.</p></li>
<li><p>However, if we know that the total populations generating our counts
differ substantially, it does not make sense to write a log-linear
model to consider expected counts direct.</p></li>
<li><p>What we really wnat is to consider, the rate per person year
<span class="math display">\[r_i = \frac{\lambda_i } {Pop_i}=\frac{E(count_i )}{Pop_i}\]</span> and
model that by a log-linear model.</p></li>
<li><p>Based on this model, we can still say:
<span class="math display">\[Y_i \sim Poisson(\lambda_i )=Poisson(r_i \cdot Pop_i)\]</span></p></li>
<li><p>On a log scale, our model will be:</p></li>
</ul>
<p><span class="math display">\[\begin{aligned}
                \log(r_{i})&amp;=\log(\frac{\lambda_i}{Pop_i} )=\beta_0 +\beta_1 I(Age55-59_i)+\beta_2 I(Age60-64_i) \\
                &amp;+\beta_3 I(Age65-69_i)+\beta_4 I(Age70-74_i)+\beta_5 I(Age&gt;74_i)
            \end{aligned}
\]</span></p>
<ul>
<li>Exponentiating, we get:</li>
</ul>
<p><span class="math display">\[\begin{aligned}
                \frac{\lambda_i}{Pop_i} &amp;= exp\{ \beta_0 +\beta_1 I(Age55-59_i)+\beta_2 I(Age60-64_i) \\
                &amp;+\beta_3 I(Age65-69_i)+\beta_4 I(Age70-74_i)+\beta_5 I(Age&gt;74_i) \}
            \end{aligned}
\]</span></p>
<ul>
<li><p>Divide the <span class="math inline">\(\lambda_i\)</span>s by the population size that yielded each
count to get rates "per <span class="math inline">\(4\)</span> person-years".</p></li>
<li><p>Now that we have a model of rates per <span class="math inline">\(4\)</span> person-years, we can
divide by <span class="math inline">\(4\)</span> to get rate per person-year.</p></li>
<li><p>We can then multiply by <span class="math inline">\(10,000\)</span> to get rates per <span class="math inline">\(10,000\)</span>
person-years (maybe easier to interpret than person-years).</p></li>
<li><p>Dividing by 4 and multiplying by <span class="math inline">\(10,000\)</span> is the same as multiplying
by <span class="math inline">\(2500\)</span>:</p></li>
<li><p>Exponentiating, we get:</p></li>
</ul>
<p><span class="math display">\[\begin{aligned}
                &amp;\frac{\lambda_i}{Pop_i/2500} = exp\{ \beta_0 +\beta_1 I(Age55-59_i)+\beta_2 I(Age60-64_i) \\
                &amp;+\beta_3 I(Age65-69_i)+\beta_4 I(Age70-74_i)+\beta_5 I(Age&gt;74_i) \}
            \end{aligned}
\]</span></p>
<ul>
<li>Finally, we should take a log-transform to get back our log-linear
model:</li>
</ul>
<p><span class="math display">\[\begin{aligned}
                &amp;\log(\frac{\lambda_i}{Pop_i /2500} ) \\
                &amp;=\beta_0 +\beta_1 I(Age55-59_i)+\beta_2 I(Age60-64_i) \\
                &amp;+\beta_3 I(Age65-69_i)+\beta_4 I(Age70-74_i)+\beta_5 I(Age&gt;74_i)
            \end{aligned}
\]</span></p>
<ul>
<li>Further, we can move the population denominator to the other size of
the equation:</li>
</ul>
<p><span class="math display">\[\begin{aligned}
                &amp;\log(\lambda_i ) \\
                &amp;= \log(Pop_i/2500) + \beta_0 +\beta_1 I(Age55-59_i)+\beta_2 I(Age60-64_i) \\
                &amp;+\beta_3 I(Age65-69_i)+\beta_4 I(Age70-74_i)+\beta_5 I(Age&gt;74_i)
            \end{aligned}
\]</span></p>
<ul>
<li><p>Here, we call the amount <span class="math inline">\(\log(Pop_i/2500)\)</span> the <strong>offset</strong>.</p></li>
<li><p>using the offset is just a way of accounting for population sizes,
which could vary by age, region, etc.</p></li>
<li><p>The term "offset" is jargon for predictor terms whose <span class="math inline">\(\beta\)</span>
coefficient is forced to be <span class="math inline">\(+1\)</span>.</p></li>
<li><p>Using an offset gives us a convenient way to model rates per
person-years, instead of just modeling the raw counts.</p></li>
<li><p>If all the observations have the same exposure, the model does not
need an offset term and we can model <span class="math inline">\(\log(\lambda_i)\)</span> directly.</p></li>
</ul>
<p><br></p>
<blockquote>
<p>SAS Code for Poisson regression</p>
</blockquote>
<pre><code>data danish;\
set danish;\
logpop=log(pop/2500);\
run;\
proc genmod data=danish;\
class age(ref=&#39;age40-54&#39;);\
model cases=age/**offset=logpop** dist=poisson link=log type3;\
estimate &#39;age2&#39; age 1 0 0 0 0 -1/exp;\
estimate &#39;age3&#39; age 0 1 0 0 0 -1/exp;\
estimate &#39;age4&#39; age 0 0 1 0 0 -1/exp;\
estimate &#39;age5&#39; age 0 0 0 1 0 -1/exp;\
estimate &#39;age6&#39; age 0 0 0 0 1 -1/exp;\
run;
</code></pre>
<ul>
<li><p>Results</p>
<p><img src="glm/result2.jpg" style="width:90.0%" /></p></li>
<li><p>After including the offset in our model, we need to change our
regression coefficient interpretations a bit.</p></li>
<li><p>We should think of the outcome as <span class="math inline">\(\log(\lambda_i)-\text{offset}_i\)</span>.</p></li>
<li><p>In this case, <span class="math inline">\(\lambda_i\)</span> was the expected number of cases observed
in a particular age group and city, within a <span class="math inline">\(4\)</span> year period of
time.</p></li>
<li><p>Our offset was <span class="math inline">\(\log(Pop_i/2500)\)</span>.</p></li>
<li><p>So, we should think of the outcome as log rate per <span class="math inline">\(10,000\)</span> person
years.</p></li>
<li><p><span class="math inline">\(\beta_0\)</span> is the log rate of cancer cases per <span class="math inline">\(10,000\)</span> person years
in the baseline age group of <span class="math inline">\(40-54\)</span>.</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> is the log relative rate of cancer cases per <span class="math inline">\(10,000\)</span>
person years comparing the age group <span class="math inline">\(55-59\)</span> to the baseline age
group <span class="math inline">\(40-54\)</span>.</p></li>
<li><p><span class="math inline">\(\beta_2\)</span> is the log relative rate of cancer cases per <span class="math inline">\(10,000\)</span>
person years comparing the age group <span class="math inline">\(60-64\)</span> to the baseline age
group <span class="math inline">\(40-54\)</span>.</p></li>
<li><p>Before we put the offset in our model, none of our regression
coefficients were statistically significant.</p></li>
<li><p><span class="math inline">\(\Rightarrow\)</span> without the offset, there was no statistically
sifnificant difference in the expected counts per year at age group
compared to the baseline <span class="math inline">\(40-54\)</span> group.</p></li>
<li><p>After including the offset, we’re looking for differences in the
expected counts per person-year, across age groups.</p></li>
<li><p>Let’s perform a likelihood ratio test to look at the global
hypothesis:
<span class="math display">\[H_0 : \beta_1 = \beta_2 = \beta_3 = \beta_4 =\beta_5 =0\]</span> versus
the alternative hypothesis:
<span class="math display">\[H_a : \text{at least one of the } \beta_i \text{&#39;s is not } 0, \text{for } i\in 1,\ldots,5\]</span></p></li>
<li><p>Test statistic:</p></li>
</ul>
<span class="math display">\[\begin{aligned}
TS &amp; \\ &amp;= -2(logLik(intercept \,\, only, \,\, with \,\, offset)-logLik(Age \,\, model \,\, with \,\,
    offset)) \\ &amp;= 101.6 \sim \chi_5^2
\end{aligned}\]</span>
<ul>
<li><p>Critical value for the hypothesis at level <span class="math inline">\(\alpha=0.05\)</span>:
<span class="math inline">\(\chi_{5,0.05}^2 = 11.07\)</span>. reject the null hypothesis.</p></li>
<li><p>Model <span class="math display">\[\begin{aligned}
            &amp;\log(\lambda_i ) \\
            &amp;= \log(Pop_i/2500) + \beta_0 +\beta_1 I(Age55-59_i)+\beta_2 I(Age60-64_i) \\
            &amp;+\beta_3 I(Age65-69_i)+\beta_4 I(Age70-74_i)+\beta_5 I(Age&gt;74_i)
        \end{aligned}\]</span></p></li>
<li><p>Predicted log rate of cancer per <span class="math inline">\(10,000\)</span> person years among <span class="math inline">\(40-54\)</span>
year olds; <span class="math display">\[\hat{\beta}_0=1.96\]</span></p></li>
<li><p>Predicted rate of cancer per <span class="math inline">\(10,000\)</span> person years among <span class="math inline">\(40-54\)</span>
year olds; <span class="math display">\[exp(\hat{\beta}_0)=exp(1.96)=7.09\]</span></p></li>
<li><p>Based on this model, we predict <span class="math inline">\(7.09\)</span> new cases of lung cancer per
<span class="math inline">\(10,000\)</span> <span class="math inline">\(40-54\)</span> year olds in Denmark, per year.</p></li>
<li><p>Predicted log rate of cancer per <span class="math inline">\(10,000\)</span> person years among <span class="math inline">\(55-59\)</span>
year olds; <span class="math display">\[\hat{\beta}_0 + \hat{\beta}_1=3.04\]</span></p></li>
<li><p>Predicted rate of cancer per <span class="math inline">\(10,000\)</span> person years among <span class="math inline">\(55-59\)</span>
year olds;
<span class="math display">\[exp(\hat{\beta}_0 + \hat{\beta}_1)=exp(1.96+1.08)=20.9\]</span></p></li>
<li><p>Based on this model, we predict <span class="math inline">\(20.9\)</span> new cases of lung cancer per
<span class="math inline">\(10,000\)</span> <span class="math inline">\(55-59\)</span> year olds in Denmark, per year.</p></li>
<li><p>We predicted an incidence rate of <span class="math inline">\(7.09\)</span> cases per <span class="math inline">\(10,000\)</span> per year
among <span class="math inline">\(40-54\)</span> year olds, and a rate of <span class="math inline">\(20.9\)</span> new cases per <span class="math inline">\(10,000\)</span>
person years for <span class="math inline">\(55-59\)</span> years olds.</p></li>
</ul>
<p>      <span class="math inline">\(\Rightarrow\)</span> The relative rate per <span class="math inline">\(10,000\)</span> person years comparing
<span class="math inline">\(55-59\)</span> years olds to <span class="math inline">\(40-54\)</span> years olds is
<span class="math inline">\(20.9/7.09 \approx 2.94\)</span>.</p>
<ul>
<li><p>We could have gotten the same answer by taking
<span class="math inline">\(exp(\hat{\beta}_1)=exp(1.08)=2.94\)</span>.</p></li>
<li><p><span class="math inline">\(exp(\hat{\beta}_1)\)</span> is the relative rate cancer cases per <span class="math inline">\(10,000\)</span>
person years comparing <span class="math inline">\(55-59\)</span> years olds to <span class="math inline">\(40-54\)</span> years olds.</p></li>
<li><p><span class="math inline">\(\hat{\beta}_1\)</span> is the log relative rate.</p></li>
<li><p>There is an increasing trend in relative rates compared to the
baseline <span class="math inline">\(40-54\)</span> year old group as age increases.</p></li>
<li><p>The only exception for this trend is for the <span class="math inline">\(Age&gt;74\)</span> group.</p></li>
<li><p>The model fit matches what we know from biology: the risk of cancer
does increase with age, but trails of for the oldest individuals
perhaps because</p>
<ul>
<li><p>Those surviving to age <span class="math inline">\(74\)</span> and beyond have genes which protect
against cancer.</p></li>
<li><p>Cell growth slows down at older ages, slowing the growth of
tumors.</p></li>
</ul></li>
<li><p>The purpose of an offset is the change the denominator or units of a
rate.</p></li>
<li><p>Often, the model without an offset does not make much sense, and
likely fails our Poisson assumptions.</p></li>
<li><p>We should always try to use an offset if we suspect that the
underlying population sizes differ for each of the observed counts.</p></li>
<li><p>Typically the offset will take value <span class="math inline">\(\log(N)\)</span> where <span class="math inline">\(N\)</span> is the
sample size, or the number of person-years.</p></li>
<li><p>If the underlying population sizes are not available, we just have
to do our best - but be careful about applying the Poisson model.</p></li>
<li><p>Log-linear models can be a good way to approach count data.</p></li>
<li><p>If population sizes or denominators are available, it’s a good idea
to include an offset.</p></li>
<li><p>Log-linear models can also be useful in analyzing binary data from
cohort studies, but with care.</p></li>
</ul>
<!-------------------------------------->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="psm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Statistical Analysis.pdf", "Statistical Analysis.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
