<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Support Vector Machines | Statistical Learning</title>
  <meta name="description" content="This is a Statistical Learning" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Support Vector Machines | Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Statistical Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Support Vector Machines | Statistical Learning" />
  
  <meta name="twitter:description" content="This is a Statistical Learning" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2023-05-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter9.html"/>
<link rel="next" href="chapter11.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>머리말</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#understanding-data"><i class="fa fa-check"></i><b>1.1</b> Understanding Data</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="chapter1.html"><a href="chapter1.html#example-1-supervised-learning-continuous-output"><i class="fa fa-check"></i><b>1.1.1</b> Example 1 (Supervised learning: Continuous output)</a></li>
<li class="chapter" data-level="1.1.2" data-path="chapter1.html"><a href="chapter1.html#example-2-supervised-leraning-categorical-output"><i class="fa fa-check"></i><b>1.1.2</b> Example 2 (Supervised leraning: Categorical output)</a></li>
<li class="chapter" data-level="1.1.3" data-path="chapter1.html"><a href="chapter1.html#example-3-unsupervised-learning-clustering-observations"><i class="fa fa-check"></i><b>1.1.3</b> Example 3 (Unsupervised learning: Clustering observations)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#brief-history-of-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Brief History of Statistical Learning</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#notation-and-simple-matrix-algebra"><i class="fa fa-check"></i><b>1.3</b> Notation and Simple Matrix Algebra</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#what-is-statitical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statitical Learning</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chapter2.html"><a href="chapter2.html#prediction"><i class="fa fa-check"></i><b>2.1.1</b> Prediction</a></li>
<li class="chapter" data-level="2.1.2" data-path="chapter2.html"><a href="chapter2.html#inference"><i class="fa fa-check"></i><b>2.1.2</b> Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="chapter2.html"><a href="chapter2.html#estimating-f"><i class="fa fa-check"></i><b>2.1.3</b> Estimating <span class="math inline">\(f\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="chapter2.html"><a href="chapter2.html#prediction-accuaracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.4</b> Prediction Accuaracy and Model Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter2.html"><a href="chapter2.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="chapter2.html"><a href="chapter2.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="2.2.3" data-path="chapter2.html"><a href="chapter2.html#classification-problems"><i class="fa fa-check"></i><b>2.2.3</b> Classification Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#basic-commands"><i class="fa fa-check"></i><b>3.1</b> Basic Commands</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#graphics"><i class="fa fa-check"></i><b>3.2</b> Graphics</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#indexing-data"><i class="fa fa-check"></i><b>3.3</b> Indexing Data</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#loading-data"><i class="fa fa-check"></i><b>3.4</b> Loading Data</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#additional-graphical-and-numerical-summaries"><i class="fa fa-check"></i><b>3.5</b> Additional Graphical and Numerical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#simple-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Regression</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#multiple-regression"><i class="fa fa-check"></i><b>4.2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chapter4.html"><a href="chapter4.html#importance-of-predictors-statistical-significance-of-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Importance of Predictors: Statistical Significance of Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="chapter4.html"><a href="chapter4.html#selecting-important-variables"><i class="fa fa-check"></i><b>4.2.2</b> Selecting Important Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="chapter4.html"><a href="chapter4.html#model-fit"><i class="fa fa-check"></i><b>4.2.3</b> Model Fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="chapter4.html"><a href="chapter4.html#prediction-1"><i class="fa fa-check"></i><b>4.2.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#other-consideration"><i class="fa fa-check"></i><b>4.3</b> Other Consideration</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chapter4.html"><a href="chapter4.html#qualitative-predictors"><i class="fa fa-check"></i><b>4.3.1</b> Qualitative Predictors</a></li>
<li class="chapter" data-level="" data-path="chapter4.html"><a href="chapter4.html#example-8"><i class="fa fa-check"></i>Example 8</a></li>
<li class="chapter" data-level="4.3.2" data-path="chapter4.html"><a href="chapter4.html#potential-problems"><i class="fa fa-check"></i><b>4.3.2</b> Potential Problems</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#non-parametric-regressions"><i class="fa fa-check"></i><b>4.4</b> Non-parametric Regressions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#logit-and-probit-models"><i class="fa fa-check"></i><b>5.1</b> Logit and Probit Models</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#confounding"><i class="fa fa-check"></i><b>5.2</b> Confounding</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#logit-model-for-multiple-classes"><i class="fa fa-check"></i><b>5.3</b> Logit Model for Multiple Classes</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#discriminant-analysis"><i class="fa fa-check"></i><b>5.4</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chapter5.html"><a href="chapter5.html#bayes-theorem-for-classification"><i class="fa fa-check"></i><b>5.4.1</b> Bayes Theorem for Classification</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapter5.html"><a href="chapter5.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="5.4.3" data-path="chapter5.html"><a href="chapter5.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.3</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#naive-bayes"><i class="fa fa-check"></i><b>5.5</b> Naive Bayes</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#knn-classification"><i class="fa fa-check"></i><b>5.6</b> KNN Classification</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#example-10"><i class="fa fa-check"></i><b>5.7</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Resampling Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#validation-set-approach"><i class="fa fa-check"></i><b>6.1</b> Validation Set Approach</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.2</b> K-fold Cross-validation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chapter6.html"><a href="chapter6.html#example-11"><i class="fa fa-check"></i><b>6.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#bootstrap"><i class="fa fa-check"></i><b>6.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Model Selection</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#variable-selection"><i class="fa fa-check"></i><b>7.1</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chapter7.html"><a href="chapter7.html#best-subset-selection"><i class="fa fa-check"></i><b>7.1.1</b> Best Subset Selection</a></li>
<li class="chapter" data-level="7.1.2" data-path="chapter7.html"><a href="chapter7.html#forward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.2</b> Forward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.3" data-path="chapter7.html"><a href="chapter7.html#backward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.3</b> Backward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.4" data-path="chapter7.html"><a href="chapter7.html#choosing-the-optimal-model"><i class="fa fa-check"></i><b>7.1.4</b> Choosing the Optimal Model</a></li>
<li class="chapter" data-level="7.1.5" data-path="chapter7.html"><a href="chapter7.html#example-13"><i class="fa fa-check"></i><b>7.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#shrinkage-methods"><i class="fa fa-check"></i><b>7.2</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chapter7.html"><a href="chapter7.html#ridge-regression"><i class="fa fa-check"></i><b>7.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="chapter7.html"><a href="chapter7.html#rasso-regression"><i class="fa fa-check"></i><b>7.2.2</b> RASSO Regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="chapter7.html"><a href="chapter7.html#example-14"><i class="fa fa-check"></i><b>7.2.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#dimension-reduction-methods"><i class="fa fa-check"></i><b>7.3</b> Dimension Reduction Methods</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="chapter7.html"><a href="chapter7.html#principal-components-regression"><i class="fa fa-check"></i><b>7.3.1</b> Principal Components Regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="chapter7.html"><a href="chapter7.html#example-15"><i class="fa fa-check"></i><b>7.3.2</b> Example</a></li>
<li class="chapter" data-level="7.3.3" data-path="chapter7.html"><a href="chapter7.html#partial-least-squares"><i class="fa fa-check"></i><b>7.3.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="7.3.4" data-path="chapter7.html"><a href="chapter7.html#example-16"><i class="fa fa-check"></i><b>7.3.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#polynomial-regression"><i class="fa fa-check"></i><b>8.1</b> Polynomial Regression</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#step-functions"><i class="fa fa-check"></i><b>8.2</b> Step Functions</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#piecewise-polynomials---splines"><i class="fa fa-check"></i><b>8.3</b> Piecewise Polynomials - Splines</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#local-regression"><i class="fa fa-check"></i><b>8.4</b> Local Regression</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#generalized-additive-models"><i class="fa fa-check"></i><b>8.5</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Tree-based Methods</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#the-basiscs-of-decision-trees"><i class="fa fa-check"></i><b>9.1</b> The Basiscs of Decision Trees</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#classification-trees"><i class="fa fa-check"></i><b>9.2</b> Classification Trees</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#bagging"><i class="fa fa-check"></i><b>9.3</b> Bagging</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#random-forests"><i class="fa fa-check"></i><b>9.4</b> Random Forests</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#boosing"><i class="fa fa-check"></i><b>9.5</b> Boosing</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Support Vector Machines</a></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#principal-components-anlsysis"><i class="fa fa-check"></i><b>11.1</b> Principal Components Anlsysis</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#clustering"><i class="fa fa-check"></i><b>11.2</b> Clustering</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chapter11.html"><a href="chapter11.html#k-means-clustering"><i class="fa fa-check"></i><b>11.2.1</b> <span class="math inline">\(K\)</span>-means clustering</a></li>
<li class="chapter" data-level="11.2.2" data-path="chapter11.html"><a href="chapter11.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.2.2</b> Hierarchical Clustering</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter10" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Support Vector Machines<a href="chapter10.html#chapter10" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li>Here we approach the two-class classification problem in a direct way:</li>
</ul>
<blockquote>
<p>We try and find a plane that separates the classes in feature space.</p>
</blockquote>
<ul>
<li>If we cannot, we get creative in two ways:
<ul>
<li>We soften what we mean by “separates”, and</li>
<li>We enrich and enlarge the feature space so that separation is possible.</li>
</ul></li>
</ul>
<div id="what-is-a-hyperplane" class="section level4 unnumbered hasAnchor">
<h4>What is a Hyperplane?<a href="chapter10.html#what-is-a-hyperplane" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>A hyperplane in <span class="math inline">\(p\)</span> dimensions is a flat affine subspace of dimension <span class="math inline">\(p-1\)</span>.</p></li>
<li><p>In general the equation for a hyperplane has the form</p></li>
</ul>
<p><span class="math display">\[
\beta_0+\beta_1X_1 +\beta_2X_2 +\cdots +\beta_pX_p=0
\]</span></p>
<ul>
<li><p>In <span class="math inline">\(p=2\)</span> dimensions a hyperplane is a line.</p></li>
<li><p>If <span class="math inline">\(\beta_0=0\)</span>, the hyperplane goes through the origin, otherwise not.</p></li>
<li><p>The vecctor <span class="math inline">\(\beta=(\beta_1,\beta_2, \cdots, \beta_p)\)</span> is called the normal vector - it points in a direction orthogonal to the surface of a hyperplane.</p></li>
<li><p>Hyperplane in 2 dimensions</p></li>
</ul>
<p><img src="fig10/f1.png" /></p>
<ul>
<li>The hyperplane <span class="math inline">\(1+2x_1+3x_2=0\)</span> divides the <span class="math inline">\(xy\)</span>-plane in which <span class="math inline">\(1+2x_1+3x_2&gt;0\)</span> is indicated by green points and <span class="math inline">\(1+2x_1+3x_2&lt;0\)</span> by red points.</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chapter10.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb1-2"><a href="chapter10.html#cb1-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">bquote</span>(x[<span class="dv">1</span>]), <span class="at">ylab =</span> <span class="fu">bquote</span>(x[<span class="dv">2</span>]),</span>
<span id="cb1-3"><a href="chapter10.html#cb1-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Affine hyperplane: 1 + 2&quot;</span>, x[<span class="dv">1</span>], <span class="st">&quot; + 3&quot;</span>, x[<span class="dv">2</span>], <span class="st">&quot; = 0&quot;</span>, <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)))</span>
<span id="cb1-4"><a href="chapter10.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="do">## add line</span></span>
<span id="cb1-5"><a href="chapter10.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>, <span class="at">b =</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>)</span>
<span id="cb1-6"><a href="chapter10.html#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="chapter10.html#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="do">## points</span></span>
<span id="cb1-8"><a href="chapter10.html#cb1-8" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> x2 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">2</span>, <span class="at">to =</span> <span class="dv">2</span>, <span class="at">length =</span> <span class="dv">40</span>)</span>
<span id="cb1-9"><a href="chapter10.html#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x1)) {</span>
<span id="cb1-10"><a href="chapter10.html#cb1-10" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">rep</span>(x1[i], <span class="at">times =</span> <span class="fu">length</span>(x1))</span>
<span id="cb1-11"><a href="chapter10.html#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(<span class="at">x =</span> x, <span class="at">y =</span> x2, <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span> .<span class="dv">5</span>,</span>
<span id="cb1-12"><a href="chapter10.html#cb1-12" aria-hidden="true" tabindex="-1"></a>           <span class="at">col =</span> <span class="fu">ifelse</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x2 <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>))</span>
<span id="cb1-13"><a href="chapter10.html#cb1-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-14"><a href="chapter10.html#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> <span class="dv">1</span>, <span class="at">y =</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb1-15"><a href="chapter10.html#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> <span class="dv">1</span>, <span class="at">y =</span> <span class="dv">1</span>, <span class="at">labels =</span> <span class="st">&quot;(1, 1)&quot;</span>, <span class="at">cex =</span> .<span class="dv">9</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pos =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex1011-1.png" width="672" /></p>
</div>
<div id="separating-hyperplanes" class="section level4 unnumbered hasAnchor">
<h4>Separating hyperplanes<a href="chapter10.html#separating-hyperplanes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig10/f2.png" /></p>
<ul>
<li><p>If <span class="math inline">\(f(X)=\beta_0+\beta_1X_1 +\beta_2X_2 +\cdots +\beta_pX_p\)</span>, then <span class="math inline">\(f(X)&gt;0\)</span> for points on one side of the hyperplane, and <span class="math inline">\(f(X)&lt;0\)</span> for points on the other.</p></li>
<li><p>If we code the colored points as <span class="math inline">\(Y_i=+1\)</span> for blue, say, and <span class="math inline">\(Y_i=-1\)</span> for mauve, then if $Y_if(X_i)&gt;0 for all <span class="math inline">\(i\)</span>, <span class="math inline">\(f(X)=0\)</span> defines a <em>separating hyperplane</em>.</p></li>
</ul>
</div>
<div id="maximal-margin-classifier" class="section level4 unnumbered hasAnchor">
<h4>Maximal margin classifier<a href="chapter10.html#maximal-margin-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Among all separating hyperplanes, find the one that makes the biggest gap or margin between the two classes.</li>
</ul>
<p><img src="fig10/f3.png" /></p>
<ul>
<li>This can be rephrased as a convex quadratic program, and solved efficiently.
<ul>
<li>The function svm() in package e1071 solves this problem efficiently.</li>
</ul></li>
</ul>
</div>
<div id="non-separable-data" class="section level4 unnumbered hasAnchor">
<h4>Non-separable data<a href="chapter10.html#non-separable-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig10/f4.png" /></p>
</div>
<div id="noisy-data" class="section level4 unnumbered hasAnchor">
<h4>Noisy data<a href="chapter10.html#noisy-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig10/f5.png" /></p>
<ul>
<li>Sometimes the data are separable, but noisy.
<ul>
<li>This can lead to a poor solution for the maximal-margin classifier.</li>
</ul></li>
<li>The <em>support vector classifier</em> maximizes a <em>soft</em> margin.</li>
</ul>
</div>
<div id="support-vector-classifier" class="section level4 unnumbered hasAnchor">
<h4>Support vector classifier<a href="chapter10.html#support-vector-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig10/f6.png" /></p>
<ul>
<li><span class="math inline">\(C\)</span> is a regularization parameter</li>
</ul>
<p><img src="fig10/f7.png" /></p>
</div>
<div id="example-19" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter10.html#example-19" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>R livrary e1071 contains svm() function to perform support vector machines and a number of other methods.</p></li>
<li><p>To illustrate the support vector classifier, consider simulated data generated from <span class="math inline">\(N(\mu_i, \Sigma)\)</span>, <span class="math inline">\(i=1,2\)</span>, with <span class="math inline">\(\mu_1=(0,0)&#39;\)</span>, <span class="math inline">\(\mu_2=(1,1)&#39;\)</span>, and <span class="math inline">\(\Sigma=I_2\)</span>, a <span class="math inline">\(2\times 2\)</span> identity matrix.</p></li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="chapter10.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># for exact replication</span></span>
<span id="cb2-2"><a href="chapter10.html#cb2-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">20</span> <span class="sc">*</span> <span class="dv">2</span>), <span class="at">ncol =</span> <span class="dv">2</span>) <span class="co"># two bivariate standard normal variates</span></span>
<span id="cb2-3"><a href="chapter10.html#cb2-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="at">times =</span> <span class="dv">10</span>), <span class="fu">rep</span>(<span class="dv">1</span>, <span class="at">times =</span> <span class="dv">10</span>)) <span class="co"># class identifiers -1 and +1</span></span>
<span id="cb2-4"><a href="chapter10.html#cb2-4" aria-hidden="true" tabindex="-1"></a>x[y <span class="sc">==</span> <span class="dv">1</span>, ] <span class="ot">&lt;-</span> x[y <span class="sc">==</span> <span class="dv">1</span>, ] <span class="sc">+</span> <span class="dv">1</span> <span class="co"># set the mean of the second to mu = (1, 1)</span></span>
<span id="cb2-5"><a href="chapter10.html#cb2-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">as.factor</span>(y)) <span class="co"># data frame for svm</span></span>
<span id="cb2-6"><a href="chapter10.html#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(data) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>, <span class="st">&quot;y&quot;</span>) <span class="co"># ccolumn names </span></span>
<span id="cb2-7"><a href="chapter10.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data) </span></code></pre></div>
<pre><code>##           x1          x2  y
## 1 -0.6264538  0.91897737 -1
## 2  0.1836433  0.78213630 -1
## 3 -0.8356286  0.07456498 -1
## 4  1.5952808 -1.98935170 -1
## 5  0.3295078  0.61982575 -1
## 6 -0.8204684 -0.05612874 -1</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="chapter10.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> data<span class="sc">$</span>x2, <span class="at">y =</span> data<span class="sc">$</span>x1, <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>]), <span class="at">ylab =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]),</span>
<span id="cb4-2"><a href="chapter10.html#cb4-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="sc">-</span><span class="dv">1</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="co"># blue for group -1 and red for +1</span></span>
<span id="cb4-3"><a href="chapter10.html#cb4-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Two normal populations&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex1021-1.png" width="672" /></p>
<ul>
<li>The figure shows that the classes are not linearly separable.</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="chapter10.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages(&quot;e1071&quot;, repos = &quot;https://cran.uib.no&quot;)</span></span>
<span id="cb5-2"><a href="chapter10.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb5-3"><a href="chapter10.html#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">#library(help = e1071) # a brief description of the methods</span></span>
<span id="cb5-4"><a href="chapter10.html#cb5-4" aria-hidden="true" tabindex="-1"></a>svmfit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> data,</span>
<span id="cb5-5"><a href="chapter10.html#cb5-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">kernel =</span> <span class="st">&quot;linear&quot;</span>, <span class="co"># svm with kernel &quot;linear&quot; produces sv classifier</span></span>
<span id="cb5-6"><a href="chapter10.html#cb5-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">cost =</span> <span class="dv">10</span>, <span class="co"># this is related to C in equation (13)</span></span>
<span id="cb5-7"><a href="chapter10.html#cb5-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">scale =</span> <span class="cn">FALSE</span> <span class="co"># here no need to scale predictors</span></span>
<span id="cb5-8"><a href="chapter10.html#cb5-8" aria-hidden="true" tabindex="-1"></a>              ) <span class="co"># svm</span></span>
<span id="cb5-9"><a href="chapter10.html#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svmfit, <span class="at">data =</span> data, <span class="at">formula =</span> x2 <span class="sc">~</span> x1) <span class="co"># (see help(plot.svm)),</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/ex1022-1.png" width="672" /></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="chapter10.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">##          by default x1 is on the vertical axis, using formula = x2 ~ x1 changes the axes</span></span>
<span id="cb6-2"><a href="chapter10.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="do">## help(plot.svm)</span></span>
<span id="cb6-3"><a href="chapter10.html#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="do">## dev.print(pdf, file = &quot;../lectures/figures/ex82b.pdf&quot;)</span></span>
<span id="cb6-4"><a href="chapter10.html#cb6-4" aria-hidden="true" tabindex="-1"></a>svmfit<span class="sc">$</span>index <span class="co"># print support vector indexes</span></span></code></pre></div>
<pre><code>## [1]  1  2  5  7 14 16 17</code></pre>
<ul>
<li><p>The decision boundary is liner (although appears jigged).</p></li>
<li><p>The seven support vectors here are indicated by crosses.</p></li>
<li><p>Due to the linearity one training observation becomes misclassified.</p></li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="chapter10.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(svmfit) <span class="co"># some summary</span></span></code></pre></div>
<pre><code>## 
## Call:
## svm(formula = y ~ ., data = data, kernel = &quot;linear&quot;, cost = 10, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  10 
## 
## Number of Support Vectors:  7
## 
##  ( 4 3 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="chapter10.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">coef</span>(svmfit), <span class="dv">3</span>) <span class="co"># coefficients of the linear decision boundary</span></span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##       1.507      -1.556      -1.612</code></pre>
<ul>
<li><p>The tune() can be used for (10-fold) cross-validation.</p></li>
<li><p>The function needs a range for the cost parameter.</p></li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="chapter10.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># for exact replication</span></span>
<span id="cb12-2"><a href="chapter10.html#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="chapter10.html#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="do">## help(tune)</span></span>
<span id="cb12-4"><a href="chapter10.html#cb12-4" aria-hidden="true" tabindex="-1"></a>tune.out <span class="ot">&lt;-</span> <span class="fu">tune</span>(svm, <span class="co"># function to be tuned</span></span>
<span id="cb12-5"><a href="chapter10.html#cb12-5" aria-hidden="true" tabindex="-1"></a>                 y<span class="sc">~</span>., <span class="at">data =</span> data, <span class="co"># formula expression alternative</span></span>
<span id="cb12-6"><a href="chapter10.html#cb12-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">kernel =</span> <span class="st">&quot;linear&quot;</span>, <span class="co"># SV classifier </span></span>
<span id="cb12-7"><a href="chapter10.html#cb12-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">ranges =</span> <span class="fu">list</span>(<span class="at">cost =</span> <span class="fu">c</span>(.<span class="dv">001</span>, .<span class="dv">01</span>, .<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">100</span>))</span>
<span id="cb12-8"><a href="chapter10.html#cb12-8" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb12-9"><a href="chapter10.html#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tune.out) <span class="co"># Cross-validation results</span></span></code></pre></div>
<pre><code>## 
## Parameter tuning of &#39;svm&#39;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost
##   0.1
## 
## - best performance: 0.05 
## 
## - Detailed performance results:
##    cost error dispersion
## 1 1e-03  0.55  0.4377975
## 2 1e-02  0.55  0.4377975
## 3 1e-01  0.05  0.1581139
## 4 1e+00  0.15  0.2415229
## 5 5e+00  0.15  0.2415229
## 6 1e+01  0.15  0.2415229
## 7 1e+02  0.15  0.2415229</code></pre>
<ul>
<li><p>Cost=0.1 results the lowest cross-validation error.</p></li>
<li><p>The tune() function sotres the best model.</p></li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="chapter10.html#cb14-1" aria-hidden="true" tabindex="-1"></a>bestmod <span class="ot">&lt;-</span> tune.out<span class="sc">$</span>best.model <span class="co"># extract the best model.</span></span>
<span id="cb14-2"><a href="chapter10.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(bestmod)</span></code></pre></div>
<pre><code>## 
## Call:
## best.tune(method = svm, train.x = y ~ ., data = data, ranges = list(cost = c(0.001, 
##     0.01, 0.1, 1, 5, 10, 100)), kernel = &quot;linear&quot;)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  0.1 
## 
## Number of Support Vectors:  16
## 
##  ( 8 8 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="chapter10.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">coef</span>(bestmod), <span class="dv">3</span>) <span class="co"># coefficients best separating linear decision boundary</span></span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##       0.061      -0.516      -0.532</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="chapter10.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bestmod, <span class="at">data =</span> data)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex1025-1.png" width="672" /></p>
<ul>
<li><p>The predict() function can be used to predict the class labels at any given value of the cost parameter.</p></li>
<li><p>Generate test observations and produce predictions.</p></li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="chapter10.html#cb19-1" aria-hidden="true" tabindex="-1"></a>xtest <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">20</span> <span class="sc">*</span> <span class="dv">2</span>), <span class="at">ncol =</span> <span class="dv">2</span>) <span class="co"># test observatons.</span></span>
<span id="cb19-2"><a href="chapter10.html#cb19-2" aria-hidden="true" tabindex="-1"></a>ytest <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">size =</span> <span class="dv">20</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="co"># randomly assigned groups</span></span>
<span id="cb19-3"><a href="chapter10.html#cb19-3" aria-hidden="true" tabindex="-1"></a>xtest[ytest <span class="sc">==</span> <span class="dv">1</span>, ] <span class="ot">&lt;-</span> xtest[ytest <span class="sc">==</span> <span class="dv">1</span>, ] <span class="sc">+</span> <span class="dv">1</span> <span class="co"># class means of class 1 observations</span></span>
<span id="cb19-4"><a href="chapter10.html#cb19-4" aria-hidden="true" tabindex="-1"></a>testdata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> xtest, <span class="at">y =</span> <span class="fu">as.factor</span>(ytest)) <span class="co"># test data frame</span></span>
<span id="cb19-5"><a href="chapter10.html#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(testdata) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>, <span class="st">&quot;y&quot;</span>) <span class="co"># test data need to have correct var names</span></span>
<span id="cb19-6"><a href="chapter10.html#cb19-6" aria-hidden="true" tabindex="-1"></a>ypred <span class="ot">&lt;-</span> <span class="fu">predict</span>(bestmod, testdata) <span class="co"># predictions</span></span>
<span id="cb19-7"><a href="chapter10.html#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">predicted =</span> ypred, <span class="at">realized =</span> testdata<span class="sc">$</span>y) <span class="co"># predictions vs true observed</span></span></code></pre></div>
<pre><code>##          realized
## predicted -1 1
##        -1  9 1
##        1   2 8</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="chapter10.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 out of 20 are misclassified</span></span></code></pre></div>
<ul>
<li>With cost=0.1, 19 out of the 20 observations become correctly classified.</li>
</ul>
</div>
<div id="linear-boundary-can-fail" class="section level4 unnumbered hasAnchor">
<h4>Linear boundary can fail<a href="chapter10.html#linear-boundary-can-fail" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig10/f8.png" /></p>
</div>
<div id="feature-expansion" class="section level4 unnumbered hasAnchor">
<h4>Feature expansion<a href="chapter10.html#feature-expansion" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Enlarge the space of features by including transformation; e.g. <span class="math inline">\(X_1^2, X_1^3, X_1X_2, X_1X_2^2, \ldots\)</span></p>
<ul>
<li>Hence go from a <span class="math inline">\(p\)</span>-dimensional space to a <span class="math inline">\(M&gt;p\)</span> dimensional space.</li>
</ul></li>
<li><p>Fit a support-vector classifier in the enlarged space.</p></li>
<li><p>This results in non-linear decision boundaries in the original space.</p></li>
<li><p>Example</p>
<ul>
<li>Suppose we use (<span class="math inline">\(X_1, X_2, X_1^2, X_2^2, X_1X_2\)</span>) instead of just (<span class="math inline">\(X_1, X_2\)</span>).</li>
<li>Then the decision boundary would be of the form</li>
</ul></li>
</ul>
<p><span class="math display">\[
\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_1^2+\beta_4X_2^2+\beta_5X_1X_2=0
\]</span></p>
<ul>
<li>This leads to nonlinear decision boundaries in the original space (quadratic conic sections).</li>
</ul>
</div>
<div id="cubic-polynomials" class="section level4 unnumbered hasAnchor">
<h4>Cubic polynomials<a href="chapter10.html#cubic-polynomials" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Here we use a basis expansion of cubic polynomials.</p></li>
<li><p>From 2 variables to 9.</p></li>
<li><p>The support-vector classifier in the enlarged space solves the problem in the lower-dimensional space</p></li>
</ul>
<p><img src="fig10/f9.png" /></p>
<p><span class="math display">\[
\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_1^2+\beta_4X_2^2+\beta_5X_1X_2+\beta_6X_1^3+\beta_7X_2^3+\beta_8X_1X_2^2+\beta_9X_1^2X_2=0
\]</span>
#### Nonlinearities and kernels {-}</p>
<ul>
<li><p>Polynomials (especially high-dimensional ones) get wild rather fast.</p></li>
<li><p>There is a more elegant and controlled way to introduce nonlinearities in support-vector classifiers - through the use of <em>kernels.</em></p></li>
<li><p>Before we discuss these, we must understand the role of <em>inner products</em> in support-vector classifiers.</p></li>
</ul>
</div>
<div id="inner-products-and-support-vectors" class="section level4 unnumbered hasAnchor">
<h4>Inner products and support vectors<a href="chapter10.html#inner-products-and-support-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Inner product between vectors</li>
</ul>
<p><span class="math display">\[
&lt;x_i,x_i&#39;&gt;=\sum_{j=1}^p x_{ij}x_{i&#39;j}
\]</span></p>
<ul>
<li>The linear support vector classifier can be represented as (<span class="math inline">\(n\)</span> parameters)</li>
</ul>
<p><span class="math display">\[
f(x)=\beta_0+\sum_{i=1}^n \alpha_i &lt;x_i,x_i&#39;&gt;
\]</span></p>
<ul>
<li><p>To estimate the parameters <span class="math inline">\(\alpha_1,\ldots, \alpha_n\)</span> and <span class="math inline">\(\beta_0\)</span>, all we need are the <span class="math inline">\(\binom{n}{2}\)</span> inner products <span class="math inline">\(&lt;x_i,x_i&#39;&gt;\)</span> between all pairs of training observations.</p></li>
<li><p>It turns out that most of the <span class="math inline">\(\hat{\alpha}_i\)</span> can be zero:</p></li>
</ul>
<p><span class="math display">\[
f(x)=\beta_0+\sum_{i\in S}\hat{\alpha}&lt;x_i,x_i&#39;&gt;
\]</span>
- <span class="math inline">\(S\)</span> is the <em>support set</em> of indices <span class="math inline">\(i\)</span> such that <span class="math inline">\(\hat{\alpha}_i&gt;0\)</span></p>
</div>
<div id="kernels-and-support-vector-machines" class="section level4 unnumbered hasAnchor">
<h4>Kernels and support vector machines<a href="chapter10.html#kernels-and-support-vector-machines" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>If we can compute inner-products between observations, we can fit a SV classifier.</p></li>
<li><p>Some special <em>kernel functions</em> can do this for us. E.g.</p></li>
</ul>
<p><span class="math display">\[
K(x_i,x_i&#39;)=(1+\sum_{j=1}^p x_{ij}x_{i&#39;j})^d
\]</span>
      computes the inner-products needed for <span class="math inline">\(d\)</span> dimensional polynomials - <span class="math inline">\(\binom{p+d}{d}\)</span> basis function!</p>
<ul>
<li>The solution has the form</li>
</ul>
<p><span class="math display">\[
f(x)=\beta_0+\sum_{i\in S} \hat{\alpha}_i K(x,x_i)
\]</span></p>
</div>
<div id="radial-kernel" class="section level4 unnumbered hasAnchor">
<h4>Radial Kernel<a href="chapter10.html#radial-kernel" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
K(x_i,x_i&#39;)=exp(-\gamma \sum_{j=1}^p (x_{ij}-x_{i&#39;j})^2)
\]</span>
<img src="fig10/f10.png" /></p>
</div>
<div id="example-heart-data-1" class="section level4 unnumbered hasAnchor">
<h4>Example: Heart data<a href="chapter10.html#example-heart-data-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig10/f11.png" /></p>
<ul>
<li><p>ROC curve is obtained by changing the threshold 0 to threshold <span class="math inline">\(t\)</span> <span class="math inline">\(\hat{f}(X)&gt;t\)</span>, and recording <em>false positive</em> and <em>true positive</em> rates as <span class="math inline">\(t\)</span> varies.</p></li>
<li><p>Here we see ROC curves on training data.</p></li>
<li><p>Example: Heart test data</p></li>
</ul>
<p><img src="fig10/f12.png" /></p>
</div>
<div id="example-20" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter10.html#example-20" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Generate bivariate normal with class 1 <span class="math inline">\(\mu=(2,2)&#39;\)</span> or <span class="math inline">\(\mu_1=(-2,-2)&#39;\)</span> and class 2 <span class="math inline">\(\mu_2=(0,0)&#39;\)</span></li>
</ul>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="chapter10.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb22-2"><a href="chapter10.html#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># for exact replication</span></span>
<span id="cb22-3"><a href="chapter10.html#cb22-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">200</span> <span class="sc">*</span> <span class="dv">2</span>), <span class="at">ncol =</span> <span class="dv">2</span>) <span class="co"># bivariate normal</span></span>
<span id="cb22-4"><a href="chapter10.html#cb22-4" aria-hidden="true" tabindex="-1"></a>x[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, ] <span class="ot">&lt;-</span> x[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, ] <span class="sc">+</span> <span class="dv">2</span> <span class="co"># shift mean of the first 100 obs</span></span>
<span id="cb22-5"><a href="chapter10.html#cb22-5" aria-hidden="true" tabindex="-1"></a>x[<span class="dv">101</span><span class="sc">:</span><span class="dv">150</span>, ] <span class="ot">&lt;-</span> x[<span class="dv">101</span><span class="sc">:</span><span class="dv">150</span>, ] <span class="sc">-</span> <span class="dv">2</span> <span class="co"># nex 50 obs mean -2 (bot cols)</span></span>
<span id="cb22-6"><a href="chapter10.html#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="do">## the rest of bothe variables (x1 and x2) have means zero</span></span>
<span id="cb22-7"><a href="chapter10.html#cb22-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="at">times =</span> <span class="dv">150</span>), <span class="fu">rep</span>(<span class="dv">2</span>, <span class="at">times =</span> <span class="dv">50</span>))</span>
<span id="cb22-8"><a href="chapter10.html#cb22-8" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">as.factor</span>(y))</span>
<span id="cb22-9"><a href="chapter10.html#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(data) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>, <span class="st">&quot;y&quot;</span>)</span>
<span id="cb22-10"><a href="chapter10.html#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data)</span></code></pre></div>
<pre><code>##         x1         x2 y
## 1 1.373546  2.4094018 1
## 2 2.183643  3.6888733 1
## 3 1.164371  3.5865884 1
## 4 3.595281  1.6690922 1
## 5 2.329508 -0.2852355 1
## 6 1.179532  4.4976616 1</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="chapter10.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> x[, <span class="dv">2</span>], <span class="at">y =</span> x[, <span class="dv">1</span>],</span>
<span id="cb24-2"><a href="chapter10.html#cb24-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="at">pch =</span> <span class="dv">20</span>,</span>
<span id="cb24-3"><a href="chapter10.html#cb24-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>]), <span class="at">ylab =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]),</span>
<span id="cb24-4"><a href="chapter10.html#cb24-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Non-linearly Separating Classes&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex2022-1.png" width="672" /></p>
<ul>
<li><p>Split data randomly to training and test sets.</p></li>
<li><p>Fitting SVM with gamma=1 to the training set gives the following results.</p></li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="chapter10.html#cb25-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">200</span>, <span class="at">size =</span> <span class="dv">100</span>) <span class="co"># taining observations</span></span>
<span id="cb25-2"><a href="chapter10.html#cb25-2" aria-hidden="true" tabindex="-1"></a>svmfit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> data[train, ], <span class="at">kernel =</span> <span class="st">&quot;radial&quot;</span>, <span class="at">gamma =</span> <span class="dv">1</span>, <span class="at">cost =</span> <span class="dv">1</span>)</span>
<span id="cb25-3"><a href="chapter10.html#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svmfit, data[train, ])</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex2034-1.png" width="672" /></p>
<ul>
<li>The decision boundary is highly non-linear.</li>
</ul>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="chapter10.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="do">## summary of results</span></span>
<span id="cb26-2"><a href="chapter10.html#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(svmfit)</span></code></pre></div>
<pre><code>## 
## Call:
## svm(formula = y ~ ., data = data[train, ], kernel = &quot;radial&quot;, gamma = 1, 
##     cost = 1)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
## 
## Number of Support Vectors:  31
## 
##  ( 16 15 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  1 2</code></pre>
<ul>
<li>We can use CV to find best fitting gamma(<span class="math inline">\(\gamma\)</span>) and cost.</li>
</ul>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="chapter10.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># for exact replication</span></span>
<span id="cb28-2"><a href="chapter10.html#cb28-2" aria-hidden="true" tabindex="-1"></a>tune.out <span class="ot">&lt;-</span> <span class="fu">tune</span>(svm, y <span class="sc">~</span> ., <span class="at">data =</span> data[train, ], <span class="at">kernel =</span> <span class="st">&quot;radial&quot;</span>,</span>
<span id="cb28-3"><a href="chapter10.html#cb28-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">ranges =</span> <span class="fu">list</span>(<span class="at">cost =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>),</span>
<span id="cb28-4"><a href="chapter10.html#cb28-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">gamma =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)))</span>
<span id="cb28-5"><a href="chapter10.html#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tune.out) <span class="co"># cross-validataion results</span></span></code></pre></div>
<pre><code>## 
## Parameter tuning of &#39;svm&#39;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost gamma
##     1   0.5
## 
## - best performance: 0.07 
## 
## - Detailed performance results:
##     cost gamma error dispersion
## 1  1e-01   0.5  0.26 0.15776213
## 2  1e+00   0.5  0.07 0.08232726
## 3  1e+01   0.5  0.07 0.08232726
## 4  1e+02   0.5  0.14 0.15055453
## 5  1e+03   0.5  0.11 0.07378648
## 6  1e-01   1.0  0.22 0.16193277
## 7  1e+00   1.0  0.07 0.08232726
## 8  1e+01   1.0  0.09 0.07378648
## 9  1e+02   1.0  0.12 0.12292726
## 10 1e+03   1.0  0.11 0.11005049
## 11 1e-01   2.0  0.27 0.15670212
## 12 1e+00   2.0  0.07 0.08232726
## 13 1e+01   2.0  0.11 0.07378648
## 14 1e+02   2.0  0.12 0.13165612
## 15 1e+03   2.0  0.16 0.13498971
## 16 1e-01   3.0  0.27 0.15670212
## 17 1e+00   3.0  0.07 0.08232726
## 18 1e+01   3.0  0.08 0.07888106
## 19 1e+02   3.0  0.13 0.14181365
## 20 1e+03   3.0  0.15 0.13540064
## 21 1e-01   4.0  0.27 0.15670212
## 22 1e+00   4.0  0.07 0.08232726
## 23 1e+01   4.0  0.09 0.07378648
## 24 1e+02   4.0  0.13 0.14181365
## 25 1e+03   4.0  0.15 0.13540064</code></pre>
<ul>
<li><p>The best choice is cost=1 and gamma=2.</p></li>
<li><p>Using these values in the test set we get</p></li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="chapter10.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="do">## test set results</span></span>
<span id="cb30-2"><a href="chapter10.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">true =</span> data[<span class="sc">-</span>train, <span class="st">&quot;y&quot;</span>], <span class="at">pred =</span> <span class="fu">predict</span>(tune.out<span class="sc">$</span>best.model, <span class="at">newx =</span> data[<span class="sc">-</span>train, ]))</span></code></pre></div>
<pre><code>##     pred
## true  1  2
##    1 54 23
##    2 17  6</code></pre>
<ul>
<li><p>39 out of 100 become misclassified.</p></li>
<li><p>Let us next produce ROC curves for the classifier.</p></li>
<li><p>R has a package, ROCR, for the purpose.</p></li>
<li><p>SVMs and support vector classifiers output class labels for each observation.</p></li>
<li><p>However, it is also possible to obtain fitted values for each observation that are produced.</p></li>
<li><p>svm() produces fitted values when setting the parameter decision.values=TRUE in the function call.</p></li>
</ul>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="chapter10.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ROC curves</span></span>
<span id="cb32-2"><a href="chapter10.html#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages(&quot;ROCR&quot;, repos = &quot;https://cran.uib.no&quot;)</span></span>
<span id="cb32-3"><a href="chapter10.html#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb32-4"><a href="chapter10.html#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="do">## library(help = ROCR)</span></span>
<span id="cb32-5"><a href="chapter10.html#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="chapter10.html#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="do">## fit svm with optimal gamma and cost, and produce also decison values</span></span>
<span id="cb32-7"><a href="chapter10.html#cb32-7" aria-hidden="true" tabindex="-1"></a>svmfit.opt <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> data[train, ], <span class="at">kernel =</span> <span class="st">&quot;radial&quot;</span>,</span>
<span id="cb32-8"><a href="chapter10.html#cb32-8" aria-hidden="true" tabindex="-1"></a>                  <span class="at">gamma =</span> <span class="dv">2</span>, <span class="at">cost =</span> <span class="dv">1</span>, <span class="at">decision.values =</span> <span class="cn">TRUE</span>) </span>
<span id="cb32-9"><a href="chapter10.html#cb32-9" aria-hidden="true" tabindex="-1"></a>fitted <span class="ot">&lt;-</span> <span class="fu">attributes</span>(<span class="fu">predict</span>(svmfit.opt, <span class="at">newdata =</span> data[train, ],</span>
<span id="cb32-10"><a href="chapter10.html#cb32-10" aria-hidden="true" tabindex="-1"></a>                             <span class="at">decision.values =</span> <span class="cn">TRUE</span>))<span class="sc">$</span>decision.values</span>
<span id="cb32-11"><a href="chapter10.html#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(fitted)</span></code></pre></div>
<pre><code>##           1/2
## 148  1.130044
## 192 -1.038648
## 87   1.188065
## 20   1.204742
## 112  1.195899
## 177 -1.096820</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="chapter10.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># help(performance)</span></span>
<span id="cb34-2"><a href="chapter10.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="do">## help(prediction)</span></span>
<span id="cb34-3"><a href="chapter10.html#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb34-4"><a href="chapter10.html#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="do">## training ROC plot</span></span>
<span id="cb34-5"><a href="chapter10.html#cb34-5" aria-hidden="true" tabindex="-1"></a>perf <span class="ot">&lt;-</span> <span class="fu">performance</span>(<span class="fu">prediction</span>(fitted, <span class="at">labels =</span> data[train, <span class="st">&quot;y&quot;</span>]),</span>
<span id="cb34-6"><a href="chapter10.html#cb34-6" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;tpr&quot;</span>, <span class="co"># true positive rate</span></span>
<span id="cb34-7"><a href="chapter10.html#cb34-7" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;fpr&quot;</span>  <span class="co"># false positive rate</span></span>
<span id="cb34-8"><a href="chapter10.html#cb34-8" aria-hidden="true" tabindex="-1"></a>                    ) <span class="co"># training perform</span></span>
<span id="cb34-9"><a href="chapter10.html#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(perf, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Training Data&quot;</span>)</span>
<span id="cb34-10"><a href="chapter10.html#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="chapter10.html#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="do">## add ROC plot for a more flexible fit</span></span>
<span id="cb34-12"><a href="chapter10.html#cb34-12" aria-hidden="true" tabindex="-1"></a>svmfit.flex <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> data[train, ], <span class="at">kernel =</span> <span class="st">&quot;radial&quot;</span>,</span>
<span id="cb34-13"><a href="chapter10.html#cb34-13" aria-hidden="true" tabindex="-1"></a>                  <span class="at">gamma =</span> <span class="dv">50</span>, <span class="at">cost =</span> <span class="dv">1</span>, <span class="at">decision.values =</span> <span class="cn">TRUE</span>) </span>
<span id="cb34-14"><a href="chapter10.html#cb34-14" aria-hidden="true" tabindex="-1"></a>fitted <span class="ot">&lt;-</span> <span class="fu">attributes</span>(<span class="fu">predict</span>(svmfit.flex, <span class="at">newdata =</span> data[train, ],</span>
<span id="cb34-15"><a href="chapter10.html#cb34-15" aria-hidden="true" tabindex="-1"></a>                             <span class="at">decision.values =</span> <span class="cn">TRUE</span>))<span class="sc">$</span>decision.values</span>
<span id="cb34-16"><a href="chapter10.html#cb34-16" aria-hidden="true" tabindex="-1"></a>perf <span class="ot">&lt;-</span> <span class="fu">performance</span>(<span class="fu">prediction</span>(fitted, <span class="at">labels =</span> data[train, <span class="st">&quot;y&quot;</span>]),</span>
<span id="cb34-17"><a href="chapter10.html#cb34-17" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;tpr&quot;</span>, <span class="co"># true positive rate</span></span>
<span id="cb34-18"><a href="chapter10.html#cb34-18" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;fpr&quot;</span>  <span class="co"># false positive rate</span></span>
<span id="cb34-19"><a href="chapter10.html#cb34-19" aria-hidden="true" tabindex="-1"></a>                    ) <span class="co"># training perform</span></span>
<span id="cb34-20"><a href="chapter10.html#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(perf, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb34-21"><a href="chapter10.html#cb34-21" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Optimal fit&quot;</span>, <span class="st">&quot;Flexible&quot;</span>),</span>
<span id="cb34-22"><a href="chapter10.html#cb34-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb34-23"><a href="chapter10.html#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="chapter10.html#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="do">## test ROC plot</span></span>
<span id="cb34-25"><a href="chapter10.html#cb34-25" aria-hidden="true" tabindex="-1"></a>fitted <span class="ot">&lt;-</span> <span class="fu">attributes</span>(<span class="fu">predict</span>(svmfit.opt, <span class="at">newdata =</span> data[<span class="sc">-</span>train, ],</span>
<span id="cb34-26"><a href="chapter10.html#cb34-26" aria-hidden="true" tabindex="-1"></a>                             <span class="at">decision.values =</span> <span class="cn">TRUE</span>))<span class="sc">$</span>decision.values</span>
<span id="cb34-27"><a href="chapter10.html#cb34-27" aria-hidden="true" tabindex="-1"></a>perf <span class="ot">&lt;-</span> <span class="fu">performance</span>(<span class="fu">prediction</span>(fitted, <span class="at">labels =</span> data[<span class="sc">-</span>train, <span class="st">&quot;y&quot;</span>]),</span>
<span id="cb34-28"><a href="chapter10.html#cb34-28" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;tpr&quot;</span>, <span class="co"># true positive rate</span></span>
<span id="cb34-29"><a href="chapter10.html#cb34-29" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;fpr&quot;</span>  <span class="co"># false positive rate</span></span>
<span id="cb34-30"><a href="chapter10.html#cb34-30" aria-hidden="true" tabindex="-1"></a>                    ) <span class="co"># training perform</span></span>
<span id="cb34-31"><a href="chapter10.html#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(perf, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Test Data&quot;</span>)</span>
<span id="cb34-32"><a href="chapter10.html#cb34-32" aria-hidden="true" tabindex="-1"></a><span class="do">## flexible</span></span>
<span id="cb34-33"><a href="chapter10.html#cb34-33" aria-hidden="true" tabindex="-1"></a>fitted <span class="ot">&lt;-</span> <span class="fu">attributes</span>(<span class="fu">predict</span>(svmfit.flex, <span class="at">newdata =</span> data[<span class="sc">-</span>train, ],</span>
<span id="cb34-34"><a href="chapter10.html#cb34-34" aria-hidden="true" tabindex="-1"></a>                             <span class="at">decision.values =</span> <span class="cn">TRUE</span>))<span class="sc">$</span>decision.values</span>
<span id="cb34-35"><a href="chapter10.html#cb34-35" aria-hidden="true" tabindex="-1"></a>perf <span class="ot">&lt;-</span> <span class="fu">performance</span>(<span class="fu">prediction</span>(fitted, <span class="at">labels =</span> data[<span class="sc">-</span>train, <span class="st">&quot;y&quot;</span>]),</span>
<span id="cb34-36"><a href="chapter10.html#cb34-36" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;tpr&quot;</span>, <span class="co"># true positive rate</span></span>
<span id="cb34-37"><a href="chapter10.html#cb34-37" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;fpr&quot;</span>  <span class="co"># false positive rate</span></span>
<span id="cb34-38"><a href="chapter10.html#cb34-38" aria-hidden="true" tabindex="-1"></a>                    ) <span class="co"># training perform</span></span>
<span id="cb34-39"><a href="chapter10.html#cb34-39" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(perf, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb34-40"><a href="chapter10.html#cb34-40" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Optimal fit&quot;</span>, <span class="st">&quot;Flexible&quot;</span>),</span>
<span id="cb34-41"><a href="chapter10.html#cb34-41" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex2038-1.png" width="672" /></p>
</div>
<div id="svms-more-than-2-classes" class="section level4 unnumbered hasAnchor">
<h4>SVMs: more than 2 classes?<a href="chapter10.html#svms-more-than-2-classes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The SVM as defined works for <span class="math inline">\(K=2\)</span> classes.</p></li>
<li><p>What do we do if we have <span class="math inline">\(K&gt;2\)</span> classes?</p></li>
<li><p><strong>OVA</strong>: One versus All</p>
<ul>
<li>Fit <span class="math inline">\(K\)</span> different 2-class SVM classifiers <span class="math inline">\(\hat{f}_k(x)\)</span>, <span class="math inline">\(k=1,\ldots, K\)</span>; each class versus the rest.</li>
<li>Classify <span class="math inline">\(x*\)</span> to the class for which <span class="math inline">\(\hat{f}_k(x*)\)</span> is largest.</li>
</ul></li>
<li><p><strong>OVO</strong>: One versus One</p>
<ul>
<li>Fit all <span class="math inline">\(\binom{K}{2}\)</span> pairwise classifiers $_{kl}(x).</li>
<li>Classify <span class="math inline">\(x*\)</span> to the class that wins the most pairwise competitions.</li>
</ul></li>
<li><p>Which to choose?</p>
<ul>
<li>If <span class="math inline">\(K\)</span> is not too large, use OVO.</li>
</ul></li>
</ul>
</div>
<div id="support-vector-versus-logistic-regression" class="section level4 unnumbered hasAnchor">
<h4>Support vector versus logistic regression?<a href="chapter10.html#support-vector-versus-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>With <span class="math inline">\(f(X)=\beta_0+\beta_1X_1+\cdots +\beta_pX_o\)</span> can rephrase support-vector classifier optimization as</li>
</ul>
<p><span class="math display">\[
minimize_{\beta_0,\beta_1,\ldots, \beta_p}\{\sum_{i=1}^n max[0, 1-y_if(x_i)]+\lambda\sum_{ij=1}^p\beta_j^2\}
\]</span></p>
<p><img src="fig10/f13.png" /></p>
<ul>
<li><p>This has the form loss plus penalty.</p></li>
<li><p>The loss is known as the hinge loss.</p></li>
<li><p>Very similar to “loss” in logistic regression (negative log-likelihood).</p></li>
</ul>
</div>
<div id="which-to-use-svm-or-logistic-regression" class="section level4 unnumbered hasAnchor">
<h4>Which to use: SVM or logistic regression<a href="chapter10.html#which-to-use-svm-or-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>When classes are (nearly) separable, SVM does better than LR. So does LDA.</p></li>
<li><p>When not, LR (with ridge penalty) and SVM very similar.</p></li>
<li><p>If you wish to estimate probabilities, LR is the choice.</p></li>
<li><p>For nonlinear boundaries, kernel SVMs are popular.</p>
<ul>
<li>Can use kernels with LR and LDA as well, but computations are more expensive.</li>
</ul></li>
</ul>
<p><br></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter9.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter11.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ILR.pdf", "ILR.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
