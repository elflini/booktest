<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Unsupervised Learning | Statistical Learning</title>
  <meta name="description" content="This is a Statistical Learning" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Unsupervised Learning | Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Statistical Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Unsupervised Learning | Statistical Learning" />
  
  <meta name="twitter:description" content="This is a Statistical Learning" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2023-05-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter10.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>머리말</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#understanding-data"><i class="fa fa-check"></i><b>1.1</b> Understanding Data</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="chapter1.html"><a href="chapter1.html#example-1-supervised-learning-continuous-output"><i class="fa fa-check"></i><b>1.1.1</b> Example 1 (Supervised learning: Continuous output)</a></li>
<li class="chapter" data-level="1.1.2" data-path="chapter1.html"><a href="chapter1.html#example-2-supervised-leraning-categorical-output"><i class="fa fa-check"></i><b>1.1.2</b> Example 2 (Supervised leraning: Categorical output)</a></li>
<li class="chapter" data-level="1.1.3" data-path="chapter1.html"><a href="chapter1.html#example-3-unsupervised-learning-clustering-observations"><i class="fa fa-check"></i><b>1.1.3</b> Example 3 (Unsupervised learning: Clustering observations)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#brief-history-of-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Brief History of Statistical Learning</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#notation-and-simple-matrix-algebra"><i class="fa fa-check"></i><b>1.3</b> Notation and Simple Matrix Algebra</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#what-is-statitical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statitical Learning</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chapter2.html"><a href="chapter2.html#prediction"><i class="fa fa-check"></i><b>2.1.1</b> Prediction</a></li>
<li class="chapter" data-level="2.1.2" data-path="chapter2.html"><a href="chapter2.html#inference"><i class="fa fa-check"></i><b>2.1.2</b> Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="chapter2.html"><a href="chapter2.html#estimating-f"><i class="fa fa-check"></i><b>2.1.3</b> Estimating <span class="math inline">\(f\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="chapter2.html"><a href="chapter2.html#prediction-accuaracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.4</b> Prediction Accuaracy and Model Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter2.html"><a href="chapter2.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="chapter2.html"><a href="chapter2.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="2.2.3" data-path="chapter2.html"><a href="chapter2.html#classification-problems"><i class="fa fa-check"></i><b>2.2.3</b> Classification Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#basic-commands"><i class="fa fa-check"></i><b>3.1</b> Basic Commands</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#graphics"><i class="fa fa-check"></i><b>3.2</b> Graphics</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#indexing-data"><i class="fa fa-check"></i><b>3.3</b> Indexing Data</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#loading-data"><i class="fa fa-check"></i><b>3.4</b> Loading Data</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#additional-graphical-and-numerical-summaries"><i class="fa fa-check"></i><b>3.5</b> Additional Graphical and Numerical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#simple-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Regression</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#multiple-regression"><i class="fa fa-check"></i><b>4.2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chapter4.html"><a href="chapter4.html#importance-of-predictors-statistical-significance-of-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Importance of Predictors: Statistical Significance of Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="chapter4.html"><a href="chapter4.html#selecting-important-variables"><i class="fa fa-check"></i><b>4.2.2</b> Selecting Important Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="chapter4.html"><a href="chapter4.html#model-fit"><i class="fa fa-check"></i><b>4.2.3</b> Model Fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="chapter4.html"><a href="chapter4.html#prediction-1"><i class="fa fa-check"></i><b>4.2.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#other-consideration"><i class="fa fa-check"></i><b>4.3</b> Other Consideration</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chapter4.html"><a href="chapter4.html#qualitative-predictors"><i class="fa fa-check"></i><b>4.3.1</b> Qualitative Predictors</a></li>
<li class="chapter" data-level="" data-path="chapter4.html"><a href="chapter4.html#example-8"><i class="fa fa-check"></i>Example 8</a></li>
<li class="chapter" data-level="4.3.2" data-path="chapter4.html"><a href="chapter4.html#potential-problems"><i class="fa fa-check"></i><b>4.3.2</b> Potential Problems</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#non-parametric-regressions"><i class="fa fa-check"></i><b>4.4</b> Non-parametric Regressions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#logit-and-probit-models"><i class="fa fa-check"></i><b>5.1</b> Logit and Probit Models</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#confounding"><i class="fa fa-check"></i><b>5.2</b> Confounding</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#logit-model-for-multiple-classes"><i class="fa fa-check"></i><b>5.3</b> Logit Model for Multiple Classes</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#discriminant-analysis"><i class="fa fa-check"></i><b>5.4</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chapter5.html"><a href="chapter5.html#bayes-theorem-for-classification"><i class="fa fa-check"></i><b>5.4.1</b> Bayes Theorem for Classification</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapter5.html"><a href="chapter5.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="5.4.3" data-path="chapter5.html"><a href="chapter5.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.3</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#naive-bayes"><i class="fa fa-check"></i><b>5.5</b> Naive Bayes</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#knn-classification"><i class="fa fa-check"></i><b>5.6</b> KNN Classification</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#example-10"><i class="fa fa-check"></i><b>5.7</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Resampling Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#validation-set-approach"><i class="fa fa-check"></i><b>6.1</b> Validation Set Approach</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.2</b> K-fold Cross-validation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chapter6.html"><a href="chapter6.html#example-11"><i class="fa fa-check"></i><b>6.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#bootstrap"><i class="fa fa-check"></i><b>6.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Model Selection</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#variable-selection"><i class="fa fa-check"></i><b>7.1</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chapter7.html"><a href="chapter7.html#best-subset-selection"><i class="fa fa-check"></i><b>7.1.1</b> Best Subset Selection</a></li>
<li class="chapter" data-level="7.1.2" data-path="chapter7.html"><a href="chapter7.html#forward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.2</b> Forward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.3" data-path="chapter7.html"><a href="chapter7.html#backward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.3</b> Backward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.4" data-path="chapter7.html"><a href="chapter7.html#choosing-the-optimal-model"><i class="fa fa-check"></i><b>7.1.4</b> Choosing the Optimal Model</a></li>
<li class="chapter" data-level="7.1.5" data-path="chapter7.html"><a href="chapter7.html#example-13"><i class="fa fa-check"></i><b>7.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#shrinkage-methods"><i class="fa fa-check"></i><b>7.2</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chapter7.html"><a href="chapter7.html#ridge-regression"><i class="fa fa-check"></i><b>7.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="chapter7.html"><a href="chapter7.html#rasso-regression"><i class="fa fa-check"></i><b>7.2.2</b> RASSO Regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="chapter7.html"><a href="chapter7.html#example-14"><i class="fa fa-check"></i><b>7.2.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#dimension-reduction-methods"><i class="fa fa-check"></i><b>7.3</b> Dimension Reduction Methods</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="chapter7.html"><a href="chapter7.html#principal-components-regression"><i class="fa fa-check"></i><b>7.3.1</b> Principal Components Regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="chapter7.html"><a href="chapter7.html#example-15"><i class="fa fa-check"></i><b>7.3.2</b> Example</a></li>
<li class="chapter" data-level="7.3.3" data-path="chapter7.html"><a href="chapter7.html#partial-least-squares"><i class="fa fa-check"></i><b>7.3.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="7.3.4" data-path="chapter7.html"><a href="chapter7.html#example-16"><i class="fa fa-check"></i><b>7.3.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#polynomial-regression"><i class="fa fa-check"></i><b>8.1</b> Polynomial Regression</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#step-functions"><i class="fa fa-check"></i><b>8.2</b> Step Functions</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#piecewise-polynomials---splines"><i class="fa fa-check"></i><b>8.3</b> Piecewise Polynomials - Splines</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#local-regression"><i class="fa fa-check"></i><b>8.4</b> Local Regression</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#generalized-additive-models"><i class="fa fa-check"></i><b>8.5</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Tree-based Methods</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#the-basiscs-of-decision-trees"><i class="fa fa-check"></i><b>9.1</b> The Basiscs of Decision Trees</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#classification-trees"><i class="fa fa-check"></i><b>9.2</b> Classification Trees</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#bagging"><i class="fa fa-check"></i><b>9.3</b> Bagging</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#random-forests"><i class="fa fa-check"></i><b>9.4</b> Random Forests</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#boosing"><i class="fa fa-check"></i><b>9.5</b> Boosing</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Support Vector Machines</a></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#principal-components-anlsysis"><i class="fa fa-check"></i><b>11.1</b> Principal Components Anlsysis</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#clustering"><i class="fa fa-check"></i><b>11.2</b> Clustering</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chapter11.html"><a href="chapter11.html#k-means-clustering"><i class="fa fa-check"></i><b>11.2.1</b> <span class="math inline">\(K\)</span>-means clustering</a></li>
<li class="chapter" data-level="11.2.2" data-path="chapter11.html"><a href="chapter11.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.2.2</b> Hierarchical Clustering</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter11" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">Chapter 11</span> Unsupervised Learning<a href="chapter11.html#chapter11" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="unsupervised-vs.-supervised-learning" class="section level4 unnumbered hasAnchor">
<h4>Unsupervised vs. supervised learning<a href="chapter11.html#unsupervised-vs.-supervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Most of this course focuses on <em>supervised learning</em> methods such as regression and classification.</p></li>
<li><p>Is that setting we observe both a set of features <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span> for each object, as well as a response or outcome variable <span class="math inline">\(Y\)</span>.</p>
<ul>
<li>The goal is then to predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>.</li>
</ul></li>
<li><p>Here we instead focus on <em>unsupervised learning</em>, we where observe only the features <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>.</p>
<ul>
<li>We are not interested in prediction, because we do not have an associated response variable <span class="math inline">\(Y\)</span>.</li>
</ul></li>
</ul>
</div>
<div id="the-goals-of-unsupervised-learning" class="section level4 unnumbered hasAnchor">
<h4>The goals of unsupervised learning<a href="chapter11.html#the-goals-of-unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The goal is to discover interesting things about the measurements: is there an informative way to visualize the data?
<ul>
<li>Can we discover subgroups among the variables or among the observations?</li>
</ul></li>
<li>We discuss two methods:
<ul>
<li><em>Principal components analysis</em>, a tool used for data visualization or data pre-processing before supervised techniques are applied, and</li>
<li><em>Clustering</em>, a broad class of methods for discovering unknown subgroups in data.</li>
</ul></li>
</ul>
</div>
<div id="the-challenge-of-unsupervised-learning" class="section level4 unnumbered hasAnchor">
<h4>The challenge of unsupervised learning<a href="chapter11.html#the-challenge-of-unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Unsupervised learning is more subjective than supervised learning, as there is no simple goal for the analysis, such as prediction of a response.</p></li>
<li><p>But techniques for unsupervised learning are of growing importance in a number of fields:</p>
<ul>
<li>Subgroup of breast cancer patients grouped by their gene expression measurements,</li>
<li>Groups of shoppers characterized by their browsing and purchase histories,</li>
<li>Movies grouped by the ratings assigned by movie viewers.</li>
</ul></li>
</ul>
</div>
<div id="another-advantage" class="section level4 unnumbered hasAnchor">
<h4>Another advantage<a href="chapter11.html#another-advantage" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>It is often easier to obtain <em>unlabeled data</em> - from a lab instrument or a computer - than <em>labeled data</em>, which can require human intervention.</p></li>
<li><p>For example it is difficult to automatically assess the overall sentiment of a movie review: is it favorable or not?</p></li>
</ul>
</div>
<div id="principal-components-anlsysis" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Principal Components Anlsysis<a href="chapter11.html#principal-components-anlsysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>PCA produces a low-dimensional representation of a dataset.</p>
<ul>
<li>It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.</li>
</ul></li>
<li><p>Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization.</p></li>
<li><p>The <em>first principal component</em> of a set of features <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span> is the normalized linear combination of the features</p></li>
</ul>
<p><span class="math display">\[
Z_1=\phi_{11}X_1 +\phi_{21}X_2+\cdots +\phi_{p1}X_p
\]</span></p>
<p>      that has the largest variance.</p>
<ul>
<li><p>By <em>normalized</em>, we mean that <span class="math inline">\(\sum_{j=1}^p\phi_{j1}^2\)</span>.</p></li>
<li><p>We refer to the elements <span class="math inline">\(\phi_{11},\ldots,\phi_{p1}\)</span> as the loadings of the first principal component; together, the loadings make up the principal component loading vector, <span class="math inline">\(\phi_1 = (\phi_{11} \phi_{21}\ldots \phi_{p1})^T\)</span>.</p></li>
<li><p>We constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.</p></li>
</ul>
<div id="pca-example" class="section level4 unnumbered hasAnchor">
<h4>PCA: example<a href="chapter11.html#pca-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig11/f1.png" /></p>
<ul>
<li><p>The population size (pop) and ad spending (ad) for 100 different cities are shown as purple circles.</p></li>
<li><p>The green solid line indicates the first principal component direction, and the blue dashed line indicates the second principal component direction.</p></li>
</ul>
</div>
<div id="computation-of-principal-components" class="section level4 unnumbered hasAnchor">
<h4>Computation of principal components<a href="chapter11.html#computation-of-principal-components" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Suppose we have a <span class="math inline">\(n\times p\)</span> data set <span class="math inline">\(X\)</span>.
<ul>
<li>Since we are only interested in variance, we assume that each of the variables in <span class="math inline">\(X\)</span> has been centered to have mean zero (that is, the column means of <span class="math inline">\(X\)</span> are zero).</li>
</ul></li>
<li>We then look for the linear combination of the sample feature values of the form</li>
</ul>
<p><span class="math display">\[
z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+\cdots +\phi_{p1}x_{ip}
\]</span></p>
<p>      for <span class="math inline">\(i=1,\ldots, n\)</span> that has largest sample variance, subject to the constraint that <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span>.</p>
<ul>
<li>Since each of the <span class="math inline">\(x_{ij}\)</span> has mean zero, then so does <span class="math inline">\(z_{i1}\)</span> (for any values of <span class="math inline">\(\phi_{j1}\)</span>).
<ul>
<li>Hence the sample variance of the <span class="math inline">\(z_{i1}\)</span> can be written as <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n z_{i1}^2\)</span>.</li>
</ul></li>
<li>Plugging the first principal component loading vector solves the optimization problem</li>
</ul>
<p><span class="math display">\[
maximize_{\phi_{11},\ldots,\phi_{p1}}\frac{1}{n}\sum_{i=1}^n(\sum_{j=1}^p \phi_{j1}x_{ij})^2 \,\,\, subject \,\,\, to \,\,\, \sum_{j=1}^p \phi_{j1}^2=1
\]</span></p>
<ul>
<li><p>This problem can be solved via a singular-value decomposition of the matrix <span class="math inline">\(X\)</span>, a standard technique in linear algebra.</p></li>
<li><p>We refer to <span class="math inline">\(Z_1\)</span> as the first principal component, with realized values <span class="math inline">\(z_{11},\ldots, z_{n1}\)</span>.</p></li>
</ul>
</div>
<div id="geometry-of-pca" class="section level4 unnumbered hasAnchor">
<h4>Geometry of PCA<a href="chapter11.html#geometry-of-pca" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The loading vector <span class="math inline">\(\phi_1\)</span> with elements <span class="math inline">\(\phi_{11},\phi_{21}, \ldots,\phi_{p1}\)</span> defines a direction in feature space along which the data vary the most.</p></li>
<li><p>If we project the <span class="math inline">\(n\)</span> data points <span class="math inline">\(x_1, \ldots, x_n\)</span> onto this direction, the projected values are the principal component scores <span class="math inline">\(z_{11},\ldots,z_{n1}\)</span> themselves.</p></li>
<li><p>The second principal component is the linear combination of <span class="math inline">\(x_1, \ldots, X_p\)</span> that has maximal variance among all linear combinations that after <em>uncorrelated</em> with <span class="math inline">\(Z_1\)</span>.</p></li>
<li><p>The second principal component scores <span class="math inline">\(z_{12}, z_{22},\ldots, z_{n2}\)</span> take the form</p></li>
</ul>
<p><span class="math display">\[
z_{i2}=\phi_{12}x_{i1}+\phi_{22}x_{i2}+\cdots +\phi_{p2}x_{ip}
\]</span></p>
<p>      where <span class="math inline">\(\phi_2\)</span> is the second principal component loading vector, with elements <span class="math inline">\(\phi_{12},\phi_{22}, \ldots,\phi_{p2}\)</span>.</p>
<ul>
<li><p>It turns out that constraining <span class="math inline">\(Z_2\)</span> to be uncorrelated with <span class="math inline">\(Z_1\)</span> is equivalent to constraining the direction <span class="math inline">\(\phi_2\)</span> to be orthogonal (perpendicular) to the direction <span class="math inline">\(\phi_1\)</span>. And so on.</p></li>
<li><p>The principal component directions <span class="math inline">\(\phi_1, \phi_2, \phi_3, \ldots\)</span> are the ordered sequence of right singular vectors of the matrix <span class="math inline">\(X\)</span>, and the variances of the components are <span class="math inline">\(\frac{1}{n}\)</span> times the squares of the singular values.</p>
<ul>
<li>There are at most <span class="math inline">\(min(n-1,p)\)</span> principal components.</li>
</ul></li>
</ul>
</div>
<div id="illustration" class="section level4 unnumbered hasAnchor">
<h4>Illustration<a href="chapter11.html#illustration" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>USAarrests data: For each of the fifty states in the United States, the data set contains the number of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape.</p>
<ul>
<li>We also record UrbanPop (the percent of the population in each state living in urban areas).</li>
</ul></li>
<li><p>The principal component score vectors have length <span class="math inline">\(n=50\)</span>, and the principal component loading vectors have length <span class="math inline">\(p=4\)</span>.</p></li>
<li><p>PCA was performed after standardizing each variable to have mean zero and standard deviation one.</p></li>
</ul>
<p><img src="fig11/f2.png" /></p>
<ul>
<li>The first two principal components for the ISArrests data.
<ul>
<li>The blue state names represent the scores for the first two principal components.</li>
<li>The orange arrows indicate the first two principal component loading vectors (with axes on the top and right).
<ul>
<li>For example, the loading for Rape on the first component is 0.54, and its loading on the second principal component 0.17 [the word Rape is centered at the point (0.54, 0.17)].</li>
</ul></li>
<li>This figure is known as a <em>biplot</em>, because it displays both the principal component scores and the principal component loadings.</li>
</ul></li>
</ul>
<p><img src="fig11/f3.png" /></p>
</div>
<div id="another-interpretation-of-principal-components" class="section level4 unnumbered hasAnchor">
<h4>Another interpretation of principal components<a href="chapter11.html#another-interpretation-of-principal-components" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig11/f4.png" /></p>
<ul>
<li><p>The first principal component loading vector has a very special property: it defines the line in <span class="math inline">\(p\)</span>-dimensional space that is closest to then <span class="math inline">\(n\)</span> observations (using average squared Euclidean distance as a measure of closeness).</p></li>
<li><p>The notion of principal components as the dimensions that are closest to the <span class="math inline">\(n\)</span> observations extends beyond just the first principal component.</p></li>
<li><p>For instance, the first two principal components of a data set span the plane that is closest to the <span class="math inline">\(n\)</span> observations, in terms of average squared Euclidean distance.</p></li>
</ul>
</div>
<div id="scaling-of-the-variables-matters" class="section level4 unnumbered hasAnchor">
<h4>Scaling of the variables matters<a href="chapter11.html#scaling-of-the-variables-matters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>If the variables are in different units, scaling each to have standard deviation equal to one is recommended.</p></li>
<li><p>If they are in the same units, you might or might not scale the variables.</p></li>
</ul>
<p><img src="fig11/f5.png" /></p>
</div>
<div id="proportion-variance-explained" class="section level4 unnumbered hasAnchor">
<h4>Proportion variance explained<a href="chapter11.html#proportion-variance-explained" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>To understand the strength of each component, we are intereste in knowing the proportion of variance explained (PVE) by each one.</p></li>
<li><p>The <em>total variance</em> present in a data set (assuming that the variables have been centered to have mean zero) is defined as</p></li>
</ul>
<p><span class="math display">\[
\sum_{j=1}^p Var(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^n x_{ij}^2
\]</span></p>
<p>      and the variance explained by the <span class="math inline">\(m\)</span>th principal component is</p>
<p><span class="math display">\[
Var(Z_m)=\frac{1}{n}\sum_{i=1}^n z_{im}^2
\]</span></p>
<ul>
<li><p>It can be shown that <span class="math inline">\(\sum_{j=1}^p Var(x_j)=\sum_{m=1}^M Var(Z_m)\)</span>, with <span class="math inline">\(M=min(n-1,p)\)</span>.</p></li>
<li><p>Therefore, the PVE of the <span class="math inline">\(m\)</span>th principal component is given by the positive quantity between 0 and 1</p></li>
</ul>
<p><span class="math display">\[
\frac{\sum_{i=1}^n z_{im}^2}{\sum_{j=1}^p \sum_{i=1}^n x_{ij}^2}
\]</span></p>
<ul>
<li>The PVEs sum to one. We sometimes disply the cumulative PVEs.</li>
</ul>
<p><img src="fig11/f6.png" /></p>
</div>
<div id="how-many-principal-components-should-we-use" class="section level4 unnumbered hasAnchor">
<h4>How many principal components should we use?<a href="chapter11.html#how-many-principal-components-should-we-use" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>If we use principle components as a summary of our data, how many components are sufficient?
<ul>
<li>No simple answer to this question, as cross-validation is not available for this purpose.
<ul>
<li>Why not?</li>
<li>When could we use cross-validation to select the number of components?</li>
</ul></li>
<li>The “scree plot” on the previous slide can be used as a guide: we look for an “elbow”.</li>
</ul></li>
</ul>
</div>
<div id="example-21" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter11.html#example-21" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Crime rates in the USA in 1973 per 100,000 perple and urban population percentages by states.</p></li>
<li><p>Using apply(), we can compute e.g. means and variances:</p></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chapter11.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(USArrests) <span class="co"># variables</span></span></code></pre></div>
<pre><code>## [1] &quot;Murder&quot;   &quot;Assault&quot;  &quot;UrbanPop&quot; &quot;Rape&quot;</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="chapter11.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">apply</span>(<span class="at">X =</span> USArrests, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean), <span class="at">digits =</span> <span class="dv">2</span>) <span class="co"># average crime rates</span></span></code></pre></div>
<pre><code>##   Murder  Assault UrbanPop     Rape 
##     7.79   170.76    65.54    21.23</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="chapter11.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">apply</span>(USArrests, <span class="dv">2</span>, var), <span class="dv">1</span>) <span class="co"># variances</span></span></code></pre></div>
<pre><code>##   Murder  Assault UrbanPop     Rape 
##     19.0   6945.2    209.5     87.7</code></pre>
<ul>
<li><p>Means and in particular variances differ vastly.</p></li>
<li><p>Function <strong>prcomp()</strong> belongs again into the default package (stat) and performs basic PCA.</p></li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="chapter11.html#cb7-1" aria-hidden="true" tabindex="-1"></a>pca.out <span class="ot">&lt;-</span> <span class="fu">princomp</span>(<span class="at">x =</span> USArrests, <span class="at">cor =</span> <span class="cn">TRUE</span>) <span class="co"># extract compontents using correlation matrix</span></span>
<span id="cb7-2"><a href="chapter11.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pca.out)</span></code></pre></div>
<pre><code>## Importance of components:
##                           Comp.1    Comp.2    Comp.3     Comp.4
## Standard deviation     1.5748783 0.9948694 0.5971291 0.41644938
## Proportion of Variance 0.6200604 0.2474413 0.0891408 0.04335752
## Cumulative Proportion  0.6200604 0.8675017 0.9566425 1.00000000</code></pre>
<ul>
<li>The two first components explain 86.8 % of the total variation (the first component explains 62 %).</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="chapter11.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(pca.out) <span class="co"># quantities produced by princomp()</span></span></code></pre></div>
<pre><code>## [1] &quot;sdev&quot;     &quot;loadings&quot; &quot;center&quot;   &quot;scale&quot;    &quot;n.obs&quot;    &quot;scores&quot;   &quot;call&quot;</code></pre>
<ul>
<li><strong>center</strong> and <strong>scale</strong> are used (here mean and standard deviation) are used to standardize the variables (<strong>sdev</strong> contains standard deviations (square roots of the eigenvalues) of the components printed in the above summary).</li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="chapter11.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(pca.out<span class="sc">$</span>center, <span class="dv">2</span>) <span class="co"># means</span></span></code></pre></div>
<pre><code>##   Murder  Assault UrbanPop     Rape 
##     7.79   170.76    65.54    21.23</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="chapter11.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(pca.out<span class="sc">$</span>scale, <span class="dv">2</span>) <span class="co"># standard deviations</span></span></code></pre></div>
<pre><code>##   Murder  Assault UrbanPop     Rape 
##     4.31    82.50    14.33     9.27</code></pre>
<ul>
<li><strong>loadings</strong> contain the eigenvectors.</li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="chapter11.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">loadings</span>(pca.out), <span class="dv">3</span>) <span class="co"># eigenvectors in R!</span></span></code></pre></div>
<pre><code>## 
## Loadings:
##          Comp.1 Comp.2 Comp.3 Comp.4
## Murder    0.536  0.418  0.341  0.649
## Assault   0.583  0.188  0.268 -0.743
## UrbanPop  0.278 -0.873  0.378  0.134
## Rape      0.543 -0.167 -0.818       
## 
##                Comp.1 Comp.2 Comp.3 Comp.4
## SS loadings     0.999   1.00   1.00  0.999
## Proportion Var  0.250   0.25   0.25  0.250
## Cumulative Var  0.250   0.50   0.75  1.000</code></pre>
<ul>
<li><p>The loadings of the first component are roughly the same for the three criminality measures, and much less weight on <strong>UrbanPop</strong>.</p></li>
<li><p>Thus, the first component represents overall (serious) criminality (high values on this component indicate high rates of these kinds of serious criminality in the state).</p></li>
<li><p>The second component represents the level of urbanization of the state (because of the negative sign, high negative values on the component indicate high urbanization, therefore in order to make interpretation more intuitive it is better to reverse the signs of the loadings, here we do not change the signs).</p></li>
<li><p><strong>scores</strong> contains the principal component scores.</p></li>
<li><p>Using the function <strong>biblot()</strong> the first two components can be plotted.</p></li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="chapter11.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">biplot</span>(pca.out, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;steel blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="co"># colors for the sate and variable names</span></span>
<span id="cb17-2"><a href="chapter11.html#cb17-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">scale =</span> <span class="dv">0</span>, <span class="co"># arrow lengths represent the loadings</span></span>
<span id="cb17-3"><a href="chapter11.html#cb17-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;PC1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;PC2&quot;</span>, <span class="at">cex =</span> .<span class="dv">8</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex2206-1.png" width="672" /></p>
<ul>
<li><p>From the biplot we find that on the first component for example Florida, Nevada, and California obtain high scores, suggesting that criminality rates in those states tend to be higher than in the others.</p></li>
<li><p>Mississippi, North Carolina, and South Caroline obtain high scores on the second component, so because of the negative loading, these states are less urbanized (criminality is there still to some extend above average (positive side on the first PC).</p></li>
</ul>
</div>
</div>
<div id="clustering" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> Clustering<a href="chapter11.html#clustering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><em>Clustering</em> refers to a very broad set of techniques for finding <em>subgroups</em>, or <em>clusters</em>, in a data set.</p></li>
<li><p>We seek a partition of the data into distinct groups so that the observations within each group are quite similar to each other.</p></li>
<li><p>It make this concrete, we must define what it means for two or more observations to be <em>similar</em> or <em>different.</em></p></li>
<li><p>Indeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied.</p></li>
</ul>
<div id="pca-vs-clustering" class="section level4 hasAnchor" number="11.2.0.1">
<h4><span class="header-section-number">11.2.0.1</span> PCA vs Clustering<a href="chapter11.html#pca-vs-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>PCA looks for a low-dimensional representation of the observations that explains a good fraction of the variance.</p></li>
<li><p>Clustering looks for homogeneous subgroups among the observations.</p></li>
</ul>
</div>
<div id="clustering-for-market-segmentation" class="section level4 unnumbered hasAnchor">
<h4>Clustering for market segmentation<a href="chapter11.html#clustering-for-market-segmentation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Suppose we have access to a large number of measurements (e.g. median household income, occupation, distance from nearest urban area, and so forth) for a large number of people.</p></li>
<li><p>Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product.</p></li>
<li><p>The task of performing market segmentation amounts to clustering the perple in the data set.</p></li>
</ul>
</div>
<div id="two-clustering-methods" class="section level4 unnumbered hasAnchor">
<h4>Two clustering methods<a href="chapter11.html#two-clustering-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>In <em><span class="math inline">\(K\)</span>-means clustering</em>, we seek to partition the observations into a pre-specified number of clusters.</p></li>
<li><p>In <em>hierarchical clustering</em>, we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a <em>dendrogram</em>, that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to <span class="math inline">\(n\)</span>.</p></li>
</ul>
</div>
<div id="k-means-clustering" class="section level3 hasAnchor" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> <span class="math inline">\(K\)</span>-means clustering<a href="chapter11.html#k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="fig11/f7.png" /></p>
<ul>
<li><p>A simulated data set with 150 observations in 2-dimensional space.</p></li>
<li><p>Panels show the results of applying <span class="math inline">\(K\)</span>-means clustering with different values of <span class="math inline">\(K\)</span>, the number of clusters.</p></li>
<li><p>The color of each observation indicates the cluster to which it was assigned using the <span class="math inline">\(K\)</span>-means clustering algorithm.</p></li>
<li><p>Note that there is no ordering of the clusters, so the cluster coloring is arbitrary.</p></li>
<li><p>These cluster labels were not used in clustering; instead, they are the outputs of the clustering procedure.</p></li>
</ul>
<div id="details-of-k-means-clustering" class="section level4 hasAnchor" number="11.2.1.1">
<h4><span class="header-section-number">11.2.1.1</span> Details of <span class="math inline">\(K\)</span>-means clustering<a href="chapter11.html#details-of-k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Let <span class="math inline">\(C_1, \ldots, C_k\)</span> denotes sets containing the indices of the observations in each cluster.</p></li>
<li><p>These sets satisfy two properties:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(C_1 \cup C_2 \cup \ldots \cup C_K=\{1,\ldots,n \}\)</span>, In other words, each observation belongs to at least one of the <span class="math inline">\(K\)</span> clusters.</p></li>
<li><p><span class="math inline">\(C_k\cap C_{k&#39;}=\phi\)</span> for all <span class="math inline">\(k \ne k&#39;\)</span>. In other words, the clusters are non-overlapping: no observation belongs to more than one cluster.</p></li>
</ol>
<ul>
<li><p>For instance, if the <span class="math inline">\(i\)</span>th observation is in the <span class="math inline">\(k\)</span>th cluster, then <span class="math inline">\(i\in C_k\)</span>.</p></li>
<li><p>The idea behind <span class="math inline">\(K\)</span>-means clustering is that a <em>good</em> clustering is one for which the <em>within-cluster variation</em> is as small as possible.</p></li>
<li><p>The within-cluster variation for cluster <span class="math inline">\(C_k\)</span> is a measure <span class="math inline">\(WCV(C_k)\)</span> of the amount by which the observations within a cluster differ from each other.</p></li>
<li><p>Hence we want to solve the problem</p></li>
</ul>
<p><span class="math display">\[
minimize_{C_1,\ldots,C_k}\{\sum_{k=1}^K WCV(C_k) \}
\]</span></p>
<ul>
<li>In other words, this formula says that we want to partition the observations into <span class="math inline">\(K\)</span> cluster such that the total within-cluster variation, summed over all <span class="math inline">\(K\)</span> clusters, is a small as possible.</li>
</ul>
</div>
<div id="how-to-define-within-cluster-variation" class="section level4 unnumbered hasAnchor">
<h4>How to define within-cluster variation?<a href="chapter11.html#how-to-define-within-cluster-variation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Typically we use Euclidean distance</li>
</ul>
<p><span class="math display">\[
WCV(C_k)=\frac{1}{|C_k|}\sum_{i,i&#39;\in C_k}\sum_{j=1}^p (x_{ij}-x_{i&#39;j})^2
\]</span></p>
<p>      where <span class="math inline">\(|C_k|\)</span> denotes the number of observations in the <span class="math inline">\(k\)</span>th cluster.</p>
<ul>
<li>Combining two equations gives the optimization problem that defines <span class="math inline">\(K\)</span>-means clustering,</li>
</ul>
<p><span class="math display">\[
minimize_{C_1,\ldots,C_k}\{\sum_{k=1}^K \frac{1}{|C_k|}\sum_{i,i&#39;\in C_k}\sum_{j=1}^p (x_{ij}-x_{i&#39;j})^2 \}
\]</span></p>
</div>
<div id="k-means-clustering-algorithm" class="section level4 unnumbered hasAnchor">
<h4><span class="math inline">\(K\)</span>-means clustering algorithm<a href="chapter11.html#k-means-clustering-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p>Randomly assign a number, from 1 to <span class="math inline">\(k\)</span>, to each of the observation. These serve as initial cluster assignments for the observations.</p></li>
<li><p>Iterate until the cluster assignments stop changing:</p></li>
</ol>
<p>      2.1 For each of the <span class="math inline">\(K\)</span> clusters, compute the cluster centroid. The <span class="math inline">\(k\)</span>th cluster <em>centroid</em> is the vector of the <span class="math inline">\(p\)</span> feature means for the observations in the <span class="math inline">\(k\)</span>th cluster.</p>
<p>      2.2 Assign each observation to the cluster whose centroid is closest (where <em>closest</em> is defined using Euclidean distance).</p>
</div>
<div id="properties-of-the-algorithm" class="section level4 unnumbered hasAnchor">
<h4>Properties of the Algorithm<a href="chapter11.html#properties-of-the-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>This algorithm is guaranteed to decrease the value of the objective function at each step. Why?</p></li>
<li><p>Note that</p></li>
</ul>
<p><span class="math display">\[
\frac{1}{|C_k|}\sum_{i,i&#39;\in C_k}\sum_{j=1}^p (x_{ij}-x_{i&#39;j})^2=2\sum_{i\in C_k}\sum_{j=1}^p (x_{ij}-\bar{x}_{kj})^2
\]</span>
      where <span class="math inline">\(\bar{x}_{kj}=\frac{1}{|C_k|} \sum_{i\in C_k}x_{ij}\)</span> is the mean for feature <span class="math inline">\(j\)</span> in cluster <span class="math inline">\(C_k\)</span>.</p>
<ul>
<li>However it is not guaranteed to give the global minimum. Why not?</li>
</ul>
</div>
<div id="example-22" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter11.html#example-22" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig11/f8.png" /></p>
<ul>
<li>The progress of the <span class="math inline">\(K\)</span>-means algorithm with <span class="math inline">\(K=3\)</span>.
<ul>
<li>Top left: The observations are shown.</li>
<li>Top center: In Step 1 of the algorithm, each observation is randomly assigned to a cluster.</li>
<li>Top right: In Step 2(a), the cluster centroids are computed. These are shown as large colored disks. Initially the centroids are almost completely overlapping because the initial cluster assignments were chosen at random.</li>
<li>Bottom left: In Step 2(b), each observation is assigned to the nearest centroid.</li>
<li>Bottom center: Step 2(a) is once again performed, leading to new cluster centroids.</li>
<li>Bottom right: The results obtained after 10 iterations.</li>
</ul></li>
</ul>
</div>
<div id="example-different-starting-values" class="section level4 unnumbered hasAnchor">
<h4>Example: different starting values<a href="chapter11.html#example-different-starting-values" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig11/f9.png" /></p>
<ul>
<li><p><span class="math inline">\(K\)</span>-means clustering performed six times on the data from previous figure with <span class="math inline">\(K=3\)</span>, each time with a different random assignment of the observations in Step 1 of the <span class="math inline">\(K\)</span>-means algorithm.</p></li>
<li><p>Above each plot is the value of the objective function.</p></li>
<li><p>Three different local optima were obtained, one of which resulted in a smaller value of the objective and provides better separation between the clusters.</p></li>
<li><p>Those labeled in red all achieved the same best solution, with an objective value of 235.8.</p></li>
</ul>
</div>
<div id="example---beer-data" class="section level4 unnumbered hasAnchor">
<h4>Example - Beer data<a href="chapter11.html#example---beer-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="chapter11.html#cb18-1" aria-hidden="true" tabindex="-1"></a>beer <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="at">header =</span> <span class="cn">TRUE</span>, <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span>,</span>
<span id="cb18-2"><a href="chapter11.html#cb18-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">text =</span> <span class="st">&quot;</span></span>
<span id="cb18-3"><a href="chapter11.html#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="st">beer,                  calories,  sodium,  alcohol,  cost</span></span>
<span id="cb18-4"><a href="chapter11.html#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="chapter11.html#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="st">Budweiser,               144,       15,      4.7,    0.43</span></span>
<span id="cb18-6"><a href="chapter11.html#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="st">Schlitz,                 151,       19,      4.9,    0.43</span></span>
<span id="cb18-7"><a href="chapter11.html#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="st">Lowenbrau,               157,       15,      0.9,    0.48</span></span>
<span id="cb18-8"><a href="chapter11.html#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="st">Kronenbourg,             170,        7,      5.2,    0.73</span></span>
<span id="cb18-9"><a href="chapter11.html#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="st">Heineken,                152,       11,      5.0,    0.77</span></span>
<span id="cb18-10"><a href="chapter11.html#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="st">Old Milwaukee,           145,       23,      4.6,    0.28</span></span>
<span id="cb18-11"><a href="chapter11.html#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="st">Augsberger,              175,       24,      5.5,    0.40</span></span>
<span id="cb18-12"><a href="chapter11.html#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="st">Srohs Bohemian Style,    149,       27,      4.7,    0.42</span></span>
<span id="cb18-13"><a href="chapter11.html#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="st">Miller Lite,              99,       10,      4.3,    0.43</span></span>
<span id="cb18-14"><a href="chapter11.html#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="st">Budweiser Light,         113,        8,      3.7,    0.40</span></span>
<span id="cb18-15"><a href="chapter11.html#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="st">Coors,                   140,       18,      4.6,    0.44</span></span>
<span id="cb18-16"><a href="chapter11.html#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="st">Coors Light,             102,       15,      4.1,    0.46</span></span>
<span id="cb18-17"><a href="chapter11.html#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="st">Michelob Light,          135,       11,      4.2,    0.50</span></span>
<span id="cb18-18"><a href="chapter11.html#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="st">Becks,                   150,       19,      4.7,    0.76</span></span>
<span id="cb18-19"><a href="chapter11.html#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="st">Kirin,                   149,        6,      5.0,    0.79</span></span>
<span id="cb18-20"><a href="chapter11.html#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="st">Pabst Extra Light,        68,       15,      2.3,    0.38</span></span>
<span id="cb18-21"><a href="chapter11.html#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="st">Hamms,                   139,       19,      4.4,    0.43</span></span>
<span id="cb18-22"><a href="chapter11.html#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="st">Heilemans Old Style,     144,       24,      4.9,    0.43</span></span>
<span id="cb18-23"><a href="chapter11.html#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="st">Olympia Goled Light,      72,        6,      2.9,    0.46</span></span>
<span id="cb18-24"><a href="chapter11.html#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="st">Schlitz Light,            97,        7,      4.2,    0.47</span></span>
<span id="cb18-25"><a href="chapter11.html#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;</span>)</span>
<span id="cb18-26"><a href="chapter11.html#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(beer) <span class="ot">&lt;-</span> beer<span class="sc">$</span>beer <span class="co"># use beer brands as rownames</span></span>
<span id="cb18-27"><a href="chapter11.html#cb18-27" aria-hidden="true" tabindex="-1"></a>beer<span class="sc">$</span>beer <span class="ot">&lt;-</span> <span class="cn">NULL</span> <span class="co"># drop beer variable</span></span>
<span id="cb18-28"><a href="chapter11.html#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="chapter11.html#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(beer)</span></code></pre></div>
<pre><code>##               calories sodium alcohol cost
## Budweiser          144     15     4.7 0.43
## Schlitz            151     19     4.9 0.43
## Lowenbrau          157     15     0.9 0.48
## Kronenbourg        170      7     5.2 0.73
## Heineken           152     11     5.0 0.77
## Old Milwaukee      145     23     4.6 0.28</code></pre>
<ul>
<li><p>A potentially intersting question might be are some beers more alike than the others.</p></li>
<li><p>I.e., are there natural groups of the beers.</p></li>
<li><p>Before clustering, check descriptive statistics and plots.</p></li>
</ul>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="chapter11.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car) <span class="co"># car library </span></span></code></pre></div>
<pre><code>## 필요한 패키지를 로딩중입니다: carData</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="chapter11.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">#library(help = car) # some help about car</span></span>
<span id="cb22-2"><a href="chapter11.html#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(beer), <span class="dv">3</span>) <span class="co"># correlation matrix </span></span></code></pre></div>
<pre><code>##          calories sodium alcohol   cost
## calories    1.000  0.415   0.486  0.328
## sodium      0.415  1.000   0.223 -0.433
## alcohol     0.486  0.223   1.000  0.259
## cost        0.328 -0.433   0.259  1.000</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="chapter11.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">scatterplotMatrix</span>(beer, <span class="at">smooth =</span> <span class="cn">FALSE</span>, <span class="at">regLine =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex2302-1.png" width="672" /></p>
<ul>
<li>It turns out that Lowenbrau is an outlier in particular in the relation of alcohol to others.</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="chapter11.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">scatterplot</span>(<span class="at">x =</span> beer<span class="sc">$</span>alcohol, <span class="at">y =</span> beer<span class="sc">$</span>calories, <span class="at">regLine =</span> <span class="cn">FALSE</span>, <span class="at">smooth =</span> <span class="cn">FALSE</span>,</span>
<span id="cb25-2"><a href="chapter11.html#cb25-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">boxplots =</span> <span class="cn">FALSE</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">6</span>), </span>
<span id="cb25-3"><a href="chapter11.html#cb25-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Alcohol&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Calories&quot;</span>,</span>
<span id="cb25-4"><a href="chapter11.html#cb25-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Calories vs Alcohol&quot;</span>)</span>
<span id="cb25-5"><a href="chapter11.html#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x =</span> beer<span class="sc">$</span>alcohol, <span class="at">y =</span> beer<span class="sc">$</span>calories, <span class="at">labels =</span> <span class="fu">rownames</span>(beer),</span>
<span id="cb25-6"><a href="chapter11.html#cb25-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex =</span> .<span class="dv">8</span>, <span class="at">pos =</span> <span class="dv">2</span>) <span class="co"># add brands</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/ex2303-1.png" width="672" /></p>
<ul>
<li>After removing Lowenbrau, no more obvious outliers.</li>
</ul>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="chapter11.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">scatterplotMatrix</span>(beer[<span class="fu">rownames</span>(beer) <span class="sc">!=</span> <span class="st">&quot;Lowenbrau&quot;</span>, ], <span class="co"># drop Lowenbrau</span></span>
<span id="cb26-2"><a href="chapter11.html#cb26-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">smooth =</span> <span class="cn">FALSE</span>, <span class="at">regLine =</span> <span class="cn">FALSE</span>) <span class="co"># scattrplot without Lowenbrau</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/ex2304-1.png" width="672" /></p>
</div>
<div id="example---k-means-clustering" class="section level4 unnumbered hasAnchor">
<h4>Example - <span class="math inline">\(K\)</span>-means clustering<a href="chapter11.html#example---k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="chapter11.html#cb27-1" aria-hidden="true" tabindex="-1"></a>km2 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(beer, <span class="at">centers =</span> <span class="dv">2</span>, <span class="at">nstart =</span> <span class="dv">50</span>) <span class="co"># K = 2, nstart defines the number of random starts</span></span>
<span id="cb27-2"><a href="chapter11.html#cb27-2" aria-hidden="true" tabindex="-1"></a>km2 <span class="co"># print results</span></span></code></pre></div>
<pre><code>## K-means clustering with 2 clusters of sizes 6, 14
## 
## Cluster means:
##    calories   sodium  alcohol      cost
## 1  91.83333 10.16667 3.583333 0.4333333
## 2 150.00000 17.00000 4.521429 0.5207143
## 
## Clustering vector:
##            Budweiser              Schlitz            Lowenbrau 
##                    2                    2                    2 
##          Kronenbourg             Heineken        Old Milwaukee 
##                    2                    2                    2 
##           Augsberger Srohs Bohemian Style          Miller Lite 
##                    2                    2                    1 
##      Budweiser Light                Coors          Coors Light 
##                    1                    2                    1 
##       Michelob Light                Becks                Kirin 
##                    2                    2                    2 
##    Pabst Extra Light                Hamms  Heilemans Old Style 
##                    1                    2                    2 
##  Olympia Goled Light        Schlitz Light 
##                    1                    1 
## 
## Within cluster sum of squares by cluster:
## [1] 1672.962 2187.863
##  (between_SS / total_SS =  78.9 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot;
## [6] &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<ul>
<li>Generate the “scree plot” to help finding out an appropriate number of clusters.</li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="chapter11.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="do">## plot within sums of squares for K = 1, 2, 3, ..., 8 (K = 1, no clustering)</span></span>
<span id="cb29-2"><a href="chapter11.html#cb29-2" aria-hidden="true" tabindex="-1"></a>ssw <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="dv">8</span>)</span>
<span id="cb29-3"><a href="chapter11.html#cb29-3" aria-hidden="true" tabindex="-1"></a>ssw[<span class="dv">1</span>] <span class="ot">&lt;-</span> km2<span class="sc">$</span>totss <span class="co"># total sum of squares represents no clustering</span></span>
<span id="cb29-4"><a href="chapter11.html#cb29-4" aria-hidden="true" tabindex="-1"></a>ssw[<span class="dv">2</span>] <span class="ot">&lt;-</span> km2<span class="sc">$</span>tot.withinss <span class="co"># within sum of squares with K = 2</span></span>
<span id="cb29-5"><a href="chapter11.html#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">8</span>) ssw[k] <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(beer, <span class="at">centers =</span> k, <span class="at">nstart =</span> <span class="dv">50</span>)<span class="sc">$</span>tot.withinss <span class="co"># SSWs</span></span>
<span id="cb29-6"><a href="chapter11.html#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="chapter11.html#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ssw, <span class="at">xlab =</span> <span class="st">&quot;Number of Clusters&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Within Sum of Squares&quot;</span>,</span>
<span id="cb29-8"><a href="chapter11.html#cb29-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Within Sum of Squares &#39;Scree plot&#39;&quot;</span>, <span class="at">font.main =</span> <span class="dv">1</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex2306-1.png" width="672" /></p>
<ul>
<li><p>Potential candidates are <span class="math inline">\(K=2\)</span>, <span class="math inline">\(K=3\)</span>, <span class="math inline">\(K=4\)</span>.</p></li>
<li><p>Below are some summaries of the results.</p></li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="chapter11.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="do">## K = 3 and 4 clusterings</span></span>
<span id="cb30-2"><a href="chapter11.html#cb30-2" aria-hidden="true" tabindex="-1"></a>km3 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(beer, <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">50</span>) <span class="co"># K = 3</span></span>
<span id="cb30-3"><a href="chapter11.html#cb30-3" aria-hidden="true" tabindex="-1"></a>km4 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(beer, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">50</span>) <span class="co"># K = 4</span></span>
<span id="cb30-4"><a href="chapter11.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="do">## collect clusterings into a matrix</span></span>
<span id="cb30-5"><a href="chapter11.html#cb30-5" aria-hidden="true" tabindex="-1"></a>clmat <span class="ot">&lt;-</span> <span class="fu">cbind</span>(km2<span class="sc">$</span>cluster, km3<span class="sc">$</span>cluster, km4<span class="sc">$</span>cluster) <span class="co"># matrix of 2 to 4 clusterings</span></span>
<span id="cb30-6"><a href="chapter11.html#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(clmat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;K = 2&quot;</span>, <span class="st">&quot;K = 3&quot;</span>, <span class="st">&quot;K = 4&quot;</span>)</span>
<span id="cb30-7"><a href="chapter11.html#cb30-7" aria-hidden="true" tabindex="-1"></a>clmat[<span class="fu">order</span>(clmat[, <span class="st">&quot;K = 2&quot;</span>]), ] <span class="co"># print results ordered by K = 2 clusters</span></span></code></pre></div>
<pre><code>##                      K = 2 K = 3 K = 4
## Miller Lite              1     3     3
## Budweiser Light          1     3     3
## Coors Light              1     3     3
## Pabst Extra Light        1     2     4
## Olympia Goled Light      1     2     4
## Schlitz Light            1     3     3
## Budweiser                2     1     1
## Schlitz                  2     1     1
## Lowenbrau                2     1     1
## Kronenbourg              2     1     2
## Heineken                 2     1     1
## Old Milwaukee            2     1     1
## Augsberger               2     1     2
## Srohs Bohemian Style     2     1     1
## Coors                    2     1     1
## Michelob Light           2     1     1
## Becks                    2     1     1
## Kirin                    2     1     1
## Hamms                    2     1     1
## Heilemans Old Style      2     1     1</code></pre>
<ul>
<li><p>With <span class="math inline">\(K=2\)</span> the clusters consist essentially of ordinary and light beers (similar to hierarchical clustering).</p></li>
<li><p>With <span class="math inline">\(K=3\)</span>, the light beers become split into two groups so that in cluster 1 there are only two brands.</p></li>
<li><p>Increasing <span class="math inline">\(K\)</span> to 4, two of the ordinary beers make up a new cluster.</p></li>
<li><p>Thus, with <span class="math inline">\(K=2\)</span> the pattern seems most clear cut.</p></li>
<li><p>Below are cluster means of the variables in different solutions.</p></li>
</ul>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="chapter11.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="do">## cluster means</span></span>
<span id="cb32-2"><a href="chapter11.html#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(km2<span class="sc">$</span>centers, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##   calories sodium alcohol cost
## 1    91.83  10.17    3.58 0.43
## 2   150.00  17.00    4.52 0.52</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="chapter11.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(km3<span class="sc">$</span>centers, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##   calories sodium alcohol cost
## 1   150.00   17.0    4.52 0.52
## 2    70.00   10.5    2.60 0.42
## 3   102.75   10.0    4.08 0.44</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="chapter11.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(km4<span class="sc">$</span>centers, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##   calories sodium alcohol cost
## 1   146.25  17.25    4.38 0.51
## 2   172.50  15.50    5.35 0.56
## 3   102.75  10.00    4.08 0.44
## 4    70.00  10.50    2.60 0.42</code></pre>
</div>
</div>
<div id="hierarchical-clustering" class="section level3 hasAnchor" number="11.2.2">
<h3><span class="header-section-number">11.2.2</span> Hierarchical Clustering<a href="chapter11.html#hierarchical-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p><span class="math inline">\(K\)</span>-means clustering requires us to pre-specify the number of clusters <span class="math inline">\(K\)</span>.</p>
<ul>
<li>This can be a disadvantage.</li>
</ul></li>
<li><p><em>Hierarchical clustering</em> is an alternative approach which does not require that we commit to a particular choice of <span class="math inline">\(K\)</span>.</p></li>
<li><p>In this section, we describe <em>bottom-up</em> or <em>agglomerative</em> clustering.</p>
<ul>
<li>This is the most common type of hierarchical clustering, and refers to the fact that a dendrogram is built starting from the leaves and combining clusters up to the trunk.</li>
</ul></li>
</ul>
<div id="hierarchical-clustering-the-idea" class="section level4 hasAnchor" number="11.2.2.1">
<h4><span class="header-section-number">11.2.2.1</span> Hierarchical clustering: the idea<a href="chapter11.html#hierarchical-clustering-the-idea" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Builds a hierarchy in a “bottom-up” fashion…</li>
</ul>
<p><img src="fig11/f10.png" /></p>
<p><img src="fig11/f11.png" /></p>
<p><img src="fig11/f12.png" /></p>
<p><img src="fig11/f13.png" /></p>
<p><img src="fig11/f14.png" /></p>
</div>
<div id="hierarchical-clustering-algorithm" class="section level4 unnumbered hasAnchor">
<h4>Hierarchical clustering algorithm<a href="chapter11.html#hierarchical-clustering-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The approach in words:
<ul>
<li>Start with each point in its own cluster.</li>
<li>Identify the <em>closest</em> two clusters and merge them.</li>
<li>Repeat.</li>
<li>Ends when all points are in a single cluster.</li>
</ul></li>
</ul>
<p><img src="fig11/f15.png" /></p>
</div>
<div id="example-23" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter11.html#example-23" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig11/f16.png" /></p>
<ul>
<li>45 observations generated in 2-dimensional space.
<ul>
<li>In reality there are three distinct classes, shown in separate colors.</li>
</ul></li>
<li>However, we will treat these class labels as unknown and will seek to cluster the observations in order to discover the classes from the data.</li>
</ul>
</div>
<div id="application-of-hierarchical-clustering" class="section level4 unnumbered hasAnchor">
<h4>Application of hierarchical clustering<a href="chapter11.html#application-of-hierarchical-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig11/f17.png" /></p>
<ul>
<li><p>Left: Dendrogram obtained from hierarchically clustering the data from previous slide, with complete linkage and Euclidean distance.</p></li>
<li><p>Center: The dendrogram from the left-hand panel, cut at a height of 9 (indicated by the dashed line).</p>
<ul>
<li>This cut results in two distinct clusters, shown in different colors.</li>
</ul></li>
<li><p>Right: The dendrogram from the left-hand panel, now cut at a height of 5.</p>
<ul>
<li>This cut results in three distinct clusters, shown in different colors.</li>
<li>Note that the colors were not used in clustering, but are simply used for display purposes in this figure.</li>
</ul></li>
</ul>
</div>
<div id="types-of-linkage" class="section level4 unnumbered hasAnchor">
<h4>Types of linkage<a href="chapter11.html#types-of-linkage" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<table>
<colgroup>
<col width="41%" />
<col width="58%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Linkage</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Complete</td>
<td align="left">Maximal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the <em>largest</em> of these dissimilarities.</td>
</tr>
<tr class="even">
<td align="center">Single</td>
<td align="left">Minimal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the <em>smallest</em> of these dissimilarities.</td>
</tr>
<tr class="odd">
<td align="center">Average</td>
<td align="left">Mean inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the <em>average</em> of these dissimilarities.</td>
</tr>
<tr class="even">
<td align="center">Centroid</td>
<td align="left">Dissimilarity between the centroid for cluster A (a mean vector of length <span class="math inline">\(p\)</span>) and the centroid for cluster B. Centroid linkage can result in undesirable <em>inversions.</em></td>
</tr>
</tbody>
</table>
</div>
<div id="choice-of-dissimilarity-measure" class="section level4 unnumbered hasAnchor">
<h4>Choice of dissimilarity measure<a href="chapter11.html#choice-of-dissimilarity-measure" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>So far have used Euclidean distance.</p></li>
<li><p>An alternative is correlation-based distance which considers two observations to be similar if their features are highly correlated.</p></li>
<li><p>This is an unusual use of correlation, which is normally computed between variables; here it is computed between the observation profiles for each pair of observations.</p></li>
</ul>
<p><img src="fig11/f18.png" /></p>
</div>
<div id="practical-issues" class="section level4 unnumbered hasAnchor">
<h4>Practical issues<a href="chapter11.html#practical-issues" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Scaling of the variables matters!
<ul>
<li>Should the observations or features first be standardized in some way?</li>
<li>For instance, maybethe variables should be centered to have mean zero and scaled to have standard deviation one.</li>
</ul></li>
<li>In the case of hierarchical clustering,
<ul>
<li>What dissimilarity measure should be used?</li>
<li>What type of linkage should be used?</li>
</ul></li>
<li>How many clusters to choose? (in both <span class="math inline">\(K\)</span>-means or hierarchical clustering).
<ul>
<li>Difficult problem.</li>
<li>No agreed-upon method.</li>
</ul></li>
<li>Which features should we use to drive the clustering?</li>
</ul>
</div>
<div id="example-breast-cancder-microarray-study" class="section level4 unnumbered hasAnchor">
<h4>Example: breast cancder microarray study<a href="chapter11.html#example-breast-cancder-microarray-study" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>“Repeated observation of breast tumor subtypes in independent gene expression data sets;” Sorlie et al, PNAS 2003</p></li>
<li><p>Gene expression measurements for about ~ 8000 genes, for each of 88 breast cancer patients.</p></li>
<li><p>Average linkage, correlation metric</p></li>
<li><p>Clustered samples using 500 <em>intrinsic genes</em>: each woman was measured before and after chemotherapy.</p>
<ul>
<li>Intrinsic genes have smallest within/between variation.</li>
</ul></li>
</ul>
<p><img src="fig11/f19.png" /></p>
<p><img src="fig11/f20.png" /></p>
</div>
<div id="conclusions-1" class="section level4 unnumbered hasAnchor">
<h4>Conclusions<a href="chapter11.html#conclusions-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Unsupervised learning is important for understanding the variation and grouping structure of a set of unlabeled data, and can be a useful pre-processor for supervised learning.</p></li>
<li><p>It is intrinsically more difficult than supervised learning because there is no gold standard (like an outcome variable) and no single objective (like test set accuracy).</p></li>
<li><p>It is an active field of research, with many recently developed tools such as self-organizing maps, independent components analysis and spectral clustering.</p></li>
</ul>
</div>
<div id="example---beer" class="section level4 unnumbered hasAnchor">
<h4>Example - beer<a href="chapter11.html#example---beer" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Beer brands using hclust() function of stat package, which is a default R package.</li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="chapter11.html#cb38-1" aria-hidden="true" tabindex="-1"></a>hc.single <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(beer), <span class="at">method =</span> <span class="st">&quot;single&quot;</span>) <span class="co"># single linkage</span></span>
<span id="cb38-2"><a href="chapter11.html#cb38-2" aria-hidden="true" tabindex="-1"></a>hc.complete <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(beer), <span class="at">method =</span> <span class="st">&quot;complete&quot;</span>) <span class="co"># complete</span></span>
<span id="cb38-3"><a href="chapter11.html#cb38-3" aria-hidden="true" tabindex="-1"></a>hc.average <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(beer), <span class="at">method =</span> <span class="st">&quot;average&quot;</span>)</span>
<span id="cb38-4"><a href="chapter11.html#cb38-4" aria-hidden="true" tabindex="-1"></a>hc.centroid <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(beer), <span class="at">method =</span> <span class="st">&quot;centroid&quot;</span>)</span>
<span id="cb38-5"><a href="chapter11.html#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="chapter11.html#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="do">## dentrogram plots</span></span>
<span id="cb38-7"><a href="chapter11.html#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="chapter11.html#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb38-9"><a href="chapter11.html#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc.single, <span class="at">main =</span> <span class="st">&quot;Single Linkage&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">sub =</span> <span class="st">&quot;&quot;</span>, <span class="at">cex =</span> .<span class="dv">8</span>, <span class="at">cex.main =</span> .<span class="dv">9</span>)</span>
<span id="cb38-10"><a href="chapter11.html#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc.complete, <span class="at">main =</span> <span class="st">&quot;Complete Linkage&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">sub =</span> <span class="st">&quot;&quot;</span>, <span class="at">cex =</span> .<span class="dv">8</span>, <span class="at">cex.main =</span> .<span class="dv">9</span>)</span>
<span id="cb38-11"><a href="chapter11.html#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc.average, <span class="at">main =</span> <span class="st">&quot;Average Linkage&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">sub =</span> <span class="st">&quot;&quot;</span>, <span class="at">cex =</span> .<span class="dv">8</span>, <span class="at">cex.main =</span> .<span class="dv">9</span>)</span>
<span id="cb38-12"><a href="chapter11.html#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc.centroid, <span class="at">main =</span> <span class="st">&quot;Centroid&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">sub =</span> <span class="st">&quot;&quot;</span>, <span class="at">cex =</span> .<span class="dv">8</span>, <span class="at">cex.main =</span> .<span class="dv">9</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex2501-1.png" width="672" /></p>
<ul>
<li><p>In the single linkage no obvious structure.</p></li>
<li><p>The other methods produce two main clusters of ordinary and light beers.</p></li>
</ul>
<p><br></p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter10.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ILR.pdf", "ILR.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
