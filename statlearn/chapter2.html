<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Statistical Learning | Statistical Learning</title>
  <meta name="description" content="This is a Statistical Learning" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Statistical Learning | Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Statistical Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Statistical Learning | Statistical Learning" />
  
  <meta name="twitter:description" content="This is a Statistical Learning" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2023-05-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter1.html"/>
<link rel="next" href="chapter3.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>머리말</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#understanding-data"><i class="fa fa-check"></i><b>1.1</b> Understanding Data</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="chapter1.html"><a href="chapter1.html#example-1-supervised-learning-continuous-output"><i class="fa fa-check"></i><b>1.1.1</b> Example 1 (Supervised learning: Continuous output)</a></li>
<li class="chapter" data-level="1.1.2" data-path="chapter1.html"><a href="chapter1.html#example-2-supervised-leraning-categorical-output"><i class="fa fa-check"></i><b>1.1.2</b> Example 2 (Supervised leraning: Categorical output)</a></li>
<li class="chapter" data-level="1.1.3" data-path="chapter1.html"><a href="chapter1.html#example-3-unsupervised-learning-clustering-observations"><i class="fa fa-check"></i><b>1.1.3</b> Example 3 (Unsupervised learning: Clustering observations)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#brief-history-of-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Brief History of Statistical Learning</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#notation-and-simple-matrix-algebra"><i class="fa fa-check"></i><b>1.3</b> Notation and Simple Matrix Algebra</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#what-is-statitical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statitical Learning</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chapter2.html"><a href="chapter2.html#prediction"><i class="fa fa-check"></i><b>2.1.1</b> Prediction</a></li>
<li class="chapter" data-level="2.1.2" data-path="chapter2.html"><a href="chapter2.html#inference"><i class="fa fa-check"></i><b>2.1.2</b> Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="chapter2.html"><a href="chapter2.html#estimating-f"><i class="fa fa-check"></i><b>2.1.3</b> Estimating <span class="math inline">\(f\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="chapter2.html"><a href="chapter2.html#prediction-accuaracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.4</b> Prediction Accuaracy and Model Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter2.html"><a href="chapter2.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="chapter2.html"><a href="chapter2.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="2.2.3" data-path="chapter2.html"><a href="chapter2.html#classification-problems"><i class="fa fa-check"></i><b>2.2.3</b> Classification Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#basic-commands"><i class="fa fa-check"></i><b>3.1</b> Basic Commands</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#graphics"><i class="fa fa-check"></i><b>3.2</b> Graphics</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#indexing-data"><i class="fa fa-check"></i><b>3.3</b> Indexing Data</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#loading-data"><i class="fa fa-check"></i><b>3.4</b> Loading Data</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#additional-graphical-and-numerical-summaries"><i class="fa fa-check"></i><b>3.5</b> Additional Graphical and Numerical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#simple-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Regression</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#multiple-regression"><i class="fa fa-check"></i><b>4.2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chapter4.html"><a href="chapter4.html#importance-of-predictors-statistical-significance-of-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Importance of Predictors: Statistical Significance of Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="chapter4.html"><a href="chapter4.html#selecting-important-variables"><i class="fa fa-check"></i><b>4.2.2</b> Selecting Important Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="chapter4.html"><a href="chapter4.html#model-fit"><i class="fa fa-check"></i><b>4.2.3</b> Model Fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="chapter4.html"><a href="chapter4.html#prediction-1"><i class="fa fa-check"></i><b>4.2.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#other-consideration"><i class="fa fa-check"></i><b>4.3</b> Other Consideration</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chapter4.html"><a href="chapter4.html#qualitative-predictors"><i class="fa fa-check"></i><b>4.3.1</b> Qualitative Predictors</a></li>
<li class="chapter" data-level="" data-path="chapter4.html"><a href="chapter4.html#example-8"><i class="fa fa-check"></i>Example 8</a></li>
<li class="chapter" data-level="4.3.2" data-path="chapter4.html"><a href="chapter4.html#potential-problems"><i class="fa fa-check"></i><b>4.3.2</b> Potential Problems</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#non-parametric-regressions"><i class="fa fa-check"></i><b>4.4</b> Non-parametric Regressions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#logit-and-probit-models"><i class="fa fa-check"></i><b>5.1</b> Logit and Probit Models</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#confounding"><i class="fa fa-check"></i><b>5.2</b> Confounding</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#logit-model-for-multiple-classes"><i class="fa fa-check"></i><b>5.3</b> Logit Model for Multiple Classes</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#discriminant-analysis"><i class="fa fa-check"></i><b>5.4</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chapter5.html"><a href="chapter5.html#bayes-theorem-for-classification"><i class="fa fa-check"></i><b>5.4.1</b> Bayes Theorem for Classification</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapter5.html"><a href="chapter5.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="5.4.3" data-path="chapter5.html"><a href="chapter5.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.3</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#naive-bayes"><i class="fa fa-check"></i><b>5.5</b> Naive Bayes</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#knn-classification"><i class="fa fa-check"></i><b>5.6</b> KNN Classification</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#example-10"><i class="fa fa-check"></i><b>5.7</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Resampling Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#validation-set-approach"><i class="fa fa-check"></i><b>6.1</b> Validation Set Approach</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.2</b> K-fold Cross-validation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chapter6.html"><a href="chapter6.html#example-11"><i class="fa fa-check"></i><b>6.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#bootstrap"><i class="fa fa-check"></i><b>6.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Model Selection</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#variable-selection"><i class="fa fa-check"></i><b>7.1</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chapter7.html"><a href="chapter7.html#best-subset-selection"><i class="fa fa-check"></i><b>7.1.1</b> Best Subset Selection</a></li>
<li class="chapter" data-level="7.1.2" data-path="chapter7.html"><a href="chapter7.html#forward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.2</b> Forward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.3" data-path="chapter7.html"><a href="chapter7.html#backward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.3</b> Backward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.4" data-path="chapter7.html"><a href="chapter7.html#choosing-the-optimal-model"><i class="fa fa-check"></i><b>7.1.4</b> Choosing the Optimal Model</a></li>
<li class="chapter" data-level="7.1.5" data-path="chapter7.html"><a href="chapter7.html#example-13"><i class="fa fa-check"></i><b>7.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#shrinkage-methods"><i class="fa fa-check"></i><b>7.2</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chapter7.html"><a href="chapter7.html#ridge-regression"><i class="fa fa-check"></i><b>7.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="chapter7.html"><a href="chapter7.html#rasso-regression"><i class="fa fa-check"></i><b>7.2.2</b> RASSO Regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="chapter7.html"><a href="chapter7.html#example-14"><i class="fa fa-check"></i><b>7.2.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#dimension-reduction-methods"><i class="fa fa-check"></i><b>7.3</b> Dimension Reduction Methods</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="chapter7.html"><a href="chapter7.html#principal-components-regression"><i class="fa fa-check"></i><b>7.3.1</b> Principal Components Regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="chapter7.html"><a href="chapter7.html#example-15"><i class="fa fa-check"></i><b>7.3.2</b> Example</a></li>
<li class="chapter" data-level="7.3.3" data-path="chapter7.html"><a href="chapter7.html#partial-least-squares"><i class="fa fa-check"></i><b>7.3.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="7.3.4" data-path="chapter7.html"><a href="chapter7.html#example-16"><i class="fa fa-check"></i><b>7.3.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#polynomial-regression"><i class="fa fa-check"></i><b>8.1</b> Polynomial Regression</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#step-functions"><i class="fa fa-check"></i><b>8.2</b> Step Functions</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#piecewise-polynomials---splines"><i class="fa fa-check"></i><b>8.3</b> Piecewise Polynomials - Splines</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#local-regression"><i class="fa fa-check"></i><b>8.4</b> Local Regression</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#generalized-additive-models"><i class="fa fa-check"></i><b>8.5</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Tree-based Methods</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#the-basiscs-of-decision-trees"><i class="fa fa-check"></i><b>9.1</b> The Basiscs of Decision Trees</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#classification-trees"><i class="fa fa-check"></i><b>9.2</b> Classification Trees</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#bagging"><i class="fa fa-check"></i><b>9.3</b> Bagging</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#random-forests"><i class="fa fa-check"></i><b>9.4</b> Random Forests</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#boosing"><i class="fa fa-check"></i><b>9.5</b> Boosing</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Support Vector Machines</a></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#principal-components-anlsysis"><i class="fa fa-check"></i><b>11.1</b> Principal Components Anlsysis</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#clustering"><i class="fa fa-check"></i><b>11.2</b> Clustering</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chapter11.html"><a href="chapter11.html#k-means-clustering"><i class="fa fa-check"></i><b>11.2.1</b> <span class="math inline">\(K\)</span>-means clustering</a></li>
<li class="chapter" data-level="11.2.2" data-path="chapter11.html"><a href="chapter11.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.2.2</b> Hierarchical Clustering</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter2" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Statistical Learning<a href="chapter2.html#chapter2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="what-is-statitical-learning" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> What is Statitical Learning<a href="chapter2.html#what-is-statitical-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>According to the literature statistical learning seems to refer
approaches that help formalizing and estimating relationships
between variables from data sets.</li>
</ul>
<p><img src="fig1/l1.png" /></p>
<ul>
<li><p>Shown are Sales vs TV, Radio and Newspaper, with a blue linear-regression line fit separately to each.</p></li>
<li><p>Can we predict Sales using these three?</p></li>
<li><p>Perhaps we can do better using a model</p></li>
</ul>
<p><span class="math display">\[
Sales \approx f(TV, Radio, Newspaper)
\]</span></p>
<ul>
<li><p>Here Sales is a response or target that we wish to predict.</p></li>
<li><p>We generically refer to the response as <span class="math inline">\(Y\)</span>.</p></li>
<li><p>TV is a feature, or input, or predictor; we name it <span class="math inline">\(X_1\)</span>.</p></li>
<li><p>Likewise name Radio as <span class="math inline">\(X_2\)</span>, and so on.</p></li>
<li><p>We can refer to the input vector collectively as</p></li>
</ul>
<p><span class="math display">\[
\mathbf{X}=\begin{pmatrix}
X_{1}\\
X_{2}\\
X_{3}\\
\end{pmatrix}
\]</span></p>
<ul>
<li>In the case of output (<span class="math inline">\(Y\)</span>) and input (<span class="math inline">\(X\)</span>) variables (supervised
learning) the situation reduces to finding a function <span class="math inline">\(f\)</span> so that we can write</li>
</ul>
<p><span class="math display">\[
Y=f(X)+\epsilon
\]</span></p>
<p>      where <span class="math inline">\(\epsilon\)</span> captures measurement errors and other discrepancies.</p>
<ul>
<li>Here <span class="math inline">\(f\)</span> is a fixed but unknown function, <span class="math inline">\(X = (X_1, X_2 , X_3)\)</span> is a set of predictors, and <span class="math inline">\(\epsilon\)</span> is a random error term, which is independent of <span class="math inline">\(x\)</span> and averages to zero.</li>
</ul>
<div id="what-is-fx-good-for" class="section level4 unnumbered hasAnchor">
<h4>What is <span class="math inline">\(f(X)\)</span> good for?<a href="chapter2.html#what-is-fx-good-for" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>With a good <span class="math inline">\(f\)</span> we can make predictions of <span class="math inline">\(Y\)</span> at new points <span class="math inline">\(X=x\)</span>.</p></li>
<li><p>We can understand which components of <span class="math inline">\(X=(X_1, X_2, \ldots, X_p)\)</span> are important in explaining <span class="math inline">\(Y\)</span>, and which are irrelevant.</p>
<ul>
<li>e.g. Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.</li>
</ul></li>
<li><p>Depending on the complexity of <span class="math inline">\(f\)</span>, we may be able to understand how each component <span class="math inline">\(X_j\)</span> of <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<p><img src="fig1/l2.png" /></p>
<ul>
<li>Is there an ideal <span class="math inline">\(f(X)\)</span>?
<ul>
<li>In particular, what is a good value for <span class="math inline">\(f(X)\)</span> at any selected value of <span class="math inline">\(X\)</span>, say <span class="math inline">\(X=4\)</span>?</li>
<li>There can be many <span class="math inline">\(Y\)</span> values at <span class="math inline">\(X=4\)</span>.</li>
</ul></li>
<li>A good value is</li>
</ul>
<p><span class="math display">\[
f(4)=E(Y|X=4)
\]</span></p>
<ul>
<li><p><span class="math inline">\(f(Y|X=4)\)</span> means <em>expected value</em> (average) of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=4\)</span>.</p></li>
<li><p>This ideal <span class="math inline">\(f(x)=E(Y|X=x)\)</span> is called the <em>regression function</em>.</p>
<ul>
<li>is also defined for vector <span class="math inline">\(X\)</span>; e.g.</li>
</ul></li>
</ul>
<p><span class="math display">\[
f(x)=f(x1,x2,x3)=E(Y|X_1=x_1, X_2=x_2, X_3=x_3)
\]</span></p>
<ul>
<li><p>The approach formalized by equation is general in statistics.</p></li>
<li><p>That is, when there is variability on <span class="math inline">\(Y\)</span>, then we can try to split the
total variability in explained or systematic component and
unexplained (or unsystematic, or random noise) component, i.e.,</p></li>
</ul>
<p><span class="math display">\[
Y = systematic + unsystematic
\]</span></p>
<ul>
<li>In equation <span class="math inline">\(f(X)\)</span> is the systematic part and <span class="math inline">\(\epsilon\)</span> is the unsystematic part.</li>
</ul>
</div>
<div id="goal" class="section level4 unnumbered hasAnchor">
<h4>Goal<a href="chapter2.html#goal" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The goal is to estimate <span class="math inline">\(f\)</span> from available data.</p></li>
<li><p>The book by James et al. actually defines statistical learning to a set of approaches for estimating <span class="math inline">\(f\)</span>.</p></li>
<li><p>Two main reasons for estimating <span class="math inline">\(f\)</span> is prediction and inference.</p></li>
</ul>
</div>
<div id="prediction" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Prediction<a href="chapter2.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Given an estimate <span class="math inline">\(\hat{f}\)</span> of <span class="math inline">\(f\)</span> and inputs <span class="math inline">\(X\)</span>, because the error term <span class="math inline">\(\epsilon\)</span> averages to zero, we can predict <span class="math inline">\(Y\)</span> as</li>
</ul>
<p><span class="math display">\[
\hat{Y}=\hat{f}(X)
\]</span></p>
<ul>
<li>In prediction context <span class="math inline">\(\hat{f}\)</span> is often treated as a black box, i.e., the
exact form of <span class="math inline">\(\hat{f}\)</span> is not of concern, provided it yields accurate
predictions for the outcome <span class="math inline">\(Y\)</span>,</li>
</ul>
<p><span class="math display">\[
input \rightarrow black box \rightarrow prediction
\]</span></p>
<div id="example-wages-and-tenure" class="section level4 unnumbered hasAnchor">
<h4>Example: wages and tenure<a href="chapter2.html#example-wages-and-tenure" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Suppose the relationship of wage and tenure (i.e., the number of years at the
current employer) is</li>
</ul>
<p><span class="math display">\[
log(wage) = 2.3 + 0.025 tenure − 0.00045 tenure2 + \epsilon
\]</span></p>
<p>      where <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span> with <span class="math inline">\(\sigma = 0.025\)</span>.</p>
<ul>
<li>Below are depicted observations from a
sample of 50 observations (left panel) and the underlying <span class="math inline">\(f\)</span> and deviations of the
observed values (right panel).</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chapter2.html#cb1-1" aria-hidden="true" tabindex="-1"></a>tenure <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">40</span> <span class="co"># years at the same employer</span></span>
<span id="cb1-2"><a href="chapter2.html#cb1-2" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="fl">2.3</span>; b1 <span class="ot">&lt;-</span> .<span class="dv">025</span>; b2 <span class="ot">&lt;-</span> <span class="sc">-</span>.<span class="dv">00045</span> <span class="co"># model parameters</span></span>
<span id="cb1-3"><a href="chapter2.html#cb1-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span> <span class="co"># sample size</span></span>
<span id="cb1-4"><a href="chapter2.html#cb1-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> .<span class="dv">025</span> <span class="co"># standard deviation of the error term</span></span>
<span id="cb1-5"><a href="chapter2.html#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="chapter2.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="at">seed =</span> <span class="dv">111</span>) <span class="co"># initialize random generator seed for exact replication of the example</span></span>
<span id="cb1-7"><a href="chapter2.html#cb1-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sample</span>(tenure, <span class="at">size =</span> n, <span class="at">replace =</span> <span class="cn">TRUE</span>); x2 <span class="ot">&lt;-</span> x<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb1-8"><a href="chapter2.html#cb1-8" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> n, <span class="at">sd =</span> sigma) <span class="co"># error term &#39;epsilon&#39;</span></span>
<span id="cb1-9"><a href="chapter2.html#cb1-9" aria-hidden="true" tabindex="-1"></a>logwage <span class="ot">&lt;-</span> b0 <span class="sc">+</span> b1 <span class="sc">*</span> x <span class="sc">+</span> b2 <span class="sc">*</span> x<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> e</span>
<span id="cb1-10"><a href="chapter2.html#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="co"># split garph window</span></span>
<span id="cb1-11"><a href="chapter2.html#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">exp</span>(logwage), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">20</span>, </span>
<span id="cb1-12"><a href="chapter2.html#cb1-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Tenure&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Wage per Hour&quot;</span>)</span>
<span id="cb1-13"><a href="chapter2.html#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="at">expr =</span> <span class="fu">exp</span>(b0 <span class="sc">+</span> b1 <span class="sc">*</span> x <span class="sc">+</span> b2 <span class="sc">*</span> x<span class="sc">^</span><span class="dv">2</span>), <span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">40</span>,</span>
<span id="cb1-14"><a href="chapter2.html#cb1-14" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">9</span>, <span class="dv">15</span>), </span>
<span id="cb1-15"><a href="chapter2.html#cb1-15" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab =</span> <span class="st">&quot;Tenure&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Wage per Hour&quot;</span>) <span class="co"># plot the underlying relation</span></span>
<span id="cb1-16"><a href="chapter2.html#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">exp</span>(logwage), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">20</span>) <span class="co"># add observations</span></span>
<span id="cb1-17"><a href="chapter2.html#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> x, <span class="at">y0 =</span> <span class="fu">exp</span>(logwage), <span class="at">x1 =</span> x, <span class="at">y1 =</span> <span class="fu">exp</span>(b0 <span class="sc">+</span> b1 <span class="sc">*</span> x <span class="sc">+</span> b2 <span class="sc">*</span> x<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb1-18"><a href="chapter2.html#cb1-18" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="co"># connect observations and the underlying relation</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>      in which the first component is due to the estimation and the
second term due to the random error that by definition cannot be
predicted by <span class="math inline">\(X\)</span>.</p>
<ul>
<li><p>The <em>ideal</em> or <em>optimal</em> predictor of <span class="math inline">\(Y\)</span> with regard to mean-squared prediction error: <span class="math inline">\(f(x)=E(Y|X=x)\)</span> is the function that minimizes <span class="math inline">\(E[(Y-f(X))^2|X=x]\)</span> over all functions <span class="math inline">\(f\)</span> at all points <span class="math inline">\(X=x\)</span>.</p></li>
<li><p>The estimation error <span class="math inline">\(f(x)-\hat{f}(x)\)</span> can be potentially reduced by
using the most appropriate statistical technique to estimate <span class="math inline">\(f\)</span>.</p>
<ul>
<li>Therefore, this error is called <em>reducible error</em>.</li>
</ul></li>
<li><p>Even with a perfect estimate <span class="math inline">\(\hat{f}=f\)</span>, <span class="math inline">\(\hat{y}\)</span> deviates from <span class="math inline">\(y\)</span> because of <span class="math inline">\(\epsilon\)</span>.</p>
<ul>
<li>This error, <span class="math inline">\(\epsilon=Y-f(x)\)</span>, is called <em>irreducible error</em>.</li>
<li>I.e. even if we knew <span class="math inline">\(f(x)\)</span>, we would still make errors in prediction, since at each <span class="math inline">\(X=x\)</span> there is typically a distribution of possible <span class="math inline">\(Y\)</span> values.</li>
</ul></li>
<li><p>Given an estimate <span class="math inline">\(\hat{f}\)</span> of <span class="math inline">\(f\)</span>, we can decompose the prediction error <span class="math inline">\(Y − \hat{Y}\)</span> as</p></li>
</ul>
<p><span class="math display">\[
Y-\hat{Y}=(f(X)-\hat{f}(X))+\epsilon
\]</span></p>
<ul>
<li>Assuming for a moment that both <span class="math inline">\(\hat{f}\)</span> and <span class="math inline">\(X\)</span> are fixed, then</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
E(Y−\hat{Y}|X=x)^2 &amp;= E[f(x) + \epsilon− \hat{f}(x)]^2 \\
&amp;= \underbrace{[f(x) − \hat{f}(x)]2}_{Reducible}+\underbrace{var(\epsilon)}_{Irreducible}
\end{aligned}
\]</span></p>
<ul>
<li>Here, <span class="math inline">\(E(Y − \hat{Y})^2\)</span> is the expected squared prediction error (difference of
actual value and its prediction), and <span class="math inline">\(var(\epsilon)\)</span> is the variance of the error
term.</li>
</ul>
</div>
<div id="remark" class="section level4 unnumbered hasAnchor">
<h4>Remark<a href="chapter2.html#remark" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Irreducible error will always provide the upper bound on the accuracy of a
prediction for <span class="math inline">\(Y\)</span>, or equivalently, it is the lower bound for prediction error, i.e.,</li>
</ul>
<p><span class="math display">\[
E(y−\hat{y})^2 \ge var(\epsilon)
\]</span></p>
</div>
</div>
<div id="inference" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Inference<a href="chapter2.html#inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Inference problems are related to understanding the way <span class="math inline">\(y\)</span> changes
as a function of <span class="math inline">\(\mathbf{X} = (X_1, \ldots , X_p)\)</span>, i.e., inference relies on
model based approaches.</p></li>
<li><p>As a results, unlike in prediction, <span class="math inline">\(f\)</span> cannot be considered any more
as a black box.</p></li>
<li><p>Rather, the explicit form is of primary interest to find out</p>
<ul>
<li>predictors that are associated with the response,</li>
<li>the particular relationship between the response with each
predictor,</li>
<li>the functional form of <span class="math inline">\(f\)</span> (linear or more complicated).</li>
</ul></li>
<li><p>Of course the interest can be a combination of these inferential an
prediction purposes.</p></li>
</ul>
<div id="example-advertising-data-set" class="section level4 unnumbered hasAnchor">
<h4>Example: Advertising data set<a href="chapter2.html#example-advertising-data-set" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Below are plots of sales and budgets for TV, radio, and newspaper advertising.</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="chapter2.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ISLR2&quot;</span>) <span class="co"># ISLR package (use install.package if needed to install the package)</span></span>
<span id="cb2-2"><a href="chapter2.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="at">help =</span> ISLR2) <span class="co"># base infor about the library</span></span>
<span id="cb2-3"><a href="chapter2.html#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="chapter2.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="do">## as the Advertising data set does not seem to be included into the library, download it over internet</span></span>
<span id="cb2-5"><a href="chapter2.html#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="chapter2.html#cb2-6" aria-hidden="true" tabindex="-1"></a>adv <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;https://www.statlearning.com/s/Advertising.csv&quot;</span>) <span class="co"># read over internet</span></span>
<span id="cb2-7"><a href="chapter2.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(adv) <span class="co"># first few lines</span></span></code></pre></div>
<pre><code>##   X    TV radio newspaper sales
## 1 1 230.1  37.8      69.2  22.1
## 2 2  44.5  39.3      45.1  10.4
## 3 3  17.2  45.9      69.3   9.3
## 4 4 151.5  41.3      58.5  18.5
## 5 5 180.8  10.8      58.4  12.9
## 6 6   8.7  48.9      75.0   7.2</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="chapter2.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(adv) <span class="co"># key summary statistics</span></span></code></pre></div>
<pre><code>##        X                TV             radio          newspaper     
##  Min.   :  1.00   Min.   :  0.70   Min.   : 0.000   Min.   :  0.30  
##  1st Qu.: 50.75   1st Qu.: 74.38   1st Qu.: 9.975   1st Qu.: 12.75  
##  Median :100.50   Median :149.75   Median :22.900   Median : 25.75  
##  Mean   :100.50   Mean   :147.04   Mean   :23.264   Mean   : 30.55  
##  3rd Qu.:150.25   3rd Qu.:218.82   3rd Qu.:36.525   3rd Qu.: 45.10  
##  Max.   :200.00   Max.   :296.40   Max.   :49.600   Max.   :114.00  
##      sales      
##  Min.   : 1.60  
##  1st Qu.:10.38  
##  Median :12.90  
##  Mean   :14.02  
##  3rd Qu.:17.40  
##  Max.   :27.00</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="chapter2.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb6-2"><a href="chapter2.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="do">## scatter plots</span></span>
<span id="cb6-3"><a href="chapter2.html#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb6-4"><a href="chapter2.html#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> adv<span class="sc">$</span>TV, <span class="at">y =</span> adv<span class="sc">$</span>sales, <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">xlab =</span> <span class="st">&quot;TV budget (1,000 USD)&quot;</span>,</span>
<span id="cb6-5"><a href="chapter2.html#cb6-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">ylab =</span> <span class="st">&quot;Sales (1,000 units)&quot;</span>, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>) <span class="co"># sales and TV advertising</span></span>
<span id="cb6-6"><a href="chapter2.html#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(sales <span class="sc">~</span> TV, <span class="at">data =</span> adv), <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>) <span class="co"># OLS-line</span></span>
<span id="cb6-7"><a href="chapter2.html#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> adv<span class="sc">$</span>radio, <span class="at">y =</span> adv<span class="sc">$</span>sales, <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">xlab =</span> <span class="st">&quot;Radio budget (1,000 USD)&quot;</span>,</span>
<span id="cb6-8"><a href="chapter2.html#cb6-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Sales (1,000 units)&quot;</span>, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>) <span class="co"># sales and radio advertising</span></span>
<span id="cb6-9"><a href="chapter2.html#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(sales <span class="sc">~</span> radio, <span class="at">data =</span> adv), <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>) <span class="co"># OLS-line</span></span>
<span id="cb6-10"><a href="chapter2.html#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> adv<span class="sc">$</span>newspaper, <span class="at">y =</span> adv<span class="sc">$</span>sales, <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">xlab =</span> <span class="st">&quot;Newspaper budget (1,000 USD)&quot;</span>,</span>
<span id="cb6-11"><a href="chapter2.html#cb6-11" aria-hidden="true" tabindex="-1"></a>         <span class="at">ylab =</span> <span class="st">&quot;Sales (1,000 units)&quot;</span>, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>) <span class="co"># sales and TV advertising</span></span>
<span id="cb6-12"><a href="chapter2.html#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(sales <span class="sc">~</span> newspaper, <span class="at">data =</span> adv), <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>) <span class="co"># OLS-line</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="questions" class="section level4 unnumbered hasAnchor">
<h4>Questions:<a href="chapter2.html#questions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Which media contributes the sales?</p></li>
<li><p>Which media generates the biggest boost in sale?</p></li>
<li><p>How much increase in sales is associated with a given increase in
TV advertising?</p></li>
<li><p>These are examples of inference type questions.</p></li>
</ul>
</div>
</div>
<div id="estimating-f" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Estimating <span class="math inline">\(f\)</span><a href="chapter2.html#estimating-f" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="how-to-estimate-f" class="section level4 unnumbered hasAnchor">
<h4>How to estimate <span class="math inline">\(f\)</span><a href="chapter2.html#how-to-estimate-f" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Typically we have few if any data points with <span class="math inline">\(X=4\)</span> exactly.</p></li>
<li><p>So we cannot compute <span class="math inline">\(E(Y|X=x)\)</span>!</p></li>
<li><p>Relax the definition and let</p></li>
</ul>
<p><span class="math display">\[
\hat{f}(x)=Ave(Y|X\in N(x))
\]</span></p>
<p>      where <span class="math inline">\(N(x)\)</span> is some <em>neighborhood</em> of <span class="math inline">\(x\)</span>.</p>
<p><img src="fig1/l3.png" /></p>
<ul>
<li><p>Nearest neighbor averaging can be pretty good for small <span class="math inline">\(p\)</span></p>
<ul>
<li>i.e. <span class="math inline">\(p \le 4\)</span> and large-ish <span class="math inline">\(N\)</span>.</li>
</ul></li>
<li><p>We will discuss smoother versions, such as kernel and spline smoothing later in the course.</p></li>
<li><p>Nearest neighbor methods can be <em>lousy</em> when <span class="math inline">\(p\)</span> is large.</p>
<ul>
<li>Reason: the *course of dimensionality$. Nearest neighbors tend to be far away in high dimensions.</li>
<li>We need to get a reasonable fraction of the <span class="math inline">\(N\)</span> values of <span class="math inline">\(y_i\)</span> to average to bring the variance down - e.g. <span class="math inline">\(10\%\)</span>.</li>
<li>A <span class="math inline">\(10\%\)</span> neighborhood in high dimensions need no longer be local, so we lose the spirit of estimating <span class="math inline">\(E(Y|X=x)\)</span> by local averaging.</li>
</ul></li>
</ul>
<p><img src="fig1/l4.png" /></p>
<ul>
<li>Broad categories of estimation methods are: <strong>parametric</strong> and
<strong>non-parametric</strong>.</li>
</ul>
</div>
<div id="parametric-methods" class="section level4 unnumbered hasAnchor">
<h4>Parametric methods<a href="chapter2.html#parametric-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The <em>linear</em> model is an important example of a parametric model:</li>
</ul>
<p><span class="math display">\[
f_L(X)=\beta_0 +\beta_1X_1 +\beta_2X_2+\cdots +\beta_pX_p
\]</span></p>
<ul>
<li><p>A linear model is specified in terms of <span class="math inline">\(p+1\)</span> parameters <span class="math inline">\(\beta_0, \beta_1, \ldots , \beta_p\)</span>.</p></li>
<li><p>We ’estimate the parameters by fitting the mod’el to training data.</p></li>
<li><p>Although it is <em>almost never correct</em>, a linear model often serves as a good and interpretable approximation to the unknown true function <span class="math inline">\(f(X)\)</span>.</p></li>
<li><p>A linear model <span class="math inline">\(\hat{f}_L(X)=\hat{\beta}_0+\hat{\beta}_1X\)</span> gives a reasonable fit here</p></li>
</ul>
<p><img src="fig1/l5.png" /></p>
<ul>
<li>A quadratic model <span class="math inline">\(\hat{f}_Q(X)=\hat{\beta}_0+\hat{\beta}_1X + \hat{\beta_2}X^2\)</span> fits slightly better.</li>
</ul>
<p><img src="fig1/l6.png" /></p>
</div>
<div id="simulated-example" class="section level4 unnumbered hasAnchor">
<h4>Simulated example<a href="chapter2.html#simulated-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig1/l7.png" /></p>
<ul>
<li>Red points are simulated values for <em>income</em> from the model</li>
</ul>
<p><span class="math display">\[
income=f(education, seniority)+\epsilon
\]</span></p>
<p>      <span class="math inline">\(f\)</span> is the blue survace.</p>
<p><img src="fig1/l8.png" /></p>
<ul>
<li>Linear regression model fit to the simulated data.</li>
</ul>
<p><span class="math display">\[
\hat{f}_L(education, seniority)=\hat{\beta}_0+\hat{\beta}_1\times education+\hat{\beta}_2\times seniority
\]</span></p>
<p><img src="fig1/l9.png" /></p>
<ul>
<li><p>More flexible regression model <span class="math inline">\(\hat{f}_s(education, seniority)\)</span> fit to the simulated data.</p></li>
<li><p>Here we use a technique called a <em>thin-plate spline</em> to fit a flexible surface.</p></li>
<li><p>We control the roughness of the fit (Chapter 7).</p></li>
</ul>
<p><img src="fig1/l10.png" /></p>
<ul>
<li><p>Even more flexible spline regression model <span class="math inline">\(\hat{f}_S(education, seniority)\)</span> fit to the simulated data.</p></li>
<li><p>Here the fitted model makes no errors on the training data!</p></li>
<li><p>Also known as <strong>overfitting</strong>.</p></li>
</ul>
</div>
<div id="back-to-the-parametric-model" class="section level4 unnumbered hasAnchor">
<h4>Back to the parametric model<a href="chapter2.html#back-to-the-parametric-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Given <span class="math inline">\(n\)</span> observations on <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, <span class="math inline">\((x_1, y_1), \ldots ,(x_n, y_n)\)</span>, where
<span class="math inline">\(x_i = (x_{i1}, x_{i2}, \ldots , x_{ip})\)</span>, <span class="math inline">\(i = 1, \ldots , n\)</span> (the <span class="math inline">\(i\)</span>th observation).</p></li>
<li><p><span class="math inline">\(x_{ij}\)</span> denotes the value for the <span class="math inline">\(i\)</span>th observation and the <span class="math inline">\(j\)</span>th variable, (<span class="math inline">\(i = 1, \ldots , n, j = 1, \ldots , p\)</span>).</p></li>
<li><p>Training data consist of <span class="math inline">\(n\)</span> observations that is used to teach or
train how to estimate the unknown function <span class="math inline">\(f\)</span>, i.e., to find a
function <span class="math inline">\(\hat{f}\)</span>, such that <span class="math inline">\(y \approx \hat{f}(x)\)</span> for any observation <span class="math inline">\((x, y)\)</span>.</p></li>
</ul>
<p>Generally parametric methods involve two steps:</p>
<ol style="list-style-type: decimal">
<li>Specification: Make assumption to specify the functional form
of <span class="math inline">\(f\)</span>, e.g. linear in <span class="math inline">\(x\)</span>, so that</li>
</ol>
<p><span class="math display">\[
f(x) = \beta_0 + \beta_1x_1 + \cdots + \beta_px_p
\]</span></p>
<p>      where the intercept <span class="math inline">\(\beta_0\)</span> and the slope coefficients <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j = 1, \ldots , p\)</span> are unknown <strong>parameters</strong> of the model.</p>
<ol start="2" style="list-style-type: decimal">
<li>Estimation: After the model has been specified, the training
data is used to <strong>fit</strong> or <strong>train</strong> the model.</li>
</ol>
<ul>
<li><p>In the above linear case
this involves <strong>estimating</strong> the unknown <span class="math inline">\(p + 1\)</span> parameters
<span class="math inline">\(\beta_0, \beta_1, \ldots , \beta_p\)</span> on the basis of sample observations (training
data).</p></li>
<li><p>The most common method here is the <strong>ordinary least
squares</strong> (OLS).</p></li>
<li><p>Assuming parametric model for <span class="math inline">\(f\)</span> simplifies estimation of <span class="math inline">\(f\)</span> to
estimation of a set of parameters, such as the above <span class="math inline">\(β\)</span>-coefficients.</p></li>
<li><p>Parametric model are fairly <strong>restrictive</strong>; Goodness of the parametric
model depends how well the parametric specification matches the
true underlying <span class="math inline">\(f\)</span>.</p>
<ul>
<li>A poor match results to a poor fit.</li>
</ul></li>
<li><p>Flexiple models can fit different functional forms of <span class="math inline">\(f\)</span>.</p>
<ul>
<li>Flexible
models may result easily to <strong>overfitting</strong>, which means they start to
follow also the patterns of the noise component in the training
data set.</li>
</ul></li>
</ul>
</div>
<div id="non-parametric-methods" class="section level4 unnumbered hasAnchor">
<h4>Non-parametric methods<a href="chapter2.html#non-parametric-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Non-parametric methods do not make explicit assumptions about
the functional form of <span class="math inline">\(f\)</span>.</p></li>
<li><p>They seek to an estimate of <span class="math inline">\(f\)</span> that gets as close to the data points
as possible without being too rough or too wiggly.</p></li>
<li><p>These methods avoid the mis-specification problem present in the
parametric approach.</p></li>
<li><p>Non-parametric approaches need typically a large number of
observation as they virtually do not make any assumptions of the
functional form of <span class="math inline">\(f\)</span>.</p></li>
</ul>
</div>
</div>
<div id="prediction-accuaracy-and-model-interpretability" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Prediction Accuaracy and Model Interpretability<a href="chapter2.html#prediction-accuaracy-and-model-interpretability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The more functional forms of <span class="math inline">\(f\)</span> a method allows, the more flexible
it is. The cost is that highly flexible models are harder to
interpret.</p></li>
<li><p>For example, linear regression is fairly inflexible but quite
interpretative, while for example different spline methods or full
non-linear methods like support vector machines can be highly
flexible but hard to interpret.</p></li>
<li><p>If the goal is inference, simple and relatively inflexiple approaches
are preferred.</p></li>
<li><p>If the goal is (purely) prediction, more flexible approaches can lead
to better predictions.</p>
<ul>
<li>This, however, is not always the case!</li>
</ul></li>
</ul>
<div id="in-summary" class="section level4 unnumbered hasAnchor">
<h4>In Summary<a href="chapter2.html#in-summary" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Prediction accuracy versus interpretability
<ul>
<li>Linear models are easy to interpret; thin-plate splines are not.</li>
</ul></li>
<li>Good fit versus over-fit or under-fit
<ul>
<li>How do we know when the fit is just right?</li>
</ul></li>
<li>Parsimony versus black-box
<ul>
<li>We often prefer a simpler model involving fewer variables over a black-box predictor involving them all.</li>
</ul></li>
</ul>
<p><img src="fig1/l11.png" /></p>
</div>
</div>
</div>
<div id="assessing-model-accuracy" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Assessing Model Accuracy<a href="chapter2.html#assessing-model-accuracy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="question" class="section level4 unnumbered hasAnchor">
<h4>Question:<a href="chapter2.html#question" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Why so many statistical methods?</p>
</div>
<div id="answer" class="section level4 unnumbered hasAnchor">
<h4>Answer:<a href="chapter2.html#answer" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Because no one dominates the others over all possible
data sets.</p>
</div>
<div id="measuring-the-quality-of-fit" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Measuring the Quality of Fit<a href="chapter2.html#measuring-the-quality-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Suppose we fit a model <span class="math inline">\(\hat{f}(x)\)</span> to some training data <span class="math inline">\(Tr=\{x_i,y_i\}_1^N\)</span>, and we wish to see how well it performs.</p></li>
<li><p>In regression setting one of the most commonly used measured of
model performance measures is the <strong>mean square error</strong> (MSE),</p></li>
<li><p>We could compute the average squared prediction error over <span class="math inline">\(Tr\)</span>:</p></li>
</ul>
<p><span class="math display">\[
MSE_{Tr}=Ave_{i\in Tr}(y_i-\hat{f}(x_i))^2=\frac{1}{N}\sum_{i=1}^n(y_i-\hat{f}(x_i))^2
\]</span></p>
<p>      This may be biased toward overfit models.</p>
<ul>
<li>Instead we should, if possible, comput it using fresh <em>test</em> data <span class="math inline">\(Te=\{x_i,y_i\}_1^M\)</span>;</li>
</ul>
<p><span class="math display">\[
MSE_{Te}=Ave_{i\in Te}(y_i-\hat{f}(x_i))^2=\frac{1}{M}\sum_{i=1}^M(y_i-\hat{f}(x_i))^2
\]</span></p>
<p>      where <span class="math inline">\(\hat{f}(x_i)\)</span> is the prediction of the <span class="math inline">\(i\)</span>th observation.</p>
<ul>
<li><p><strong>Training MSE</strong>: computed from the training sample (overstates the
accuracy of <span class="math inline">\(\hat{f}\)</span>).</p></li>
<li><p><strong>Test MSE</strong>: computed form the test sample (out of sample) (gives realistic view of the accuracy of <span class="math inline">\(\hat{f}\)</span>).</p></li>
</ul>
<p><img src="fig1/fig1_6.jpg" /></p>
<p><strong>Figure: Model accuracy and quality of fit</strong>.</p>
<ul>
<li><p>Left: underlying true <span class="math inline">\(f\)</span> (black line), observations (open dots), linear regression (orange), and
two spline fits (blue and green), observations.</p></li>
<li><p>Right: Training MSE (grey), test MSE (red), Square represent the three training and test MSEs
for the fits in the left-hand panel.</p></li>
</ul>
<p><img src="fig1/l12.png" /></p>
<ul>
<li>Here the truth is smoother, so the smoother fit and linear model to really well.</li>
</ul>
<p><img src="fig1/l13.png" /></p>
<ul>
<li>Here the truth is wiggly and the noise is low, so the more flexible fits do the best.</li>
</ul>
<p><strong>If test data is not available, <em>cross-validation</em> and <em>bootstrapping</em> methods can be used to estimate MSE from the training data.</strong></p>
</div>
<div id="bias-variance-trade-off" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Bias-Variance Trade-off<a href="chapter2.html#bias-variance-trade-off" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Suppose we have fit a model <span class="math inline">\(\hat{f}(x)\)</span> to some training data <span class="math inline">\(Tr\)</span>, and let <span class="math inline">\((x_0,y_0)\)</span> be a test observation dwawn from the population.</p></li>
<li><p>If the true model is <span class="math inline">\(Y=f(x)+\epsilon\)</span> (with <span class="math inline">\(f(x)=E(Y|X=x)\)</span>).</p></li>
<li><p>Given test observation <span class="math inline">\((y_0, x_0)\)</span>, the <em>expected test MSE</em>, <span class="math inline">\(E(y_0 − \hat{f}(x_0))^2\)</span> decomposes as</p></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
E(y_0-\hat{f}(x_0))^2 &amp;=\underbrace{E(\hat{f}(x_0)-E[\hat{f}(x_0)]^2}_{varience}+\underbrace{[E(\hat{f}(x_0)-f(x_0))]^2}_{Bias}+\underbrace{E(ε^2)}_{Error\ variance}\\
&amp;= var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+var(\epsilon)
\end{aligned}
\]</span></p>
<ul>
<li><p>The variance, <span class="math inline">\(var(\hat{f}(x_0))\)</span>, refers to the variation that would be the
result of estimating <span class="math inline">\(f\)</span> over and over again form different training
samples (repeated sampling principle).</p></li>
<li><p>The bias, <span class="math inline">\(Bias(\hat{f}(x_0))\)</span>, refers to the average deviation of <span class="math inline">\(\hat{f}(x_0)\)</span>
from <span class="math inline">\(f(x_0)\)</span> in these repeated samples.</p>
<ul>
<li>I.e., the average approximation error <span class="math inline">\(\hat{f}(x_0) − f(x_0)\)</span>.</li>
</ul></li>
<li><p>Flexible methods tend to have higher variance and lower bias, while
non-flexible methods tend to have higher bias and lower variance.</p></li>
<li><p>Thus, in practical application we need to balance with this
<em>bias-variance trade-off</em> and try to find a solution that minimizes the
model based total variability</p></li>
</ul>
<p><span class="math display">\[
var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2
\]</span></p>
<ul>
<li><p>Note that the lower bound of MSE is <span class="math inline">\(var(\epsilon)\)</span>.</p></li>
<li><p>Bias-variance trade-off for the three examples</p></li>
</ul>
<p><img src="fig1/l14.png" /></p>
</div>
<div id="classification-problems" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Classification Problems<a href="chapter2.html#classification-problems" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Here the response variable <span class="math inline">\(Y\)</span> is qualitative - e.g. email is one of <span class="math inline">\(C=(spam, ham)\)</span> (ham=good email), digit class is one of <span class="math inline">\(C=\{0,1,\ldots,9\}\)</span>.</li>
</ul>
<p>-Goals:
- Build a classifier <span class="math inline">\(C(X)\)</span> that assigns a class label from <span class="math inline">\(C\)</span> to a future unlabeled observation <span class="math inline">\(X\)</span>.
- Assess the uncertainty in each classification.
- Understand the roles of the differenet predictors among $X=(X_1,X_2, , X_p).</p>
<p><img src="fig1/l15.png" /></p>
<ul>
<li><p>Is there an ideal <span class="math inline">\(C(X)\)</span>?</p></li>
<li><p>Suppose the <span class="math inline">\(K\)</span> elements in <span class="math inline">\(C\)</span> are numbered <span class="math inline">\(1,2, \ldots,K\)</span>.</p></li>
<li><p>Let</p></li>
</ul>
<p><span class="math display">\[
p_k(x)=Pr(Y=k|X=x), \,\,\, k=1,2,\ldots,K
\]</span></p>
<ul>
<li><p>These are the conditional class probabilities at <span class="math inline">\(x\)</span>; e.g. see little barplot at <span class="math inline">\(x=5\)</span>.</p></li>
<li><p>Then the <em>Bayes optimal</em> classifier at <span class="math inline">\(x\)</span> is</p></li>
</ul>
<p><span class="math display">\[
C(x)=j \,\,\, if \,\,\, p_j(x)=max\{p_1(x),p_2(x),\ldots,p_K(x)\}
\]</span></p>
<p><img src="fig1/l16.png" /></p>
<ul>
<li><p>Nearest-neighbor averaging can be used as before.</p></li>
<li><p>Also breaks down as dimension grows.However, the impact on <span class="math inline">\(\hat{C}\)</span> is less than on <span class="math inline">\(\hat{p}_k(x)\)</span>, <span class="math inline">\(k=1,\ldots,K\)</span>.</p></li>
</ul>
<div id="classification-setting" class="section level4 unnumbered hasAnchor">
<h4>Classification setting<a href="chapter2.html#classification-setting" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>In classification <span class="math inline">\(y_i\)</span> is qualitative (indicating the class label), so to transfer the
above bias-variance trade-off need some modification.</p></li>
<li><p>One of the most common approach for quantifying the accuracy of <span class="math inline">\(\hat{C}(x)\)</span> is the
training <em>misclassification error rate</em>, computed as</p></li>
</ul>
<p><span class="math display">\[
Err_{tr}=Ave_{i\in Tr}I(y_i\ne \hat{C}(x_i))=\frac{1}{N}\sum_{i=1}^NI(y_i \ne \hat{y}_i)
\]</span></p>
<p>     
in which <span class="math inline">\(I(y_i \ne \hat{y}_i)\)</span> is an <em>indicator variable</em> equaling one if <span class="math inline">\(y_i \ne \hat{y}_i\)</span> (i.e., <span class="math inline">\(y_i\)</span> predicts
an incorrect class label) and zero if <span class="math inline">\(y_i = \hat{y}_i\)</span> (i.e., predicts a correct class label).</p>
<ul>
<li><p>As a results <span class="math inline">\(I(y_i = \hat{y}_i) = 0\)</span> indicates correct classification of the <span class="math inline">\(i\)</span>th observation; otherwise it is misclassified.</p></li>
<li><p>The <em>test error</em> with a test data <span class="math inline">\(y_0\)</span> is</p></li>
</ul>
<p><span class="math display">\[
Err_{te}=Ave_{i\in Te}I(y_i\ne \hat{C}(x_i))=\frac{1}{M}\sum_{i=1}^MI(y_i \ne \hat{y}_i)
\]</span></p>
<ul>
<li><p>A good classifier is one for which the test error rate is smallest.</p></li>
<li><p><em>The Bayes classifier</em> (using the true <span class="math inline">\(p_k(x)\)</span>) has smallest error (in the population).</p>
<ul>
<li>It turns out that it is minimized by a classifier that <em>assigns each
observation to the most likely class, given its predictor values</em>.</li>
<li>That is, assign a test observation with predictor <span class="math inline">\(x_0\)</span> to the class <span class="math inline">\(j\)</span> for which</li>
</ul></li>
</ul>
<p><span class="math display">\[
P(Y = j|X = x_0)
\]</span></p>
<p>      is largest.</p>
<ul>
<li><p>Support-vector machines build structured models for <span class="math inline">\(C(x)\)</span>.</p></li>
<li><p>We will also build structured models for representing the <span class="math inline">\(p_k(x)\)</span>.</p>
<ul>
<li>E.g. Logistic regression, generalized additive models.</li>
</ul></li>
</ul>
</div>
<div id="example-bayes-classifier" class="section level4 unnumbered hasAnchor">
<h4>Example: Bayes classifier<a href="chapter2.html#example-bayes-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>In a two-class problem <span class="math inline">\(y\)</span> assumes two possible values, e.g. <span class="math inline">\(1\)</span> for class one
and <span class="math inline">\(2\)</span> for calss two.</p></li>
<li><p>Given predictor value <span class="math inline">\(x_0\)</span>, the Bayes rule is then to assign class one if <span class="math inline">\(P(y = 1|x_0) &gt; 0.5\)</span> and class two otherwise.</p></li>
<li><p>The borderline at which <span class="math inline">\(P(y = 1|x_0) = P(y = 0|x_0) = 0.5\)</span> is the <em>Bayes decision boundary</em>.</p></li>
<li><p>Figure below plots a simulation data set with <span class="math inline">\(x_0 = (x_1, x_2)^T\)</span>.</p>
<ul>
<li>The orange and blue circles
correspond to training observations (100 observations from each of the two class).</li>
<li>The orange shaded set reflects points where <span class="math inline">\(P(y = orange|x) &gt; 50\%\)</span>, the blue region where the probability is below <span class="math inline">\(50\%\)</span>, and the purple dashed line the Bayes decision boundary where the probability is exactly <span class="math inline">\(50\%\)</span>.</li>
</ul></li>
</ul>
<p><img src="fig1/fig1_7.jpg" /></p>
<ul>
<li><p>The Bayes classifier produces the lowest possible test error rate,
called <em>Bayes error rate</em>.</p></li>
<li><p>The largest error rate at <span class="math inline">\(x = x_0\)</span> is <span class="math inline">\(1 − max_j P(y = j|x = x_0)\)</span>.</p></li>
<li><p>The overall Bayes error rate is</p></li>
</ul>
<p><span class="math display">\[
1-E(max_{j} P(y=j|x))
\]</span></p>
<p>      where the expectation is over possible <span class="math inline">\(x\)</span>-values.</p>
<ul>
<li><p>The Bayes error rate is analogous to the irreducible error rate,
discussed earlier.</p></li>
<li><p>Unfortunately the conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> is unknown,
therefore Bayes classifier cannot be computed.</p>
<ul>
<li>Bayes classifier serves as a gold standard against which other
classifiers are compared.</li>
</ul></li>
<li><p>Because the conditional distribution of the Bayes classifier is
unknown, the best one can do is to estimate it, and classify a given
observation to the class with highest <em>estimated probability</em>.</p>
<ul>
<li>One such popular method is the <em>K-nearest neigbors</em> (KNN)
classifier.</li>
</ul></li>
</ul>
</div>
<div id="k-nearest-neighbor" class="section level4 unnumbered hasAnchor">
<h4>K-Nearest neighbor<a href="chapter2.html#k-nearest-neighbor" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Given a positive integer <span class="math inline">\(K\)</span> and test observation <span class="math inline">\(x_0\)</span>, the KNN
identifies the <span class="math inline">\(K\)</span> points in the training data that are closest to <span class="math inline">\(x_0\)</span>,
denote this set of K points by <span class="math inline">\(N_0\)</span>.</p></li>
<li><p>Next estimate the conditional probability for class <span class="math inline">\(j\)</span> as the fraction
of points in <span class="math inline">\(N_0\)</span> whose response value matches class <span class="math inline">\(j\)</span></p></li>
</ul>
<p><span class="math display">\[
P(y = j|x = x_0) = \frac{1}{K}\sum_{i\in N_0} I(y_i=j)
\]</span></p>
<ul>
<li>Finally, KNN applies Bayes rule and classifies the test observation
<span class="math inline">\(x_0\)</span> to the class with the highest probability.</li>
</ul>
</div>
<div id="example-knn" class="section level4 unnumbered hasAnchor">
<h4>Example: KNN<a href="chapter2.html#example-knn" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li>Suppose <span class="math inline">\(K = 3\)</span> and out of three observations that are closest to the observation <span class="math inline">\(x_0\)</span>, two belong to class 1 and one to class 2.</li>
</ol>
<p><img src="fig1/l17.png" /></p>
<ol start="2" style="list-style-type: decimal">
<li>The probabilities are then <span class="math inline">\(2/3\)</span> for class 1 and <span class="math inline">\(1/3\)</span> for class 2.</li>
<li>So on the basis of <span class="math inline">\(x_0\)</span> the observation is classified into class 1.</li>
</ol>
<ul>
<li><p>The major problem with KNN is the selection of <span class="math inline">\(K\)</span>, which will be
addressed later.</p></li>
<li><p>Choosing the correct level of flexibility (i.e., <span class="math inline">\(K\)</span>) is critical to the success.</p></li>
<li><p>Small <span class="math inline">\(K\)</span> implies more flexibility and a small training error (in the extreme
of <span class="math inline">\(K = 1\)</span> the training error is zero), but the test error may be high.</p></li>
<li><p>Large <span class="math inline">\(K\)</span> implies less flexibility and depending on the non-linearity of the
Bayes boundary can increase also training error as demonstrated by the
figures below.</p></li>
</ul>
<p><img src="fig1/fig1_8_1.png" /></p>
<p><img src="fig1/fig1_8_2.png" /></p>
<p>Solid curve indicates the KNN decision boundary and dashed purple line the Bayesian decision boundary.</p>
<ul>
<li>KNN training error rate</li>
</ul>
<p><img src="fig1/l18.png" /></p>
<p><br></p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ILR.pdf", "ILR.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
