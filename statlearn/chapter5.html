<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Classification | Statistical Learning</title>
  <meta name="description" content="This is a Statistical Learning" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Classification | Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Statistical Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Classification | Statistical Learning" />
  
  <meta name="twitter:description" content="This is a Statistical Learning" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2023-04-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter4.html"/>
<link rel="next" href="chapter6.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>머리말</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#understanding-data"><i class="fa fa-check"></i><b>1.1</b> Understanding Data</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="chapter1.html"><a href="chapter1.html#example-1-supervised-learning-continuous-output"><i class="fa fa-check"></i><b>1.1.1</b> Example 1 (Supervised learning: Continuous output)</a></li>
<li class="chapter" data-level="1.1.2" data-path="chapter1.html"><a href="chapter1.html#example-2-supervised-leraning-categorical-output"><i class="fa fa-check"></i><b>1.1.2</b> Example 2 (Supervised leraning: Categorical output)</a></li>
<li class="chapter" data-level="1.1.3" data-path="chapter1.html"><a href="chapter1.html#example-3-unsupervised-learning-clustering-observations"><i class="fa fa-check"></i><b>1.1.3</b> Example 3 (Unsupervised learning: Clustering observations)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#brief-history-of-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Brief History of Statistical Learning</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#notation-and-simple-matrix-algebra"><i class="fa fa-check"></i><b>1.3</b> Notation and Simple Matrix Algebra</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#what-is-statitical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statitical Learning</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chapter2.html"><a href="chapter2.html#prediction"><i class="fa fa-check"></i><b>2.1.1</b> Prediction</a></li>
<li class="chapter" data-level="2.1.2" data-path="chapter2.html"><a href="chapter2.html#inference"><i class="fa fa-check"></i><b>2.1.2</b> Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="chapter2.html"><a href="chapter2.html#estimating-f"><i class="fa fa-check"></i><b>2.1.3</b> Estimating <span class="math inline">\(f\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="chapter2.html"><a href="chapter2.html#prediction-accuaracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.4</b> Prediction Accuaracy and Model Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter2.html"><a href="chapter2.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="chapter2.html"><a href="chapter2.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="2.2.3" data-path="chapter2.html"><a href="chapter2.html#classification-problems"><i class="fa fa-check"></i><b>2.2.3</b> Classification Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#basic-commands"><i class="fa fa-check"></i><b>3.1</b> Basic Commands</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#graphics"><i class="fa fa-check"></i><b>3.2</b> Graphics</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#indexing-data"><i class="fa fa-check"></i><b>3.3</b> Indexing Data</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#loading-data"><i class="fa fa-check"></i><b>3.4</b> Loading Data</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#additional-graphical-and-numerical-summaries"><i class="fa fa-check"></i><b>3.5</b> Additional Graphical and Numerical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#simple-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Regression</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#multiple-regression"><i class="fa fa-check"></i><b>4.2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chapter4.html"><a href="chapter4.html#importance-of-predictors-statistical-significance-of-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Importance of Predictors: Statistical Significance of Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="chapter4.html"><a href="chapter4.html#selecting-important-variables"><i class="fa fa-check"></i><b>4.2.2</b> Selecting Important Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="chapter4.html"><a href="chapter4.html#model-fit"><i class="fa fa-check"></i><b>4.2.3</b> Model Fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="chapter4.html"><a href="chapter4.html#prediction-1"><i class="fa fa-check"></i><b>4.2.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#other-consideration"><i class="fa fa-check"></i><b>4.3</b> Other Consideration</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chapter4.html"><a href="chapter4.html#qualitative-predictors"><i class="fa fa-check"></i><b>4.3.1</b> Qualitative Predictors</a></li>
<li class="chapter" data-level="" data-path="chapter4.html"><a href="chapter4.html#example-8"><i class="fa fa-check"></i>Example 8</a></li>
<li class="chapter" data-level="4.3.2" data-path="chapter4.html"><a href="chapter4.html#potential-problems"><i class="fa fa-check"></i><b>4.3.2</b> Potential Problems</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#non-parametric-regressions"><i class="fa fa-check"></i><b>4.4</b> Non-parametric Regressions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#logit-and-probit-models"><i class="fa fa-check"></i><b>5.1</b> Logit and Probit Models</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#confounding"><i class="fa fa-check"></i><b>5.2</b> Confounding</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#logit-model-for-multiple-classes"><i class="fa fa-check"></i><b>5.3</b> Logit Model for Multiple Classes</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#discriminant-analysis"><i class="fa fa-check"></i><b>5.4</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chapter5.html"><a href="chapter5.html#bayes-theorem-for-classification"><i class="fa fa-check"></i><b>5.4.1</b> Bayes Theorem for Classification</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapter5.html"><a href="chapter5.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="5.4.3" data-path="chapter5.html"><a href="chapter5.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.3</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#naive-bayes"><i class="fa fa-check"></i><b>5.5</b> Naive Bayes</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#knn-classification"><i class="fa fa-check"></i><b>5.6</b> KNN Classification</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#example-10"><i class="fa fa-check"></i><b>5.7</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Resampling Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#validation-set-approach"><i class="fa fa-check"></i><b>6.1</b> Validation set approach</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.2</b> K-fold cross-validation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chapter6.html"><a href="chapter6.html#example-11"><i class="fa fa-check"></i><b>6.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#bootstrap"><i class="fa fa-check"></i><b>6.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Model Selection</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#variable-selection"><i class="fa fa-check"></i><b>7.1</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chapter7.html"><a href="chapter7.html#best-subset-selection"><i class="fa fa-check"></i><b>7.1.1</b> Best Subset Selection</a></li>
<li class="chapter" data-level="7.1.2" data-path="chapter7.html"><a href="chapter7.html#forward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.2</b> Forward Step Wise Selection</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter5" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Classification<a href="chapter5.html#chapter5" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li>In regression the response variable <span class="math inline">\(Y\)</span> is quantitative.
<ul>
<li>If <span class="math inline">\(Y\)</span> is qualitative (indicate classes), classification techniques
(classifiers) use predictors to predict in which class (category) the
object should be assigned.</li>
</ul></li>
<li>Qualitative variable take values in an unordered set <span class="math inline">\(C\)</span>, such as</li>
</ul>
<p><span class="math display">\[
eye \,\,\, color \in \{{brown,blue,green\}}
\]</span>
- Given a feature vector <span class="math inline">\(X\)</span> and a qualitative response <span class="math inline">\(Y\)</span> taking values in the set <span class="math inline">\(C\)</span>, the classification task is to build a function <span class="math inline">\(C(X)\)</span> that takes as input the feature vector <span class="math inline">\(X\)</span> and predicts its value for <span class="math inline">\(Y\)</span>; i.e. <span class="math inline">\(C(X)\in C\)</span>.</p>
<ul>
<li>Some examples:
<ul>
<li>Medical condition of a person arrived into the emergency room.<br />
</li>
<li>Spam e-mail on the basis of some key words, etc.</li>
<li>Fraudulent credit card transaction on the basis of IP address, past transactions, etc.</li>
</ul></li>
<li>Often we are more interested in estimating the <em>probabilities</em> that <span class="math inline">\(X\)</span> belongs to each category in <span class="math inline">\(C\)</span>.
<ul>
<li>For example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification fraudulent or not</li>
</ul></li>
<li>The figure below illustrates (sub sample from a simulated data set) credit card defaults (the dependent variable default is categorical (orange = Yes, blue = No)).</li>
</ul>
<p><img src="fig5/fig3_1.jpg" /></p>
<div id="can-we-use-linear-regression" class="section level4 unnumbered hasAnchor">
<h4>Can we use linear regression?<a href="chapter5.html#can-we-use-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Suppose for the default classification task that we code</li>
</ul>
<p><span class="math display">\[
Y= \begin{cases} 0\,\,\, if \,\,\, No \\
1\,\,\, if\,\,\, Yes\\
\end{cases}
\]</span></p>
<ul>
<li>Can we simply perform a linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and classify as <em>Yes</em> if <span class="math inline">\(\hat{Y}&gt;0.5?\)</span>
<ul>
<li>In this case of a binary outcome, linear regression does a good job as a classifier, and is equivalent to <em>linear discriminant analysis</em> which we discuss later.</li>
<li>Since in the population <span class="math inline">\(E(Y|X=x)=Pr(Y=1|X=x)\)</span>, we might think that regression is perfect for this task.</li>
<li>However, <em>linear regression</em> might produce probabilities less than zero or bigger than one. <em>Logistic regression</em> is more appropriate.</li>
</ul></li>
<li>For a two-class response variables with binary 0/1 coding, the conditional expectation given explanatory variables <span class="math inline">\(X\)</span> becomes</li>
</ul>
<p><span class="math display">\[
E(Y|X) = 0 · P(Y = 0|X) + 1 · P(Y = 1|X) = P(Y = 1|X)
\]</span></p>
<p>      i.e., conditional probability which by definition is between zero and one.</p>
<ul>
<li>To satisfy this restriction other than linear regression for
<span class="math inline">\(E(Y|X) = f(X)\)</span> must be adopted.
<ul>
<li>Because, here <span class="math inline">\(E(Y|X)\)</span> is probability, it is natural to select <span class="math inline">\(f\)</span> as a probability function.</li>
<li>Two most popular are <em>logit</em> (called also <em>logistic</em> ) and <em>probit</em> probability functions.</li>
</ul></li>
</ul>
<p><img src="fig5/ex2.png" /></p>
<ul>
<li>The orange marks indicate the response <span class="math inline">\(Y\)</span>, either 0 or 1.
<ul>
<li>Linear regression does not estimate <span class="math inline">\(Pr(T=1|X)\)</span> well.</li>
<li>Logistic regression seems well suited to the task.</li>
</ul></li>
<li>Now suppose we have a response variable with three possible values.
<ul>
<li>A patient presents at the emergency room, and we must classify them according to their symptoms.</li>
</ul></li>
</ul>
<p><span class="math display">\[
Y= \begin{cases} 1\,\,\, if \,\,\, stroke \\
2\,\,\, if\,\,\, drug \,\,\, overdose\\
3\,\,\, if\,\,\, epileptic \,\,\, seizure\\
\end{cases}
\]</span></p>
<ul>
<li><p>This coding suggests an ordering, and in fact implies that the difference between <em>stroke</em> and <em>drug overdose</em> is the same as between <em>drug overdose</em> and <em>epileptic seizure</em>.</p></li>
<li><p>Linear regression is not appropriate here.</p>
<ul>
<li><em>Multiclass logistic regression</em> or <em>discriminant analysis</em> are more appropriate.</li>
</ul></li>
<li><p>Examples of classifiers are:</p>
<ul>
<li>logistic regression</li>
<li>discriminant analysis</li>
<li>K-nearest neighbors</li>
<li>generalized additive models</li>
<li>trees</li>
<li>random forests</li>
<li>support vector machines.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="logit-and-probit-models" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Logit and Probit Models<a href="chapter5.html#logit-and-probit-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="simple-logistic-regression" class="section level4 unnumbered hasAnchor">
<h4>Simple logistic regression<a href="chapter5.html#simple-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>To emphasize the probabilistic nature, denote <span class="math inline">\(E(Y|X) = p(X)\)</span>.</p></li>
<li><p>Let’s write <span class="math inline">\(p(X)=Pr(Y=1|X)\)</span> for short and consider using <em>balance</em> to predict <em>default.</em></p></li>
<li><p>The logit (logistic regression) model is defined as</p></li>
</ul>
<p><span class="math display">\[
p(X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}
\]</span></p>
<p>      (<span class="math inline">\(e\approx 2.71828\)</span> is a mathematical constant [Euler’s number])</p>
<ul>
<li><p>It is easy to see that no matter what values <span class="math inline">\(\beta_0, \beta_1\)</span> or <span class="math inline">\(X\)</span> take, <span class="math inline">\(p(X)\)</span> will have values between 0 and 1.</p></li>
<li><p>A bit of rearrangement gives</p></li>
</ul>
<p><span class="math display">\[
log\big(\frac{p(X)}{1-p(X)}\big)=\beta_0 +\beta_1X
\]</span></p>
<ul>
<li>This monotone transformation is called the <em>log odds</em> or <em>logit</em> transformation of <span class="math inline">\(p(X)\)</span>. (by log we mean natural log: ln)</li>
</ul>
<p><img src="fig5/ex2.png" /></p>
<ul>
<li><p>logistic regression ensures that our estimate for <span class="math inline">\(p(X)\)</span> lies between 0 and 1.</p></li>
<li><p>We use <em>maximum likelihood</em> (ML) to estimate the parameters (<span class="math inline">\(\beta\)</span>s).</p></li>
</ul>
<p><span class="math display">\[
l(\beta_0, \beta_1)=\Pi_{i:y_i=1}p(x_i)\Pi_{i:y_0=0}(1-p(x_i))
\]</span></p>
<ul>
<li>This <em>likelihood</em> gives the probability of the observed zeros and ones in the data.
<ul>
<li>We pick <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to maximize the likelihood of the observed data.</li>
</ul></li>
<li>Most statistical packages can fit linear logistic regression models by maximum likelihood. In R we use the glm function.</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chapter5.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb1-2"><a href="chapter5.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="do">## logit model</span></span>
<span id="cb1-3"><a href="chapter5.html#cb1-3" aria-hidden="true" tabindex="-1"></a>fit_logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> balance,  <span class="at">data =</span> Default, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb1-4"><a href="chapter5.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_logit) <span class="co"># print results</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = default ~ balance, family = binomial(link = &quot;logit&quot;), 
##     data = Default)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2697  -0.1465  -0.0589  -0.0221   3.7589  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.065e+01  3.612e-01  -29.49   &lt;2e-16 ***
## balance      5.499e-03  2.204e-04   24.95   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1596.5  on 9998  degrees of freedom
## AIC: 1600.5
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p><img src="fig5/ex3.png" /></p>
<ul>
<li>What is our estimated probability of <em>default</em> for someone with a balance of $1000?</li>
</ul>
<p><span class="math display">\[
\hat{p}(X)=\frac{e^{\hat{\beta}_0+\hat{\beta}_1X}}{1+e^{\hat{\beta}_0+\hat{\beta}_1X}}=\frac{e^{-10.6513+0.0055\times 1000}}{1+e^{-10.6513+0.0055\times 1000}}=0.006
\]</span></p>
<ul>
<li>With a balance of $2000?</li>
</ul>
<p><span class="math display">\[
\hat{p}(X)=\frac{e^{\hat{\beta}_0+\hat{\beta}_1X}}{1+e^{\hat{\beta}_0+\hat{\beta}_1X}}=\frac{e^{-10.6513+0.0055\times 2000}}{1+e^{-10.6513+0.0055\times 2000}}=0.586
\]</span></p>
<ul>
<li>Lets do it again, using <em>student</em> as the predictor.</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="chapter5.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb3-2"><a href="chapter5.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="do">## logit model</span></span>
<span id="cb3-3"><a href="chapter5.html#cb3-3" aria-hidden="true" tabindex="-1"></a>fit_logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> <span class="fu">factor</span>(student),  <span class="at">data =</span> Default, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb3-4"><a href="chapter5.html#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_logit) <span class="co"># print results</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = default ~ factor(student), family = binomial(link = &quot;logit&quot;), 
##     data = Default)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.2970  -0.2970  -0.2434  -0.2434   2.6585  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        -3.50413    0.07071  -49.55  &lt; 2e-16 ***
## factor(student)Yes  0.40489    0.11502    3.52 0.000431 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 2908.7  on 9998  degrees of freedom
## AIC: 2912.7
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p><img src="fig5/ex4.png" /></p>
<p><span class="math display">\[
\hat{Pr}(default=Yes|student=Yes)==\frac{e^{-3.5041+0.4049\times 1}}{1+e^{-3.5041+0.4049\times 1}}=0.0431
\]</span></p>
<p><span class="math display">\[
\hat{Pr}(default=Yes|student=No)==\frac{e^{-3.5041+0.4049\times 0}}{1+e^{-3.5041+0.4049\times 0}}=0.0292
\]</span></p>
</div>
<div id="multiple-logistic-regression" class="section level4 unnumbered hasAnchor">
<h4>Multiple logistic regression<a href="chapter5.html#multiple-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The logit (logistic regression) model is defined as</li>
</ul>
<p><span class="math display">\[
p(X) = \frac{e^{X^T\beta}}{1+e^{X^T\beta}}
\]</span></p>
<p>      where <span class="math inline">\(X^T\beta = \beta_{0} + \beta_{1}X_{1} + \cdots + \beta_{p}X_{p}\)</span>.</p>
<ul>
<li>We can write</li>
</ul>
<p><span class="math display">\[
log \ \Bigg ({p(X)\over1 − p(X)}\Bigg) = \beta_{0} + \beta_{1}X_{1} + · · · + \beta_{p}X_{p}
\]</span></p>
</div>
<div id="probit-model" class="section level4 unnumbered hasAnchor">
<h4>Probit model<a href="chapter5.html#probit-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>In the <em>probit</em> model the adopted underlying probability model is
assumed to be normal, so that</li>
</ul>
<p><span class="math display">\[
P(Y = 1|X) =\Phi(X^Tβ)
\]</span></p>
<p>      where</p>
<p><span class="math display">\[
\Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}\ du
\]</span></p>
<p>      is the standard normal cumulative distribution function.</p>
<ul>
<li><p>Again the <span class="math inline">\(\beta\)</span>-parameters are estimated by the ML method.</p></li>
<li><p>Thus for the logit transformation</p></li>
</ul>
<p><span class="math display">\[
G(z) = \frac{e^{z}}{1 + e^z} = \frac{1}{1 + e^{-z}} = \int_{-\infty}^z \frac{e^{-u}}{(1+e^{-u})^2}du
\]</span></p>
<p>      while for probit in place of <span class="math inline">\(G(z)\)</span> is the normal cumulative
distribution function (cdf) <span class="math inline">\(\Phi(z)\)</span>.</p>
<ul>
<li>As both are probability function they assume values between zero
and one, and are pretty much similar looking.</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="chapter5.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb5-2"><a href="chapter5.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="at">expr =</span> <span class="fu">exp</span>(x) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(x)), <span class="at">from =</span> <span class="sc">-</span><span class="fl">3.5</span>, <span class="at">to =</span> <span class="fl">3.5</span>, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb5-3"><a href="chapter5.html#cb5-3" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab =</span> <span class="st">&quot;z&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Logistic probability&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Logistic Cumulative</span><span class="sc">\n</span><span class="st">Distribution&quot;</span>,</span>
<span id="cb5-4"><a href="chapter5.html#cb5-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">cex.main =</span> <span class="dv">1</span>)</span>
<span id="cb5-5"><a href="chapter5.html#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;light gray&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="co"># lower limit</span></span>
<span id="cb5-6"><a href="chapter5.html#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;light gray&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="co"># upper limit</span></span>
<span id="cb5-7"><a href="chapter5.html#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="chapter5.html#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="at">expr =</span> <span class="fu">pnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), <span class="at">from =</span> <span class="sc">-</span><span class="fl">3.5</span>, <span class="at">to =</span> <span class="fl">3.5</span>, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>,</span>
<span id="cb5-9"><a href="chapter5.html#cb5-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab =</span> <span class="st">&quot;z&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Normal probability&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Normal Cumulative</span><span class="sc">\n</span><span class="st">Distribution&quot;</span>,</span>
<span id="cb5-10"><a href="chapter5.html#cb5-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">cex.main =</span> <span class="dv">1</span>)</span>
<span id="cb5-11"><a href="chapter5.html#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;light gray&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="co"># lower limit</span></span>
<span id="cb5-12"><a href="chapter5.html#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;light gray&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="co"># upper limit</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="chapter5.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#dev.print(pdf, file = &quot;logit-probit.pdf&quot;)</span></span></code></pre></div>
<p><br></p>
</div>
<div id="example" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter5.html#example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Women attendance to labor force.</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="chapter5.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages(&quot;wooldridge&quot;, repos = &quot;https://cloud.r-project.org&quot;)</span></span>
<span id="cb7-2"><a href="chapter5.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(wooldridge) <span class="co"># make the package available</span></span>
<span id="cb7-3"><a href="chapter5.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="do">## library(help = wooldridge) # names in ht package, here we use mroz</span></span>
<span id="cb7-4"><a href="chapter5.html#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">#help(mroz) # description of the variables in the used data set</span></span>
<span id="cb7-5"><a href="chapter5.html#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="chapter5.html#cb7-6" aria-hidden="true" tabindex="-1"></a>wdf <span class="ot">&lt;-</span> mroz </span>
<span id="cb7-7"><a href="chapter5.html#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(wdf) <span class="co">#  a few first lines</span></span></code></pre></div>
<pre><code>##   inlf hours kidslt6 kidsge6 age educ   wage repwage hushrs husage huseduc
## 1    1  1610       1       0  32   12 3.3540    2.65   2708     34      12
## 2    1  1656       0       2  30   12 1.3889    2.65   2310     30       9
## 3    1  1980       1       3  35   12 4.5455    4.04   3072     40      12
## 4    1   456       0       3  34   12 1.0965    3.25   1920     53      10
## 5    1  1568       1       2  31   14 4.5918    3.60   2000     32      12
## 6    1  2032       0       0  54   12 4.7421    4.70   1040     57      11
##   huswage faminc    mtr motheduc fatheduc unem city exper  nwifeinc      lwage
## 1  4.0288  16310 0.7215       12        7  5.0    0    14 10.910060 1.21015370
## 2  8.4416  21800 0.6615        7        7 11.0    1     5 19.499981 0.32851210
## 3  3.5807  21040 0.6915       12        7  5.0    0    15 12.039910 1.51413774
## 4  3.5417   7300 0.7815        7        7  5.0    0     6  6.799996 0.09212332
## 5 10.0000  27300 0.6215       12       14  9.5    1     7 20.100058 1.52427220
## 6  6.7106  19495 0.6915       14        7  7.5    1    33  9.859054 1.55648005
##   expersq
## 1     196
## 2      25
## 3     225
## 4      36
## 5      49
## 6    1089</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="chapter5.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">## logit</span></span>
<span id="cb9-2"><a href="chapter5.html#cb9-2" aria-hidden="true" tabindex="-1"></a>fit_logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(inlf <span class="sc">~</span> huswage <span class="sc">+</span> educ <span class="sc">+</span> exper <span class="sc">+</span> <span class="fu">I</span>(exper<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> age <span class="sc">+</span> kidslt6 <span class="sc">+</span> kidsge6, <span class="at">data =</span> wdf, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb9-3"><a href="chapter5.html#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_logit) <span class="co"># print results</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = inlf ~ huswage + educ + exper + I(exper^2) + age + 
##     kidslt6 + kidsge6, family = binomial(link = &quot;logit&quot;), data = wdf)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2086  -0.8990   0.4481   0.8447   2.1953  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.592500   0.853042   0.695  0.48732    
## huswage     -0.043803   0.021312  -2.055  0.03985 *  
## educ         0.209272   0.042568   4.916 8.83e-07 ***
## exper        0.209668   0.031963   6.560 5.39e-11 ***
## I(exper^2)  -0.003181   0.001013  -3.141  0.00168 ** 
## age         -0.091342   0.014460  -6.317 2.67e-10 ***
## kidslt6     -1.431959   0.202233  -7.081 1.43e-12 ***
## kidsge6      0.047089   0.074284   0.634  0.52614    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1029.75  on 752  degrees of freedom
## Residual deviance:  805.85  on 745  degrees of freedom
## AIC: 821.85
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="chapter5.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="do">## probit</span></span>
<span id="cb11-2"><a href="chapter5.html#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="chapter5.html#cb11-3" aria-hidden="true" tabindex="-1"></a>fit_probit <span class="ot">&lt;-</span> <span class="fu">glm</span>(inlf <span class="sc">~</span> huswage <span class="sc">+</span> educ <span class="sc">+</span> exper <span class="sc">+</span> <span class="fu">I</span>(exper<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> age <span class="sc">+</span> kidslt6 <span class="sc">+</span> kidsge6, <span class="at">data =</span> wdf, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;probit&quot;</span>))</span>
<span id="cb11-4"><a href="chapter5.html#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_probit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = inlf ~ huswage + educ + exper + I(exper^2) + age + 
##     kidslt6 + kidsge6, family = binomial(link = &quot;probit&quot;), data = wdf)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2495  -0.9127   0.4288   0.8502   2.2237  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.3606454  0.5045248   0.715  0.47472    
## huswage     -0.0255438  0.0126530  -2.019  0.04351 *  
## educ         0.1248347  0.0249191   5.010 5.45e-07 ***
## exper        0.1260736  0.0187091   6.739 1.60e-11 ***
## I(exper^2)  -0.0019245  0.0005987  -3.214  0.00131 ** 
## age         -0.0547298  0.0083918  -6.522 6.95e-11 ***
## kidslt6     -0.8650224  0.1177382  -7.347 2.03e-13 ***
## kidsge6      0.0285102  0.0438420   0.650  0.51550    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1029.75  on 752  degrees of freedom
## Residual deviance:  804.69  on 745  degrees of freedom
## AIC: 820.69
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<ul>
<li><p>Qualitatively the results are similar!</p></li>
<li><p>Below are probability plots as functions of experience evaluated at the mean of other explanatory variables.</p></li>
<li><p>The predictions are virtually identical.</p></li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="chapter5.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="do">## for the purpose generate data to be utilized at plot of experience at the mean of other variables</span></span>
<span id="cb13-2"><a href="chapter5.html#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="chapter5.html#cb13-3" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">exper =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fu">min</span>(wdf<span class="sc">$</span>exper, <span class="at">na.rm =</span> <span class="cn">TRUE</span>),</span>
<span id="cb13-4"><a href="chapter5.html#cb13-4" aria-hidden="true" tabindex="-1"></a>                             <span class="at">to =</span> <span class="fu">max</span>(wdf<span class="sc">$</span>exper, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), <span class="at">length =</span> <span class="dv">100</span>),</span>
<span id="cb13-5"><a href="chapter5.html#cb13-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">huswage =</span> <span class="fu">rep</span>(<span class="fu">mean</span>(wdf<span class="sc">$</span>huswage, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), <span class="at">times =</span> <span class="dv">100</span>),</span>
<span id="cb13-6"><a href="chapter5.html#cb13-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">educ =</span> <span class="fu">rep</span>(<span class="fu">mean</span>(wdf<span class="sc">$</span>educ, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), <span class="at">times =</span> <span class="dv">100</span>),</span>
<span id="cb13-7"><a href="chapter5.html#cb13-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">age =</span> <span class="fu">rep</span>(<span class="fu">mean</span>(wdf<span class="sc">$</span>age, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), <span class="at">times =</span> <span class="dv">100</span>),</span>
<span id="cb13-8"><a href="chapter5.html#cb13-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">kidslt6 =</span> <span class="fu">rep</span>(<span class="fu">mean</span>(wdf<span class="sc">$</span>kidslt6, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), <span class="at">times =</span> <span class="dv">100</span>),</span>
<span id="cb13-9"><a href="chapter5.html#cb13-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">kidsge6 =</span> <span class="fu">rep</span>(<span class="fu">mean</span>(wdf<span class="sc">$</span>kidsge6, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), <span class="at">times =</span> <span class="dv">100</span>)) <span class="co"># dummy data set</span></span>
<span id="cb13-10"><a href="chapter5.html#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="chapter5.html#cb13-11" aria-hidden="true" tabindex="-1"></a>pred_logit <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_logit,  <span class="at">newdata =</span> xx, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="co"># logit probability predictions (see help(predict.glm))</span></span>
<span id="cb13-12"><a href="chapter5.html#cb13-12" aria-hidden="true" tabindex="-1"></a>pred_probit <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_probit, <span class="at">newdata =</span> xx, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="co"># probit probability predictions</span></span>
<span id="cb13-13"><a href="chapter5.html#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="chapter5.html#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> xx[, <span class="dv">1</span>], <span class="at">y =</span> pred_logit, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb13-15"><a href="chapter5.html#cb13-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Experience (years)&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Probability&quot;</span>,</span>
<span id="cb13-16"><a href="chapter5.html#cb13-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb13-17"><a href="chapter5.html#cb13-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Logit and Probit Predictions for</span><span class="sc">\n</span><span class="st">Probabilities of Labor Force Attentance&quot;</span>) <span class="co"># logit</span></span>
<span id="cb13-18"><a href="chapter5.html#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> xx[, <span class="dv">1</span>], <span class="at">y =</span> pred_probit, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb13-19"><a href="chapter5.html#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Logit&quot;</span>, <span class="st">&quot;Probit&quot;</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
</div>
<div id="confounding" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Confounding<a href="chapter5.html#confounding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="chapter5.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb14-2"><a href="chapter5.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="do">## logit model</span></span>
<span id="cb14-3"><a href="chapter5.html#cb14-3" aria-hidden="true" tabindex="-1"></a>fit_logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> balance <span class="sc">+</span> income<span class="sc">+</span> <span class="fu">factor</span>(student),  <span class="at">data =</span> Default, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb14-4"><a href="chapter5.html#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_logit) <span class="co"># print results</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = default ~ balance + income + factor(student), family = binomial(link = &quot;logit&quot;), 
##     data = Default)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4691  -0.1418  -0.0557  -0.0203   3.7383  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        -1.087e+01  4.923e-01 -22.080  &lt; 2e-16 ***
## balance             5.737e-03  2.319e-04  24.738  &lt; 2e-16 ***
## income              3.033e-06  8.203e-06   0.370  0.71152    
## factor(student)Yes -6.468e-01  2.363e-01  -2.738  0.00619 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.5  on 9996  degrees of freedom
## AIC: 1579.5
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p><img src="fig5/ex5.png" /></p>
<ul>
<li>Why is coefficient for <em>student</em> negative, while it was positive before?</li>
</ul>
<div id="example-of-confounding" class="section level4 unnumbered hasAnchor">
<h4>Example of confounding<a href="chapter5.html#example-of-confounding" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Credit default example.</li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="chapter5.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2) <span class="co"># load ISLR library</span></span>
<span id="cb16-2"><a href="chapter5.html#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Default) <span class="co"># A few first lines from the Default data set</span></span></code></pre></div>
<pre><code>##   default student   balance    income
## 1      No      No  729.5265 44361.625
## 2      No     Yes  817.1804 12106.135
## 3      No      No 1073.5492 31767.139
## 4      No      No  529.2506 35704.494
## 5      No      No  785.6559 38463.496
## 6      No     Yes  919.5885  7491.559</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="chapter5.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(Default)  <span class="co"># Structure of the data frame</span></span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    10000 obs. of  4 variables:
##  $ default: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ student: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 2 1 1 ...
##  $ balance: num  730 817 1074 529 786 ...
##  $ income : num  44362 12106 31767 35704 38463 ...</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="chapter5.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb20-2"><a href="chapter5.html#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Simple logit of default on student satutus</span></span>
<span id="cb20-3"><a href="chapter5.html#cb20-3" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> student, <span class="at">data =</span> Default, <span class="at">family =</span> binomial)</span>
<span id="cb20-4"><a href="chapter5.html#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit1)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = default ~ student, family = binomial, data = Default)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.2970  -0.2970  -0.2434  -0.2434   2.6585  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.50413    0.07071  -49.55  &lt; 2e-16 ***
## studentYes   0.40489    0.11502    3.52 0.000431 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 2908.7  on 9998  degrees of freedom
## AIC: 2912.7
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="chapter5.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="do">## default on student and balance</span></span>
<span id="cb22-2"><a href="chapter5.html#cb22-2" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> student <span class="sc">+</span> balance, <span class="at">data =</span> Default, <span class="at">family =</span> binomial)</span>
<span id="cb22-3"><a href="chapter5.html#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = default ~ student + balance, family = binomial, 
##     data = Default)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4578  -0.1422  -0.0559  -0.0203   3.7435  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.075e+01  3.692e-01 -29.116  &lt; 2e-16 ***
## studentYes  -7.149e-01  1.475e-01  -4.846 1.26e-06 ***
## balance      5.738e-03  2.318e-04  24.750  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.7  on 9997  degrees of freedom
## AIC: 1577.7
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p><img src="fig5/ex1.png" /></p>
<ul>
<li><p>When students are regressed alone, the sign is positive, and when balance is included, the sign is negative!</p></li>
<li><p>The reason for this seemingly contradicting result is that students tend to have higher balance than non-students. Therefore students are on average defaulting more likely.</p></li>
<li><p>However, at each level of balance, students tend to default less than non-students.</p></li>
<li><p>Due to the correlation of student and balance the effects in single (logit) regression are <em>confounded</em> and show up as positive effect due to the higher average balance for students.</p></li>
</ul>
<p><br></p>
<ul>
<li>Logit probabilities for Students and Non-Students.</li>
</ul>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="chapter5.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="co"># split widow to two areas vertically</span></span>
<span id="cb24-2"><a href="chapter5.html#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">c</span>(<span class="dv">500</span>, <span class="fu">max</span>(Default<span class="sc">$</span>balance)), <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">type =</span> <span class="st">&quot;n&quot;</span>,</span>
<span id="cb24-3"><a href="chapter5.html#cb24-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Balance&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Probability&quot;</span>)</span>
<span id="cb24-4"><a href="chapter5.html#cb24-4" aria-hidden="true" tabindex="-1"></a>bvals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">500</span>, <span class="fu">max</span>(Default<span class="sc">$</span>balance), <span class="at">length =</span> <span class="dv">200</span>) <span class="co"># range of default values</span></span>
<span id="cb24-5"><a href="chapter5.html#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> bvals,</span>
<span id="cb24-6"><a href="chapter5.html#cb24-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="fu">predict</span>(fit2, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>,</span>
<span id="cb24-7"><a href="chapter5.html#cb24-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">balance =</span> bvals, <span class="at">student =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="st">&quot;Yes&quot;</span>, <span class="fu">length</span>(bvals))))),</span>
<span id="cb24-8"><a href="chapter5.html#cb24-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>) <span class="do">## default probability curve for students</span></span>
<span id="cb24-9"><a href="chapter5.html#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> bvals,</span>
<span id="cb24-10"><a href="chapter5.html#cb24-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="fu">predict</span>(fit2, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>,</span>
<span id="cb24-11"><a href="chapter5.html#cb24-11" aria-hidden="true" tabindex="-1"></a>                  <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">balance =</span> bvals, <span class="at">student =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="st">&quot;No&quot;</span>, <span class="fu">length</span>(bvals))))),</span>
<span id="cb24-12"><a href="chapter5.html#cb24-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>) <span class="co"># non-students</span></span>
<span id="cb24-13"><a href="chapter5.html#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="fu">mean</span>(Default<span class="sc">$</span>default[Default<span class="sc">$</span>student <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>] <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>), <span class="at">b =</span> <span class="dv">0</span>,</span>
<span id="cb24-14"><a href="chapter5.html#cb24-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>) <span class="co"># average default rate for students</span></span>
<span id="cb24-15"><a href="chapter5.html#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="fu">mean</span>(Default<span class="sc">$</span>default[Default<span class="sc">$</span>student <span class="sc">==</span> <span class="st">&quot;No&quot;</span>] <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>), <span class="at">b =</span> <span class="dv">0</span>,</span>
<span id="cb24-16"><a href="chapter5.html#cb24-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="co"># average default rate for non-students</span></span>
<span id="cb24-17"><a href="chapter5.html#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;light gray&quot;</span>) <span class="co"># horizontal line at probability 0</span></span>
<span id="cb24-18"><a href="chapter5.html#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">1</span>, <span class="at">b =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;light gray&quot;</span>) <span class="co"># horizontal line at probability 1</span></span>
<span id="cb24-19"><a href="chapter5.html#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Student&quot;</span>, <span class="st">&quot;Non-strudents&quot;</span>,</span>
<span id="cb24-20"><a href="chapter5.html#cb24-20" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;Student, average default rate&quot;</span>,</span>
<span id="cb24-21"><a href="chapter5.html#cb24-21" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;Non-student, average default</span><span class="sc">\n</span><span class="st">rate&quot;</span>),</span>
<span id="cb24-22"><a href="chapter5.html#cb24-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="st">&quot;solid&quot;</span>, <span class="st">&quot;solid&quot;</span>, <span class="st">&quot;dashed&quot;</span>, <span class="st">&quot;dashed&quot;</span>),</span>
<span id="cb24-23"><a href="chapter5.html#cb24-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;orange&quot;</span>, <span class="st">&quot;steel blue&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;steel blue&quot;</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>, <span class="at">cex =</span> .<span class="dv">8</span>)</span>
<span id="cb24-24"><a href="chapter5.html#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Box plots</span></span>
<span id="cb24-25"><a href="chapter5.html#cb24-25" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> Default<span class="sc">$</span>student, <span class="at">y =</span> Default<span class="sc">$</span>balance, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;steel blue&quot;</span>, <span class="st">&quot;orange&quot;</span>),</span>
<span id="cb24-26"><a href="chapter5.html#cb24-26" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Student&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Balance&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="example-south-african-heart-disease" class="section level4 unnumbered hasAnchor">
<h4>Example: South African Heart Disease<a href="chapter5.html#example-south-african-heart-disease" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>160 cases of MI (myocardial infarction) and 302 controls (all male in age reange 15-64), from Western Cape, South Africa in early 80s.</p></li>
<li><p>Overall prevalence very high in this region: 5.1%.</p></li>
<li><p>Measurements on seven predictors (risk factors), shown in scatterplot matrix.</p></li>
<li><p>Goal is to identify relative strengths and directions of risk factors.</p></li>
<li><p>This was part of an intervention study aimed at educating the public on healthier diets.</p></li>
</ul>
<p><img src="fig5/ex6.png" /></p>
<ul>
<li>Scatterplot matrix of the South African Heart Disease data.
<ul>
<li>The response is color coded - The cases (MI) are red, the controls turquoise.</li>
<li>famhist is a binary variable, with 1 indicating family history of MI</li>
</ul></li>
</ul>
<p><img src="fig5/ex7.png" /></p>
<ul>
<li><p>In South African data, there are 160 cases, 302 controls - <span class="math inline">\(\tilde{\pi}=0.35\)</span> are cases</p>
<ul>
<li>Yet the prevalence of MI in this region is <span class="math inline">\(\pi=0.05\)</span></li>
</ul></li>
<li><p>With case-control samples, we can estimate the regression parameters <span class="math inline">\(\beta_j\)</span> accurately (if our model is correct); the constant term <span class="math inline">\(\beta_0\)</span> is incorrect.</p></li>
<li><p>We can correct the estimated intercept by a simple transformation</p></li>
</ul>
<p><span class="math display">\[
\hat{\beta}_0^* = \hat{\beta}_0 +log\frac{\pi}{1-\pi}-log\frac{\tilde{\pi}}{1-\tilde{\pi}}
\]</span></p>
<ul>
<li>Often cases are rare and we take them all; up to five times that number of controls is sufficient.</li>
</ul>
<p><img src="fig5/ex8.png" /></p>
<ul>
<li>Sampling more controls than cases reduces the variance of the parameter estimates.
<ul>
<li>But after a ratio of about 5 to 1 the variance reduction flattens out.</li>
</ul></li>
</ul>
<p><br></p>
</div>
</div>
<div id="logit-model-for-multiple-classes" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Logit Model for Multiple Classes<a href="chapter5.html#logit-model-for-multiple-classes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>The logit model has multiple-class extensions, i.e., when the
response has more than two classes.</p></li>
<li><p>It is easily generalized to more than two classes.</p></li>
<li><p>One version (used in the R package glmnet) has the symmetric form</p></li>
</ul>
<p><span class="math display">\[
Pr(Y=k|X)=\frac{e^{\beta_{ok}+\beta_{1k}X_1 +\cdots +\beta_{pk}X_p}}{\sum_{l=1}^K e^{\beta_{ol}+\beta_{1l}X_1 +\cdots +\beta_{pl}X_p}}
\]</span>
- Here there is a linear function for <em>each</em> class.</p>
<ul>
<li><p>Multiclass logistic regression is also referred to as <em>multinomial regression</em>.</p></li>
<li><p>However, seems that these extensions are not used that often.</p></li>
<li><p>Instead <em>discriminant analysis</em> is more popular in these instances.</p></li>
</ul>
<p><br></p>
</div>
<div id="discriminant-analysis" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Discriminant Analysis<a href="chapter5.html#discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Discriminant analysis is popular in particular when there are more than two classes in the response variable.</li>
<li>If the underlying populations are normal and have the same covariance matrices, <em>linear discriminant analysis</em> (LDA) can be used.</li>
<li>If the (population) covariance matrices differ between classes, <em>quadratic discriminant analysis</em> (QDA) is used.</li>
</ul>
<div id="bayes-theorem-for-classification" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Bayes Theorem for Classification<a href="chapter5.html#bayes-theorem-for-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Here the approach is to model the distribution of <span class="math inline">\(X\)</span> in each of the classes separately, and then use <em>Bayes theorem</em> to flip things around and obtain <span class="math inline">\(Pr(Y|X)\)</span>.</p></li>
<li><p>When we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.</p></li>
<li><p>However, this approach is quite general, and other distributions can be used as well.</p>
<ul>
<li>We will focus on normal distribution.</li>
</ul></li>
<li><p>Thomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling.</p></li>
<li><p>Here we focus on a simple result, known as Bayes theorem:</p></li>
</ul>
<p><span class="math display">\[
Pr(Y=k|X=x)=\frac{Pr(X=x|Y=k) \cdot Pr(Y=k)}{Pr(X=x)}
\]</span></p>
<ul>
<li><p>One writes this slightly differently for discriminant analysis.</p></li>
<li><p>Let</p>
<ul>
<li><span class="math inline">\(K ≥ 2\)</span> be the number of classes of the response variable <span class="math inline">\(Y\)</span> ,</li>
<li><span class="math inline">\(\pi_k\)</span> be the <em>prior</em> probability of class <span class="math inline">\(k = 1, \ldots ,K\)</span>,</li>
<li><span class="math inline">\(f_k (x) = P(X = x|Y = k)\)</span> denote the (conditional) <em>density function</em> of random variable <span class="math inline">\(X\)</span> in class k .</li>
<li>Then <em>Bayes theorem</em> states that</li>
</ul></li>
</ul>
<p><span class="math display">\[
P(Y = k|X = x) = \frac{\pi_k f_k (x)}{\sum_{j=1}^{K} \pi_{j}f_{j}(x)}
\]</span></p>
<ul>
<li><p>Here we will use normal densities for these, separately in each class.</p></li>
<li><p>The probability <span class="math inline">\(p(k|x) = P(Y = k|X = x)\)</span> is called the <em>posterior</em> probability that an observation with <span class="math inline">\(X = x\)</span> belongs to the <span class="math inline">\(k\)</span>th class of the response variable <span class="math inline">\(Y\)</span>.</p></li>
<li><p>The Bayes classifier that assigns the observation with <span class="math inline">\(X = x\)</span> to
class <span class="math inline">\(k\)</span> for which <span class="math inline">\(p(k|x)\)</span> is highest, has the lowest possible error rate of all classifiers.</p></li>
<li><p>The problem in practice is that <span class="math inline">\(\pi_k\)</span> s and <span class="math inline">\(f_{k} (x)\)</span>s are typically unknown.</p></li>
<li><p>In the following we deal with various approaches to estimate <span class="math inline">\(f_k(x)\)</span>s
in particular ( <span class="math inline">\(\pi_k\)</span>s can be typically estimated by the fractions of
observations in group <span class="math inline">\(k\)</span> in the training sample).</p></li>
</ul>
<p><img src="fig5/ex9.png" /></p>
<ul>
<li><p>We classify a new point according to which density is highest.</p></li>
<li><p>When priors are different, we take them into account as well, and compare <span class="math inline">\(\pi_k f_k(x)\)</span>.</p>
<ul>
<li>On the right, we favor the pink class - the decision boundary has shifted to the left.</li>
</ul></li>
<li><p>When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable.</p>
<ul>
<li>Linear discriminant analysis does not suffer from this problem.</li>
</ul></li>
<li><p>If <span class="math inline">\(n\)</span> is small and the distribution of the predictors <span class="math inline">\(X\)</span> is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.</p></li>
<li><p>Linear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.</p></li>
</ul>
<p><br></p>
</div>
<div id="linear-discriminant-analysis" class="section level3 hasAnchor" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Linear Discriminant Analysis<a href="chapter5.html#linear-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="linear-discriminant-analysis-when-p1" class="section level4 unnumbered hasAnchor">
<h4>Linear discriminant analysis when <span class="math inline">\(p=1\)</span><a href="chapter5.html#linear-discriminant-analysis-when-p1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The Gaussian density has the form</li>
</ul>
<p><span class="math display">\[
f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}e^{-\frac{1}{2}(\frac{x-\mu_k}{\sigma_k})^2}
\]</span></p>
<ul>
<li>Here <span class="math inline">\(\mu_k\)</span> is the mean, and <span class="math inline">\(\sigma_k^2\)</span> the variance (in class <span class="math inline">\(k\)</span>).
<ul>
<li>We will assume that all the <span class="math inline">\(\sigma_k=\sigma\)</span> are the same.</li>
</ul></li>
<li>Plugging this into Bayes formula, we get a rather complex expression for <span class="math inline">\(p_k(x)=Pr(Y=k|X=x)\)</span>:</li>
</ul>
<p><span class="math display">\[
p_k(x)=\frac{\pi_
k\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu_k}{\sigma})^2}}{\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu_l}{\sigma})^2}}
\]</span></p>
<ul>
<li><p>Happily, there are simplifications and cancellations.</p></li>
<li><p>To classify at the value <span class="math inline">\(X=x\)</span>, we need to see which of the <span class="math inline">\(p_k(x)\)</span> is largest.</p>
<ul>
<li>Taking logs, and discarding terms that do not depend on <span class="math inline">\(k\)</span>, we see that this is equivalent to assigning <span class="math inline">\(x\)</span> to the class with the largest <em>discriminant score</em>:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\delta_k(x)=x\cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+log(\pi_k)
\]</span></p>
<ul>
<li><p>Note that <span class="math inline">\(\delta_k(x)\)</span> is a linear function of <span class="math inline">\(x\)</span></p></li>
<li><p>If there are <span class="math inline">\(K=2\)</span> classes and <span class="math inline">\(\pi_1=\pi_2=0.5\)</span>, then one can see that the <em>decision boundary</em> is at</p></li>
</ul>
<p><span class="math display">\[
x=\frac{\mu_1+\mu_2}{2}
\]</span></p>
<ul>
<li>Example
<ul>
<li><span class="math inline">\(\mu_1=1.5, \mu_2=1.5, \pi_1=\pi_2=0.5\)</span> and <span class="math inline">\(\sigma^2=1\)</span>.</li>
</ul></li>
</ul>
<p><img src="fig5/ex10.png" /></p>
<ul>
<li>Typically we don’t know these parameters; we just have the training data.
<ul>
<li>In that case we simply estimate the parameters and plug them into the rule.</li>
</ul></li>
</ul>
<p><span class="math display">\[
\hat{\pi}_k=\frac{n_k}{n}
\]</span></p>
<p><span class="math display">\[
\hat{\mu}_k=\frac{1}{n_k}\sum_{i:y_i=k}x_i
\]</span></p>
<p><span class="math display">\[
\hat{\sigma}^2=\frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i=k}(x_i-\hat{\mu}_k)^2=\sum_{k=1}^K\frac{n_k-1}{n-K}\cdot \hat{\sigma}_k^2
\]</span></p>
<p>      where <span class="math inline">\(\hat{\sigma}_k^2=\frac{1}{n_k-1}\sum_{i:y_i=1k}(x_i-\hat{\mu}_k)^2\)</span> is the usual formula for the estimated variance in the <span class="math inline">\(k\)</span>th class.</p>
</div>
<div id="linear-discriminant-analysis-when-p1-1" class="section level4 unnumbered hasAnchor">
<h4>Linear discriminant analysis when <span class="math inline">\(p&gt;1\)</span><a href="chapter5.html#linear-discriminant-analysis-when-p1-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig5/ex11.png" /></p>
<ul>
<li>Multivariate normal density</li>
</ul>
<p><span class="math display">\[
f(x)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}{e^{1\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}}
\]</span></p>
<ul>
<li>If the random variable <span class="math inline">\(X\)</span> is multivariate <em>normal</em> or Gaussian, and
has in all groups the covariance matrices <span class="math inline">\(\Sigma\)</span> the same, so that
<span class="math inline">\(X \sim N(\mu_k, \Sigma)\)</span> in group <span class="math inline">\(k = 1, \ldots, K\)</span>, then (note that <span class="math inline">\(\pi\)</span> in the
normal density is the number <span class="math inline">\(3.14\ldots\)</span> , while <span class="math inline">\(\pi_k\)</span> s denote prior
probabilities)</li>
</ul>
<p><span class="math display">\[
p(k|x) = \frac{\pi_k\frac{1}{(2\pi)^{p/2}|Σ|^{1/2}} exp\Big(\frac{1}{2}(x-\mu_k)^T{\Sigma}^{-1}(x-\mu_k)\Big)}{\sum_{j=1}^{K}\pi_{j}f(j|x)}
\]</span></p>
<ul>
<li>Taking logarithm</li>
</ul>
<p><span class="math display">\[
log \ p(k|x) =\frac{1}{2}(x-\mu_k)^T{\Sigma}^{-1}(x-\mu_k)+log \ \pi_k + const
\]</span></p>
<p>      in which <span class="math inline">\(const = −(p/2) log(2\pi) − (1/2) log |\Sigma| − log(\sum_{j}\pi_{j}f (j|x))\)</span>
does not depend on <span class="math inline">\(k\)</span>.</p>
<ul>
<li>Dropping the constant and doing little algebra, we have a
<strong>Bayes classifier</strong> by assigning the observation with <span class="math inline">\(x\)</span> to the class for
which</li>
</ul>
<p><span class="math display">\[
\delta_k (x) = x^T\Sigma^{-1}\mu_k − \frac{1}{2}\mu^T_k\Sigma^{-1}\mu_k
+log \pi_k
\]</span></p>
<p>      is largest.</p>
<ul>
<li>The function <span class="math inline">\(\delta_k\)</span> is a linear in <span class="math inline">\(X\)</span> , therefore <span class="math inline">\(\delta_k(X)\)</span> is called a linear
discriminant function.
<ul>
<li>The Bayes decision boundaries are defined by <span class="math inline">\(\delta_k (x) = \delta_l(x)\)</span> for <span class="math inline">\(k \neq l\)</span>.</li>
</ul></li>
<li>The figure below illustrates <em>Bayes decision boundaries</em> in the case
of <span class="math inline">\(K = 3\)</span> classes, <span class="math inline">\(p = 2\)</span> variables, and <span class="math inline">\(\pi_k = 1/3\)</span>, <span class="math inline">\(k = 1, 2, 3\)</span>.</li>
</ul>
<p><img src="fig5/fig3_2.jpg" /></p>
<ul>
<li><p>Gaussian distributions in three groups with common covariance matrix. Ellipses contain 95% of
observations, dashed lines are Bayesian decision boundaries, and solid lines in the right hand panel are
estimated Bayesian decision boundaries from samples of 20 (simulated) observations from each group.</p></li>
<li><p><em>Bayes decision boundaries</em> were known, they would yield the fewest misclassification errors, among all possible classifiers.</p></li>
</ul>
</div>
<div id="fishers-iris-data" class="section level4 unnumbered hasAnchor">
<h4>Fisher’s Iris Data<a href="chapter5.html#fishers-iris-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig5/ex12.png" /></p>
<p><img src="fig5/ex13.png" /></p>
<ul>
<li><p>When there are <span class="math inline">\(K\)</span> classes, linear discriminant analysis can be viewed exactly in a <span class="math inline">\(K-1\)</span> dimensional plot.</p>
<ul>
<li>Why? Because it essentially classifies to the closest centroid, and they span a <span class="math inline">\(K-1\)</span> dimensional plane.</li>
<li>Even when <span class="math inline">\(K&gt;3\)</span>, we can find the “best” 2-dimensional plane for visualizing the discriminant rule.</li>
</ul></li>
<li><p>The unknown parameters <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(\pi_k\)</span> are estimated from the training (sample) data by sample means of <span class="math inline">\(X\)</span>-variables, <span class="math inline">\(\bar{x}_k\)</span>, common sample covariance matrix, <span class="math inline">\(S\)</span>, and <span class="math inline">\(\hat{\pi}_k = n_k /n\)</span>, the fraction of observations in group <span class="math inline">\(k\)</span>.</p></li>
<li><p>The <em>pooled sample covariance matrix</em> <span class="math inline">\(S\)</span> is computed as</p></li>
</ul>
<p><span class="math display">\[
S = \frac{1}{n}\sum_{k=1}^{K}W_k
\]</span></p>
<p>      where</p>
<p><span class="math display">\[
W_k = \sum_{i=1}^{n_k}(x_{ik} − \bar x_k)(x_{ik} − \bar x_k)^T
\]</span></p>
<p>      is a multiple of the <em>within</em> covariance matrix <span class="math inline">\(S_k = W_k /n_k\)</span>.</p>
<ul>
<li><p>Alternatively in place of <span class="math inline">\(n\)</span> is often used <span class="math inline">\(n − K\)</span> in order to make <span class="math inline">\(S\)</span>
unbiased (for the same reason in <span class="math inline">\(S_k\)</span> the divisor is replaced by
<span class="math inline">\(n_k − 1\)</span>).</p></li>
<li><p>Once we have estimates <span class="math inline">\(\hat{\delta}_k(x)\)</span>, we can turn these into estimates for class probabilities:</p></li>
</ul>
<p><span class="math display">\[
\hat{Pr}(Y=k|X=x)=\frac{e^{\hat{\delta}_k(x)}}{\sum_{l=1}^K e^{\hat{\delta}_l(x)}}
\]</span></p>
<ul>
<li><p>So classifying to the largest <span class="math inline">\(\hat{\delta}_k(x)\)</span> amounts to classifying to the class for which <span class="math inline">\(\hat{Pr}(Y=k|X=x)\)</span> is largest.</p></li>
<li><p>When <span class="math inline">\(K=2\)</span>, we classify to class 2 if <span class="math inline">\(\hat{Pr}(Y=2|X=x)\ge 0.5\)</span> else to class 1.</p></li>
<li><p>LDA on Credit data</p></li>
</ul>
<p><img src="fig5/ex14.png" /></p>
<ul>
<li><p>(24+252)/10000 errors - a 2.75/% misclassification rate!</p></li>
<li><p>Some caveats:</p>
<ul>
<li>This is training error, and we may be overfitting. Not a big concern here since <span class="math inline">\(n=10000\)</span> and <span class="math inline">\(p=1\)</span>!</li>
<li>If we classified to the prior - always to class No in this case - we would make 333/10000 errors, or only 3.33/%.</li>
<li>Of the true No’s, we make 23/96667=0.2/% errors; of the true Yes’s, we make 252/333=75.7/% errors!</li>
</ul></li>
</ul>
</div>
<div id="types-of-errors" class="section level4 unnumbered hasAnchor">
<h4>Types of errors<a href="chapter5.html#types-of-errors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>False positive rate: The fraction of negative examples that are classified as positive - 0.2/% in example.</p></li>
<li><p>False negative rate: The fraction of positive examples that are classsified as negative - 75.7/% in example.</p></li>
<li><p>We produced this table by classifying to class Yes if</p></li>
</ul>
<p><span class="math display">\[
\hat{Pr}(Default=Yes|Balance, Student)\ge 0.5
\]</span></p>
<ul>
<li>We can change the two error rates by chaning the threshold from 0.5 to some other value in [0,1];</li>
</ul>
<p><span class="math display">\[
\hat{Pr}(Default=Yes|Balance, Student)\ge threshold
\]</span></p>
<p>      and vary threshold.</p>
<p><img src="fig5/ex15.png" /></p>
<ul>
<li>In order to reduce the false negative rate, we may want to reduce the threshold to 0.1 or less</li>
</ul>
<p><img src="fig5/ex16.png" /></p>
<ul>
<li><p>The <em>ROC plot</em> displays both simultaneously.</p></li>
<li><p>Sometimes we use the <em>AUC</em> or <em>area under the curve</em> to summarize the overall performance.</p>
<ul>
<li>Higher <em>AUC</em> is good.</li>
</ul></li>
</ul>
</div>
<div id="remark-3" class="section level4 unnumbered hasAnchor">
<h4>Remark<a href="chapter5.html#remark-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>In particular in the two-group case (<span class="math inline">\(K = 2\)</span>), considering the log-odds ratio of the posterior probabilities and using the right hand side of the linear discrimanant function,</li>
</ul>
<p><span class="math display">\[
log \Big(\frac{p(k = 1|x)}{1 − p(k = 1|x)}\Big) = a + b^Tx
\]</span></p>
<p>      where</p>
<p><span class="math display">\[
a = \frac{1}{2} (\mu_2^TΣ^{−1}\mu_2 − \mu_1^TΣ^{−1}\mu_1) + log \ (\pi_1/\pi_2)
\]</span></p>
<p>      and</p>
<p><span class="math display">\[
b = Σ^{−1}(\mu_1 − \mu_2)
\]</span></p>
<p>      I.e., coincides with the linear logit-model.
- The difference is in the underlying assumptions.
- LDA relies on normal distribution with common variance, while logit-model does not.
- As a results even in this linear case logit-model can be assumed to be more flexible, while if the normal assumption with a common covariance matrix holds, LDA based classification can be expected to perform better.</p>
</div>
<div id="remark-4" class="section level4 unnumbered hasAnchor">
<h4>Remark<a href="chapter5.html#remark-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The LDA in equation can be enhanced with quadratic and higher terms as well as other transformations of <span class="math inline">\(X\)</span>-variables.
<ul>
<li>This just amounts to enhancing the <span class="math inline">\(X\)</span> vector with transformed variables.</li>
<li>However, the normality assumption of these transformed variables does not hold any more.</li>
<li>The bottom line is that in the two-group case the major difference between LDA and logit (probit) model is the estimation of the parameters (due to the different underlying assumptions).</li>
</ul></li>
</ul>
</div>
<div id="example-9" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter5.html#example-9" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Use the (artificial) Default data set in the ISLR2 library.</li>
<li>A few first lines of the total of 10,000 observations, fractions of defaulted in the sample,
defaulted according to student non-student classes, and Box plots of balance and income in
the defaulted and non-defaulted classes</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="chapter5.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2) <span class="co"># load ISLR library</span></span>
<span id="cb25-2"><a href="chapter5.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS) <span class="co"># linear discriminant analysis etc</span></span>
<span id="cb25-3"><a href="chapter5.html#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Default) <span class="co"># A few first lines from the Default data set</span></span></code></pre></div>
<pre><code>##   default student   balance    income
## 1      No      No  729.5265 44361.625
## 2      No     Yes  817.1804 12106.135
## 3      No      No 1073.5492 31767.139
## 4      No      No  529.2506 35704.494
## 5      No      No  785.6559 38463.496
## 6      No     Yes  919.5885  7491.559</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="chapter5.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(Default)  <span class="co"># Structure of the data frame</span></span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    10000 obs. of  4 variables:
##  $ default: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ student: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 2 1 1 ...
##  $ balance: num  730 817 1074 529 786 ...
##  $ income : num  44362 12106 31767 35704 38463 ...</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="chapter5.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-2"><a href="chapter5.html#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="do">## some preliminary results</span></span>
<span id="cb29-3"><a href="chapter5.html#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(Default<span class="sc">$</span>default) <span class="co"># number dealted and non-defaulted in the sample</span></span></code></pre></div>
<pre><code>## 
##   No  Yes 
## 9667  333</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="chapter5.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(Default) <span class="co"># summary statistics for the variable in the data frame</span></span></code></pre></div>
<pre><code>##  default    student       balance           income     
##  No :9667   No :7056   Min.   :   0.0   Min.   :  772  
##  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  
##                        Median : 823.6   Median :34553  
##                        Mean   : 835.4   Mean   :33517  
##                        3rd Qu.:1166.3   3rd Qu.:43808  
##                        Max.   :2654.3   Max.   :73554</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="chapter5.html#cb33-1" aria-hidden="true" tabindex="-1"></a>tb <span class="ot">&lt;-</span> <span class="fu">table</span>(Default[, <span class="fu">c</span>(<span class="st">&quot;student&quot;</span>, <span class="st">&quot;default&quot;</span>)]) <span class="co"># cross-table for students and defaulted</span></span>
<span id="cb33-2"><a href="chapter5.html#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="chapter5.html#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> tb <span class="sc">/</span> <span class="fu">c</span>(<span class="fu">sum</span>(tb[<span class="dv">1</span>, ]), <span class="fu">sum</span>(tb[<span class="dv">2</span>, ])), <span class="at">digits =</span> <span class="dv">2</span>) <span class="co"># percents rounded to two digts</span></span></code></pre></div>
<pre><code>##        default
## student    No   Yes
##     No  97.08  2.92
##     Yes 95.69  4.31</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="chapter5.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb35-2"><a href="chapter5.html#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> Default<span class="sc">$</span>default, <span class="at">y =</span> Default<span class="sc">$</span>balance, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;light green&quot;</span>, <span class="st">&quot;light blue&quot;</span>),</span>
<span id="cb35-3"><a href="chapter5.html#cb35-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Default&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Balance&quot;</span>) <span class="co"># Box plots</span></span>
<span id="cb35-4"><a href="chapter5.html#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> Default<span class="sc">$</span>default, <span class="at">y =</span> Default<span class="sc">$</span>income, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;light green&quot;</span>, <span class="st">&quot;light blue&quot;</span>),</span>
<span id="cb35-5"><a href="chapter5.html#cb35-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Default&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Income&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<ul>
<li><p>The interest is to find a classifier to identify default-risk customers.</p></li>
<li><p>On the basis of the figure, balance obviously is related to default.</p></li>
<li><p>Students seem to default a bit higher percentage.</p></li>
<li><p>Income also tends to be lower among defaulted.</p></li>
<li><p>It may be noted that even null classifier by which we trivially classify all
customers non-defaulted, the error rate in the training sample is only 3.33 %.
However, them among the defaulted (the main concern of the credit card
company), the error rate is 100%.</p></li>
<li><p>Using the logit model with student status and balance as explanatory variable produces
estimates:</p></li>
</ul>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="chapter5.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(Default<span class="sc">$</span>default) <span class="co"># show which one of default values is set to 1 (here it is &quot;Yes&quot;, defaulted.</span></span></code></pre></div>
<pre><code>##     Yes
## No    0
## Yes   1</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="chapter5.html#cb38-1" aria-hidden="true" tabindex="-1"></a>fit.logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> student <span class="sc">+</span> balance, <span class="at">data =</span> Default, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> logit))</span>
<span id="cb38-2"><a href="chapter5.html#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.logit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = default ~ student + balance, family = binomial(link = logit), 
##     data = Default)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4578  -0.1422  -0.0559  -0.0203   3.7435  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.075e+01  3.692e-01 -29.116  &lt; 2e-16 ***
## studentYes  -7.149e-01  1.475e-01  -4.846 1.26e-06 ***
## balance      5.738e-03  2.318e-04  24.750  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1571.7  on 9997  degrees of freedom
## AIC: 1577.7
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<ul>
<li>Training sample classification results.
<ul>
<li>A confusion matrix (known also as error matrix) summarizes numbers of correct and incorrect classifications.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="chapter5.html#cb40-1" aria-hidden="true" tabindex="-1"></a>pred.logit <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.logit, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="co"># probabilities for default status &quot;Yes&quot;</span></span>
<span id="cb40-2"><a href="chapter5.html#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(pred.logit) <span class="co"># show a few predictions</span></span></code></pre></div>
<pre><code>##            1            2            3            4            5            6 
## 0.0014090960 0.0011403179 0.0100571943 0.0004469571 0.0019434977 0.0020503778</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="chapter5.html#cb42-1" aria-hidden="true" tabindex="-1"></a>default.logit <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(pred.logit <span class="sc">&gt;</span> .<span class="dv">5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>)) <span class="co"># predicted defaults statuses</span></span>
<span id="cb42-2"><a href="chapter5.html#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(default.logit) <span class="co"># a few first</span></span></code></pre></div>
<pre><code>##  1  2  3  4  5  6 
## No No No No No No 
## Levels: No Yes</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="chapter5.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(default.logit) <span class="co"># n of predicted default and non default</span></span></code></pre></div>
<pre><code>## default.logit
##   No  Yes 
## 9856  144</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="chapter5.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion matrix, i.e., table for predicted vs true defaults</span></span>
<span id="cb46-2"><a href="chapter5.html#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Note: surrounding assignments x &lt;- y in parenthesis performs the assignment and prints the results</span></span>
<span id="cb46-3"><a href="chapter5.html#cb46-3" aria-hidden="true" tabindex="-1"></a>(tbl.logit <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> default.logit, <span class="at">true =</span> Default<span class="sc">$</span>default)))</span></code></pre></div>
<pre><code>##      true
## pred    No  Yes
##   No  9628  228
##   Yes   39  105</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="chapter5.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Training error rates</span></span>
<span id="cb48-2"><a href="chapter5.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(Default<span class="sc">$</span>default <span class="sc">!=</span> default.logit) <span class="co"># training sample n of false classifications</span></span></code></pre></div>
<pre><code>## [1] 267</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="chapter5.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="dv">100</span> <span class="sc">*</span> <span class="fu">mean</span>(Default<span class="sc">$</span>default <span class="sc">!=</span> default.logit) <span class="co"># % training error rate</span></span></code></pre></div>
<pre><code>## [1] 2.67</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="chapter5.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb52-2"><a href="chapter5.html#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb52-3"><a href="chapter5.html#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co">#help(lda) # some help for lda function in MASS</span></span>
<span id="cb52-4"><a href="chapter5.html#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="chapter5.html#cb52-5" aria-hidden="true" tabindex="-1"></a>fit.lda <span class="ot">&lt;-</span> <span class="fu">lda</span>(<span class="at">formula =</span> default <span class="sc">~</span> student <span class="sc">+</span> balance, <span class="at">data =</span> Default)</span>
<span id="cb52-6"><a href="chapter5.html#cb52-6" aria-hidden="true" tabindex="-1"></a>fit.lda </span></code></pre></div>
<pre><code>## Call:
## lda(default ~ student + balance, data = Default)
## 
## Prior probabilities of groups:
##     No    Yes 
## 0.9667 0.0333 
## 
## Group means:
##     studentYes   balance
## No   0.2914037  803.9438
## Yes  0.3813814 1747.8217
## 
## Coefficients of linear discriminants:
##                     LD1
## studentYes -0.249059498
## balance     0.002244397</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="chapter5.html#cb54-1" aria-hidden="true" tabindex="-1"></a>pred.lda <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.lda, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="co"># produces object containing posterior probabilities etc,</span></span>
<span id="cb54-2"><a href="chapter5.html#cb54-2" aria-hidden="true" tabindex="-1"></a>default.lda <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(pred.lda<span class="sc">$</span>posterior[, <span class="st">&quot;Yes&quot;</span>] <span class="sc">&gt;</span> .<span class="dv">5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))</span>
<span id="cb54-3"><a href="chapter5.html#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(default.lda)</span></code></pre></div>
<pre><code>##  1  2  3  4  5  6 
## No No No No No No 
## Levels: No Yes</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="chapter5.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(pred.lda<span class="sc">$</span>posterior)</span></code></pre></div>
<pre><code>##          No         Yes
## 1 0.9968680 0.003131975
## 2 0.9971925 0.002807531
## 3 0.9843970 0.015603046
## 4 0.9987769 0.001223133
## 5 0.9959254 0.004074582
## 6 0.9954627 0.004537289</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="chapter5.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion matrix</span></span>
<span id="cb58-2"><a href="chapter5.html#cb58-2" aria-hidden="true" tabindex="-1"></a>(tbl.lda <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> default.lda, <span class="at">true =</span> Default<span class="sc">$</span>default)))</span></code></pre></div>
<pre><code>##      true
## pred    No  Yes
##   No  9644  252
##   Yes   23   81</code></pre>
<ul>
<li><p>The training error rates (39 + 228)/10,000 = 2.67% for logit and
(23 + 252)/10,000 = 2.75% for LDA look pretty low.</p>
<ul>
<li>However, recall that with the null classifier we would reach also a small rate of
3.33%.</li>
<li>Also, training error tend to give too optimistic predictions.</li>
<li>So, even if the test errors were reliable, the total error rates do not look here very
informative.</li>
</ul></li>
<li><p>From the tables we see, that the logit model labels 39 out of the total of not defaulted 9,667 (i.e., 0.4%) incorrectly (lda: 0.2%) as defaulted (“false positive” or “false alarms”), but labels out of the 333 defaulted 228 incorrectly as not defaulted (“false negative”) (lda: 252), or 68.5% (lda:
75.7%)!</p>
<ul>
<li>These latter figures are most likely unacceptable from the company point of
view.</li>
</ul></li>
<li><p>Class specific performances are sometimes referred to as sensitivity and specificity.</p>
<ul>
<li>In machine learning sensitivity refers to probability of detection (“true positive cases”), here true defaulters, i.e., correct detection of defaulted individuals (logit: 105/333 = 31.5%, lda: 81/333 = 24.3%).</li>
<li>Specificity refers to detection of “true negative cases” , here true non-defaulters, logit: 9,628/9,667 = 99.6% lda: 9,644/9,667 = 99.8%</li>
<li>1 - specificity refers to probability of “false alarm” , which in our credit default example means classifying a non-defaulting individual defaulting (specificity, logit: 9, 628/9,667 = 99.6%, lda: 9, 644/9, 667 = 99.8%; “false
alarm” 0.4% (logit) and 0.2% (lda)).</li>
</ul></li>
<li><p>The problem is the poor performance in classifying defaults.</p>
<ul>
<li>LDA aims to mimic the Bayes classifier that yields the smallest possible total error rate.</li>
</ul></li>
<li><p>Recall, that the Bayesian classifier assigns the observation to the class with highest posterior probability, i.e., in the two-class case to the class with probability &gt; 0.5.</p></li>
<li><p>If the concern is incorrectly predicting defaulters, we can catch them better by lowering the posterior probability threshold of default from the 0.5, e.g. down to 0.2, i.e.,</p></li>
</ul>
<p><span class="math display">\[
P(default = Yes|X = x) &gt; 0.2
\]</span></p>
<ul>
<li>This gives:</li>
</ul>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="chapter5.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="do">## P(default = Yes | X = x) &gt; .20</span></span>
<span id="cb60-2"><a href="chapter5.html#cb60-2" aria-hidden="true" tabindex="-1"></a>default.logit20 <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(pred.logit <span class="sc">&gt;</span> .<span class="dv">2</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))</span>
<span id="cb60-3"><a href="chapter5.html#cb60-3" aria-hidden="true" tabindex="-1"></a>default.lda20 <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(pred.lda<span class="sc">$</span>posterior[, <span class="st">&quot;Yes&quot;</span>] <span class="sc">&gt;</span> .<span class="dv">2</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))</span>
<span id="cb60-4"><a href="chapter5.html#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion matrices</span></span>
<span id="cb60-5"><a href="chapter5.html#cb60-5" aria-hidden="true" tabindex="-1"></a>tbl.logit20 <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> default.logit20, <span class="at">true =</span> Default<span class="sc">$</span>default))</span>
<span id="cb60-6"><a href="chapter5.html#cb60-6" aria-hidden="true" tabindex="-1"></a>tbl.lda20 <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> default.lda20, <span class="at">true =</span> Default<span class="sc">$</span>default))</span>
<span id="cb60-7"><a href="chapter5.html#cb60-7" aria-hidden="true" tabindex="-1"></a>tbl.logit20; <span class="fu">rowSums</span>(tbl.logit20)</span></code></pre></div>
<pre><code>##      true
## pred    No  Yes
##   No  9391  130
##   Yes  276  203</code></pre>
<pre><code>##   No  Yes 
## 9521  479</code></pre>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="chapter5.html#cb63-1" aria-hidden="true" tabindex="-1"></a>tbl.lda20; <span class="fu">rowSums</span>(tbl.lda20)</span></code></pre></div>
<pre><code>##      true
## pred    No  Yes
##   No  9432  138
##   Yes  235  195</code></pre>
<pre><code>##   No  Yes 
## 9570  430</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="chapter5.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="do">## total training error</span></span>
<span id="cb66-2"><a href="chapter5.html#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(default.logit20 <span class="sc">!=</span> Default<span class="sc">$</span>default) <span class="co"># logit</span></span></code></pre></div>
<pre><code>## [1] 0.0406</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="chapter5.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(default.lda20 <span class="sc">!=</span> Default<span class="sc">$</span>default) <span class="co"># lda</span></span></code></pre></div>
<pre><code>## [1] 0.0373</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="chapter5.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Sensitivity (percents of correctly classified defaulters)</span></span>
<span id="cb70-2"><a href="chapter5.html#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span>tbl.logit20[<span class="dv">2</span>, <span class="dv">2</span>] <span class="sc">/</span> <span class="fu">sum</span>(tbl.logit20[, <span class="dv">2</span>]), <span class="at">digits =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 61</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="chapter5.html#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span>tbl.lda20[<span class="dv">2</span>, <span class="dv">2</span>] <span class="sc">/</span> <span class="fu">sum</span>(tbl.lda20[, <span class="dv">2</span>]), <span class="at">digits =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 58.6</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="chapter5.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Specificity (fractions of correctly classified non-defaulters</span></span>
<span id="cb74-2"><a href="chapter5.html#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span>tbl.logit20[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">/</span> <span class="fu">sum</span>(tbl.logit20[, <span class="dv">1</span>]), <span class="at">digits =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 97.1</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="chapter5.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span>tbl.lda20[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">/</span> <span class="fu">sum</span>(tbl.lda20[, <span class="dv">1</span>]), <span class="at">digits =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 97.6</code></pre>
<ul>
<li>The resulting training error rates and correct classification rates
<ul>
<li>Total: logit 4.06%, LDA 3.73%</li>
<li>Sensitivity (correctly defaulted): logit 61.0%, LDA 58.6%</li>
<li>Specificity (correctly not defaulted): logit 97.1%, LDA 97.6%</li>
</ul></li>
<li>Thus, there is relatively small increase in total error rate, whereas
identifying defaulters improves materially.
<ul>
<li>However, this does not come without a cost: Correctly identified
non-defaulters decrease (logit: 99.6% → 97.1%, lda: 99.8% → 97.6%).</li>
<li>The card company need to balance with these in order to minimize losses.</li>
</ul></li>
<li>Figure below illustrates the trade-off as the posterior is changed.</li>
</ul>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="chapter5.html#cb78-1" aria-hidden="true" tabindex="-1"></a>true.dflt <span class="ot">&lt;-</span> Default<span class="sc">$</span>default <span class="co"># copy for ease</span></span>
<span id="cb78-2"><a href="chapter5.html#cb78-2" aria-hidden="true" tabindex="-1"></a>pr <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length =</span> <span class="dv">1000</span>) <span class="co"># the whole range of posterior probability tresholds</span></span>
<span id="cb78-3"><a href="chapter5.html#cb78-3" aria-hidden="true" tabindex="-1"></a>sens.lgt <span class="ot">&lt;-</span> sens.lda <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="fu">length</span>(pr)) <span class="co"># vectors for logit and lda sensitivities</span></span>
<span id="cb78-4"><a href="chapter5.html#cb78-4" aria-hidden="true" tabindex="-1"></a>spec.lgt <span class="ot">&lt;-</span> spec.lda <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="fu">length</span>(pr)) <span class="co"># vectors for specifities</span></span>
<span id="cb78-5"><a href="chapter5.html#cb78-5" aria-hidden="true" tabindex="-1"></a>er.tot.lgt <span class="ot">&lt;-</span> er.tot.lda <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="fu">length</span>(pr)) <span class="co"># tota error rate vectors</span></span>
<span id="cb78-6"><a href="chapter5.html#cb78-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-7"><a href="chapter5.html#cb78-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(pr)) {</span>
<span id="cb78-8"><a href="chapter5.html#cb78-8" aria-hidden="true" tabindex="-1"></a>    dlgt <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">ifelse</span>(pred.logit <span class="sc">&gt;</span> pr[i], <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>), <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>)) <span class="co"># logit defaults</span></span>
<span id="cb78-9"><a href="chapter5.html#cb78-9" aria-hidden="true" tabindex="-1"></a>    dlda <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">ifelse</span>(pred.lda<span class="sc">$</span>posterior[, <span class="st">&quot;Yes&quot;</span>] <span class="sc">&gt;</span> pr[i], <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>), <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>)) <span class="co"># lda</span></span>
<span id="cb78-10"><a href="chapter5.html#cb78-10" aria-hidden="true" tabindex="-1"></a>    sens.lgt[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dlgt[true.dflt <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>] <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>) <span class="co"># logit sensitivities (identified true defaulters)</span></span>
<span id="cb78-11"><a href="chapter5.html#cb78-11" aria-hidden="true" tabindex="-1"></a>    sens.lda[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dlda[true.dflt <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>] <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>) <span class="co"># lda sensitivities</span></span>
<span id="cb78-12"><a href="chapter5.html#cb78-12" aria-hidden="true" tabindex="-1"></a>    spec.lgt[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dlgt[true.dflt <span class="sc">==</span> <span class="st">&quot;No&quot;</span>] <span class="sc">==</span> <span class="st">&quot;No&quot;</span>) <span class="co"># logit specificities (true non-defaulters identified)</span></span>
<span id="cb78-13"><a href="chapter5.html#cb78-13" aria-hidden="true" tabindex="-1"></a>    spec.lda[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dlda[true.dflt <span class="sc">==</span> <span class="st">&quot;No&quot;</span>] <span class="sc">==</span> <span class="st">&quot;No&quot;</span>) <span class="co"># lda specificities</span></span>
<span id="cb78-14"><a href="chapter5.html#cb78-14" aria-hidden="true" tabindex="-1"></a>    er.tot.lgt[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dlgt <span class="sc">!=</span> true.dflt) <span class="co"># logit total error rate</span></span>
<span id="cb78-15"><a href="chapter5.html#cb78-15" aria-hidden="true" tabindex="-1"></a>    er.tot.lda[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(dlda <span class="sc">!=</span> true.dflt) <span class="co"># lda total error rate</span></span>
<span id="cb78-16"><a href="chapter5.html#cb78-16" aria-hidden="true" tabindex="-1"></a>} <span class="co"># for p</span></span>
<span id="cb78-17"><a href="chapter5.html#cb78-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-18"><a href="chapter5.html#cb78-18" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)) <span class="co"># reset full window</span></span>
<span id="cb78-19"><a href="chapter5.html#cb78-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> pr, <span class="at">y =</span> <span class="dv">1</span> <span class="sc">-</span> sens.lgt, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb78-20"><a href="chapter5.html#cb78-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Posterior Threshold&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Error Rate&quot;</span>)</span>
<span id="cb78-21"><a href="chapter5.html#cb78-21" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> pr, <span class="at">y =</span> <span class="dv">1</span> <span class="sc">-</span> sens.lda, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb78-22"><a href="chapter5.html#cb78-22" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> pr, <span class="at">y =</span> er.tot.lgt, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;solid&quot;</span>)</span>
<span id="cb78-23"><a href="chapter5.html#cb78-23" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x =</span> pr, <span class="at">y =</span> er.tot.lda, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;solid&quot;</span>)</span>
<span id="cb78-24"><a href="chapter5.html#cb78-24" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> .<span class="dv">04</span>, <span class="at">y =</span> <span class="dv">1</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Rate of missed defaults (logit)&quot;</span>,</span>
<span id="cb78-25"><a href="chapter5.html#cb78-25" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;Rate of missed defaults (lda)&quot;</span>,</span>
<span id="cb78-26"><a href="chapter5.html#cb78-26" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;Total error rate (logit)&quot;</span>,</span>
<span id="cb78-27"><a href="chapter5.html#cb78-27" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;Total error rate (lda)&quot;</span>),</span>
<span id="cb78-28"><a href="chapter5.html#cb78-28" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;steel blue&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;steel blue&quot;</span>),</span>
<span id="cb78-29"><a href="chapter5.html#cb78-29" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="st">&quot;dashed&quot;</span>, <span class="st">&quot;dashed&quot;</span>, <span class="st">&quot;solid&quot;</span>, <span class="st">&quot;solid&quot;</span>),</span>
<span id="cb78-30"><a href="chapter5.html#cb78-30" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<ul>
<li><p>Default error rate indicates the rate of assigning defaulted individuals not
defaulted in the training sample.</p></li>
<li><p>The appropriate error rate must be defined by the credit card company
(analyzing costs incurred by the error classifications).</p></li>
<li><p>Both error rates (defaulted mis-classified as non-defaulted; non-defaulted mis-classified as defaulted) can be simultaneously displayed by the <em>ROC curve</em> for different posterior probability thresholds.</p>
<ul>
<li>The ROC is acronym for receiver operating characteristic coming from communications theory (see Wiki, <a href="https://en.wikipedia.org/wiki/Receiver" class="uri">https://en.wikipedia.org/wiki/Receiver</a> operating characteristic, for general description).</li>
<li>The ROC is a plot of true positive rate (TPR) against the false positive rate (FPR)
<ul>
<li>Positive and negative rates stem from testing a medical condition in which a positive result signals presence of a disease while negative results signals no disease.</li>
<li>True positive rate (TPR) equals sensitivity (probability of detection) and false positive rate (FPR) false alarm = 1 - specificity.</li>
</ul></li>
<li>The overall performance of a classifier is given by the area under the (ROC) curve (AUC) over all possible thresholds.</li>
<li>The closer AUC is one the better the classifier.</li>
<li>AUC of 0.5 corresponds classifying by chance.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="chapter5.html#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span>  <span class="dv">1</span> <span class="sc">-</span> spec.lgt, <span class="at">y =</span> sens.lgt, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb79-2"><a href="chapter5.html#cb79-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb79-3"><a href="chapter5.html#cb79-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;False positive rate (Non-defaulted classified as defaulted)&quot;</span>,</span>
<span id="cb79-4"><a href="chapter5.html#cb79-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;True positive rate (Defaulted classified correctly)&quot;</span>,</span>
<span id="cb79-5"><a href="chapter5.html#cb79-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;ROC for the logit classifier&quot;</span>)</span>
<span id="cb79-6"><a href="chapter5.html#cb79-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;dotted&quot;</span>) <span class="co"># 0.5 line</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="chapter5.html#cb80-1" aria-hidden="true" tabindex="-1"></a>auc.lgt <span class="ot">&lt;-</span> auc.lda <span class="ot">&lt;-</span> <span class="dv">0</span> <span class="co"># initialize</span></span>
<span id="cb80-2"><a href="chapter5.html#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(spec.lgt)){</span>
<span id="cb80-3"><a href="chapter5.html#cb80-3" aria-hidden="true" tabindex="-1"></a>    auc.lgt <span class="ot">&lt;-</span> auc.lgt <span class="sc">+</span> <span class="fu">abs</span>(spec.lgt[i] <span class="sc">-</span> spec.lgt[i<span class="dv">-1</span>]) <span class="sc">*</span> (sens.lgt[i<span class="dv">-1</span>] <span class="sc">+</span> sens.lgt[i])<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb80-4"><a href="chapter5.html#cb80-4" aria-hidden="true" tabindex="-1"></a>    auc.lda <span class="ot">&lt;-</span> auc.lda <span class="sc">+</span> <span class="fu">abs</span>(spec.lda[i] <span class="sc">-</span> spec.lda[i<span class="dv">-1</span>]) <span class="sc">*</span> (sens.lda[i<span class="dv">-1</span>] <span class="sc">+</span> sens.lda[i])<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb80-5"><a href="chapter5.html#cb80-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb80-6"><a href="chapter5.html#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot; AUC logit:&quot;</span>, auc.lgt, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>, <span class="st">&quot;AUC LDA:  &quot;</span>, auc.lda, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>##  AUC logit: 0.9488711 
##  AUC LDA:   0.9495384</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="chapter5.html#cb82-1" aria-hidden="true" tabindex="-1"></a>auc.lgt <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">abs</span>(spec.lgt[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">-</span> spec.lgt[<span class="sc">-</span><span class="fu">length</span>(spec.lgt)]) <span class="sc">*</span> (sens.lgt[<span class="sc">-</span><span class="fu">length</span>(sens.lgt)] <span class="sc">+</span> sens.lgt[<span class="sc">-</span><span class="dv">1</span>])<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb82-2"><a href="chapter5.html#cb82-2" aria-hidden="true" tabindex="-1"></a>auc.lda <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">abs</span>(spec.lda[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">-</span> spec.lda[<span class="sc">-</span><span class="fu">length</span>(spec.lda)]) <span class="sc">*</span> (sens.lda[<span class="sc">-</span><span class="fu">length</span>(sens.lda)] <span class="sc">+</span> sens.lda[<span class="sc">-</span><span class="dv">1</span>])<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb82-3"><a href="chapter5.html#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot; AUC logit:&quot;</span>, auc.lgt, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>, <span class="st">&quot;AUC LDA:  &quot;</span>, auc.lda, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>##  AUC logit: 0.9488711 
##  AUC LDA:   0.9495384</code></pre>
<ul>
<li>For the logit classifier AUC is 94.9%, i.e. very high (for the LDA classifier AUC = 95%).</li>
</ul>
<p><br></p>
</div>
</div>
<div id="quadratic-discriminant-analysis" class="section level3 hasAnchor" number="5.4.3">
<h3><span class="header-section-number">5.4.3</span> Quadratic Discriminant Analysis<a href="chapter5.html#quadratic-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>LDA is based on the assumption that the covariance matrix is the
same in all <span class="math inline">\(K\)</span> classes.</li>
<li><em>Quadratic discriminant analysis</em> (QDA) is based again on normal distribution theory, but gives up the assumption of the same covariance matrices across classes, so that <span class="math inline">\(X\sim N(\mu_k, \Sigma_k)\)</span>, <span class="math inline">\(k=1,\ldots,K\)</span>.</li>
<li>Under this assumption the Bayes rule assigns and observation <span class="math inline">\(x\)</span> to the class for which</li>
</ul>
<p><span class="math display">\[
\delta_k (x) = −\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac{1}{ 2} \mu^T_k\Sigma_k^{-1}\mu_k- \frac{1}{2} log|\Sigma_k| + log\pi_k
\]</span></p>
<p>      is largest.</p>
<ul>
<li>Thus, the QDA involves estimating in addition to the class means also
class specific covariance matrices and plug them in to the classification
function.</li>
<li>The classifier is a quadratic function of <span class="math inline">\(X\)</span>, hence the name.</li>
</ul>
<p><img src="fig5/ex17.png" /></p>
<ul>
<li>LDA implies linear decision boundary and QDA produces quadratic decision boundary.</li>
<li>The number of estimated mean and variance-covariance parameters in LDA is <span class="math inline">\(Kp + p(p + 1)/2\)</span> which grows in QDA to <span class="math inline">\(K(p + p(p + 1)/2)\)</span> , i.e., by ( <span class="math inline">\(K − 1)p(p + 1)/2\)</span>.</li>
<li>Due to this increment QDA need large sample sizes to estimate parameters accurately.</li>
<li>Because of giving up the equality of covariance matrices, QDA is more flexible than LDA.</li>
<li>However, in small samples LDA may perform better even in the case of different covariance matrices due to smaller number of estimated parameters (less sampling error).</li>
</ul>
<p><br></p>
</div>
</div>
<div id="naive-bayes" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Naive Bayes<a href="chapter5.html#naive-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>LDA and QDA are based on the Bayes’ theorem,
i.e.,</li>
</ul>
<p><span class="math display">\[
P(Y = k|X = x) = \frac{\pi_k f_k (x)}{\sum_{j=1}^{K} \pi_j f_j(x)}
\]</span></p>
<ul>
<li><p>The prior probabilities <span class="math inline">\(\pi_1, \ldots, \pi_K\)</span> are typically fairly
straightforwardly obtained.</p></li>
<li><p>The difficulty is estimating <span class="math inline">\(f_k (x)\)</span>, <span class="math inline">\(k = 1,\ldots,K\)</span> as they are multivariate joint densities (or probabilities).</p></li>
<li><p>Naive Bayesian approach makes a simplifying approach by assuming the that the random variables <span class="math inline">\(X_i\)</span>s in <span class="math inline">\(X = (X1, . . . , Xp)^T\)</span> are independent.</p></li>
<li><p>Then joint the distributions <span class="math inline">\(f_k(x)\)</span>, <span class="math inline">\(k = 1,\ldots,K\)</span> can be represented in terms of the marginal distribution <span class="math inline">\(f_{kj}(xj)\)</span> of each <span class="math inline">\(X_j\)</span>, <span class="math inline">\(j = 1,\ldots, p\)</span>,</p></li>
</ul>
<p><span class="math display">\[
f_k (x) = f_{k1}(x_1) × f_{k2}(x_2) × · · · × f_{kp}(x_p)
\]</span></p>
<p>      i.e., as a product of marginal distributions.</p>
<ul>
<li><p>This can simplify materially matters.</p></li>
<li><p>For example, for QDA, instead of estimating <span class="math inline">\(Kp\)</span> means and <span class="math inline">\(K(p + 1)p/2\)</span>, covariances and variances, in Naive Bayes one needs to estimate only <span class="math inline">\(Kp\)</span> means and <span class="math inline">\(Kp\)</span> variances (e.g., if <span class="math inline">\(K = 2\)</span> and <span class="math inline">\(p = 10\)</span>, in QDA there <span class="math inline">\(2 × 10 + 2 × (10 + 1) × 10/2 = 130\)</span> parameters to be estimated compared to <span class="math inline">\(40\)</span> in Naive Bayes).</p></li>
<li><p>Assumes features are independent in each class.</p>
<ul>
<li>Useful when <span class="math inline">\(p\)</span> is large, and so multivariate methods like QDA and even LDA break down.</li>
</ul></li>
<li><p>Gaussian naive Bayes assumes each <span class="math inline">\(\Sigma_k\)</span> is diagonal:</p></li>
</ul>
<p><span class="math display">\[
\delta_k(x) \propto log \big[ \pi_k \Pi_{j=1}^p f_{kj}(x_j) \big]=-\frac{1}{2}\sum_{j=1}^p \big[\frac{(x_j-\mu_{kj})^2}{\sigma_{kj}^2}+log\sigma_{kj}^2  \big] + log\pi_k
\]</span></p>
<ul>
<li>Can use for <em>mixed</em> feature vectors (qualitative and quantitative).
<ul>
<li>If <span class="math inline">\(X_j\)</span> is qualitative, replace <span class="math inline">\(f_{kj}(x_j)\)</span> with probability mass function (histogram) over discrete categories.</li>
</ul></li>
<li><strong>Despite strong assumptions, naive Bayes often produces good classification results</strong>.</li>
</ul>
<div id="logistic-regression-versus-lda" class="section level4 unnumbered hasAnchor">
<h4>Logistic regression versus LDA<a href="chapter5.html#logistic-regression-versus-lda" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>For a two-class problem, one can show that for LDA</li>
</ul>
<p><span class="math display">\[
log(\frac{p_1(x)}{1-p_1(x)})=log(\frac{p_1(x)}{p_2(x)})=c_0 + c_1x_1 + \cdots +c_px_p
\]</span></p>
<ul>
<li><p>So it has the same form as logistic regression.</p></li>
<li><p>The difference is in how the parameters are estimated.</p>
<ul>
<li>Logistic regression uses the conditional likelihood based on <span class="math inline">\(Pr(Y|X)\)</span> (known as <em>discriminative learning</em>).</li>
<li>LDA uses the full likelihood based on $Pr(X,Y) (Known as <em>generative learning</em>).</li>
<li>Despite these differences, in practice the results are often very similar.</li>
</ul></li>
<li><p>Logistic regression can also fit quadratic boundaries like QDA, by explicitly including quadratic terms in the model.</p></li>
</ul>
<p><br></p>
</div>
</div>
<div id="knn-classification" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> KNN Classification<a href="chapter5.html#knn-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>KNN classification classifies the new observation with value x according to “majority voice” of <span class="math inline">\(K\)</span> nearest neighbors in the training data (in the case of tied classes, i.e., two or more classes are equally close, programs like R randomly break the tie).</p></li>
<li><p>Note: Here <span class="math inline">\(K\)</span> refers to the number of nearest neighbors, not the number of classes!</p></li>
<li><p>The closeness if computed by an appropriate distance measure (e.g. Euclidian distance).</p></li>
<li><p>The function <em>knn()</em> of the <em>class</em> package needs four inputs:</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>matrix (or data frame) containing predictors (<span class="math inline">\(X\)</span> variables)
associated with the training data (the first argument, train, in the
knn() function call).</li>
<li>A matrix (or data frame) containing predictors (<span class="math inline">\(x\)</span> variables)
associated with the data the observations of which we wish to classify
(the second argument, test, in knn()).</li>
<li>A vector containing containing class labels for the training
observations (the third argument, cl, in knn()).</li>
<li>value for <span class="math inline">\(K\)</span> , the number of nearest neighbors to be used by the
classifier (the fourth argument, <span class="math inline">\(k\)</span>, in knn()).</li>
</ol>
<p><br></p>
</div>
<div id="example-10" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Example<a href="chapter5.html#example-10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Predict next stock market direction (Up, Down) on the basis of 5
previous day returns on Helsinki Stock Exchange (OMX Helsinki) by
OMXH25 index.</li>
<li>Sample period is from March 13, 2013 to Dec 30, 2021.
Basic return (ret) sample statistics</li>
</ul>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="chapter5.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages(&quot;quantmod&quot;)</span></span>
<span id="cb84-2"><a href="chapter5.html#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="at">package =</span> <span class="st">&quot;quantmod&quot;</span>) <span class="co"># load fImport package</span></span>
<span id="cb84-3"><a href="chapter5.html#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class) <span class="co"># base package class has knn function</span></span>
<span id="cb84-4"><a href="chapter5.html#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb84-5"><a href="chapter5.html#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="chapter5.html#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="do">### Helsinki OMX 25 index</span></span>
<span id="cb84-7"><a href="chapter5.html#cb84-7" aria-hidden="true" tabindex="-1"></a>h25 <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">getSymbols</span>(<span class="at">Symbols =</span> <span class="st">&quot;^OMXH25&quot;</span>, <span class="co"># OMX Helsinki 25</span></span>
<span id="cb84-8"><a href="chapter5.html#cb84-8" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">from =</span> <span class="st">&quot;2004-01-01&quot;</span>, <span class="co"># Starting January 1, 2004 (if available), truns out the first date is 2013-03-05</span></span>
<span id="cb84-9"><a href="chapter5.html#cb84-9" aria-hidden="true" tabindex="-1"></a>                                        <span class="co"># data is coersed as a data frame (for convenience)</span></span>
<span id="cb84-10"><a href="chapter5.html#cb84-10" aria-hidden="true" tabindex="-1"></a>                                        <span class="co"># Dates are coersed as rownames of the resulting data frame</span></span>
<span id="cb84-11"><a href="chapter5.html#cb84-11" aria-hidden="true" tabindex="-1"></a>                                <span class="at">to =</span> <span class="st">&quot;2021-12-31&quot;</span>, <span class="co"># Endin date</span></span>
<span id="cb84-12"><a href="chapter5.html#cb84-12" aria-hidden="true" tabindex="-1"></a>                                <span class="at">env =</span> <span class="cn">NULL</span> <span class="co"># returns data to dfr</span></span>
<span id="cb84-13"><a href="chapter5.html#cb84-13" aria-hidden="true" tabindex="-1"></a>                                ) <span class="co"># getSynbols</span></span>
<span id="cb84-14"><a href="chapter5.html#cb84-14" aria-hidden="true" tabindex="-1"></a>                     ) <span class="co"># as.data.frame</span></span></code></pre></div>
<pre><code>## Warning: ^OMXH25 contains missing values. Some functions will not work if
## objects contain missing values in the middle of the series. Consider using
## na.omit(), na.approx(), na.fill(), etc to remove or replace them.</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="chapter5.html#cb86-1" aria-hidden="true" tabindex="-1"></a>h25 <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(h25) <span class="co"># remove missing values</span></span>
<span id="cb86-2"><a href="chapter5.html#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="chapter5.html#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(h25) <span class="co"># show a few first lines</span></span></code></pre></div>
<pre><code>##            OMXH25.Open OMXH25.High OMXH25.Low OMXH25.Close OMXH25.Volume
## 2013-03-05     2351.12     2381.78    2351.12      2378.81             0
## 2013-03-06     2384.10     2391.14    2376.58      2376.75             0
## 2013-03-07     2386.29     2389.44    2370.58      2376.89             0
## 2013-03-08     2379.53     2400.87    2379.53      2388.12             0
## 2013-03-11     2388.33     2388.93    2368.74      2379.26             0
## 2013-03-12     2380.01     2393.99    2380.01      2380.43             0
##            OMXH25.Adjusted
## 2013-03-05         2378.81
## 2013-03-06         2376.75
## 2013-03-07         2376.89
## 2013-03-08         2388.12
## 2013-03-11         2379.26
## 2013-03-12         2380.43</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="chapter5.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(h25) <span class="co"># show a few last lines</span></span></code></pre></div>
<pre><code>##            OMXH25.Open OMXH25.High OMXH25.Low OMXH25.Close OMXH25.Volume
## 2021-12-22     5443.45     5472.59    5426.49      5472.59             0
## 2021-12-23     5497.20     5537.14    5496.95      5534.49             0
## 2021-12-27     5522.93     5600.74    5522.93      5584.77             0
## 2021-12-28     5591.05     5627.86    5588.81      5605.85             0
## 2021-12-29     5594.81     5618.91    5563.80      5581.27             0
## 2021-12-30     5576.30     5592.60    5571.37      5571.97             0
##            OMXH25.Adjusted
## 2021-12-22         5472.59
## 2021-12-23         5534.49
## 2021-12-27         5584.77
## 2021-12-28         5605.85
## 2021-12-29         5581.27
## 2021-12-30         5571.97</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="chapter5.html#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  Rename series names</span></span>
<span id="cb90-2"><a href="chapter5.html#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(h25) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;open&quot;</span>, <span class="st">&quot;high&quot;</span>, <span class="st">&quot;low&quot;</span>, <span class="st">&quot;close&quot;</span>, <span class="st">&quot;vol&quot;</span>, <span class="st">&quot;aclose&quot;</span>) <span class="co"># change column names</span></span>
<span id="cb90-3"><a href="chapter5.html#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="chapter5.html#cb90-4" aria-hidden="true" tabindex="-1"></a>h25<span class="sc">$</span>date <span class="ot">&lt;-</span> <span class="fu">as.Date</span>(<span class="at">x =</span> <span class="fu">rownames</span>(h25), <span class="at">format =</span> <span class="st">&quot;%Y-%m-%d&quot;</span>) <span class="co"># create R dates from the rownames</span></span>
<span id="cb90-5"><a href="chapter5.html#cb90-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-6"><a href="chapter5.html#cb90-6" aria-hidden="true" tabindex="-1"></a>h25<span class="sc">$</span>vol <span class="ot">&lt;-</span> <span class="cn">NULL</span> <span class="co"># remove vol as no volume information</span></span>
<span id="cb90-7"><a href="chapter5.html#cb90-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-8"><a href="chapter5.html#cb90-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Add returns to data frame. Note the replacement of the first obseravation by missing value NA</span></span>
<span id="cb90-9"><a href="chapter5.html#cb90-9" aria-hidden="true" tabindex="-1"></a>h25<span class="sc">$</span>ret <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">100</span><span class="sc">*</span>(h25<span class="sc">$</span>aclose[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">-</span> h25<span class="sc">$</span>aclose[<span class="sc">-</span><span class="fu">nrow</span>(h25)]) <span class="sc">/</span> h25<span class="sc">$</span>aclose[<span class="sc">-</span><span class="fu">nrow</span>(h25)])</span>
<span id="cb90-10"><a href="chapter5.html#cb90-10" aria-hidden="true" tabindex="-1"></a>h25<span class="sc">$</span>lag1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="cn">NA</span>, h25<span class="sc">$</span>ret[<span class="sc">-</span><span class="fu">nrow</span>(h25)])</span>
<span id="cb90-11"><a href="chapter5.html#cb90-11" aria-hidden="true" tabindex="-1"></a>h25<span class="sc">$</span>lag2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">2</span>), h25<span class="sc">$</span>ret[<span class="sc">-</span>((<span class="fu">nrow</span>(h25)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">:</span><span class="fu">nrow</span>(h25))])</span>
<span id="cb90-12"><a href="chapter5.html#cb90-12" aria-hidden="true" tabindex="-1"></a>h25<span class="sc">$</span>lag3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">3</span>), h25<span class="sc">$</span>ret[<span class="sc">-</span>((<span class="fu">nrow</span>(h25)<span class="sc">-</span><span class="dv">2</span>)<span class="sc">:</span><span class="fu">nrow</span>(h25))])</span>
<span id="cb90-13"><a href="chapter5.html#cb90-13" aria-hidden="true" tabindex="-1"></a>h25<span class="sc">$</span>lag4 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">4</span>), h25<span class="sc">$</span>ret[<span class="sc">-</span>((<span class="fu">nrow</span>(h25)<span class="sc">-</span><span class="dv">3</span>)<span class="sc">:</span><span class="fu">nrow</span>(h25))])</span>
<span id="cb90-14"><a href="chapter5.html#cb90-14" aria-hidden="true" tabindex="-1"></a>h25<span class="sc">$</span>lag5 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">5</span>), h25<span class="sc">$</span>ret[<span class="sc">-</span>((<span class="fu">nrow</span>(h25)<span class="sc">-</span><span class="dv">4</span>)<span class="sc">:</span><span class="fu">nrow</span>(h25))])</span>
<span id="cb90-15"><a href="chapter5.html#cb90-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-16"><a href="chapter5.html#cb90-16" aria-hidden="true" tabindex="-1"></a>h25 <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(h25) <span class="co"># remove missing values</span></span>
<span id="cb90-17"><a href="chapter5.html#cb90-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-18"><a href="chapter5.html#cb90-18" aria-hidden="true" tabindex="-1"></a>h25<span class="sc">$</span>dir <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(h25<span class="sc">$</span>ret <span class="sc">&lt;</span> <span class="dv">0</span>, <span class="st">&quot;Down&quot;</span>, <span class="st">&quot;Up&quot;</span>)) <span class="co"># directions</span></span>
<span id="cb90-19"><a href="chapter5.html#cb90-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-20"><a href="chapter5.html#cb90-20" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(h25) <span class="ot">&lt;-</span> <span class="cn">NULL</span> <span class="co"># restate row names for convenience</span></span>
<span id="cb90-21"><a href="chapter5.html#cb90-21" aria-hidden="true" tabindex="-1"></a><span class="do">## few first and last lines</span></span>
<span id="cb90-22"><a href="chapter5.html#cb90-22" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">cbind</span>(h25<span class="sc">$</span>date, h25<span class="sc">$</span>dir, <span class="fu">round</span>(h25[, <span class="fu">c</span>(<span class="st">&quot;ret&quot;</span>, <span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)], <span class="at">digits =</span> <span class="dv">3</span>))) <span class="co"># check the retsults</span></span></code></pre></div>
<pre><code>##     h25$date h25$dir    ret   lag1   lag2   lag3   lag4   lag5
## 1 2013-03-13    Down -0.353  0.049 -0.371  0.472  0.006 -0.087
## 2 2013-03-14      Up  0.385 -0.353  0.049 -0.371  0.472  0.006
## 3 2013-03-15    Down -0.418  0.385 -0.353  0.049 -0.371  0.472
## 4 2013-03-18    Down -0.437 -0.418  0.385 -0.353  0.049 -0.371
## 5 2013-03-19    Down -0.078 -0.437 -0.418  0.385 -0.353  0.049
## 6 2013-03-20      Up  0.613 -0.078 -0.437 -0.418  0.385 -0.353</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="chapter5.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(<span class="fu">cbind</span>(h25<span class="sc">$</span>date, h25<span class="sc">$</span>dir, <span class="fu">round</span>(h25[, <span class="fu">c</span>(<span class="st">&quot;ret&quot;</span>, <span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)], <span class="at">digits =</span> <span class="dv">3</span>))) <span class="co"># check the retsults</span></span></code></pre></div>
<pre><code>##        h25$date h25$dir    ret   lag1   lag2   lag3   lag4   lag5
## 2193 2021-12-22      Up  0.830  1.287 -1.422 -0.607  1.184  0.581
## 2194 2021-12-23      Up  1.131  0.830  1.287 -1.422 -0.607  1.184
## 2195 2021-12-27      Up  0.908  1.131  0.830  1.287 -1.422 -0.607
## 2196 2021-12-28      Up  0.377  0.908  1.131  0.830  1.287 -1.422
## 2197 2021-12-29    Down -0.438  0.377  0.908  1.131  0.830  1.287
## 2198 2021-12-30    Down -0.167 -0.438  0.377  0.908  1.131  0.830</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="chapter5.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb94-2"><a href="chapter5.html#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">summary</span>(h25<span class="sc">$</span>ret), <span class="at">digits =</span> <span class="dv">3</span>) <span class="co"># sumary statistc for returns</span></span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -10.128  -0.538   0.071   0.045   0.688   6.892</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="chapter5.html#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb96-2"><a href="chapter5.html#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sd</span>(h25<span class="sc">$</span>ret), <span class="at">digits =</span> <span class="dv">3</span>) <span class="co"># return standard deviation</span></span></code></pre></div>
<pre><code>## [1] 1.136</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="chapter5.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="do">## correlations</span></span>
<span id="cb98-2"><a href="chapter5.html#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(h25[, <span class="fu">c</span>(<span class="st">&quot;ret&quot;</span>, <span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)]), <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##         ret   lag1   lag2   lag3   lag4   lag5
## ret   1.000  0.044 -0.005  0.031 -0.010 -0.007
## lag1  0.044  1.000  0.044 -0.005  0.031 -0.010
## lag2 -0.005  0.044  1.000  0.044 -0.005  0.031
## lag3  0.031 -0.005  0.044  1.000  0.044 -0.005
## lag4 -0.010  0.031 -0.005  0.044  1.000  0.043
## lag5 -0.007 -0.010  0.031 -0.005  0.043  1.000</code></pre>
<ul>
<li>ret is today’s (percentage) return, lag1 is yesterday’s return,
lag2 is return two days ago, and so forth</li>
</ul>
<p><br></p>
<p>We use logit, LDA, QDA, NBayes, and KNN methods to predict directions of the
daily returns using returns to the end of 2020 as a training set year 2021 returns
as the test set.
Below are frequencies of directions in training and test sets</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="chapter5.html#cb100-1" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> (<span class="fu">format</span>(h25<span class="sc">$</span>date, <span class="at">format =</span> <span class="st">&quot;%Y&quot;</span>) <span class="sc">&gt;</span> <span class="dv">2020</span>) <span class="co"># Year 2021 is the test period</span></span>
<span id="cb100-2"><a href="chapter5.html#cb100-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="sc">!</span>test <span class="co"># training period</span></span>
<span id="cb100-3"><a href="chapter5.html#cb100-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Up and Down pecentages</span></span>
<span id="cb100-4"><a href="chapter5.html#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(h25) <span class="co"># total number of training days</span></span></code></pre></div>
<pre><code>## [1] 2198</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="chapter5.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="do">## training sample</span></span>
<span id="cb102-2"><a href="chapter5.html#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(h25<span class="sc">$</span>dir[train]) <span class="co"># frequencies</span></span></code></pre></div>
<pre><code>## Down   Up 
##  922 1026</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="chapter5.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(h25[train, ]) <span class="co"># n of ttrain observations</span></span></code></pre></div>
<pre><code>## [1] 1948</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="chapter5.html#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span><span class="fu">summary</span>(h25<span class="sc">$</span>dir[train]) <span class="sc">/</span> <span class="fu">sum</span>(train), <span class="dv">1</span>) <span class="co"># percentage</span></span></code></pre></div>
<pre><code>## Down   Up 
## 47.3 52.7</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="chapter5.html#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="do">## test set</span></span>
<span id="cb108-2"><a href="chapter5.html#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(h25<span class="sc">$</span>dir[test]) <span class="co"># frequencies</span></span></code></pre></div>
<pre><code>## Down   Up 
##  105  145</code></pre>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="chapter5.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(h25[test, ]) <span class="co"># n of test observations</span></span></code></pre></div>
<pre><code>## [1] 250</code></pre>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="chapter5.html#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span><span class="fu">summary</span>(h25<span class="sc">$</span>dir[test]) <span class="sc">/</span> <span class="fu">sum</span>(test), <span class="dv">1</span>) <span class="co"># percentage</span></span></code></pre></div>
<pre><code>## Down   Up 
##   42   58</code></pre>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="chapter5.html#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="do">## 1. training sample logistic estimations</span></span>
<span id="cb114-2"><a href="chapter5.html#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Logit</span></span>
<span id="cb114-3"><a href="chapter5.html#cb114-3" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(h25<span class="sc">$</span>dir) <span class="co"># check which one of the direction labels is 1, the probability of which is predicted by glm (here logit model)</span></span></code></pre></div>
<pre><code>##      Up
## Down  0
## Up    1</code></pre>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="chapter5.html#cb116-1" aria-hidden="true" tabindex="-1"></a>glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(dir <span class="sc">~</span> lag1 <span class="sc">+</span> lag2 <span class="sc">+</span> lag3 <span class="sc">+</span> lag4 <span class="sc">+</span> lag5,</span>
<span id="cb116-2"><a href="chapter5.html#cb116-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> logit), <span class="at">data =</span> h25, <span class="at">subset =</span> train) <span class="co"># training sample</span></span>
<span id="cb116-3"><a href="chapter5.html#cb116-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm.fit) <span class="co"># logit results</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = dir ~ lag1 + lag2 + lag3 + lag4 + lag5, family = binomial(link = logit), 
##     data = h25, subset = train)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.532  -1.218   1.061   1.133   1.399  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)  0.10853    0.04556   2.382   0.0172 *
## lag1         0.05654    0.03933   1.438   0.1506  
## lag2        -0.02007    0.03940  -0.509   0.6105  
## lag3         0.01332    0.03929   0.339   0.7346  
## lag4        -0.04831    0.03951  -1.223   0.2215  
## lag5        -0.03762    0.03929  -0.957   0.3384  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2694.9  on 1947  degrees of freedom
## Residual deviance: 2690.2  on 1942  degrees of freedom
## AIC: 2702.2
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="chapter5.html#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="do">## LDA</span></span>
<span id="cb118-2"><a href="chapter5.html#cb118-2" aria-hidden="true" tabindex="-1"></a>lda.fit <span class="ot">&lt;-</span> <span class="fu">lda</span>(dir <span class="sc">~</span> lag1 <span class="sc">+</span> lag2 <span class="sc">+</span> lag3 <span class="sc">+</span> lag4 <span class="sc">+</span> lag5,</span>
<span id="cb118-3"><a href="chapter5.html#cb118-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> h25, <span class="at">subset =</span> train)</span>
<span id="cb118-4"><a href="chapter5.html#cb118-4" aria-hidden="true" tabindex="-1"></a>lda.fit <span class="co"># show estimation results</span></span></code></pre></div>
<pre><code>## Call:
## lda(dir ~ lag1 + lag2 + lag3 + lag4 + lag5, data = h25, subset = train)
## 
## Prior probabilities of groups:
##     Down       Up 
## 0.473306 0.526694 
## 
## Group means:
##             lag1       lag2       lag3        lag4       lag5
## Down 0.002870379 0.05270834 0.03319058 0.072918626 0.06864224
## Up   0.075077291 0.02910391 0.04594629 0.009602557 0.01267777
## 
## Coefficients of linear discriminants:
##             LD1
## lag1  0.5673811
## lag2 -0.1999142
## lag3  0.1333366
## lag4 -0.4834099
## lag5 -0.3777617</code></pre>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="chapter5.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="do">## QDA</span></span>
<span id="cb120-2"><a href="chapter5.html#cb120-2" aria-hidden="true" tabindex="-1"></a>qda.fit <span class="ot">&lt;-</span> <span class="fu">qda</span>(dir <span class="sc">~</span> lag1 <span class="sc">+</span> lag2 <span class="sc">+</span> lag3 <span class="sc">+</span> lag4 <span class="sc">+</span> lag5,</span>
<span id="cb120-3"><a href="chapter5.html#cb120-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> h25, <span class="at">subset =</span> train)</span>
<span id="cb120-4"><a href="chapter5.html#cb120-4" aria-hidden="true" tabindex="-1"></a>qda.fit <span class="co"># shows only priors and sample group means</span></span></code></pre></div>
<pre><code>## Call:
## qda(dir ~ lag1 + lag2 + lag3 + lag4 + lag5, data = h25, subset = train)
## 
## Prior probabilities of groups:
##     Down       Up 
## 0.473306 0.526694 
## 
## Group means:
##             lag1       lag2       lag3        lag4       lag5
## Down 0.002870379 0.05270834 0.03319058 0.072918626 0.06864224
## Up   0.075077291 0.02910391 0.04594629 0.009602557 0.01267777</code></pre>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="chapter5.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="do">### NBayes with separate variances</span></span>
<span id="cb122-2"><a href="chapter5.html#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="do">### Naive Bayes is in package e1071</span></span>
<span id="cb122-3"><a href="chapter5.html#cb122-3" aria-hidden="true" tabindex="-1"></a><span class="do">## install.packages(pkgs = &quot;e1071&quot;, repos = &quot;https://cloud.r-project.org&quot;) # install if needed</span></span>
<span id="cb122-4"><a href="chapter5.html#cb122-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span></code></pre></div>
<pre><code>## Warning: 패키지 &#39;e1071&#39;는 R 버전 4.2.3에서 작성되었습니다</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="chapter5.html#cb124-1" aria-hidden="true" tabindex="-1"></a>nb.fit <span class="ot">&lt;-</span> <span class="fu">naiveBayes</span>(dir <span class="sc">~</span> lag1 <span class="sc">+</span> lag2 <span class="sc">+</span> lag3 <span class="sc">+</span> lag4 <span class="sc">+</span> lag5, <span class="at">data =</span> h25, <span class="at">subset =</span> train) <span class="co"># fit training data</span></span>
<span id="cb124-2"><a href="chapter5.html#cb124-2" aria-hidden="true" tabindex="-1"></a>nb.fit <span class="co"># show estimation results</span></span></code></pre></div>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##     Down       Up 
## 0.473306 0.526694 
## 
## Conditional probabilities:
##       lag1
## Y             [,1]     [,2]
##   Down 0.002870379 1.160224
##   Up   0.075077291 1.165455
## 
##       lag2
## Y            [,1]     [,2]
##   Down 0.05270834 1.127840
##   Up   0.02910391 1.194404
## 
##       lag3
## Y            [,1]     [,2]
##   Down 0.03319058 1.128859
##   Up   0.04594629 1.193155
## 
##       lag4
## Y             [,1]     [,2]
##   Down 0.072918626 1.165713
##   Up   0.009602557 1.159927
## 
##       lag5
## Y            [,1]     [,2]
##   Down 0.06864224 1.147725
##   Up   0.01267777 1.175936</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="chapter5.html#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(h25[train <span class="sc">&amp;</span> h25<span class="sc">$</span>dir <span class="sc">==</span> <span class="st">&quot;Down&quot;</span>, <span class="st">&quot;lag1&quot;</span>])</span></code></pre></div>
<pre><code>## [1] 0.002870379</code></pre>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="chapter5.html#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(h25[train <span class="sc">&amp;</span> h25<span class="sc">$</span>dir <span class="sc">==</span> <span class="st">&quot;Down&quot;</span>, <span class="st">&quot;lag1&quot;</span>])</span></code></pre></div>
<pre><code>## [1] 1.160224</code></pre>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="chapter5.html#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(h25[train <span class="sc">&amp;</span> h25<span class="sc">$</span>dir <span class="sc">==</span> <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;lag1&quot;</span>])</span></code></pre></div>
<pre><code>## [1] 0.07507729</code></pre>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="chapter5.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(h25[train <span class="sc">&amp;</span> h25<span class="sc">$</span>dir <span class="sc">==</span> <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;lag1&quot;</span>])</span></code></pre></div>
<pre><code>## [1] 1.165455</code></pre>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="chapter5.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(h25[train, ])</span></code></pre></div>
<pre><code>##      open    high     low   close  aclose       date         ret        lag1
## 1 2381.21 2384.33 2366.79 2372.03 2372.03 2013-03-13 -0.35287336  0.04917168
## 2 2376.36 2385.45 2371.20 2381.16 2381.16 2013-03-14  0.38489745 -0.35287336
## 3 2384.63 2384.98 2368.38 2371.20 2371.20 2013-03-15 -0.41828190  0.38489745
## 4 2338.66 2362.51 2336.75 2360.83 2360.83 2013-03-18 -0.43732596 -0.41828190
## 5 2359.13 2380.13 2352.64 2359.00 2359.00 2013-03-19 -0.07751841 -0.43732596
## 6 2365.53 2376.89 2365.45 2373.46 2373.46 2013-03-20  0.61296994 -0.07751841
##          lag2        lag3         lag4         lag5  dir
## 1 -0.37100759  0.47247557  0.005885895 -0.086600399 Down
## 2  0.04917168 -0.37100759  0.472475567  0.005885895   Up
## 3 -0.35287336  0.04917168 -0.371007595  0.472475567 Down
## 4  0.38489745 -0.35287336  0.049171675 -0.371007595 Down
## 5 -0.41828190  0.38489745 -0.352873356  0.049171675 Down
## 6 -0.43732596 -0.41828190  0.384897446 -0.352873356   Up</code></pre>
<p><br></p>
<ul>
<li><p>In each lag-matrix the first column gives the means and the second
the standard deviations of the lagged returns in the Down and Up
classes.</p></li>
<li><p>Below are confusion matrices for the 2018 test data (predictions).</p></li>
</ul>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="chapter5.html#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="do">## 2. test sample predictions</span></span>
<span id="cb136-2"><a href="chapter5.html#cb136-2" aria-hidden="true" tabindex="-1"></a>glm.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(glm.fit, <span class="at">newdata =</span> h25[test, ], <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb136-3"><a href="chapter5.html#cb136-3" aria-hidden="true" tabindex="-1"></a>lda.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda.fit, <span class="at">newdata =</span> h25[test, ], <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb136-4"><a href="chapter5.html#cb136-4" aria-hidden="true" tabindex="-1"></a>qda.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(qda.fit, <span class="at">newdata =</span> h25[test, ], <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb136-5"><a href="chapter5.html#cb136-5" aria-hidden="true" tabindex="-1"></a>nb.probs <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb.fit, <span class="at">newdata =</span> h25[test, ], <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>)</span>
<span id="cb136-6"><a href="chapter5.html#cb136-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(nb.probs)</span></code></pre></div>
<pre><code>##           Down        Up
## [1,] 0.5108355 0.4891645
## [2,] 0.5001210 0.4998790
## [3,] 0.5215819 0.4784181
## [4,] 0.4506993 0.5493007
## [5,] 0.4234230 0.5765770
## [6,] 0.4206130 0.5793870</code></pre>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="chapter5.html#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(lda.probs<span class="sc">$</span>posterior)</span></code></pre></div>
<pre><code>##           Down        Up
## 1949 0.4988495 0.5011505
## 1950 0.4789847 0.5210153
## 1951 0.5030180 0.4969820
## 1952 0.4263443 0.5736557
## 1953 0.4900092 0.5099908
## 1954 0.4764840 0.5235160</code></pre>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="chapter5.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">predict</span>(nb.fit, <span class="at">newdata =</span> h25[test, ]), h25[test, <span class="st">&quot;dir&quot;</span>])</span></code></pre></div>
<pre><code>##       
##        Down Up
##   Down   42 58
##   Up     63 87</code></pre>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="chapter5.html#cb142-1" aria-hidden="true" tabindex="-1"></a>dir.test <span class="ot">&lt;-</span> h25<span class="sc">$</span>dir[test] <span class="co"># 2021 actual directions</span></span>
<span id="cb142-2"><a href="chapter5.html#cb142-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-3"><a href="chapter5.html#cb142-3" aria-hidden="true" tabindex="-1"></a>glm.test <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(glm.probs <span class="sc">&gt;</span> .<span class="dv">5</span>, <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;Down&quot;</span>))</span>
<span id="cb142-4"><a href="chapter5.html#cb142-4" aria-hidden="true" tabindex="-1"></a>lda.test <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(lda.probs<span class="sc">$</span>posterior[, <span class="st">&quot;Up&quot;</span>] <span class="sc">&gt;</span> .<span class="dv">5</span>, <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;Down&quot;</span>))</span>
<span id="cb142-5"><a href="chapter5.html#cb142-5" aria-hidden="true" tabindex="-1"></a>qda.test <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(qda.probs<span class="sc">$</span>posterior[, <span class="st">&quot;Up&quot;</span>] <span class="sc">&gt;</span> .<span class="dv">5</span>, <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;Down&quot;</span>))</span>
<span id="cb142-6"><a href="chapter5.html#cb142-6" aria-hidden="true" tabindex="-1"></a>nb.test  <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(nb.probs[, <span class="st">&quot;Up&quot;</span>] <span class="sc">&gt;</span> .<span class="dv">5</span>, <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;Down&quot;</span>))</span>
<span id="cb142-7"><a href="chapter5.html#cb142-7" aria-hidden="true" tabindex="-1"></a>knn.test.k1 <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> h25[train, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-8"><a href="chapter5.html#cb142-8" aria-hidden="true" tabindex="-1"></a>                     <span class="at">test =</span> h25[test, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-9"><a href="chapter5.html#cb142-9" aria-hidden="true" tabindex="-1"></a>                     <span class="at">cl =</span> h25<span class="sc">$</span>dir[train],</span>
<span id="cb142-10"><a href="chapter5.html#cb142-10" aria-hidden="true" tabindex="-1"></a>                     <span class="at">k =</span> <span class="dv">1</span>) <span class="co"># knn with K = 1</span></span>
<span id="cb142-11"><a href="chapter5.html#cb142-11" aria-hidden="true" tabindex="-1"></a>knn.test.k2 <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> h25[train, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-12"><a href="chapter5.html#cb142-12" aria-hidden="true" tabindex="-1"></a>                     <span class="at">test =</span> h25[test, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-13"><a href="chapter5.html#cb142-13" aria-hidden="true" tabindex="-1"></a>                     <span class="at">cl =</span> h25<span class="sc">$</span>dir[train],</span>
<span id="cb142-14"><a href="chapter5.html#cb142-14" aria-hidden="true" tabindex="-1"></a>                     <span class="at">k =</span> <span class="dv">2</span>) <span class="co"># knn with K = 2</span></span>
<span id="cb142-15"><a href="chapter5.html#cb142-15" aria-hidden="true" tabindex="-1"></a>knn.test.k3 <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> h25[train, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-16"><a href="chapter5.html#cb142-16" aria-hidden="true" tabindex="-1"></a>                     <span class="at">test =</span> h25[test, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-17"><a href="chapter5.html#cb142-17" aria-hidden="true" tabindex="-1"></a>                     <span class="at">cl =</span> h25<span class="sc">$</span>dir[train],</span>
<span id="cb142-18"><a href="chapter5.html#cb142-18" aria-hidden="true" tabindex="-1"></a>                     <span class="at">k =</span> <span class="dv">3</span>) <span class="co"># knn with K = 3</span></span>
<span id="cb142-19"><a href="chapter5.html#cb142-19" aria-hidden="true" tabindex="-1"></a>knn.test.k5 <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> h25[train, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-20"><a href="chapter5.html#cb142-20" aria-hidden="true" tabindex="-1"></a>                     <span class="at">test =</span> h25[test, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-21"><a href="chapter5.html#cb142-21" aria-hidden="true" tabindex="-1"></a>                     <span class="at">cl =</span> h25<span class="sc">$</span>dir[train],</span>
<span id="cb142-22"><a href="chapter5.html#cb142-22" aria-hidden="true" tabindex="-1"></a>                     <span class="at">k =</span> <span class="dv">5</span>) <span class="co"># knn with K = 5</span></span>
<span id="cb142-23"><a href="chapter5.html#cb142-23" aria-hidden="true" tabindex="-1"></a>knn.test.k10 <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> h25[train, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-24"><a href="chapter5.html#cb142-24" aria-hidden="true" tabindex="-1"></a>                     <span class="at">test =</span> h25[test, <span class="fu">c</span>(<span class="st">&quot;lag1&quot;</span>, <span class="st">&quot;lag2&quot;</span>, <span class="st">&quot;lag3&quot;</span>, <span class="st">&quot;lag4&quot;</span>, <span class="st">&quot;lag5&quot;</span>)],</span>
<span id="cb142-25"><a href="chapter5.html#cb142-25" aria-hidden="true" tabindex="-1"></a>                     <span class="at">cl =</span> h25<span class="sc">$</span>dir[train],</span>
<span id="cb142-26"><a href="chapter5.html#cb142-26" aria-hidden="true" tabindex="-1"></a>                     <span class="at">k =</span> <span class="dv">10</span>) <span class="co"># knn with K = 10</span></span>
<span id="cb142-27"><a href="chapter5.html#cb142-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-28"><a href="chapter5.html#cb142-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-29"><a href="chapter5.html#cb142-29" aria-hidden="true" tabindex="-1"></a><span class="co">#help(knn)</span></span>
<span id="cb142-30"><a href="chapter5.html#cb142-30" aria-hidden="true" tabindex="-1"></a>cfm <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> <span class="dv">9</span> <span class="sc">*</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb142-31"><a href="chapter5.html#cb142-31" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(cfm) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;lgtDown&quot;</span>, <span class="st">&quot;lgtUp&quot;</span>, <span class="st">&quot;ldaDown&quot;</span>, <span class="st">&quot;ldaUp&quot;</span>, <span class="st">&quot;qdaDown&quot;</span>, <span class="st">&quot;qdaUp&quot;</span>, <span class="st">&quot;nbUp&quot;</span>, <span class="st">&quot;nbDown&quot;</span>,</span>
<span id="cb142-32"><a href="chapter5.html#cb142-32" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;knn1Up&quot;</span>, <span class="st">&quot;knn1Down&quot;</span>, <span class="st">&quot;knn2Up&quot;</span>, <span class="st">&quot;knn2Down&quot;</span>, <span class="st">&quot;knn3Up&quot;</span>, <span class="st">&quot;knn3Down&quot;</span>, <span class="st">&quot;knn5Up&quot;</span>, <span class="st">&quot;knn5Down&quot;</span>, <span class="st">&quot;knn10Up&quot;</span>, <span class="st">&quot;knn10Down&quot;</span>,</span>
<span id="cb142-33"><a href="chapter5.html#cb142-33" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;Act Total&quot;</span>) <span class="co"># column names for the matrix</span></span>
<span id="cb142-34"><a href="chapter5.html#cb142-34" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(cfm) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Actual Down&quot;</span>, <span class="st">&quot;Actual Up&quot;</span>, <span class="st">&quot;Pred Total&quot;</span>)</span>
<span id="cb142-35"><a href="chapter5.html#cb142-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-36"><a href="chapter5.html#cb142-36" aria-hidden="true" tabindex="-1"></a><span class="do">## collect confusion matrices into one table</span></span>
<span id="cb142-37"><a href="chapter5.html#cb142-37" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">table</span>(dir.test, glm.test), <span class="fu">colSums</span>(<span class="fu">table</span>(dir.test, glm.test))) <span class="co"># logit</span></span>
<span id="cb142-38"><a href="chapter5.html#cb142-38" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">table</span>(dir.test, lda.test), <span class="fu">colSums</span>(<span class="fu">table</span>(dir.test, lda.test))) <span class="co"># LDA</span></span>
<span id="cb142-39"><a href="chapter5.html#cb142-39" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">5</span><span class="sc">:</span><span class="dv">6</span>] <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">table</span>(dir.test, qda.test), <span class="fu">colSums</span>(<span class="fu">table</span>(dir.test, qda.test))) <span class="co"># QDA</span></span>
<span id="cb142-40"><a href="chapter5.html#cb142-40" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">7</span><span class="sc">:</span><span class="dv">8</span>] <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">table</span>(dir.test, nb.test), <span class="fu">colSums</span>(<span class="fu">table</span>(dir.test, nb.test))) <span class="co"># NBayes</span></span>
<span id="cb142-41"><a href="chapter5.html#cb142-41" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">9</span><span class="sc">:</span><span class="dv">10</span>] <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">table</span>(dir.test, knn.test.k1), <span class="fu">colSums</span>(<span class="fu">table</span>(dir.test, knn.test.k1))) <span class="co"># KNN, K = 1</span></span>
<span id="cb142-42"><a href="chapter5.html#cb142-42" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">11</span><span class="sc">:</span><span class="dv">12</span>] <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">table</span>(dir.test, knn.test.k2), <span class="fu">colSums</span>(<span class="fu">table</span>(dir.test, knn.test.k2))) <span class="co"># KNN, K = 2</span></span>
<span id="cb142-43"><a href="chapter5.html#cb142-43" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">13</span><span class="sc">:</span><span class="dv">14</span>] <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">table</span>(dir.test, knn.test.k3), <span class="fu">colSums</span>(<span class="fu">table</span>(dir.test, knn.test.k3))) <span class="co"># KNN, K = 3</span></span>
<span id="cb142-44"><a href="chapter5.html#cb142-44" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">15</span><span class="sc">:</span><span class="dv">16</span>] <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">table</span>(dir.test, knn.test.k5), <span class="fu">colSums</span>(<span class="fu">table</span>(dir.test, knn.test.k5))) <span class="co"># KNN, K = 5</span></span>
<span id="cb142-45"><a href="chapter5.html#cb142-45" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">17</span><span class="sc">:</span><span class="dv">18</span>] <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">table</span>(dir.test, knn.test.k10), <span class="fu">colSums</span>(<span class="fu">table</span>(dir.test, knn.test.k10))) <span class="co"># KNN, K = 10</span></span>
<span id="cb142-46"><a href="chapter5.html#cb142-46" aria-hidden="true" tabindex="-1"></a>cfm[ , <span class="dv">19</span>] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">sum</span>(dir.test <span class="sc">==</span> <span class="st">&quot;Down&quot;</span>), <span class="fu">sum</span>(dir.test <span class="sc">==</span> <span class="st">&quot;Up&quot;</span>), <span class="fu">length</span>(dir.test)) <span class="co"># Actual totals</span></span>
<span id="cb142-47"><a href="chapter5.html#cb142-47" aria-hidden="true" tabindex="-1"></a>cfm</span></code></pre></div>
<pre><code>##             lgtDown lgtUp ldaDown ldaUp qdaDown qdaUp nbUp nbDown knn1Up
## Actual Down       8    97       7    98      54    51   42     63     54
## Actual Up        11   134      11   134      62    83   58     87     71
## Pred Total       19   231      18   232     116   134  100    150    125
##             knn1Down knn2Up knn2Down knn3Up knn3Down knn5Up knn5Down knn10Up
## Actual Down       51     54       51     56       49     53       52      49
## Actual Up         74     65       80     68       77     68       77      63
## Pred Total       125    119      131    124      126    121      129     112
##             knn10Down Act Total
## Actual Down        56       105
## Actual Up          82       145
## Pred Total        138       250</code></pre>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="chapter5.html#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="do">## percents of correct predicted directions</span></span>
<span id="cb144-2"><a href="chapter5.html#cb144-2" aria-hidden="true" tabindex="-1"></a>pcts <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">mean</span>(glm.test <span class="sc">==</span> dir.test), <span class="fu">mean</span>(lda.test <span class="sc">==</span> dir.test), <span class="fu">mean</span>(qda.test <span class="sc">==</span> dir.test), <span class="fu">mean</span>(nb.test <span class="sc">==</span> dir.test),</span>
<span id="cb144-3"><a href="chapter5.html#cb144-3" aria-hidden="true" tabindex="-1"></a>          <span class="fu">mean</span>(knn.test.k1 <span class="sc">==</span> dir.test), <span class="fu">mean</span>(knn.test.k2 <span class="sc">==</span> dir.test), <span class="fu">mean</span>(knn.test.k3 <span class="sc">==</span> dir.test),</span>
<span id="cb144-4"><a href="chapter5.html#cb144-4" aria-hidden="true" tabindex="-1"></a>          <span class="fu">mean</span>(knn.test.k5 <span class="sc">==</span> dir.test), <span class="fu">mean</span>(knn.test.k10 <span class="sc">==</span> dir.test),</span>
<span id="cb144-5"><a href="chapter5.html#cb144-5" aria-hidden="true" tabindex="-1"></a>          <span class="fu">mean</span>(dir.test <span class="sc">==</span> <span class="st">&quot;Up&quot;</span>)) <span class="co"># the last one is percentage of correct prediction of a &quot;Null&quot; classifier that predicts always &quot;Up&quot;.</span></span>
<span id="cb144-6"><a href="chapter5.html#cb144-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-7"><a href="chapter5.html#cb144-7" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(pcts) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Logit&quot;</span>, <span class="st">&quot;LDA&quot;</span>, <span class="st">&quot;QDA&quot;</span>, <span class="st">&quot;NBayes&quot;</span>, <span class="st">&quot;KNN K=1&quot;</span>, <span class="st">&quot;KNN K=2&quot;</span>, <span class="st">&quot;KNN K=3&quot;</span>, <span class="st">&quot;KNN K=5&quot;</span>, <span class="st">&quot;KNN K=10&quot;</span>, <span class="st">&quot;Null&quot;</span>)  <span class="co"># for printing</span></span>
<span id="cb144-8"><a href="chapter5.html#cb144-8" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> pcts, <span class="at">digits =</span> <span class="dv">1</span>) <span class="co"># show results in percents rounded to 1 decimal</span></span></code></pre></div>
<pre><code>##    Logit      LDA      QDA   NBayes  KNN K=1  KNN K=2  KNN K=3  KNN K=5 
##     56.8     56.4     54.8     51.6     51.2     53.6     53.2     52.0 
## KNN K=10     Null 
##     52.4     58.0</code></pre>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="chapter5.html#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="do">### Error rates:</span></span>
<span id="cb146-2"><a href="chapter5.html#cb146-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Total error rates: Misclassifcation percentages</span></span>
<span id="cb146-3"><a href="chapter5.html#cb146-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> pcts), <span class="at">digits =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##    Logit      LDA      QDA   NBayes  KNN K=1  KNN K=2  KNN K=3  KNN K=5 
##     43.2     43.6     45.2     48.4     48.8     46.4     46.8     48.0 
## KNN K=10     Null 
##     47.6     42.0</code></pre>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="chapter5.html#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Error rates in predicting Down directions</span></span>
<span id="cb148-2"><a href="chapter5.html#cb148-2" aria-hidden="true" tabindex="-1"></a>down.errors <span class="ot">&lt;-</span> cfm[<span class="dv">1</span>, <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">2</span>, <span class="at">to =</span> <span class="dv">18</span>, <span class="at">by =</span> <span class="dv">2</span>)] <span class="sc">/</span> cfm[<span class="dv">1</span>, <span class="fu">ncol</span>(cfm)] <span class="co"># pick up &quot;Down&quot; columns from the table</span></span>
<span id="cb148-3"><a href="chapter5.html#cb148-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(down.errors) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Logit&quot;</span>, <span class="st">&quot;LDA&quot;</span>, <span class="st">&quot;QDA&quot;</span>, <span class="st">&quot;NBayes&quot;</span>, <span class="st">&quot;KNN 1&quot;</span>, <span class="st">&quot;KNN 2&quot;</span>, <span class="st">&quot;KNN 3&quot;</span>, <span class="st">&quot;KNN 5&quot;</span>, <span class="st">&quot;KNN 10&quot;</span>) <span class="co"># name the elements</span></span>
<span id="cb148-4"><a href="chapter5.html#cb148-4" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> down.errors, <span class="at">digits =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##  Logit    LDA    QDA NBayes  KNN 1  KNN 2  KNN 3  KNN 5 KNN 10 
##   92.4   93.3   48.6   60.0   48.6   48.6   46.7   49.5   53.3</code></pre>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="chapter5.html#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Error rates of predicting Up directions</span></span>
<span id="cb150-2"><a href="chapter5.html#cb150-2" aria-hidden="true" tabindex="-1"></a>up.errors <span class="ot">&lt;-</span> cfm[<span class="dv">2</span>, <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">1</span>, <span class="at">to =</span> <span class="dv">17</span>, <span class="at">by =</span> <span class="dv">2</span>)] <span class="sc">/</span> cfm[<span class="dv">2</span>, <span class="fu">ncol</span>(cfm)] <span class="co"># pick up &quot;Up&quot; columns</span></span>
<span id="cb150-3"><a href="chapter5.html#cb150-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(up.errors) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Logit&quot;</span>, <span class="st">&quot;LDA&quot;</span>, <span class="st">&quot;QDA&quot;</span>, <span class="st">&quot;NBayes&quot;</span>, <span class="st">&quot;KNN 1&quot;</span>, <span class="st">&quot;KNN 2&quot;</span>, <span class="st">&quot;KNN 3&quot;</span>, <span class="st">&quot;KNN 5&quot;</span>, <span class="st">&quot;KNN 10&quot;</span>) <span class="co"># name the columns</span></span>
<span id="cb150-4"><a href="chapter5.html#cb150-4" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> up.errors, <span class="at">digits =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##  Logit    LDA    QDA NBayes  KNN 1  KNN 2  KNN 3  KNN 5 KNN 10 
##    7.6    7.6   42.8   40.0   49.0   44.8   46.9   46.9   43.4</code></pre>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="chapter5.html#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb152-2"><a href="chapter5.html#cb152-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Sensitivity: Here percentage of &quot;true positive cases, i.e. true Up directions&quot; that are identified</span></span>
<span id="cb152-3"><a href="chapter5.html#cb152-3" aria-hidden="true" tabindex="-1"></a>snsty <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> up.errors</span>
<span id="cb152-4"><a href="chapter5.html#cb152-4" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> snsty, <span class="at">digits =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##  Logit    LDA    QDA NBayes  KNN 1  KNN 2  KNN 3  KNN 5 KNN 10 
##   92.4   92.4   57.2   60.0   51.0   55.2   53.1   53.1   56.6</code></pre>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="chapter5.html#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Specificity: Here percentage of &quot;true negative cases, i.e., Down directions&quot; that are identified</span></span>
<span id="cb154-2"><a href="chapter5.html#cb154-2" aria-hidden="true" tabindex="-1"></a>spcty <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> down.errors</span>
<span id="cb154-3"><a href="chapter5.html#cb154-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> spcty, <span class="at">digits =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##  Logit    LDA    QDA NBayes  KNN 1  KNN 2  KNN 3  KNN 5 KNN 10 
##    7.6    6.7   51.4   40.0   51.4   51.4   53.3   50.5   46.7</code></pre>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="chapter5.html#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb156-2"><a href="chapter5.html#cb156-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Null classifier, i.e. classify all returns &quot;Up&quot;</span></span>
<span id="cb156-3"><a href="chapter5.html#cb156-3" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb156-4"><a href="chapter5.html#cb156-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Total Error rate</span></span>
<span id="cb156-5"><a href="chapter5.html#cb156-5" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> <span class="fu">mean</span>(dir.test <span class="sc">==</span> <span class="st">&quot;Down&quot;</span>), <span class="at">digits =</span> <span class="dv">1</span>) <span class="co"># error rate equals the rate of down directions</span></span></code></pre></div>
<pre><code>## [1] 42</code></pre>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="chapter5.html#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Up error rate (percentage of missed actual Up directions) is trivially zero as all predictions are &quot;Up&quot;,</span></span>
<span id="cb158-2"><a href="chapter5.html#cb158-2" aria-hidden="true" tabindex="-1"></a><span class="co"># therfore here sensitivity is trivially 100%</span></span>
<span id="cb158-3"><a href="chapter5.html#cb158-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Down error rate (percentage of missed actual Down directions) is trivially 100% as no &quot;Down&quot; predictions,</span></span>
<span id="cb158-4"><a href="chapter5.html#cb158-4" aria-hidden="true" tabindex="-1"></a><span class="co"># therefore specificity is trivially 0%.</span></span></code></pre></div>
<ul>
<li>According the correct prediction rates
<ul>
<li>Logit is the winner with close to 56.8 % correct prediction rate.</li>
<li>LDA, and QDA close to Logit with percentages 56.4 and 54.8, respectively.</li>
<li>KNN with <span class="math inline">\(K = 3\)</span> performs best of the remaining ones, not much better than pure guessing.
<ul>
<li>It is notable that in the test period 58 % of the returns were positive, i.e., Up direction. Thus, with the null classifier, predicting continuously Up would have produced 58 % (i.e., the actual Up-percentage in the test period which happens to be the same as that of the training period) rate of correct predictions. Thus, the null classier is the ultimate winner here!!</li>
</ul></li>
</ul></li>
</ul>
<p><br></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ILR.pdf", "ILR.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
