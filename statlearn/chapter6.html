<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Resampling Methods | Statistical Learning</title>
  <meta name="description" content="This is a Statistical Learning" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Resampling Methods | Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Statistical Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Resampling Methods | Statistical Learning" />
  
  <meta name="twitter:description" content="This is a Statistical Learning" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2023-05-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter5.html"/>
<link rel="next" href="chapter7.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>머리말</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#understanding-data"><i class="fa fa-check"></i><b>1.1</b> Understanding Data</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="chapter1.html"><a href="chapter1.html#example-1-supervised-learning-continuous-output"><i class="fa fa-check"></i><b>1.1.1</b> Example 1 (Supervised learning: Continuous output)</a></li>
<li class="chapter" data-level="1.1.2" data-path="chapter1.html"><a href="chapter1.html#example-2-supervised-leraning-categorical-output"><i class="fa fa-check"></i><b>1.1.2</b> Example 2 (Supervised leraning: Categorical output)</a></li>
<li class="chapter" data-level="1.1.3" data-path="chapter1.html"><a href="chapter1.html#example-3-unsupervised-learning-clustering-observations"><i class="fa fa-check"></i><b>1.1.3</b> Example 3 (Unsupervised learning: Clustering observations)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#brief-history-of-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Brief History of Statistical Learning</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#notation-and-simple-matrix-algebra"><i class="fa fa-check"></i><b>1.3</b> Notation and Simple Matrix Algebra</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#what-is-statitical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statitical Learning</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chapter2.html"><a href="chapter2.html#prediction"><i class="fa fa-check"></i><b>2.1.1</b> Prediction</a></li>
<li class="chapter" data-level="2.1.2" data-path="chapter2.html"><a href="chapter2.html#inference"><i class="fa fa-check"></i><b>2.1.2</b> Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="chapter2.html"><a href="chapter2.html#estimating-f"><i class="fa fa-check"></i><b>2.1.3</b> Estimating <span class="math inline">\(f\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="chapter2.html"><a href="chapter2.html#prediction-accuaracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.4</b> Prediction Accuaracy and Model Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter2.html"><a href="chapter2.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="chapter2.html"><a href="chapter2.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="2.2.3" data-path="chapter2.html"><a href="chapter2.html#classification-problems"><i class="fa fa-check"></i><b>2.2.3</b> Classification Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#basic-commands"><i class="fa fa-check"></i><b>3.1</b> Basic Commands</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#graphics"><i class="fa fa-check"></i><b>3.2</b> Graphics</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#indexing-data"><i class="fa fa-check"></i><b>3.3</b> Indexing Data</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#loading-data"><i class="fa fa-check"></i><b>3.4</b> Loading Data</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#additional-graphical-and-numerical-summaries"><i class="fa fa-check"></i><b>3.5</b> Additional Graphical and Numerical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#simple-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Regression</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#multiple-regression"><i class="fa fa-check"></i><b>4.2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chapter4.html"><a href="chapter4.html#importance-of-predictors-statistical-significance-of-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Importance of Predictors: Statistical Significance of Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="chapter4.html"><a href="chapter4.html#selecting-important-variables"><i class="fa fa-check"></i><b>4.2.2</b> Selecting Important Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="chapter4.html"><a href="chapter4.html#model-fit"><i class="fa fa-check"></i><b>4.2.3</b> Model Fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="chapter4.html"><a href="chapter4.html#prediction-1"><i class="fa fa-check"></i><b>4.2.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#other-consideration"><i class="fa fa-check"></i><b>4.3</b> Other Consideration</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chapter4.html"><a href="chapter4.html#qualitative-predictors"><i class="fa fa-check"></i><b>4.3.1</b> Qualitative Predictors</a></li>
<li class="chapter" data-level="" data-path="chapter4.html"><a href="chapter4.html#example-8"><i class="fa fa-check"></i>Example 8</a></li>
<li class="chapter" data-level="4.3.2" data-path="chapter4.html"><a href="chapter4.html#potential-problems"><i class="fa fa-check"></i><b>4.3.2</b> Potential Problems</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#non-parametric-regressions"><i class="fa fa-check"></i><b>4.4</b> Non-parametric Regressions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#logit-and-probit-models"><i class="fa fa-check"></i><b>5.1</b> Logit and Probit Models</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#confounding"><i class="fa fa-check"></i><b>5.2</b> Confounding</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#logit-model-for-multiple-classes"><i class="fa fa-check"></i><b>5.3</b> Logit Model for Multiple Classes</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#discriminant-analysis"><i class="fa fa-check"></i><b>5.4</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chapter5.html"><a href="chapter5.html#bayes-theorem-for-classification"><i class="fa fa-check"></i><b>5.4.1</b> Bayes Theorem for Classification</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapter5.html"><a href="chapter5.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="5.4.3" data-path="chapter5.html"><a href="chapter5.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.3</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#naive-bayes"><i class="fa fa-check"></i><b>5.5</b> Naive Bayes</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#knn-classification"><i class="fa fa-check"></i><b>5.6</b> KNN Classification</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#example-10"><i class="fa fa-check"></i><b>5.7</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Resampling Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#validation-set-approach"><i class="fa fa-check"></i><b>6.1</b> Validation Set Approach</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.2</b> K-fold Cross-validation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chapter6.html"><a href="chapter6.html#example-11"><i class="fa fa-check"></i><b>6.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#bootstrap"><i class="fa fa-check"></i><b>6.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Model Selection</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#variable-selection"><i class="fa fa-check"></i><b>7.1</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chapter7.html"><a href="chapter7.html#best-subset-selection"><i class="fa fa-check"></i><b>7.1.1</b> Best Subset Selection</a></li>
<li class="chapter" data-level="7.1.2" data-path="chapter7.html"><a href="chapter7.html#forward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.2</b> Forward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.3" data-path="chapter7.html"><a href="chapter7.html#backward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.3</b> Backward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.4" data-path="chapter7.html"><a href="chapter7.html#choosing-the-optimal-model"><i class="fa fa-check"></i><b>7.1.4</b> Choosing the Optimal Model</a></li>
<li class="chapter" data-level="7.1.5" data-path="chapter7.html"><a href="chapter7.html#example-13"><i class="fa fa-check"></i><b>7.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#shrinkage-methods"><i class="fa fa-check"></i><b>7.2</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chapter7.html"><a href="chapter7.html#ridge-regression"><i class="fa fa-check"></i><b>7.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="chapter7.html"><a href="chapter7.html#rasso-regression"><i class="fa fa-check"></i><b>7.2.2</b> RASSO Regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="chapter7.html"><a href="chapter7.html#example-14"><i class="fa fa-check"></i><b>7.2.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#dimension-reduction-methods"><i class="fa fa-check"></i><b>7.3</b> Dimension Reduction Methods</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="chapter7.html"><a href="chapter7.html#principal-components-regression"><i class="fa fa-check"></i><b>7.3.1</b> Principal Components Regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="chapter7.html"><a href="chapter7.html#example-15"><i class="fa fa-check"></i><b>7.3.2</b> Example</a></li>
<li class="chapter" data-level="7.3.3" data-path="chapter7.html"><a href="chapter7.html#partial-least-squares"><i class="fa fa-check"></i><b>7.3.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="7.3.4" data-path="chapter7.html"><a href="chapter7.html#example-16"><i class="fa fa-check"></i><b>7.3.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#polynomial-regression"><i class="fa fa-check"></i><b>8.1</b> Polynomial Regression</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#step-functions"><i class="fa fa-check"></i><b>8.2</b> Step Functions</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#piecewise-polynomials---splines"><i class="fa fa-check"></i><b>8.3</b> Piecewise Polynomials - Splines</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#local-regression"><i class="fa fa-check"></i><b>8.4</b> Local Regression</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#generalized-additive-models"><i class="fa fa-check"></i><b>8.5</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Tree-based Methods</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#the-basiscs-of-decision-trees"><i class="fa fa-check"></i><b>9.1</b> The Basiscs of Decision Trees</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#classification-trees"><i class="fa fa-check"></i><b>9.2</b> Classification Trees</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#bagging"><i class="fa fa-check"></i><b>9.3</b> Bagging</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#random-forests"><i class="fa fa-check"></i><b>9.4</b> Random Forests</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#boosing"><i class="fa fa-check"></i><b>9.5</b> Boosing</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Support Vector Machines</a></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#principal-components-anlsysis"><i class="fa fa-check"></i><b>11.1</b> Principal Components Anlsysis</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#clustering"><i class="fa fa-check"></i><b>11.2</b> Clustering</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chapter11.html"><a href="chapter11.html#k-means-clustering"><i class="fa fa-check"></i><b>11.2.1</b> <span class="math inline">\(K\)</span>-means clustering</a></li>
<li class="chapter" data-level="11.2.2" data-path="chapter11.html"><a href="chapter11.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.2.2</b> Hierarchical Clustering</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter6" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Resampling Methods<a href="chapter6.html#chapter6" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li><p>In this section we discuss two resampling methods: <em>cross-validation</em> and the <em>bootstrap</em>.</p></li>
<li><p>Resampling</p>
<ul>
<li>Repeatedly drawing samples from a training sample</li>
<li>Refitting a model in each sample to obtain additional information of the fitted model.
<ul>
<li>Variability of the estimated coefficients.</li>
<li>More accurate and robust standard errors.</li>
<li>Prediction errors (MSE)</li>
</ul></li>
</ul></li>
<li><p>Two most commonly methods are <em>cross-validation</em> and <em>bootstrap</em>.</p></li>
<li><p>These both can be utilized in</p>
<ul>
<li>Model assessment.
<ul>
<li>The process of evaluating model performance.</li>
</ul></li>
<li>Model selection.
<ul>
<li>Process of selection the proper level of flexibility for a model.
<br></li>
</ul></li>
</ul></li>
</ul>
<div id="training-error-versus-test-error" class="section level4 unnumbered hasAnchor">
<h4>Training Error versus Test error<a href="chapter6.html#training-error-versus-test-error" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Test error rate
<ul>
<li>The test error rate is the average error rate that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.</li>
</ul></li>
<li>Training error rate
<ul>
<li>The training error rate can be easily calculated by applying the statistical learning method to the observations used in its training data.</li>
</ul></li>
<li>The training error rate often is quite different from the test error rate, and in particular the former can <em>dramatically underestimate</em> the latter.</li>
</ul>
<p><img src="fig6/f1.png" /></p>
<ul>
<li>Best solution: a large designated test set.
<ul>
<li>Often not available.</li>
</ul></li>
<li>Some methods make a <em>mathematical adjustment</em> to the training error rate in order to estimate the test error rate.
<ul>
<li>These include the <em><span class="math inline">\(C_p\)</span> statistic</em>, <em>AIC</em> and <em>BIC</em></li>
</ul></li>
<li>In the absence of test set that can be used to directly estimate the error rate, cross-validation approaches estimate the test error rate by <em>holding out</em> a randomly selected subset of the training set and estimate the rates from the held out observations.</li>
</ul>
</div>
<div id="validation-set-approach" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Validation Set Approach<a href="chapter6.html#validation-set-approach" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Here we randomly divide the available set of samples into two parts: a <em>training set</em> and a <em>validation</em> or <em>hold-out set</em>.</p></li>
<li><p>The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.</p></li>
<li><p>The resulting validation-set error provides an estimate of the test error.</p>
<ul>
<li>This is typically assessed using MSE in the case of a quantitative response and misclassification rate in the case of a qualitative (discrete) response.</li>
</ul></li>
<li><p>A random splitting into two halves: left part is training set, right part is validation set.</p></li>
</ul>
<p><img src="fig6/ex41.JPG" /></p>
<p>(Source James et al. (2013), Fig 5.2)</p>
<div id="example-automobile-data" class="section level4 unnumbered hasAnchor">
<h4>Example: automobile data<a href="chapter6.html#example-automobile-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Want to compare linear vs higher-order polynomial terms in a linear regression.</p></li>
<li><p>We randomly split the 392 observations into two sets, a training set containing 196 of the data points, and a validation set containing the remaining 196 observations.</p></li>
</ul>
<p><img src="fig6/f2.png" /></p>
<ul>
<li><p>Left panel shows single split; right panel shows multiple splits.</p></li>
<li><p>Issues</p>
<ul>
<li>Validation estimate of the test error (like MSE) can be highly variable, depending on precisely which observations are included in the training set and which one in the validation set.</li>
<li>In the validation approach, only a subset of the observations - those that are included in the training set rather than in the validation set - are used to fit the model.</li>
<li>This suggests that the validation set error may tend to <em>overestimate</em> the test error for the model fit on the entire data set.</li>
</ul></li>
</ul>
</div>
</div>
<div id="k-fold-cross-validation" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> K-fold Cross-validation<a href="chapter6.html#k-fold-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><em>Widely used approach</em> for estimating test error.</p></li>
<li><p>Estimates can be used to select best model, and to give an idea of the test error of the final chosen model.</p></li>
<li><p>Idea is to randomly divide the data into <span class="math inline">\(K\)</span> equal-sized parts.</p>
<ul>
<li>We leave out part <span class="math inline">\(k\)</span>, fit the model to the other <span class="math inline">\(K-1\)</span> parts (combined), and then obtain predictions for the left-out <span class="math inline">\(k\)</span>th part.</li>
</ul></li>
<li><p>This is done in turn for each part <span class="math inline">\(k=1,2, \ldots, K\)</span>, and then the results are combined.</p></li>
<li><p>Devide data into <span class="math inline">\(K\)</span> roughly equal-sized parts (<span class="math inline">\(K=5\)</span> here)</p></li>
</ul>
<p><img src="fig6/f3.png" /></p>
<ul>
<li>Randomly divide <span class="math inline">\(n\)</span> observations into <span class="math inline">\(K\)</span> groups, or <em>folds</em>, of approximately equal size, treat the first fold as a validation set, fit the model on each of the ramaining <span class="math inline">\(K-1\)</span> folds at a time, compute <span class="math inline">\(MSE_k\)</span>, <span class="math inline">\(k=1,\ldots, K\)</span>, and</li>
</ul>
<p><span class="math display">\[
CV_K=\frac{1}{K}\sum_{k=1}^K MSE_k
\]</span></p>
<p>      is the <span class="math inline">\(k\)</span>-fold approach to estimate the test MSE.</p>
<div id="other-approach" class="section level4 unnumbered hasAnchor">
<h4>Other approach<a href="chapter6.html#other-approach" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Let the <span class="math inline">\(K\)</span> parts be <span class="math inline">\(C_1, C2, \ldots, C_K\)</span>, where <span class="math inline">\(C_k\)</span> denotes the indices of the observations in part <span class="math inline">\(k\)</span>.
<ul>
<li>There are <span class="math inline">\(n_k\)</span> observations in part <span class="math inline">\(k\)</span>: if <span class="math inline">\(N\)</span> is a multiple of <span class="math inline">\(K\)</span>, then <span class="math inline">\(n_k=n/K\)</span>.</li>
</ul></li>
<li>Compute</li>
</ul>
<p><span class="math display">\[
CV_k=\sum_{k=1}^K\frac{n_k}{n}MSE_k
\]</span></p>
<p>      where <span class="math inline">\(MSE_k=\sum_{i \in C_k}(y_i-\hat{y}_i)^2/n_k\)</span>, and <span class="math inline">\(\hat{y}_i\)</span> is the fit for observation <span class="math inline">\(i\)</span>, obtained from the data with part <span class="math inline">\(k\)</span> removed.</p>
<ul>
<li>Setting <span class="math inline">\(K=n\)</span> yields <span class="math inline">\(n\)</span>-fold or <em>leave-one out cross-validation</em> (LOOCV).</li>
</ul>
</div>
<div id="leave-one-out-cross-validation-loocv" class="section level4 unnumbered hasAnchor">
<h4>Leave-one-out cross-validation (LOOCV)<a href="chapter6.html#leave-one-out-cross-validation-loocv" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig6/ex42.JPG" /></p>
<p>(Source James et al. (2013), Fig 5.3)</p>
<ul>
<li><p>Leave out each observation at a time, estimate the model <span class="math inline">\(n\)</span> times
from each of remaining <span class="math inline">\(n-1\)</span> observation, compute <span class="math inline">\(MSE_i=(Y_i-\hat{Y}_{(i)})^2\)</span>, where <span class="math inline">\(\hat{Y}_{(i)}\)</span> is estimated from the sample without the <span class="math inline">\(i\)</span>th observation.</p></li>
<li><p>With least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit!</p></li>
<li><p>The following formula holds:</p></li>
</ul>
<p><span class="math display">\[
CV_n = \frac{1}{n}\sum_{i=1}^n MSE_i = \frac{1}{n}\sum_{i=1}^n (y_i-\hat{y}_{(i)})^2
\]</span></p>
<ul>
<li>For OLS regression it can be shown that</li>
</ul>
<p><span class="math display">\[
CV_n = \frac{1}{n}\sum_{i=1}^n (\frac{y_i-\hat{y}_{(i)}}{1-h_i})^2
\]</span></p>
<p>      where <span class="math inline">\(h_i=x_i^T(X^TX)^{-1}x_i\)</span> is the leverage of the <span class="math inline">\(i\)</span>th observation and <span class="math inline">\(\hat{Y}_i\)</span> is the fitted value (prediction) of the full sample regression.</p>
<ul>
<li>LOOCV sometimes useful, but typically doesn’t <em>shake up</em> the data enough.
<ul>
<li>The estimates from each fold are highly correlated and hence their average can have high variance.</li>
</ul></li>
<li>Typical (or better) choice is <span class="math inline">\(k=5\)</span> or <span class="math inline">\(10\)</span>.</li>
</ul>
</div>
<div id="auto-data-example" class="section level4 unnumbered hasAnchor">
<h4>Auto data example<a href="chapter6.html#auto-data-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig6/f4.png" /></p>
<ul>
<li>True and estimated test MSE for the simulated data</li>
</ul>
<p><img src="fig6/f5.png" /></p>
<ul>
<li>True test MSE: blue, LOOCV: black, 10-fold CV: orange</li>
</ul>
</div>
<div id="bias-variance-trade-off-1" class="section level4 unnumbered hasAnchor">
<h4>Bias-variance trade-off<a href="chapter6.html#bias-variance-trade-off-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Recall</li>
</ul>
<p><span class="math display">\[
MSE=Var(est. error)+(bias)^2 +Var(irreducible\_error)
\]</span></p>
<ul>
<li>The population MSE is the true test error (which remains unknown in practice).</li>
<li>Mostly the major interest is not in the true test error as such, rather the interest is in the correct degree of flexibility of the specified model which is determined by the minimum of the test set MSE (test error).</li>
<li>However if the interest is in the test error as such, the question reduces to the bias-variance trade-off stemming from <span class="math inline">\(var(est.error)+(bias)^2\)</span> in the MSE.</li>
<li>It turns out that <span class="math inline">\(k\)</span>-fold CV often gives loser error rates than LOOCV.
<ul>
<li>It has been empirically observed that selecting <span class="math inline">\(k\)</span> suitably (usually 5 or 10), <span class="math inline">\(k\)</span>-fold CV tends balance better bias and variance than LOOCV.</li>
</ul></li>
</ul>
</div>
<div id="cross-validation-in-classification" class="section level4 unnumbered hasAnchor">
<h4>Cross-validation in classification<a href="chapter6.html#cross-validation-in-classification" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Cross-validation works in classification in the same manner as in regression with the exception that MSE is replaced by the misclassification rate.</p></li>
<li><p>For example in LOOCV</p></li>
</ul>
<p><span class="math display">\[
CV_n = \frac{1}{n}\sum_{i=1}^n I(Y_i\ne \hat{Y}_{(i)})
\]</span></p>
<p>      where <span class="math inline">\(\hat{Y}_i{(i)}\)</span> is the predicted class by the model estimated without the <span class="math inline">\(i\)</span>th observation.</p>
<ul>
<li><p>In the same fashion, using CV approaches one can select the optimal value for <span class="math inline">\(K\)</span>.</p></li>
<li><p>We divide the data into <span class="math inline">\(K\)</span> roughly equal-sized parts <span class="math inline">\(C_1, C2, \ldots, C_K\)</span>, where <span class="math inline">\(C_k\)</span> denotes the indices of the observations in part <span class="math inline">\(k\)</span>.</p>
<ul>
<li>There are <span class="math inline">\(n_k\)</span> observations in part <span class="math inline">\(k\)</span>: if <span class="math inline">\(N\)</span> is a multiple of <span class="math inline">\(K\)</span>, then <span class="math inline">\(n_k=n/K\)</span>.</li>
</ul></li>
<li><p>Compute</p></li>
</ul>
<p><span class="math display">\[
CV_K=\frac{1}{K} \sum_{k=1}^K Err_k
\]</span></p>
<p>      where <span class="math inline">\(Err_k=\sum_{i \in C_k}I(y_i \ne \hat{y}_i)/n_k\)</span></p>
<ul>
<li>The estimated standard deviation of <span class="math inline">\(CV_K\)</span> is</li>
</ul>
<p><span class="math display">\[
\hat{SE}(CV_K)=\sqrt{\frac{1}{K}\sum_{k=1}^K\frac{(Err_k-\bar{Err_k)^2}}{K-1}}
\]</span></p>
<ul>
<li>This is a useful estimate, but strictly speaking, not quite valid.</li>
</ul>
</div>
<div id="cross-validation-right-and-wrong" class="section level4 unnumbered hasAnchor">
<h4>Cross-validation: right and wrong<a href="chapter6.html#cross-validation-right-and-wrong" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Consider a simple classifier applied to some two-class data:</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Starting with 5000 peredictors and 50 samples, find the 100 predictors having the largest correlation with the classs labels.</p></li>
<li><p>We then apply a classifier such as logistic regression, using only these 100 predictors.</p></li>
</ol>
<ul>
<li><p>How do we estimate the test set performance of this classifier?</p></li>
<li><p>Can we apply cross-validation in step 2, forgetting about step 1?</p></li>
<li><p>This would ignore the fact that in Step 1, the procedure has already seen the labels of the training data, and made use of them.</p>
<ul>
<li>This is a form of training and must be included in the validation process.</li>
</ul></li>
<li><p>It is easy to simulate realistic data with the class labels independent of the outcome, so that true test error=50%, but the CV error estimate that ignores Step 1 is zero!</p></li>
<li><p>We have seen this error made in many high profile genomics papers.</p></li>
<li><p>Wrong: Apply cross-validation in step 2.</p></li>
<li><p>Right: Apply cross-validation to steps 1 and 2.</p></li>
</ul>
<p><img src="fig6/f6.png" /></p>
<p><img src="fig6/f7.png" /></p>
</div>
<div id="example-11" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Example<a href="chapter6.html#example-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>In the following examples we utilize the Auto data set available in the ISLR2 package.</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chapter6.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2) <span class="co"># load ISLR package</span></span>
<span id="cb1-2"><a href="chapter6.html#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="chapter6.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Auto)</span></code></pre></div>
<pre><code>##   mpg cylinders displacement horsepower weight acceleration year origin
## 1  18         8          307        130   3504         12.0   70      1
## 2  15         8          350        165   3693         11.5   70      1
## 3  18         8          318        150   3436         11.0   70      1
## 4  16         8          304        150   3433         12.0   70      1
## 5  17         8          302        140   3449         10.5   70      1
## 6  15         8          429        198   4341         10.0   70      1
##                        name
## 1 chevrolet chevelle malibu
## 2         buick skylark 320
## 3        plymouth satellite
## 4             amc rebel sst
## 5               ford torino
## 6          ford galaxie 500</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="chapter6.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(Auto)</span></code></pre></div>
<pre><code>##     mpg cylinders displacement horsepower weight acceleration year origin
## 392  27         4          151         90   2950         17.3   82      1
## 393  27         4          140         86   2790         15.6   82      1
## 394  44         4           97         52   2130         24.6   82      2
## 395  32         4          135         84   2295         11.6   82      1
## 396  28         4          120         79   2625         18.6   82      1
## 397  31         4          119         82   2720         19.4   82      1
##                 name
## 392 chevrolet camaro
## 393  ford mustang gl
## 394        vw pickup
## 395    dodge rampage
## 396      ford ranger
## 397       chevy s-10</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="chapter6.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(Auto) <span class="co"># display the number of obsevations</span></span></code></pre></div>
<pre><code>## [1] 392</code></pre>
<ul>
<li>The interest is the consumption measured by mileage per gallon
(mpg) (miles covered per gallon) as a function of just the horse
power (for simplicity).</li>
</ul>
<div id="validation-set-approach-1" class="section level4 unnumbered hasAnchor">
<h4>Validation set approach<a href="chapter6.html#validation-set-approach-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="chapter6.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># to allow replicate the results exactly</span></span>
<span id="cb7-2"><a href="chapter6.html#cb7-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">392</span>, <span class="at">size =</span> <span class="dv">196</span>) <span class="co"># see help(sample)</span></span>
<span id="cb7-3"><a href="chapter6.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb7-4"><a href="chapter6.html#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="do">## fit simple linear regression, save results into object lm.fit, and print summary</span></span>
<span id="cb7-5"><a href="chapter6.html#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto, <span class="at">subset =</span> train)) <span class="co"># lm.fit contains results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ horsepower, data = Auto, subset = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.3177 -3.5428 -0.5591  2.3910 14.6836 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 41.283548   1.044352   39.53   &lt;2e-16 ***
## horsepower  -0.169659   0.009556  -17.75   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.032 on 194 degrees of freedom
## Multiple R-squared:  0.619,  Adjusted R-squared:  0.6171 
## F-statistic: 315.2 on 1 and 194 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="chapter6.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">## prediction MSE </span></span>
<span id="cb9-2"><a href="chapter6.html#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit, <span class="at">newdata =</span> Auto))[<span class="sc">-</span>train]<span class="sc">^</span><span class="dv">2</span>), <span class="dv">2</span>) <span class="co"># validation set (-train selects these) MSE</span></span></code></pre></div>
<pre><code>## [1] 23.27</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="chapter6.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb11-2"><a href="chapter6.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="do">## second order polynomial regression y = a + b x + c x^2 + e</span></span>
<span id="cb11-3"><a href="chapter6.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(<span class="at">x =</span> horsepower, <span class="dv">2</span>), <span class="at">data =</span> Auto, <span class="at">subset =</span> train)) <span class="co"># quadratic</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ poly(x = horsepower, 2), data = Auto, subset = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -12.8711  -2.6655  -0.0096   2.0806  16.1063 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                23.5496     0.3175  74.182  &lt; 2e-16 ***
## poly(x = horsepower, 2)1 -123.5881     6.4587 -19.135  &lt; 2e-16 ***
## poly(x = horsepower, 2)2   47.7189     6.3613   7.501 2.25e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.439 on 193 degrees of freedom
## Multiple R-squared:  0.705,  Adjusted R-squared:  0.702 
## F-statistic: 230.6 on 2 and 193 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="chapter6.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb13-2"><a href="chapter6.html#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit2, <span class="at">newdata =</span> Auto))[<span class="sc">-</span>train]<span class="sc">^</span><span class="dv">2</span>), <span class="dv">2</span>) <span class="co"># validation set MSE</span></span></code></pre></div>
<pre><code>## [1] 18.72</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="chapter6.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="do">## third order</span></span>
<span id="cb15-2"><a href="chapter6.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(<span class="at">x =</span> horsepower, <span class="dv">3</span>), <span class="at">data =</span> Auto, <span class="at">subset =</span> train)) <span class="co"># cubic</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ poly(x = horsepower, 3), data = Auto, subset = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -12.6625  -2.7108   0.0805   2.0724  16.1378 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                23.5527     0.3185  73.946  &lt; 2e-16 ***
## poly(x = horsepower, 3)1 -123.6143     6.4755 -19.089  &lt; 2e-16 ***
## poly(x = horsepower, 3)2   47.8284     6.3935   7.481 2.58e-12 ***
## poly(x = horsepower, 3)3    1.3825     5.8107   0.238    0.812    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.45 on 192 degrees of freedom
## Multiple R-squared:  0.7051, Adjusted R-squared:  0.7005 
## F-statistic:   153 on 3 and 192 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="chapter6.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb17-2"><a href="chapter6.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit3, <span class="at">newdata =</span> Auto))[<span class="sc">-</span>train]<span class="sc">^</span><span class="dv">2</span>), <span class="dv">2</span>) <span class="co"># validation set MSE</span></span></code></pre></div>
<pre><code>## [1] 18.79</code></pre>
<ul>
<li>MSE for the second order polynomial reduces down to 18.72, for the third order model it is 18.79, i.e., no material improvement from the second order model.</li>
<li>If we choose a different training set, these numbers change.</li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="chapter6.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>) <span class="co"># to allow replicate the results exactly</span></span>
<span id="cb19-2"><a href="chapter6.html#cb19-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">392</span>, <span class="at">size =</span> <span class="dv">196</span>) <span class="co"># see help(sample)</span></span>
<span id="cb19-3"><a href="chapter6.html#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb19-4"><a href="chapter6.html#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="do">## fit simple linear regression, save results into object lm.fit, and print summary</span></span>
<span id="cb19-5"><a href="chapter6.html#cb19-5" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto, <span class="at">subset =</span> train)</span>
<span id="cb19-6"><a href="chapter6.html#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb19-7"><a href="chapter6.html#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="do">## prediction MSE </span></span>
<span id="cb19-8"><a href="chapter6.html#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit, <span class="at">newdata =</span> Auto))[<span class="sc">-</span>train]<span class="sc">^</span><span class="dv">2</span>), <span class="dv">2</span>) <span class="co"># validation set (-train selects these) MSE</span></span></code></pre></div>
<pre><code>## [1] 25.73</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="chapter6.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb21-2"><a href="chapter6.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="do">## second order polynomial regression y = a + b x + c x^2 + e</span></span>
<span id="cb21-3"><a href="chapter6.html#cb21-3" aria-hidden="true" tabindex="-1"></a>lm.fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(<span class="at">x =</span> horsepower, <span class="dv">2</span>), <span class="at">data =</span> Auto, <span class="at">subset =</span> train) <span class="co"># qyuadratic</span></span>
<span id="cb21-4"><a href="chapter6.html#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb21-5"><a href="chapter6.html#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit2, <span class="at">newdata =</span> Auto))[<span class="sc">-</span>train]<span class="sc">^</span><span class="dv">2</span>), <span class="dv">2</span>) <span class="co"># validation set MSE</span></span></code></pre></div>
<pre><code>## [1] 20.43</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="chapter6.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="do">## third order</span></span>
<span id="cb23-2"><a href="chapter6.html#cb23-2" aria-hidden="true" tabindex="-1"></a>lm.fit3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(<span class="at">x =</span> horsepower, <span class="dv">3</span>), <span class="at">data =</span> Auto, <span class="at">subset =</span> train) <span class="co"># cubic</span></span>
<span id="cb23-3"><a href="chapter6.html#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb23-4"><a href="chapter6.html#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit3, <span class="at">newdata =</span> Auto))[<span class="sc">-</span>train]<span class="sc">^</span><span class="dv">2</span>), <span class="dv">2</span>) <span class="co"># validation set MSE</span></span></code></pre></div>
<pre><code>## [1] 20.39</code></pre>
<ul>
<li>The results are qualitatively similar and support the quadratic function of horsepower.</li>
</ul>
</div>
<div id="loocv" class="section level4 unnumbered hasAnchor">
<h4>LOOCV<a href="chapter6.html#loocv" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Leave-one-out cross-validation can be worked out by cv.glm() function available in the boot package, see help (cv.glm).</p></li>
<li><p>In glm() the default family is gaussian which results to the same estimation results as lm().</p></li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="chapter6.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto)) <span class="co"># estimate and show coefficients</span></span></code></pre></div>
<pre><code>## (Intercept)  horsepower 
##  39.9358610  -0.1578447</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="chapter6.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(boot) <span class="co">#</span></span>
<span id="cb27-2"><a href="chapter6.html#cb27-2" aria-hidden="true" tabindex="-1"></a>cv.err <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(<span class="at">data =</span> Auto, <span class="at">glmfit =</span> glm.fit)</span>
<span id="cb27-3"><a href="chapter6.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="do">## cross-validation error, the second number is a bias corrected version,</span></span>
<span id="cb27-4"><a href="chapter6.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="do">## here they are in two decimal places the same</span></span>
<span id="cb27-5"><a href="chapter6.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(cv.err<span class="sc">$</span>delta, <span class="at">digits =</span> <span class="dv">2</span>) </span></code></pre></div>
<pre><code>## [1] 24.23 24.23</code></pre>
<ul>
<li><p>LOOCV estimate for the test MSE of the linear model is 24.23 (compared to
26.14 of the validation set).</p></li>
<li><p>Next we repeat cv.glm() for regressions up to 5th order polynomial.</p></li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="chapter6.html#cb29-1" aria-hidden="true" tabindex="-1"></a>cv.errv <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="dv">5</span>) <span class="co"># initialize a vector of length 5</span></span>
<span id="cb29-2"><a href="chapter6.html#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb29-3"><a href="chapter6.html#cb29-3" aria-hidden="true" tabindex="-1"></a>    gfit <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, i), <span class="at">data =</span> Auto)</span>
<span id="cb29-4"><a href="chapter6.html#cb29-4" aria-hidden="true" tabindex="-1"></a>    cv.errv[i] <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(<span class="at">data =</span> Auto, <span class="at">glmfit =</span> gfit)<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb29-5"><a href="chapter6.html#cb29-5" aria-hidden="true" tabindex="-1"></a>} <span class="co"># end for</span></span>
<span id="cb29-6"><a href="chapter6.html#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(cv.errv, <span class="at">digits =</span> <span class="dv">2</span>) <span class="co"># show results</span></span></code></pre></div>
<pre><code>## [1] 24.23 19.25 19.33 19.42 19.03</code></pre>
<ul>
<li>Sharp drop in the estimated test MSE after the linear fit, but then no clear improvement.</li>
</ul>
</div>
<div id="k-fold-cv" class="section level4 unnumbered hasAnchor">
<h4><span class="math inline">\(K\)</span>-fold CV<a href="chapter6.html#k-fold-cv" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The cv.glm() function can be used for <span class="math inline">\(K\)</span>-fold CV by setting argument <span class="math inline">\(K\)</span> in the function call equal to 10 (default is <span class="math inline">\(n\)</span>, the number of observations which results to LOOCV).</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="chapter6.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">17</span>) <span class="co"># to replicate results</span></span>
<span id="cb31-2"><a href="chapter6.html#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="do">## as in previous example, fit polynomial regression,</span></span>
<span id="cb31-3"><a href="chapter6.html#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="do">## but here up to order 10, and set k = 10 also.</span></span>
<span id="cb31-4"><a href="chapter6.html#cb31-4" aria-hidden="true" tabindex="-1"></a>cv.err10 <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="dv">10</span>) <span class="co"># initialize error vector</span></span>
<span id="cb31-5"><a href="chapter6.html#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb31-6"><a href="chapter6.html#cb31-6" aria-hidden="true" tabindex="-1"></a>    gfit <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, i), <span class="at">data =</span> Auto)</span>
<span id="cb31-7"><a href="chapter6.html#cb31-7" aria-hidden="true" tabindex="-1"></a>    cv.err10[i] <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(<span class="at">data =</span> Auto, <span class="at">glmfit =</span> gfit, <span class="at">K =</span> <span class="dv">10</span>)<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb31-8"><a href="chapter6.html#cb31-8" aria-hidden="true" tabindex="-1"></a>} <span class="co"># end for</span></span>
<span id="cb31-9"><a href="chapter6.html#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(cv.err10, <span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##  [1] 24.27 19.27 19.35 19.29 19.03 18.90 19.12 19.15 18.87 20.96</code></pre>
<ul>
<li>Again, no material improvement after the quadratic model.</li>
</ul>
</div>
</div>
</div>
<div id="bootstrap" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Bootstrap<a href="chapter6.html#bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>The bootstrap approach is extremely powerful statistical tool that can be used to estimate standard errors and other model precision
statistics (generally uncertainty) of a fitted model.</p></li>
<li><p>The bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.</p>
<ul>
<li>For example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient.</li>
</ul></li>
<li><p>In particular the power is in cases where analytic form of the standard errors is difficult to derive.</p></li>
<li><p>The use of the term bootstrap derives from the phrase to pull oneself up by one’s bootstraps, widely thought to be based on one of the eighteenth century “The Surprising Adventures of Baron Munchausen” by Rudolph Erich Raspe:</p>
<ul>
<li>The Baron had fallen to the bottom of a deep lake. Just when it looked like all was lost, he thought to pick himself up by his own bootstraps.</li>
</ul></li>
<li><p>It is not the same as the term “bootstrap” used in computer science meaning to “boot” a computer from a set of core instruction, though the derivation is similar.</p></li>
</ul>
<div id="simple-example" class="section level4 unnumbered hasAnchor">
<h4>Simple example<a href="chapter6.html#simple-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Suppose that we wish to invest a fixed sum of meney in two financial assets that yield teturns of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random quantities.</p></li>
<li><p>We will invest a fraction <span class="math inline">\(\alpha\)</span> of our money in <span class="math inline">\(X\)</span>, and will invest the remaining <span class="math inline">\(1-\alpha\)</span> in <span class="math inline">\(Y\)</span>.</p></li>
<li><p>We wish to choose <span class="math inline">\(\alpha\)</span> to minimize the total risk, or variance, of our investment. In other words, we want to minimize <span class="math inline">\(Var(\alpha X+(1-\alpha)Y)\)</span>.</p></li>
<li><p>One can show that the value that minimized the risk is given by</p></li>
</ul>
<p><span class="math display">\[
\alpha=\frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 +\sigma_Y^2 -2\sigma_{XY}}
\]</span></p>
<p>      <span class="math inline">\(\sigma_X^2=Var(X)\)</span>, <span class="math inline">\(\sigma_Y^2=Var(Y)\)</span>, and <span class="math inline">\(\sigma_{XY}=Cov(X,Y)\)</span>.</p>
<ul>
<li><p>But the values of <span class="math inline">\(\sigma_X^2\)</span>, <span class="math inline">\(\Sigma_Y^2\)</span>, and <span class="math inline">\(\sigma_{XY}\)</span> are unknown.</p></li>
<li><p>We can compute estimates for these quantities, <span class="math inline">\(\hat{\sigma}_X^2\)</span>, <span class="math inline">\(\hat{\Sigma}_Y^2\)</span>, and <span class="math inline">\(\hat{\sigma}_{XY}\)</span>, using a data set that contains measurements for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
<li><p>We can then estimate the value of <span class="math inline">\(\alpha\)</span> that minimized the variance of our investment using</p></li>
</ul>
<p><span class="math display">\[
\hat{\alpha}=\frac{\hat{\sigma}_Y^2 - \hat{\sigma}_{XY}}{\hat{\sigma}_X^2 +\hat{\sigma}_Y^2 -2\hat{\sigma}_{XY}}
\]</span></p>
<p><img src="fig6/f8.png" /></p>
<ul>
<li><p>Each panel displays 100 simulated returns for investments <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. From left to right and top to bottom, the resulting estimates for <span class="math inline">\(\alpha\)</span> are 0.576, 0.532, 0.657, and 0.651.</p></li>
<li><p>To estimate the standard deviation of <span class="math inline">\(\hat{\alpha}\)</span>, we repeated the process of simulating 100 paired observatins of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and estimating <span class="math inline">\(\alpha\)</span> 1,000 times.</p></li>
<li><p>We thereby obtained 1,000 estimates for <span class="math inline">\(\alpha\)</span>, which we can call <span class="math inline">\(\hat{\alpha}_1, \hat{\alpha}_2, \ldots , \hat{\alpha}_{1000}\)</span>.</p></li>
</ul>
<p><img src="fig6/f9.png" /></p>
<ul>
<li><p>Left: A histogram of the estimates of <span class="math inline">\(\alpha\)</span> obtained by generating 1,000 simulated data sets from the true population. Center: A histogram of the estimates of <span class="math inline">\(\alpha\)</span> obtained from 1,000 bootstrap samples from a single data set. Right: The estimates of <span class="math inline">\(\alpha\)</span> displayed in the left and center panels are shown as boxplots. In each panel, the pink line indicatees the true value of <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>The left-hand panel of the Figure displays a histogram of the resulting estimates.</p></li>
<li><p>For these simulations the parameters were set to <span class="math inline">\(\sigma_X^2=1\)</span>, <span class="math inline">\(\sigma_Y^2=1.25\)</span>, and <span class="math inline">\(\sigma_{XY}=0.5\)</span>, and so we know that the true value of <span class="math inline">\(\alpha\)</span> is 0.6 (indicated by the red line).</p></li>
<li><p>The mean over all 1,000 estimates for <span class="math inline">\(\alpha\)</span> is</p></li>
</ul>
<p><span class="math display">\[
\bar{\alpha}=\frac{1}{1000}\sum_{r=1}^{1000} \hat{\alpha}_r=0.5996
\]</span></p>
<p>      very close to <span class="math inline">\(\alpha=0.6\)</span>, and the standard deviation of the estimates is</p>
<p><span class="math display">\[
\sqrt{\frac{1}{1000-1}\sum_{r=1}^{1000} (\hat{\alpha}_r-\bar{\alpha})^2}=0.083
\]</span></p>
<ul>
<li><p>This gives us a very good idea of the accuracy of <span class="math inline">\(\hat{\alpha}\)</span>: <span class="math inline">\(SE(\hat{\alpha})\approx 0.083\)</span>.</p></li>
<li><p>So roughly speaking, for a random sample from the population, we would expect <span class="math inline">\(\hat{\alpha}\)</span> to differ from <span class="math inline">\(\alpha\)</span> by approximately 0.08, on average.</p></li>
<li><p>The procedure outlined above cannot be applied, because for read data we cannot generate new samples from the original population.</p></li>
<li><p>However, the bootstrap approach allows us to use a computer to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.</p></li>
<li><p>Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data wet <em>with replacement</em>.</p></li>
<li><p>Each of these “bootstrap data sets” is created by sampling <em>with replacement</em>, and is the <em>same size</em> as our original data set. As a result some observations may appear more than once in a given bootstrap data set and some not at all.</p></li>
</ul>
<p><img src="fig6/f10.png" /></p>
<ul>
<li><p>A graphical illustration of the bootstrap approach on a small sample containing <span class="math inline">\(n=3\)</span> observations.</p>
<ul>
<li>Each bootstrap data set contains <span class="math inline">\(n\)</span> observations, sampled with replacement from the original data set.</li>
<li>Each bootstrap data set is used to obtain an estimate of <span class="math inline">\(\alpha\)</span>.</li>
</ul></li>
<li><p>Denoting the first bootstrap data set by <span class="math inline">\(Z^{*1}\)</span>, we use <span class="math inline">\(Z^{*1}\)</span> to produce a new bootstrap estimate for <span class="math inline">\(\alpha\)</span>, which we call <span class="math inline">\(\hat{\alpha}^{*1}\)</span>.</p></li>
<li><p>This procedure is repeated <span class="math inline">\(B\)</span> times for some large value of <span class="math inline">\(B\)</span> (say 100 or 1000), in order to produce <span class="math inline">\(B\)</span> different bootstrap data sets, <span class="math inline">\(Z^{*1}, Z^{*2}, \ldots, Z^{*B}\)</span>, and <span class="math inline">\(B\)</span> corresponding <span class="math inline">\(\alpha\)</span> estimates, <span class="math inline">\(\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, \ldots, \hat{\alpha}^{*B}\)</span>.</p></li>
<li><p>We estimate the standard error of these bootstrap estimates using the formula</p></li>
</ul>
<p><span class="math display">\[
SE_B(\hat{\alpha}) = \sqrt{\frac{1}{B-1}     \sum_{b=1}^{B} (\hat {\alpha}^{*r}- \bar{\hat{\alpha}}^*)^2  }
\]</span></p>
<ul>
<li>This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set.
<ul>
<li>See center and right panels of Figure above. Bootstrap results are in blue. For this example <span class="math inline">\(SE_B(\hat{\alpha})=0.087\)</span></li>
</ul></li>
</ul>
<p><img src="fig6/f11.png" /></p>
<ul>
<li><p>In more complex data situations, figuring out the appropriate way to generate bootstrap samples can require some thought.</p></li>
<li><p>For example, if the data is a time series, we can’t simply sample the observations with replacement.</p></li>
<li><p>We can instead create blocks of consecutive observations, and sample those with replacements. Then we paste together sample blocks to obtain a bootstrap data set.</p></li>
<li><p>Primarily used to obtain standard errors of an estimate.</p></li>
<li><p>Also provides approximate confidence intervals for a population parameter.</p>
<ul>
<li>For example, looking at the histogram in the middle panel of the Figure above, the 5% and 95% quantiles of the 1000 values is (0.43, 0.72).</li>
<li>This represents an approximate 90% confidence interval for the true <span class="math inline">\(\alpha\)</span></li>
<li>How do we interpret this confidence interval?</li>
</ul></li>
<li><p>The above interval is called a <em>Bootstrap Percentile</em> confidence interval.</p>
<ul>
<li>It is the simplest method (among many approaches) for obtaining a confidence interval from the bootstrap.</li>
</ul></li>
</ul>
</div>
<div id="can-the-bootstrap-estimate-prediction-error" class="section level4 unnumbered hasAnchor">
<h4>Can the bootstrap estimate prediction error?<a href="chapter6.html#can-the-bootstrap-estimate-prediction-error" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>In cross-validation, each of the <span class="math inline">\(K\)</span> validation folds is distinctfrom the other <span class="math inline">\(K-1\)</span> folds used for training: <em>there is no overlap</em>.</p>
<ul>
<li>This is crucial for its success.</li>
</ul></li>
<li><p>To estimate prediction error using the bootstrap, we could think about using each bootstrap data set as our training sample, and the original sample as our validation sample.</p></li>
<li><p>But each bootstrap sample has significant overlap with the original data.</p>
<ul>
<li>About two-thirds of the original data points appear in each bootstrap sample.</li>
</ul></li>
<li><p>This will cause the bootstrap to seriously underestimate the true prediction error.</p></li>
<li><p>The other way around - with original sample = training sample, bootstrap data set = validation sample - is worse!</p></li>
<li><p>Can partly fix this problem by only using predictions for those observations that did not (by chance) occur in the current bootstrap sample.</p></li>
<li><p>But the method gets complicated, and in the end, cross-validation provides a simpler, more attractive approach for estimating prediction error.</p></li>
<li><p>In microarray and other genomic studies, an important problem is to compare a predictor of disease outcome derived from a large number of “biomarkers” to standard clinical predictors.</p></li>
<li><p>Comparing them on the same data set that was used to derive the biomarker predictor can lead to results strongly biased in favor of the biomarker predictor.</p></li>
<li><p><em>Pre-validation</em> can be used to make a fairer comparison between the two sets of predictors.</p></li>
</ul>
</div>
<div id="motivating-example" class="section level4 unnumbered hasAnchor">
<h4>Motivating example<a href="chapter6.html#motivating-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>An example of this problem arose in the paper of van’t Veer et al. Nature (2002).
<ul>
<li>The microarray data has 4918 genes measured over 78 cases, taken from a study of breast cancer.</li>
<li>There are 44 cases in the good prognosis group and 34 in the poor prognosis group</li>
<li>A “microarray” predictor was constructed as follows:</li>
</ul></li>
</ul>
<ol style="list-style-type: decimal">
<li>70 genes were selected, having largest absolute correlation with the 78 class labels.</li>
<li>Using these 70 genes, a nearest-centroid classifier <span class="math inline">\(C(x)\)</span> was constructed.</li>
<li>Applying the classifier to the 78 microarrays gave a dichotomous predictor <span class="math inline">\(z_i=C(x_i)\)</span> for each case <span class="math inline">\(i\)</span>.</li>
</ol>
<ul>
<li>Comparison of the microarray predictor with some clinical predictors, using logistic regression with outcome prognosis:</li>
</ul>
<p><img src="fig6/f12.png" /></p>
<ul>
<li><p>Designed for comparison of adaptively derived predictors to fixed, pre-defined predictors.</p></li>
<li><p>The idea is to from a “pre-validated” verson of the adaptive predictor: specifically, a “fairer” verson that hasn’t “seen” the response <span class="math inline">\(y\)</span>.</p></li>
</ul>
<p><img src="fig6/f13.png" /></p>
<ol style="list-style-type: decimal">
<li><p>Divide the cases up into <span class="math inline">\(K=13\)</span> equal-sized parts of 6 cases each.</p></li>
<li><p>Set aside one of parts. Using only the data from the other 12 parts, select the features having absolute correlation at least 0.3 with the class labels, and form a nearest centroid classification rule.</p></li>
<li><p>Use the rule to predict the class labels for the 13th part.</p></li>
<li><p>Do steps 2 and 3 for each of the 13 parts, yielding a “pre-validated” microarray predictor <span class="math inline">\(\tilde{z}_i\)</span> for each of the 78 cases.</p></li>
<li><p>Fit a logistic regression model to the pre-validated microarray predictor and the 6 clinical predictors.</p></li>
</ol>
</div>
<div id="bootstrap-versus-permutation-tests" class="section level4 unnumbered hasAnchor">
<h4>Bootstrap versus Permutation tests<a href="chapter6.html#bootstrap-versus-permutation-tests" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The bootstrap samples from the estimated population, and use the results to estimate standard errors and confidence intervals.
-Permutation methods sample from an estimated <em>null distribution</em> for the data, and use this to estimate p-values and False Discovery Rates for hypothesis tests.</li>
<li>The bootstrap can be used to test a null hypothesis in simple situations.
<ul>
<li>For example, if <span class="math inline">\(\theta=0\)</span> is the null hypothesis, we chect whether the confidence interval for <span class="math inline">\(\theta\)</span> contains zero.</li>
</ul></li>
<li>Can also adapt the bootstrap to sample from a null distribution (See Efron and Tibshirani book “An Introduction to the Bootstrap” (1993), ch. 16) but there’s no real advantage over permutations.</li>
</ul>
</div>
<div id="example-12" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter6.html#example-12" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Consider the married women labor force attendance.</li>
</ul>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="chapter6.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">#if wooldridge not installed</span></span>
<span id="cb33-2"><a href="chapter6.html#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages(&quot;wooldridge&quot;, repos = &quot;https://cloud.r-project.org&quot;)</span></span></code></pre></div>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="chapter6.html#cb34-1" aria-hidden="true" tabindex="-1"></a>mroz <span class="ot">&lt;-</span> mroz[, <span class="fu">c</span>(<span class="st">&quot;inlf&quot;</span>, <span class="st">&quot;huswage&quot;</span>, <span class="st">&quot;educ&quot;</span>, <span class="st">&quot;exper&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;kidslt6&quot;</span>, <span class="st">&quot;kidsge6&quot;</span>)] <span class="co"># keep only needed</span></span>
<span id="cb34-2"><a href="chapter6.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(mroz)</span></code></pre></div>
<pre><code>##   inlf huswage educ exper age kidslt6 kidsge6
## 1    1  4.0288   12    14  32       1       0
## 2    1  8.4416   12     5  30       0       2
## 3    1  3.5807   12    15  35       1       3
## 4    1  3.5417   12     6  34       0       3
## 5    1 10.0000   14     7  31       1       2
## 6    1  6.7106   12    33  54       0       0</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="chapter6.html#cb36-1" aria-hidden="true" tabindex="-1"></a>fit.logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(inlf <span class="sc">~</span> huswage <span class="sc">+</span> educ <span class="sc">+</span> exper <span class="sc">+</span> <span class="fu">I</span>(exper<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> age <span class="sc">+</span> kidslt6 <span class="sc">+</span> kidsge6, <span class="at">data =</span> mroz, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb36-2"><a href="chapter6.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit.logit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = inlf ~ huswage + educ + exper + I(exper^2) + age + 
##     kidslt6 + kidsge6, family = binomial(link = &quot;logit&quot;), data = mroz)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2086  -0.8990   0.4481   0.8447   2.1953  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.592500   0.853042   0.695  0.48732    
## huswage     -0.043803   0.021312  -2.055  0.03985 *  
## educ         0.209272   0.042568   4.916 8.83e-07 ***
## exper        0.209668   0.031963   6.560 5.39e-11 ***
## I(exper^2)  -0.003181   0.001013  -3.141  0.00168 ** 
## age         -0.091342   0.014460  -6.317 2.67e-10 ***
## kidslt6     -1.431959   0.202233  -7.081 1.43e-12 ***
## kidsge6      0.047089   0.074284   0.634  0.52614    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1029.75  on 752  degrees of freedom
## Residual deviance:  805.85  on 745  degrees of freedom
## AIC: 821.85
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="chapter6.html#cb38-1" aria-hidden="true" tabindex="-1"></a>nboot <span class="ot">&lt;-</span> <span class="dv">1000</span> <span class="co"># n of bootstrap samples</span></span>
<span id="cb38-2"><a href="chapter6.html#cb38-2" aria-hidden="true" tabindex="-1"></a>coefm <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> nboot, <span class="at">ncol =</span> <span class="fu">length</span>(<span class="fu">coef</span>(fit.logit))) <span class="co"># matrix for bootstrap slope coefficients</span></span>
<span id="cb38-3"><a href="chapter6.html#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(coefm) <span class="ot">&lt;-</span> <span class="fu">names</span>(<span class="fu">coef</span>(fit.logit))</span>
<span id="cb38-4"><a href="chapter6.html#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="chapter6.html#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nboot) {</span>
<span id="cb38-6"><a href="chapter6.html#cb38-6" aria-hidden="true" tabindex="-1"></a>    bsample <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(mroz), <span class="at">size =</span> <span class="fu">nrow</span>(mroz), <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="co"># bootstrap sample</span></span>
<span id="cb38-7"><a href="chapter6.html#cb38-7" aria-hidden="true" tabindex="-1"></a>    coefm[b, ] <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">glm</span>(inlf <span class="sc">~</span> huswage <span class="sc">+</span> educ <span class="sc">+</span> exper <span class="sc">+</span> <span class="fu">I</span>(exper<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> age <span class="sc">+</span> kidslt6 <span class="sc">+</span> kidsge6,</span>
<span id="cb38-8"><a href="chapter6.html#cb38-8" aria-hidden="true" tabindex="-1"></a>                           <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>), <span class="at">data =</span> mroz, <span class="at">subset =</span> bsample))</span>
<span id="cb38-9"><a href="chapter6.html#cb38-9" aria-hidden="true" tabindex="-1"></a>} <span class="co"># end of for i</span></span>
<span id="cb38-10"><a href="chapter6.html#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="chapter6.html#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">head</span>(coefm), <span class="at">digits =</span> <span class="dv">3</span>) <span class="co"># a few first bootsrap estimates of the coefficients</span></span></code></pre></div>
<pre><code>##      (Intercept) huswage  educ exper I(exper^2)    age kidslt6 kidsge6
## [1,]      -0.153  -0.057 0.242 0.249     -0.004 -0.089  -1.644   0.145
## [2,]      -0.857  -0.061 0.233 0.218     -0.003 -0.067  -1.261   0.112
## [3,]       0.757  -0.053 0.196 0.181     -0.003 -0.087  -1.505   0.145
## [4,]       0.940  -0.066 0.238 0.218     -0.003 -0.109  -1.558   0.127
## [5,]       1.595  -0.060 0.211 0.249     -0.005 -0.117  -1.683   0.042
## [6,]       1.577  -0.059 0.234 0.165     -0.001 -0.114  -1.391   0.037</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="chapter6.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb40-2"><a href="chapter6.html#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="do">## bootstrab satandard error esimates</span></span>
<span id="cb40-3"><a href="chapter6.html#cb40-3" aria-hidden="true" tabindex="-1"></a>se.boot <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> coefm, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> sd)</span>
<span id="cb40-4"><a href="chapter6.html#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(se.boot, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## (Intercept)     huswage        educ       exper  I(exper^2)         age 
##       0.903       0.023       0.045       0.033       0.001       0.015 
##     kidslt6     kidsge6 
##       0.205       0.081</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="chapter6.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cbind</span>(<span class="fu">coef</span>(<span class="fu">summary</span>(fit.logit))[, <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)], se.boot), <span class="at">digits =</span> <span class="dv">3</span>) <span class="co"># original and bootstap s.e</span></span></code></pre></div>
<pre><code>##             Estimate Std. Error se.boot
## (Intercept)    0.592      0.853   0.903
## huswage       -0.044      0.021   0.023
## educ           0.209      0.043   0.045
## exper          0.210      0.032   0.033
## I(exper^2)    -0.003      0.001   0.001
## age           -0.091      0.014   0.015
## kidslt6       -1.432      0.202   0.205
## kidsge6        0.047      0.074   0.081</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="chapter6.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb44-2"><a href="chapter6.html#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="do">## boot backages includes advance bootstrapping procedures based on the book:</span></span>
<span id="cb44-3"><a href="chapter6.html#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Davison, A.C and D.V. Hinkley (1997). Bootstrap Methods and Their Applications.</span></span>
<span id="cb44-4"><a href="chapter6.html#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="do">##                                       Cambridge Univesity Press.</span></span>
<span id="cb44-5"><a href="chapter6.html#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="do">## web: http://statwww.epfl.ch/davison/BMA/</span></span>
<span id="cb44-6"><a href="chapter6.html#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb44-7"><a href="chapter6.html#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb44-8"><a href="chapter6.html#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(boot) <span class="co"># boot package</span></span>
<span id="cb44-9"><a href="chapter6.html#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb44-10"><a href="chapter6.html#cb44-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#help(boot) # for help</span></span>
<span id="cb44-11"><a href="chapter6.html#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb44-12"><a href="chapter6.html#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="do">## the function to be called is boot() of which the three key arguments are</span></span>
<span id="cb44-13"><a href="chapter6.html#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="do">## data, indluding the original sample (above the mroz data frame</span></span>
<span id="cb44-14"><a href="chapter6.html#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="do">## statistic, the statistic to be bootstrapped (abvove the coefficient vetor)</span></span>
<span id="cb44-15"><a href="chapter6.html#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="do">## R, the numebr of repliactions (above 100)</span></span>
<span id="cb44-16"><a href="chapter6.html#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb44-17"><a href="chapter6.html#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="do">## To replicate the above bootstrap example with boot we need to define</span></span>
<span id="cb44-18"><a href="chapter6.html#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="do">## for statistic an appropriate funciton</span></span>
<span id="cb44-19"><a href="chapter6.html#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb44-20"><a href="chapter6.html#cb44-20" aria-hidden="true" tabindex="-1"></a>coef.fn <span class="ot">&lt;-</span> <span class="cf">function</span>(data, index){ <span class="co"># data will include data and index the sampled rows set by boot</span></span>
<span id="cb44-21"><a href="chapter6.html#cb44-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">coef</span>(<span class="fu">glm</span>(inlf <span class="sc">~</span> huswage <span class="sc">+</span> educ <span class="sc">+</span> exper <span class="sc">+</span> <span class="fu">I</span>(exper<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> age <span class="sc">+</span> kidslt6 <span class="sc">+</span> kidsge6,</span>
<span id="cb44-22"><a href="chapter6.html#cb44-22" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> data, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>), <span class="at">subset =</span> index)))</span>
<span id="cb44-23"><a href="chapter6.html#cb44-23" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb44-24"><a href="chapter6.html#cb44-24" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">boot</span>(<span class="at">data =</span> mroz, <span class="at">statistic =</span> coef.fn, <span class="at">R =</span> <span class="dv">1000</span>) <span class="co"># bootstrap object</span></span>
<span id="cb44-25"><a href="chapter6.html#cb44-25" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(b) <span class="co"># structure of the b object</span></span></code></pre></div>
<pre><code>## List of 11
##  $ t0       : Named num [1:8] 0.5925 -0.0438 0.20927 0.20967 -0.00318 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;(Intercept)&quot; &quot;huswage&quot; &quot;educ&quot; &quot;exper&quot; ...
##  $ t        : num [1:1000, 1:8] 0.189 0.671 0.127 1.42 0.234 ...
##  $ R        : num 1000
##  $ data     :&#39;data.frame&#39;:   753 obs. of  7 variables:
##   ..$ inlf   : int [1:753] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ huswage: num [1:753] 4.03 8.44 3.58 3.54 10 ...
##   ..$ educ   : int [1:753] 12 12 12 12 14 12 16 12 12 12 ...
##   ..$ exper  : int [1:753] 14 5 15 6 7 33 11 35 24 21 ...
##   ..$ age    : int [1:753] 32 30 35 34 31 54 37 54 48 39 ...
##   ..$ kidslt6: int [1:753] 1 0 1 0 1 0 0 0 0 0 ...
##   ..$ kidsge6: int [1:753] 0 2 3 3 2 0 2 0 2 2 ...
##  $ seed     : int [1:626] 10403 323 -185828344 -1690977408 2063634284 2126413505 1457180909 1464198481 1352935467 1433208526 ...
##  $ statistic:function (data, index)  
##   ..- attr(*, &quot;srcref&quot;)= &#39;srcref&#39; int [1:8] 43 12 46 5 12 5 43 46
##   .. ..- attr(*, &quot;srcfile&quot;)=Classes &#39;srcfilecopy&#39;, &#39;srcfile&#39; &lt;environment: 0x0000000014d5b998&gt; 
##  $ sim      : chr &quot;ordinary&quot;
##  $ call     : language boot(data = mroz, statistic = coef.fn, R = 1000)
##  $ stype    : chr &quot;i&quot;
##  $ strata   : num [1:753] 1 1 1 1 1 1 1 1 1 1 ...
##  $ weights  : num [1:753] 0.00133 0.00133 0.00133 0.00133 0.00133 ...
##  - attr(*, &quot;class&quot;)= chr &quot;boot&quot;
##  - attr(*, &quot;boot_type&quot;)= chr &quot;boot&quot;</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="chapter6.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(b<span class="sc">$</span>t) <span class="co"># six first lines of the bootstrapped coefficients</span></span></code></pre></div>
<pre><code>##           [,1]        [,2]      [,3]      [,4]         [,5]        [,6]
## [1,] 0.1887090 -0.02365513 0.2372744 0.1914703 -0.003236789 -0.08717578
## [2,] 0.6705535 -0.06306132 0.2485857 0.2195537 -0.002233515 -0.10961670
## [3,] 0.1271517 -0.05613699 0.2842374 0.1544412 -0.001611106 -0.08879443
## [4,] 1.4197738 -0.06222460 0.1565783 0.1814791 -0.002397134 -0.08888121
## [5,] 0.2340466 -0.02962024 0.1907106 0.2081037 -0.003297360 -0.08539290
## [6,] 0.5556055 -0.03951345 0.1589620 0.1926016 -0.002483546 -0.07537368
##           [,7]        [,8]
## [1,] -1.176191 -0.03535283
## [2,] -1.522841  0.16300002
## [3,] -1.618449  0.02880192
## [4,] -1.608595  0.08704198
## [5,] -1.327913  0.09914998
## [6,] -1.278560  0.06837032</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="chapter6.html#cb48-1" aria-hidden="true" tabindex="-1"></a>se.boot2 <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> b<span class="sc">$</span>t, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> sd) <span class="co"># bootstrap standard errors</span></span>
<span id="cb48-2"><a href="chapter6.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cbind</span>(<span class="fu">coef</span>(<span class="fu">summary</span>(fit.logit))[, <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)], se.boot2), <span class="at">digits =</span> <span class="dv">3</span>) <span class="co"># original and bootstap s.e</span></span></code></pre></div>
<pre><code>##             Estimate Std. Error se.boot2
## (Intercept)    0.592      0.853    0.853
## huswage       -0.044      0.021    0.022
## educ           0.209      0.043    0.045
## exper          0.210      0.032    0.033
## I(exper^2)    -0.003      0.001    0.001
## age           -0.091      0.014    0.014
## kidslt6       -1.432      0.202    0.213
## kidsge6        0.047      0.074    0.080</code></pre>
<ul>
<li>Here we observe that there are no big differences between the bootstrapped standard errors and the original ones.</li>
</ul>
</div>
<div id="remark-5" class="section level4 unnumbered hasAnchor">
<h4>Remark<a href="chapter6.html#remark-5" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Because of the sampling with replacement some observations do not enter into a
particular bootstrap sample and some observations appear multiple times.</p></li>
<li><p>When the response variable is a dichotomous variable (0/1) like in the above
example, there is always a possibility to get a bootstrap sample where all response
values are the same (either 0 or 1), in which case estimation fails and it may
crash the whole bootstrap sampling.</p></li>
<li><p>There are special designed bootstrap procedures to avoid these situation (one
simple procedure is to simply discard those samples).</p></li>
<li><p>With <span class="math inline">\(n\)</span> observations, there are</p></li>
</ul>
<p><span class="math display">\[
{\left( \begin{matrix}
2n-1 \\
n \\
\end{matrix} \right)}
\]</span></p>
<p>      different bootstrap samples.</p>
<p><br></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter5.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter7.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ILR.pdf", "ILR.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
