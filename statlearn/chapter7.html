<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Model Selection | Statistical Learning</title>
  <meta name="description" content="This is a Statistical Learning" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Model Selection | Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Statistical Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Model Selection | Statistical Learning" />
  
  <meta name="twitter:description" content="This is a Statistical Learning" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2023-04-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter6.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>머리말</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#understanding-data"><i class="fa fa-check"></i><b>1.1</b> Understanding Data</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="chapter1.html"><a href="chapter1.html#example-1-supervised-learning-continuous-output"><i class="fa fa-check"></i><b>1.1.1</b> Example 1 (Supervised learning: Continuous output)</a></li>
<li class="chapter" data-level="1.1.2" data-path="chapter1.html"><a href="chapter1.html#example-2-supervised-leraning-categorical-output"><i class="fa fa-check"></i><b>1.1.2</b> Example 2 (Supervised leraning: Categorical output)</a></li>
<li class="chapter" data-level="1.1.3" data-path="chapter1.html"><a href="chapter1.html#example-3-unsupervised-learning-clustering-observations"><i class="fa fa-check"></i><b>1.1.3</b> Example 3 (Unsupervised learning: Clustering observations)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#brief-history-of-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Brief History of Statistical Learning</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#notation-and-simple-matrix-algebra"><i class="fa fa-check"></i><b>1.3</b> Notation and Simple Matrix Algebra</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#what-is-statitical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statitical Learning</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chapter2.html"><a href="chapter2.html#prediction"><i class="fa fa-check"></i><b>2.1.1</b> Prediction</a></li>
<li class="chapter" data-level="2.1.2" data-path="chapter2.html"><a href="chapter2.html#inference"><i class="fa fa-check"></i><b>2.1.2</b> Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="chapter2.html"><a href="chapter2.html#estimating-f"><i class="fa fa-check"></i><b>2.1.3</b> Estimating <span class="math inline">\(f\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="chapter2.html"><a href="chapter2.html#prediction-accuaracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.4</b> Prediction Accuaracy and Model Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter2.html"><a href="chapter2.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="chapter2.html"><a href="chapter2.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="2.2.3" data-path="chapter2.html"><a href="chapter2.html#classification-problems"><i class="fa fa-check"></i><b>2.2.3</b> Classification Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#basic-commands"><i class="fa fa-check"></i><b>3.1</b> Basic Commands</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#graphics"><i class="fa fa-check"></i><b>3.2</b> Graphics</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#indexing-data"><i class="fa fa-check"></i><b>3.3</b> Indexing Data</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#loading-data"><i class="fa fa-check"></i><b>3.4</b> Loading Data</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#additional-graphical-and-numerical-summaries"><i class="fa fa-check"></i><b>3.5</b> Additional Graphical and Numerical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#simple-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Regression</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#multiple-regression"><i class="fa fa-check"></i><b>4.2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chapter4.html"><a href="chapter4.html#importance-of-predictors-statistical-significance-of-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Importance of Predictors: Statistical Significance of Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="chapter4.html"><a href="chapter4.html#selecting-important-variables"><i class="fa fa-check"></i><b>4.2.2</b> Selecting Important Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="chapter4.html"><a href="chapter4.html#model-fit"><i class="fa fa-check"></i><b>4.2.3</b> Model Fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="chapter4.html"><a href="chapter4.html#prediction-1"><i class="fa fa-check"></i><b>4.2.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#other-consideration"><i class="fa fa-check"></i><b>4.3</b> Other Consideration</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chapter4.html"><a href="chapter4.html#qualitative-predictors"><i class="fa fa-check"></i><b>4.3.1</b> Qualitative Predictors</a></li>
<li class="chapter" data-level="" data-path="chapter4.html"><a href="chapter4.html#example-8"><i class="fa fa-check"></i>Example 8</a></li>
<li class="chapter" data-level="4.3.2" data-path="chapter4.html"><a href="chapter4.html#potential-problems"><i class="fa fa-check"></i><b>4.3.2</b> Potential Problems</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#non-parametric-regressions"><i class="fa fa-check"></i><b>4.4</b> Non-parametric Regressions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#logit-and-probit-models"><i class="fa fa-check"></i><b>5.1</b> Logit and Probit Models</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#confounding"><i class="fa fa-check"></i><b>5.2</b> Confounding</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#logit-model-for-multiple-classes"><i class="fa fa-check"></i><b>5.3</b> Logit Model for Multiple Classes</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#discriminant-analysis"><i class="fa fa-check"></i><b>5.4</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chapter5.html"><a href="chapter5.html#bayes-theorem-for-classification"><i class="fa fa-check"></i><b>5.4.1</b> Bayes Theorem for Classification</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapter5.html"><a href="chapter5.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="5.4.3" data-path="chapter5.html"><a href="chapter5.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.3</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#naive-bayes"><i class="fa fa-check"></i><b>5.5</b> Naive Bayes</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#knn-classification"><i class="fa fa-check"></i><b>5.6</b> KNN Classification</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#example-10"><i class="fa fa-check"></i><b>5.7</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Resampling Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#validation-set-approach"><i class="fa fa-check"></i><b>6.1</b> Validation Set Approach</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.2</b> K-fold Cross-validation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chapter6.html"><a href="chapter6.html#example-11"><i class="fa fa-check"></i><b>6.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#bootstrap"><i class="fa fa-check"></i><b>6.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Model Selection</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#variable-selection"><i class="fa fa-check"></i><b>7.1</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chapter7.html"><a href="chapter7.html#best-subset-selection"><i class="fa fa-check"></i><b>7.1.1</b> Best Subset Selection</a></li>
<li class="chapter" data-level="7.1.2" data-path="chapter7.html"><a href="chapter7.html#forward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.2</b> Forward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.3" data-path="chapter7.html"><a href="chapter7.html#backward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.3</b> Backward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.4" data-path="chapter7.html"><a href="chapter7.html#choosing-the-optimal-model"><i class="fa fa-check"></i><b>7.1.4</b> Choosing the Optimal Model</a></li>
<li class="chapter" data-level="7.1.5" data-path="chapter7.html"><a href="chapter7.html#example-13"><i class="fa fa-check"></i><b>7.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#shrinkage-methods"><i class="fa fa-check"></i><b>7.2</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chapter7.html"><a href="chapter7.html#ridge-regression"><i class="fa fa-check"></i><b>7.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="chapter7.html"><a href="chapter7.html#rasso-regression"><i class="fa fa-check"></i><b>7.2.2</b> RASSO Regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="chapter7.html"><a href="chapter7.html#example-14"><i class="fa fa-check"></i><b>7.2.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#dimension-reduction-methods"><i class="fa fa-check"></i><b>7.3</b> Dimension Reduction Methods</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="chapter7.html"><a href="chapter7.html#principal-components-regression"><i class="fa fa-check"></i><b>7.3.1</b> Principal Components Regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="chapter7.html"><a href="chapter7.html#example-15"><i class="fa fa-check"></i><b>7.3.2</b> Example</a></li>
<li class="chapter" data-level="7.3.3" data-path="chapter7.html"><a href="chapter7.html#partial-least-squares"><i class="fa fa-check"></i><b>7.3.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="7.3.4" data-path="chapter7.html"><a href="chapter7.html#example-16"><i class="fa fa-check"></i><b>7.3.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter7" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Model Selection<a href="chapter7.html#chapter7" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li>Model selection (subset selection, feature seletion, variable selection) in the linear regression</li>
</ul>
<p><span class="math display">\[
Y=\beta_0+\beta_1X_1 +\cdots +\beta_pX_p +\epsilon
\]</span></p>
<p>      refers typically to the selection of the most appropriate subset from the <span class="math inline">\(p\)</span> explanatory variables that best predict and capture variability in <span class="math inline">\(Y\)</span>.</p>
<ul>
<li><p>Despite its simplicity, the linear model has distinct advantages in terms of its <em>interpretability</em> and often shows good <em>predictive performance</em>.</p></li>
<li><p>Hence we discuss some ways in which the simple linear model can be improved, by replacing ordinary least squares fitting with some alternative fitting procedures.</p></li>
</ul>
<div id="why-consider-alternatives-to-least-squares" class="section level4 unnumbered hasAnchor">
<h4>Why consider alternatives to least squares?<a href="chapter7.html#why-consider-alternatives-to-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Prediction accuracy: especially when <span class="math inline">\(p&gt;n\)</span>, to control the variance.</li>
<li>Model interpretability: By removing irrelevant features - that is, by setting the corresponding coefficient estimates to zero - we can obtain a model that is more easily interpreted.
<ul>
<li>We will present some approaches for automatically performing <em>feature selection</em>.</li>
</ul></li>
</ul>
</div>
<div id="three-classes-of-methods" class="section level4 unnumbered hasAnchor">
<h4>Three classes of methods<a href="chapter7.html#three-classes-of-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Subset selection</strong>
<ul>
<li>We identify a subset of the <span class="math inline">\(p\)</span> predictors that we believe to be related to the response.</li>
<li>We then fit a model using least squares on the reduced set of variables.</li>
</ul></li>
<li><strong>Shrinkage</strong>
<ul>
<li>We fit a model involving all <span class="math inline">\(p\)</span> predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates.</li>
<li>This shrinkage (also known as <em>regularization</em> or <em>penalization</em>) has th effect of reducing variance and can also perform variable selection.</li>
</ul></li>
<li><strong>Dimension reduction</strong>
<ul>
<li>We project the <span class="math inline">\(p\)</span> predictors into a <span class="math inline">\(M\)</span>-dimensional subspace, where <span class="math inline">\(M&lt;p\)</span>.</li>
<li>This is achieved by computing <span class="math inline">\(M\)</span> different <em>linear combinations</em>, or <em>projections</em>, of the variables.</li>
<li>Then these <span class="math inline">\(M\)</span> projections are used as predictors to fit a linear regression model by least squares.</li>
</ul></li>
</ul>
</div>
<div id="variable-selection" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Variable Selection<a href="chapter7.html#variable-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Best subset and stepwise model selection procedures.</li>
</ul>
<div id="best-subset-selection" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Best Subset Selection<a href="chapter7.html#best-subset-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the <em>null model</em>, which contains no predictors.</li>
</ol>
<p>      - This model simply predicts the sample mean for each observation.</p>
<ol start="2" style="list-style-type: decimal">
<li>For <span class="math inline">\(k=1,2,\ldots,p\)</span>:</li>
</ol>
<p>      (a) Fit all <span class="math inline">\(\binom{p}{k}\)</span> models that contains exactly <span class="math inline">\(k\)</span> predictors.</p>
<p>      (b) Pick the best among these <span class="math inline">\(\binom{p}{k}\)</span> models, and call it <span class="math inline">\(M_k\)</span>.</p>
<p>      - Here <em>best</em> is defined as having the smallest RSS, or equivalently largest <span class="math inline">\(R^2\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>Select a single best model from among <span class="math inline">\(M_0,\ldots, M_p\)</span> using cross-validated prediction error, <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p><img src="fig7/f1.png" /></p>
<ul>
<li>For each possible model containing a subset of the ten predictors in the Credit data set, the RSS and <span class="math inline">\(R^2\)</span> are displayed.
<ul>
<li>The red frontier tracks the <em>best</em> model for a given number of predictors, according to RSS and <span class="math inline">\(R^2\)</span>.</li>
<li>Though the data set contaings only ten predictors, the <span class="math inline">\(x\)</span>-axis ranges from 1 to 11, since one of the variables is categorical and takes on three values, leading th the creation of two dummy variables.</li>
</ul></li>
<li>Although we have presented best subset selection here for least squares regression, the same ideas apply to other types of models, such as logistic regression.
<ul>
<li>The deviance - negative two times the maximized log-likelihood - plays the role of RSS for a broader class of models.</li>
</ul></li>
</ul>
<div id="stepwise-selection" class="section level4 unnumbered hasAnchor">
<h4>Stepwise selection<a href="chapter7.html#stepwise-selection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>For computational reasons, best subset selection cannot be applied with very large <span class="math inline">\(p\)</span>. <em>why not?</em></p></li>
<li><p>Best subset selection may also suffer from statistical problems when <span class="math inline">\(p\)</span> is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive poser on future data.</p></li>
<li><p>Thus an enormous search space can lead to <em>overfitting</em> and high variance of the coefficient estimates.</p></li>
<li><p>For both of these reasons, <em>stepwise</em> methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection</p></li>
</ul>
</div>
</div>
<div id="forward-step-wise-selection" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Forward Step Wise Selection<a href="chapter7.html#forward-step-wise-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.</p></li>
<li><p>In particular, at each step the variable that gives the greatest <em>additional</em> improvement to the fit is added to the model.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the <em>null</em> model, which contains no predictors.</li>
<li>For <span class="math inline">\(k=0,\ldots,p-1\)</span>:</li>
</ol>
<p>      (a) Consider all <span class="math inline">\(p-k\)</span> models that augment the predictors in <span class="math inline">\(M_k\)</span> with one additional predictor.</p>
<p>      (b) Choose the <em>best</em> among these <span class="math inline">\(p-k\)</span> models, and call it <span class="math inline">\(M_{k+1}\)</span>.</p>
<p>      - Here <em>best</em> is defined as having smallest RSS or highest <span class="math inline">\(R^2\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>Select a single best model from among <span class="math inline">\(M_0, \ldots, M_p\)</span> using cross-validated prediction error, <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<ul>
<li><p>Computational advantage over best subset selection is clear.</p></li>
<li><p>It is not guaranteed to find the best possible model out of all <span class="math inline">\(2^p\)</span> models containing subsets of the <span class="math inline">\(p\)</span> predictors.</p></li>
</ul>
<p><img src="fig7/f2.png" /></p>
<ul>
<li>The first four selected models for best subset selection and forward stepwise selection on the Credit data set.
<ul>
<li>The first three models are identical but the fourth models differ.</li>
</ul></li>
</ul>
</div>
<div id="backward-step-wise-selection" class="section level3 hasAnchor" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Backward Step Wise Selection<a href="chapter7.html#backward-step-wise-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Like forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.</p></li>
<li><p>However, unlike forward stepwise selection, it begins with the full least squares model containing all <span class="math inline">\(p\)</span> predictors, and then iteratively removes the least useful predictor, one-at-a-time.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_p\)</span> denote the full model, which contains all <span class="math inline">\(p\)</span> predictors.</li>
<li>For <span class="math inline">\(k=p, p-1, \ldots, 1\)</span>:</li>
</ol>
<p>      (a) Consider all <span class="math inline">\(k\)</span> models that contain all but one of the predictors in <span class="math inline">\(M_k\)</span>, for a total of <span class="math inline">\(k-1\)</span> predictors.</p>
<p>      (b) Choose the best among these <span class="math inline">\(k\)</span> models, and call it <span class="math inline">\(M_k-1\)</span>.</p>
<p>      - Here best is defined as having smallest RSS or highest <span class="math inline">\(R^2\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>Select a single best model from among <span class="math inline">\(M_0,\ldots,M_p\)</span> using cross-validated prediction error, <span class="math inline">\(C_p\)</span> (AIC)
, BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<ul>
<li><p>Like forward stepwise selection, the backward selection approach searches through only <span class="math inline">\(1+p(p+1)/2\)</span> models, and so can be applied in settings where <span class="math inline">\(p\)</span> is too large to apply best subset selection.</p></li>
<li><p>Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the <em>best</em> model containing a subset of the <span class="math inline">\(p\)</span> predictors.</p></li>
<li><p>Backward selection requires that the <em>number of samples <span class="math inline">\(n\)</span> is larger than the number of variables <span class="math inline">\(p\)</span></em> (so that the full model can be fit).</p>
<ul>
<li>In contrast, forward stepwise can be used even when <span class="math inline">\(n&lt;p\)</span>, and so is the only variable subset method when <span class="math inline">\(p\)</span> is very large.</li>
</ul></li>
</ul>
</div>
<div id="choosing-the-optimal-model" class="section level3 hasAnchor" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Choosing the Optimal Model<a href="chapter7.html#choosing-the-optimal-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The model containing all of the predictors will always havethe smallest RSS and the largest <span class="math inline">\(R^2\)</span>, since these quantities are related to the training error.</p></li>
<li><p>We wish to choose a model with low test error, not a model with low training error.</p>
<ul>
<li>Recall that training error is usually a poor estimate of test error.</li>
</ul></li>
<li><p>Therefore, RSS and <span class="math inline">\(R^2\)</span> are not suitable for selecting the best model among a collection of models with different numbers of predictors.</p></li>
</ul>
<div id="estimating-test-error-two-approaches" class="section level4 unnumbered hasAnchor">
<h4>Estimating test error: two approaches<a href="chapter7.html#estimating-test-error-two-approaches" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.</p></li>
<li><p>We can directly estimate the test error, using either a validation set approach or a cross-validation approach.</p></li>
</ul>
</div>
<div id="c_p-aic-bic-and-adjusted-r2" class="section level4 unnumbered hasAnchor">
<h4><span class="math inline">\(C_p\)</span>, AIC, BIC, and Adjusted <span class="math inline">\(R^2\)</span><a href="chapter7.html#c_p-aic-bic-and-adjusted-r2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>These techniques adjust the training error for the model size, can be used to select among a set of models with different numbers of variables.</p></li>
<li><p>The next figure displays <span class="math inline">\(C_p\)</span>, BIC and adjusted <span class="math inline">\(R^2\)</span> for the best model of each size produced by bet subset selection on the Credit data set.</p></li>
</ul>
<p><img src="fig7/f3.png" /></p>
<ul>
<li><strong>Mallow’s <span class="math inline">\(C_p\)</span></strong>:</li>
</ul>
<p><span class="math display">\[
C_p = \frac{1}{n}(RSS+2d\hat{\sigma}^2)
\]</span></p>
<p>      where <span class="math inline">\(d\)</span> is the total # of parameters used and <span class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the variance of the error <span class="math inline">\(\epsilon\)</span> associated with each response measurement.</p>
<ul>
<li>The <strong>AIC</strong> criterion is defined for a large class of models fit by maximum likelihood:</li>
</ul>
<p><span class="math display">\[
AIC=-2logL+2\cdot d
\]</span></p>
<p>      where <span class="math inline">\(L\)</span> is the maximized value of the likelihood function for the etimated model.</p>
<ul>
<li><p>In the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and <span class="math inline">\(C_p\)</span> and AIC are equivalent.</p></li>
<li><p><strong>BIC</strong>:</p></li>
</ul>
<p><span class="math display">\[
BIC=\frac{1}{n}(RSS+log(n)d\hat{\sigma}^2)
\]</span></p>
<ul>
<li><p>Like <span class="math inline">\(C_p\)</span>, the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest
BIC value.</p></li>
<li><p>Notice that BIC replaces the <span class="math inline">\(2d\hat{\sigma}^2\)</span> used by <span class="math inline">\(C_p\)</span> with a <span class="math inline">\(log(n)d\hat{\sigma}^2\)</span> term, where <span class="math inline">\(n\)</span> is the number of observations.</p></li>
<li><p>Since <span class="math inline">\(logn&gt;2\)</span> for any <span class="math inline">\(n&gt;7\)</span>, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than <span class="math inline">\(C_p\)</span>.</p></li>
<li><p>For a least squares model with <span class="math inline">\(d\)</span> variables, the <strong>adjusted <span class="math inline">\(R^2\)</span></strong> statistic is calculated as</p></li>
</ul>
<p><span class="math display">\[
Adjusted \,\,\,R^2 =1-\frac{RSS/(n-d-1)}{TSS/(n-1)}
\]</span></p>
<p>      where TSS is the total sum of squares.</p>
<ul>
<li><p>Unlike <span class="math inline">\(C_p\)</span>, AIC, and BIC, for which a <em>small</em> value indicates a model with a low test error, a <em>large</em> value of adjusted <span class="math inline">\(R^2\)</span> indicates a model with a small test error.</p></li>
<li><p>Maximizing the adjusted <span class="math inline">\(R^2\)</span> is equivalent to minimizing <span class="math inline">\(\frac{RSS}{n-d-1}\)</span>.</p>
<ul>
<li>While RSS always decreases as the number of variables in the model increases, <span class="math inline">\(\frac{RSS}{n-d-1}\)</span> may increase or decrease, due to the presence of <span class="math inline">\(d\)</span> in the denominator.</li>
</ul></li>
<li><p>Unlike the <span class="math inline">\(R^2\)</span> statistic, the adjusted <span class="math inline">\(R^2\)</span> statistic <em>pays a price</em> for the inclusion of unnecessary variables in the model.</p></li>
</ul>
</div>
<div id="validation-and-cross-validation" class="section level4 unnumbered hasAnchor">
<h4>Validation and Cross-validation<a href="chapter7.html#validation-and-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Each of the procedures returns a sequence of models <span class="math inline">\(M_k\)</span> indexed by model size <span class="math inline">\(k=0,1,2,\ldots\)</span>.</p>
<ul>
<li>Our job here is to select <span class="math inline">\(\hat{k}\)</span>.</li>
<li>Once selected, we will return model <span class="math inline">\(M_{\hat{k}}\)</span></li>
</ul></li>
<li><p>We compute the validation set error or the cross-validation error for each model <span class="math inline">\(M_k\)</span> under consideration, and then select the <span class="math inline">\(k\)</span> for which the resulting estimated test error is smallest.</p></li>
<li><p>This procedure has an advantage relative to AIC, BIC, <span class="math inline">\(C_p\)</span>, and adjusted <span class="math inline">\(R^2\)</span>, in that it provides a direct estimate of the test error, and <em>doesn’t require an estimate of the error variance <span class="math inline">\(\sigma^2\)</span></em>.</p></li>
<li><p>It can also be used in a wider range of model selection tasks, even in caes where it is hard to pinpoint the model degrees of freedom (e.g. the number of predictors in the model) or hard to estimate the error variance <span class="math inline">\(\sigma^2\)</span></p></li>
</ul>
<p><img src="fig7/f4.png" /></p>
<ul>
<li><p>The validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set.</p></li>
<li><p>The cross-validation errors were computed using <span class="math inline">\(k=10\)</span> folds.</p>
<ul>
<li>In this case, the validation and cross-validation methods both result in a six-variable model.</li>
</ul></li>
<li><p>However, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.</p></li>
<li><p>In this setting, we can select a model using the <em>one-standard-error rule</em>.</p>
<ul>
<li>We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.</li>
<li>What is the rationale for this?</li>
</ul></li>
</ul>
</div>
</div>
<div id="example-13" class="section level3 hasAnchor" number="7.1.5">
<h3><span class="header-section-number">7.1.5</span> Example<a href="chapter7.html#example-13" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Consider the Hitters data set in the ISLR2 package which contains data on baseball players.</p></li>
<li><p>We predict a player’s Salary on the basis of available background data.</p></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chapter7.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet) <span class="co"># package for ridge and lasso (and elastic net, i.e., part lasso part ridge)</span></span>
<span id="cb1-2"><a href="chapter7.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps)</span>
<span id="cb1-3"><a href="chapter7.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb1-4"><a href="chapter7.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(Hitters) <span class="co"># variables in the data set</span></span></code></pre></div>
<pre><code>##  [1] &quot;AtBat&quot;     &quot;Hits&quot;      &quot;HmRun&quot;     &quot;Runs&quot;      &quot;RBI&quot;       &quot;Walks&quot;    
##  [7] &quot;Years&quot;     &quot;CAtBat&quot;    &quot;CHits&quot;     &quot;CHmRun&quot;    &quot;CRuns&quot;     &quot;CRBI&quot;     
## [13] &quot;CWalks&quot;    &quot;League&quot;    &quot;Division&quot;  &quot;PutOuts&quot;   &quot;Assists&quot;   &quot;Errors&quot;   
## [19] &quot;Salary&quot;    &quot;NewLeague&quot;</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="chapter7.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#help(Hitters) # more detailed descriotion of the data set inluding variable descriptions</span></span>
<span id="cb3-2"><a href="chapter7.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Hitters) <span class="co"># Hitters data example data in the package</span></span></code></pre></div>
<pre><code>##                   AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun
## -Andy Allanson      293   66     1   30  29    14     1    293    66      1
## -Alan Ashby         315   81     7   24  38    39    14   3449   835     69
## -Alvin Davis        479  130    18   66  72    76     3   1624   457     63
## -Andre Dawson       496  141    20   65  78    37    11   5628  1575    225
## -Andres Galarraga   321   87    10   39  42    30     2    396   101     12
## -Alfredo Griffin    594  169     4   74  51    35    11   4408  1133     19
##                   CRuns CRBI CWalks League Division PutOuts Assists Errors
## -Andy Allanson       30   29     14      A        E     446      33     20
## -Alan Ashby         321  414    375      N        W     632      43     10
## -Alvin Davis        224  266    263      A        W     880      82     14
## -Andre Dawson       828  838    354      N        E     200      11      3
## -Andres Galarraga    48   46     33      N        E     805      40      4
## -Alfredo Griffin    501  336    194      A        W     282     421     25
##                   Salary NewLeague
## -Andy Allanson        NA         A
## -Alan Ashby        475.0         N
## -Alvin Davis       480.0         A
## -Andre Dawson      500.0         N
## -Andres Galarraga   91.5         N
## -Alfredo Griffin   750.0         A</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="chapter7.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(Hitters) <span class="co"># nobs n vars</span></span></code></pre></div>
<pre><code>## [1] 322  20</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="chapter7.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(Hitters) <span class="co"># structure (shows typse of variables)</span></span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    322 obs. of  20 variables:
##  $ AtBat    : int  293 315 479 496 321 594 185 298 323 401 ...
##  $ Hits     : int  66 81 130 141 87 169 37 73 81 92 ...
##  $ HmRun    : int  1 7 18 20 10 4 1 0 6 17 ...
##  $ Runs     : int  30 24 66 65 39 74 23 24 26 49 ...
##  $ RBI      : int  29 38 72 78 42 51 8 24 32 66 ...
##  $ Walks    : int  14 39 76 37 30 35 21 7 8 65 ...
##  $ Years    : int  1 14 3 11 2 11 2 3 2 13 ...
##  $ CAtBat   : int  293 3449 1624 5628 396 4408 214 509 341 5206 ...
##  $ CHits    : int  66 835 457 1575 101 1133 42 108 86 1332 ...
##  $ CHmRun   : int  1 69 63 225 12 19 1 0 6 253 ...
##  $ CRuns    : int  30 321 224 828 48 501 30 41 32 784 ...
##  $ CRBI     : int  29 414 266 838 46 336 9 37 34 890 ...
##  $ CWalks   : int  14 375 263 354 33 194 24 12 8 866 ...
##  $ League   : Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 2 1 2 1 ...
##  $ Division : Factor w/ 2 levels &quot;E&quot;,&quot;W&quot;: 1 2 2 1 1 2 1 2 2 1 ...
##  $ PutOuts  : int  446 632 880 200 805 282 76 121 143 0 ...
##  $ Assists  : int  33 43 82 11 40 421 127 283 290 0 ...
##  $ Errors   : int  20 10 14 3 4 25 7 9 19 0 ...
##  $ Salary   : num  NA 475 480 500 91.5 750 70 100 75 1100 ...
##  $ NewLeague: Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 1 1 2 1 ...</code></pre>
<ul>
<li><p>Consider first selecting the best subset with respect to a given criterion.</p></li>
<li><p>Earlier we utilized the car package. Here we utilize leaps package.</p></li>
<li><p>Function regsubsets(), which is part of the package, can be used to identify the best subset in terms of the residual sum of squares, RSS.</p></li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="chapter7.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(Hitters) <span class="co"># basic summary statistics</span></span></code></pre></div>
<pre><code>##      AtBat            Hits         HmRun            Runs       
##  Min.   : 16.0   Min.   :  1   Min.   : 0.00   Min.   :  0.00  
##  1st Qu.:255.2   1st Qu.: 64   1st Qu.: 4.00   1st Qu.: 30.25  
##  Median :379.5   Median : 96   Median : 8.00   Median : 48.00  
##  Mean   :380.9   Mean   :101   Mean   :10.77   Mean   : 50.91  
##  3rd Qu.:512.0   3rd Qu.:137   3rd Qu.:16.00   3rd Qu.: 69.00  
##  Max.   :687.0   Max.   :238   Max.   :40.00   Max.   :130.00  
##                                                                
##       RBI             Walks            Years            CAtBat       
##  Min.   :  0.00   Min.   :  0.00   Min.   : 1.000   Min.   :   19.0  
##  1st Qu.: 28.00   1st Qu.: 22.00   1st Qu.: 4.000   1st Qu.:  816.8  
##  Median : 44.00   Median : 35.00   Median : 6.000   Median : 1928.0  
##  Mean   : 48.03   Mean   : 38.74   Mean   : 7.444   Mean   : 2648.7  
##  3rd Qu.: 64.75   3rd Qu.: 53.00   3rd Qu.:11.000   3rd Qu.: 3924.2  
##  Max.   :121.00   Max.   :105.00   Max.   :24.000   Max.   :14053.0  
##                                                                      
##      CHits            CHmRun           CRuns             CRBI        
##  Min.   :   4.0   Min.   :  0.00   Min.   :   1.0   Min.   :   0.00  
##  1st Qu.: 209.0   1st Qu.: 14.00   1st Qu.: 100.2   1st Qu.:  88.75  
##  Median : 508.0   Median : 37.50   Median : 247.0   Median : 220.50  
##  Mean   : 717.6   Mean   : 69.49   Mean   : 358.8   Mean   : 330.12  
##  3rd Qu.:1059.2   3rd Qu.: 90.00   3rd Qu.: 526.2   3rd Qu.: 426.25  
##  Max.   :4256.0   Max.   :548.00   Max.   :2165.0   Max.   :1659.00  
##                                                                      
##      CWalks        League  Division    PutOuts          Assists     
##  Min.   :   0.00   A:175   E:157    Min.   :   0.0   Min.   :  0.0  
##  1st Qu.:  67.25   N:147   W:165    1st Qu.: 109.2   1st Qu.:  7.0  
##  Median : 170.50                    Median : 212.0   Median : 39.5  
##  Mean   : 260.24                    Mean   : 288.9   Mean   :106.9  
##  3rd Qu.: 339.25                    3rd Qu.: 325.0   3rd Qu.:166.0  
##  Max.   :1566.00                    Max.   :1378.0   Max.   :492.0  
##                                                                     
##      Errors          Salary       NewLeague
##  Min.   : 0.00   Min.   :  67.5   A:176    
##  1st Qu.: 3.00   1st Qu.: 190.0   N:146    
##  Median : 6.00   Median : 425.0            
##  Mean   : 8.04   Mean   : 535.9            
##  3rd Qu.:11.00   3rd Qu.: 750.0            
##  Max.   :32.00   Max.   :2460.0            
##                  NA&#39;s   :59</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="chapter7.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters)) <span class="co"># full regression with all variables</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Salary ~ ., data = Hitters)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -907.62 -178.35  -31.11  139.09 1877.04 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  163.10359   90.77854   1.797 0.073622 .  
## AtBat         -1.97987    0.63398  -3.123 0.002008 ** 
## Hits           7.50077    2.37753   3.155 0.001808 ** 
## HmRun          4.33088    6.20145   0.698 0.485616    
## Runs          -2.37621    2.98076  -0.797 0.426122    
## RBI           -1.04496    2.60088  -0.402 0.688204    
## Walks          6.23129    1.82850   3.408 0.000766 ***
## Years         -3.48905   12.41219  -0.281 0.778874    
## CAtBat        -0.17134    0.13524  -1.267 0.206380    
## CHits          0.13399    0.67455   0.199 0.842713    
## CHmRun        -0.17286    1.61724  -0.107 0.914967    
## CRuns          1.45430    0.75046   1.938 0.053795 .  
## CRBI           0.80771    0.69262   1.166 0.244691    
## CWalks        -0.81157    0.32808  -2.474 0.014057 *  
## LeagueN       62.59942   79.26140   0.790 0.430424    
## DivisionW   -116.84925   40.36695  -2.895 0.004141 ** 
## PutOuts        0.28189    0.07744   3.640 0.000333 ***
## Assists        0.37107    0.22120   1.678 0.094723 .  
## Errors        -3.36076    4.39163  -0.765 0.444857    
## NewLeagueN   -24.76233   79.00263  -0.313 0.754218    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 315.6 on 243 degrees of freedom
##   (결측으로 인하여 59개의 관측치가 삭제되었습니다.)
## Multiple R-squared:  0.5461, Adjusted R-squared:  0.5106 
## F-statistic: 15.39 on 19 and 243 DF,  p-value: &lt; 2.2e-16</code></pre>
<ul>
<li>Significant <span class="math inline">\(t\)</span>-alues suggest that only AtBat, Hits, Walks, CWalks, Division, and PutOuts (and possibly CRuns and Assists that are 10% significant) have explanatory power.</li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="chapter7.html#cb13-1" aria-hidden="true" tabindex="-1"></a>fit.full <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters,  <span class="at">nvmax =</span> <span class="dv">19</span>) <span class="co"># all subsets</span></span>
<span id="cb13-2"><a href="chapter7.html#cb13-2" aria-hidden="true" tabindex="-1"></a>(sm.full <span class="ot">&lt;-</span> <span class="fu">summary</span>(fit.full)) <span class="co"># with smallest RSS(k), k = 1, ..., p, indicate variables included</span></span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19)
## 19 Variables  (and intercept)
##            Forced in Forced out
## AtBat          FALSE      FALSE
## Hits           FALSE      FALSE
## HmRun          FALSE      FALSE
## Runs           FALSE      FALSE
## RBI            FALSE      FALSE
## Walks          FALSE      FALSE
## Years          FALSE      FALSE
## CAtBat         FALSE      FALSE
## CHits          FALSE      FALSE
## CHmRun         FALSE      FALSE
## CRuns          FALSE      FALSE
## CRBI           FALSE      FALSE
## CWalks         FALSE      FALSE
## LeagueN        FALSE      FALSE
## DivisionW      FALSE      FALSE
## PutOuts        FALSE      FALSE
## Assists        FALSE      FALSE
## Errors         FALSE      FALSE
## NewLeagueN     FALSE      FALSE
## 1 subsets of each size up to 19
## Selection Algorithm: exhaustive
##           AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI
## 1  ( 1 )  &quot; &quot;   &quot; &quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 2  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 3  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 4  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 5  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 6  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot; 
## 7  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot; 
## 8  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot; 
## 9  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 10  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 11  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 12  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 13  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 14  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 15  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 16  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 17  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 18  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;   &quot;*&quot; 
## 19  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot; 
##           CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
## 1  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 2  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 3  ( 1 )  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 4  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 5  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 6  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 7  ( 1 )  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 8  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 9  ( 1 )  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 10  ( 1 ) &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 11  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 12  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 13  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 14  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 15  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 16  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 17  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 18  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 19  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;</code></pre>
<ul>
<li>Asterisk indicates inclusion of a variable in a model.</li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="chapter7.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(sm.full)</span></code></pre></div>
<pre><code>## [1] &quot;which&quot;  &quot;rsq&quot;    &quot;rss&quot;    &quot;adjr2&quot;  &quot;cp&quot;     &quot;bic&quot;    &quot;outmat&quot; &quot;obj&quot;</code></pre>
<ul>
<li>Printing for example <span class="math inline">\(R\)</span>-squares shows the highest values for the best combinations of <span class="math inline">\(k\)</span> explanatory variables, <span class="math inline">\(k=1,\ldots,p\)</span> (<span class="math inline">\(p=19\)</span>).</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="chapter7.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(sm.full<span class="sc">$</span>rsq, <span class="at">digits =</span> <span class="dv">3</span>) <span class="co"># show R-squares (in 3 decimals)</span></span></code></pre></div>
<pre><code>##  [1] 0.321 0.425 0.451 0.475 0.491 0.509 0.514 0.529 0.535 0.540 0.543 0.544
## [13] 0.544 0.545 0.545 0.546 0.546 0.546 0.546</code></pre>
<ul>
<li><p>Thus, for example the highest <span class="math inline">\(R^2\)</span> with one explanatory variable is <span class="math inline">\(32.1\%\)</span> when CRBI is in the regression.</p></li>
<li><p>Plotting <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, <span class="math inline">\(C_p\)</span> (here equivalent to AIC), and BIC for all subset sizes can be used to decide the final model.</p></li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="chapter7.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb19-2"><a href="chapter7.html#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">19</span>, <span class="at">y =</span> sm.full<span class="sc">$</span>rsq, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>, <span class="at">main =</span> <span class="st">&quot;R-squares&quot;</span>,</span>
<span id="cb19-3"><a href="chapter7.html#cb19-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;N of Variables&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;R-squared&quot;</span>)</span>
<span id="cb19-4"><a href="chapter7.html#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">19</span>, <span class="at">y =</span> sm.full<span class="sc">$</span>adjr2, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Adjusted R-squares&quot;</span>,</span>
<span id="cb19-5"><a href="chapter7.html#cb19-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;N of Variables&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Adjusted R-squared&quot;</span>)</span>
<span id="cb19-6"><a href="chapter7.html#cb19-6" aria-hidden="true" tabindex="-1"></a>(k.best <span class="ot">&lt;-</span> <span class="fu">which.max</span>(sm.full<span class="sc">$</span>adjr2)) <span class="co"># model with best adj R-square</span></span></code></pre></div>
<pre><code>## [1] 11</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="chapter7.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(k.best, sm.full<span class="sc">$</span>adjr2[k.best], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">20</span>) <span class="co"># show the maximum</span></span>
<span id="cb21-2"><a href="chapter7.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">19</span>, <span class="at">y =</span> sm.full<span class="sc">$</span>cp, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Cp&quot;</span>,</span>
<span id="cb21-3"><a href="chapter7.html#cb21-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;N of Variables&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Cp&quot;</span>)</span>
<span id="cb21-4"><a href="chapter7.html#cb21-4" aria-hidden="true" tabindex="-1"></a>(k.best <span class="ot">&lt;-</span> <span class="fu">which.min</span>(sm.full<span class="sc">$</span>cp)) <span class="co"># model with the smallest Cp</span></span></code></pre></div>
<pre><code>## [1] 10</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="chapter7.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(k.best, sm.full<span class="sc">$</span>cp[k.best], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">20</span>) <span class="co"># show the minimum</span></span>
<span id="cb23-2"><a href="chapter7.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">19</span>, <span class="at">y =</span> sm.full<span class="sc">$</span>bic, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>, <span class="at">main =</span> <span class="st">&quot;BIC&quot;</span>,</span>
<span id="cb23-3"><a href="chapter7.html#cb23-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;N of Variables&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;BIC&quot;</span>)</span>
<span id="cb23-4"><a href="chapter7.html#cb23-4" aria-hidden="true" tabindex="-1"></a>(k.best <span class="ot">&lt;-</span> <span class="fu">which.min</span>(sm.full<span class="sc">$</span>bic)) <span class="co"># model with the smallest BIC</span></span></code></pre></div>
<pre><code>## [1] 6</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="chapter7.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(k.best, sm.full<span class="sc">$</span>bic[k.best], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/bestm-1.png" width="672" /></p>
<ul>
<li>For example the six variables selected by BIC and the corresponding fitted model are:</li>
</ul>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="chapter7.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">coef</span>(fit.full, <span class="at">id =</span> <span class="dv">6</span>, <span class="at">vcov =</span> <span class="cn">TRUE</span>))</span></code></pre></div>
<pre><code>## [1] &quot;(Intercept)&quot; &quot;AtBat&quot;       &quot;Hits&quot;        &quot;Walks&quot;       &quot;CRBI&quot;       
## [6] &quot;DivisionW&quot;   &quot;PutOuts&quot;</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="chapter7.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit6 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Salary <span class="sc">~</span> AtBat <span class="sc">+</span> Hits <span class="sc">+</span> Walks <span class="sc">+</span> CRBI <span class="sc">+</span> Division <span class="sc">+</span> PutOuts, <span class="at">data =</span> Hitters))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Salary ~ AtBat + Hits + Walks + CRBI + Division + 
##     PutOuts, data = Hitters)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -873.11 -181.72  -25.91  141.77 2040.47 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   91.51180   65.00006   1.408 0.160382    
## AtBat         -1.86859    0.52742  -3.543 0.000470 ***
## Hits           7.60440    1.66254   4.574 7.46e-06 ***
## Walks          3.69765    1.21036   3.055 0.002488 ** 
## CRBI           0.64302    0.06443   9.979  &lt; 2e-16 ***
## DivisionW   -122.95153   39.82029  -3.088 0.002239 ** 
## PutOuts        0.26431    0.07477   3.535 0.000484 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 319.9 on 256 degrees of freedom
##   (결측으로 인하여 59개의 관측치가 삭제되었습니다.)
## Multiple R-squared:  0.5087, Adjusted R-squared:  0.4972 
## F-statistic: 44.18 on 6 and 256 DF,  p-value: &lt; 2.2e-16</code></pre>
<ul>
<li><p>The are differences with those significant in the full model (e.g. initially non-significant CRBI is included while initially significant CWalks is not).</p></li>
<li><p>In particular if our objective is to use the model for prediction, we can use a validation set to identify the best predictors.</p></li>
<li><p>First split the sample to a test set and a validation set, estimate the best explanatory variable subsets of sizes <span class="math inline">\(1,2,\ldots,p\)</span> using the estimation data set and find the minimum test set MSE.</p></li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="chapter7.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(Hitters<span class="sc">$</span>Salary)) <span class="co"># number of missing</span></span></code></pre></div>
<pre><code>## [1] 59</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="chapter7.html#cb32-1" aria-hidden="true" tabindex="-1"></a>Hitters <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(Hitters) <span class="co"># drop all rows with missing values</span></span>
<span id="cb32-2"><a href="chapter7.html#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(Hitters))</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="chapter7.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># initialize random seed for exact replication</span></span>
<span id="cb34-2"><a href="chapter7.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="do">## a random vector of TRUEs and FALSEs with length equaling the rows in Hitters</span></span>
<span id="cb34-3"><a href="chapter7.html#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="do">## and about one half of the values are TRUE</span></span>
<span id="cb34-4"><a href="chapter7.html#cb34-4" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>), <span class="at">size =</span> <span class="fu">nrow</span>(Hitters), <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="co"># </span></span>
<span id="cb34-5"><a href="chapter7.html#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="do">## in train about one half are TRUE values and one half FALSE values</span></span>
<span id="cb34-6"><a href="chapter7.html#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(train) <span class="co"># proportion of TRUEs</span></span></code></pre></div>
<pre><code>## [1] 0.5095057</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="chapter7.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Hitter[train, ] pick lines with train equaling TRUE, which we used for estimation</span></span>
<span id="cb36-2"><a href="chapter7.html#cb36-2" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="sc">!</span>train <span class="co"># complement set to identify the test set</span></span>
<span id="cb36-3"><a href="chapter7.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(test) <span class="co"># fraction of observations in the test set</span></span></code></pre></div>
<pre><code>## [1] 0.4904943</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="chapter7.html#cb38-1" aria-hidden="true" tabindex="-1"></a>fit.best <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., Hitters[train, ], <span class="at">nvmax =</span> <span class="dv">19</span>) <span class="co"># best fitting models</span></span>
<span id="cb38-2"><a href="chapter7.html#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="do">## next compute test set MSE for each best fitting explanatory variable subset</span></span>
<span id="cb38-3"><a href="chapter7.html#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="do">## for the purpose we first generate the test set into an appropriate format</span></span>
<span id="cb38-4"><a href="chapter7.html#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="do">## with model.matrix() function (see help(model.matrix))</span></span>
<span id="cb38-5"><a href="chapter7.html#cb38-5" aria-hidden="true" tabindex="-1"></a>test.mat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters[test, ]) <span class="co"># generate model matrix</span></span>
<span id="cb38-6"><a href="chapter7.html#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(test.mat) <span class="co"># a few first lines</span></span></code></pre></div>
<pre><code>##                   (Intercept) AtBat Hits HmRun Runs RBI Walks Years CAtBat
## -Alvin Davis                1   479  130    18   66  72    76     3   1624
## -Alfredo Griffin            1   594  169     4   74  51    35    11   4408
## -Andre Thornton             1   401   92    17   49  66    65    13   5206
## -Alan Trammell              1   574  159    21  107  75    59    10   4631
## -Buddy Biancalana           1   190   46     2   24   8    15     5    479
## -Bruce Bochy                1   127   32     8   16  22    14     8    727
##                   CHits CHmRun CRuns CRBI CWalks LeagueN DivisionW PutOuts
## -Alvin Davis        457     63   224  266    263       0         1     880
## -Alfredo Griffin   1133     19   501  336    194       0         1     282
## -Andre Thornton    1332    253   784  890    866       0         0       0
## -Alan Trammell     1300     90   702  504    488       0         0     238
## -Buddy Biancalana   102      5    65   23     39       0         1     102
## -Bruce Bochy        180     24    67   82     56       1         1     202
##                   Assists Errors NewLeagueN
## -Alvin Davis           82     14          0
## -Alfredo Griffin      421     25          0
## -Andre Thornton         0      0          0
## -Alan Trammell        445     22          0
## -Buddy Biancalana     177     16          0
## -Bruce Bochy           22      2          1</code></pre>
<ul>
<li>The model.matrix() function generates constant vector for the intercept and transforms factor variables to 0/1 dummy vectors by indicating also which class is labeled by 1.</li>
</ul>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="chapter7.html#cb40-1" aria-hidden="true" tabindex="-1"></a>test.mse <span class="ot">&lt;-</span> <span class="fu">double</span>(<span class="dv">19</span>) <span class="co"># vector of length 19 for validation set MSEs</span></span>
<span id="cb40-2"><a href="chapter7.html#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(test.mse)) {</span>
<span id="cb40-3"><a href="chapter7.html#cb40-3" aria-hidden="true" tabindex="-1"></a>    betai <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="at">object =</span> fit.best, <span class="at">id =</span> i) <span class="co"># extract coefficients of the model with k x-vars</span></span>
<span id="cb40-4"><a href="chapter7.html#cb40-4" aria-hidden="true" tabindex="-1"></a>    pred.salary <span class="ot">&lt;-</span> test.mat[, <span class="fu">names</span>(betai)] <span class="sc">%*%</span> betai <span class="co"># pred y  = X beta</span></span>
<span id="cb40-5"><a href="chapter7.html#cb40-5" aria-hidden="true" tabindex="-1"></a>    test.mse[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((Hitters<span class="sc">$</span>Salary[test] <span class="sc">-</span> pred.salary)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb40-6"><a href="chapter7.html#cb40-6" aria-hidden="true" tabindex="-1"></a>} <span class="co"># end for</span></span>
<span id="cb40-7"><a href="chapter7.html#cb40-7" aria-hidden="true" tabindex="-1"></a>test.mse <span class="co"># print results</span></span></code></pre></div>
<pre><code>##  [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4
##  [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4
## [17] 141767.4 142339.6 142238.2</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="chapter7.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which.min</span>(test.mse) <span class="co"># find the minimum</span></span></code></pre></div>
<pre><code>## [1] 7</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="chapter7.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit.best, <span class="at">id =</span> <span class="dv">7</span>) <span class="co"># slope coefficients of the best fitting model</span></span></code></pre></div>
<pre><code>##  (Intercept)        AtBat         Hits        Walks        CRuns       CWalks 
##   67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 
##    DivisionW      PutOuts 
## -118.4364998    0.2526925</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="chapter7.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit.full, <span class="at">id =</span> <span class="dv">7</span>) <span class="co"># slope coefficients of the best set of 10 variables from the full data set</span></span></code></pre></div>
<pre><code>##  (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun 
##   79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 
##    DivisionW      PutOuts 
## -129.9866432    0.2366813</code></pre>
<ul>
<li><p>Thus, the best model is the one with 10 predictors.</p></li>
<li><p>This set of predictors is also the best model with 10 perdictors from the full data, and would be the results selected by the <span class="math inline">\(C_p\)</span> criterion.</p></li>
<li><p>The results, however, can different for a different training and test sets (actually if we initialized the random generator with set.seed(1), a slightly different set of predictors would have been selected).</p></li>
</ul>
<div id="remark-6" class="section level4 unnumbered hasAnchor">
<h4>Remark<a href="chapter7.html#remark-6" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The practice is that the size <span class="math inline">\(k\)</span> of the best set of predictors (here <span class="math inline">\(k=10\)</span>) is selected on the basis of the validation approach, while the final best <span class="math inline">\(k\)</span> predictors are selected from the full sample and the corresponding regression is estimated (again from the full sample).</li>
<li>Thus, predictors in the final model may differ from those of the validation best predictors, only the number is the same (above in both cases the sets of best predictors happened to coincide).</li>
</ul>
</div>
<div id="cross-validation-approach" class="section level4 unnumbered hasAnchor">
<h4>Cross-validation approach<a href="chapter7.html#cross-validation-approach" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>In the same manner as in the validation set approach, we can identify the size the set of best predictors on the basis of cross-validation.</p></li>
<li><p>We demonstrate here the <span class="math inline">\(k\)</span>-fold CV with <span class="math inline">\(k=10\)</span> (here <span class="math inline">\(k\)</span> refers to the number of folds, not the number of predictors).</p></li>
<li><p>First create a vector that indicates in which of the 10 groups each observation belong to.</p></li>
</ul>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="chapter7.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb48-2"><a href="chapter7.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="do">## k-fold cross-validation approach</span></span>
<span id="cb48-3"><a href="chapter7.html#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb48-4"><a href="chapter7.html#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># for exact replication</span></span>
<span id="cb48-5"><a href="chapter7.html#cb48-5" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co"># n of folds</span></span>
<span id="cb48-6"><a href="chapter7.html#cb48-6" aria-hidden="true" tabindex="-1"></a>folds <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span>k, <span class="at">size =</span> <span class="fu">nrow</span>(Hitters), <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="co"># randomly formed k groups</span></span>
<span id="cb48-7"><a href="chapter7.html#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(folds)</span></code></pre></div>
<pre><code>## [1] 9 4 7 1 2 7</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="chapter7.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(fit.best) <span class="co"># structure of regsubsets object</span></span></code></pre></div>
<pre><code>## List of 28
##  $ np       : int 20
##  $ nrbar    : int 190
##  $ d        : num [1:20] 1.34e+02 8.80e+06 3.32e+01 8.89e+03 8.80e+02 ...
##  $ rbar     : num [1:190] 265.94 0.478 12.067 7.44 341.619 ...
##  $ thetab   : num [1:20] 567.316 0.985 33.726 6.76 5.944 ...
##  $ first    : int 2
##  $ last     : int 20
##  $ vorder   : int [1:20] 1 14 20 4 8 13 16 9 15 6 ...
##  $ tol      : num [1:20] 5.79e-09 5.71e-06 9.85e-09 1.47e-07 8.81e-08 ...
##  $ rss      : num [1:20] 26702538 18174675 18136864 17730750 17699645 ...
##  $ bound    : num [1:20] 26702538 15786734 12857104 11254676 10720451 ...
##  $ nvmax    : int 20
##  $ ress     : num [1:20, 1] 26702538 15786734 12857104 11254676 10720451 ...
##  $ ir       : int 20
##  $ nbest    : int 1
##  $ lopt     : int [1:210, 1] 1 1 12 1 12 3 1 7 10 9 ...
##  $ il       : int 210
##  $ ier      : int 0
##  $ xnames   : chr [1:20] &quot;(Intercept)&quot; &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; ...
##  $ method   : chr &quot;exhaustive&quot;
##  $ force.in : Named logi [1:20] TRUE FALSE FALSE FALSE FALSE FALSE ...
##   ..- attr(*, &quot;names&quot;)= chr [1:20] &quot;&quot; &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; ...
##  $ force.out: Named logi [1:20] FALSE FALSE FALSE FALSE FALSE FALSE ...
##   ..- attr(*, &quot;names&quot;)= chr [1:20] &quot;&quot; &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; ...
##  $ sserr    : num 8924127
##  $ intercept: logi TRUE
##  $ lindep   : logi [1:20] FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ nullrss  : num 26702538
##  $ nn       : int 134
##  $ call     : language regsubsets.formula(Salary ~ ., Hitters[train, ], nvmax = 19)
##  - attr(*, &quot;class&quot;)= chr &quot;regsubsets&quot;</code></pre>
<ul>
<li>Thus, here the first observation falls into fold 9, the second into 4, etc.</li>
<li>The following function produces predictions.</li>
</ul>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="chapter7.html#cb52-1" aria-hidden="true" tabindex="-1"></a>predict.regsubsets <span class="ot">&lt;-</span> <span class="cf">function</span>(obj, <span class="co"># object produced by regsubset()</span></span>
<span id="cb52-2"><a href="chapter7.html#cb52-2" aria-hidden="true" tabindex="-1"></a>                               newdata, <span class="co"># out of sample data</span></span>
<span id="cb52-3"><a href="chapter7.html#cb52-3" aria-hidden="true" tabindex="-1"></a>                               id, <span class="co"># id of the model</span></span>
<span id="cb52-4"><a href="chapter7.html#cb52-4" aria-hidden="true" tabindex="-1"></a>                               ... <span class="co"># potential additional argumets if needed</span></span>
<span id="cb52-5"><a href="chapter7.html#cb52-5" aria-hidden="true" tabindex="-1"></a>                               ) {  <span class="co"># Source: James et al. (2013) ISL</span></span>
<span id="cb52-6"><a href="chapter7.html#cb52-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">class</span>(obj) <span class="sc">!=</span> <span class="st">&quot;regsubsets&quot;</span>) <span class="fu">stop</span>(<span class="st">&quot;obj must be produced by regsubsets() function!&quot;</span>)</span>
<span id="cb52-7"><a href="chapter7.html#cb52-7" aria-hidden="true" tabindex="-1"></a>    fmla <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(obj<span class="sc">$</span>call[[<span class="dv">2</span>]]) <span class="co"># extract formula from the obj object</span></span>
<span id="cb52-8"><a href="chapter7.html#cb52-8" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="at">object =</span> obj, <span class="at">id =</span> id) <span class="co"># coefficients corresponding to model id</span></span>
<span id="cb52-9"><a href="chapter7.html#cb52-9" aria-hidden="true" tabindex="-1"></a>    xmat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(fmla, newdata) <span class="co"># data matrix for prediction computations</span></span>
<span id="cb52-10"><a href="chapter7.html#cb52-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(xmat[, <span class="fu">names</span>(beta)] <span class="sc">%*%</span> beta) <span class="co"># return predictions</span></span>
<span id="cb52-11"><a href="chapter7.html#cb52-11" aria-hidden="true" tabindex="-1"></a>} <span class="co"># pred.regsubsets</span></span></code></pre></div>
<ul>
<li>Next we loop through the <span class="math inline">\(k\)</span> sets, and best predictions sets to compute MSEs.</li>
</ul>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="chapter7.html#cb53-1" aria-hidden="true" tabindex="-1"></a>cv.mse <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> k, <span class="at">ncol =</span> <span class="dv">19</span>, <span class="at">dimnames =</span> <span class="fu">list</span>(<span class="dv">1</span><span class="sc">:</span>k, <span class="dv">1</span><span class="sc">:</span><span class="dv">19</span>)) <span class="co"># matrix to store MSE-values</span></span>
<span id="cb53-2"><a href="chapter7.html#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="do">## for loops to produce MSEs</span></span>
<span id="cb53-3"><a href="chapter7.html#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k) { <span class="co"># over validation sets</span></span>
<span id="cb53-4"><a href="chapter7.html#cb53-4" aria-hidden="true" tabindex="-1"></a>    best.fits <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters[folds <span class="sc">!=</span> i, ], <span class="at">nvmax =</span> <span class="dv">19</span>)</span>
<span id="cb53-5"><a href="chapter7.html#cb53-5" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> Hitters<span class="sc">$</span>Salary[folds <span class="sc">==</span> i] <span class="co"># new y values</span></span>
<span id="cb53-6"><a href="chapter7.html#cb53-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">19</span>) { <span class="co"># MSEs over n of predictors</span></span>
<span id="cb53-7"><a href="chapter7.html#cb53-7" aria-hidden="true" tabindex="-1"></a>        ypred <span class="ot">&lt;-</span> <span class="fu">predict.regsubsets</span>(best.fits, <span class="at">newdata =</span> Hitters[folds <span class="sc">==</span> i, ], <span class="at">id =</span> j) <span class="co"># predictions</span></span>
<span id="cb53-8"><a href="chapter7.html#cb53-8" aria-hidden="true" tabindex="-1"></a>        cv.mse[i, j] <span class="ot">&lt;-</span> <span class="fu">mean</span>((y <span class="sc">-</span> ypred)<span class="sc">^</span><span class="dv">2</span>) <span class="co"># store MSEs into cv.mse matrix</span></span>
<span id="cb53-9"><a href="chapter7.html#cb53-9" aria-hidden="true" tabindex="-1"></a>    } <span class="co"># for j</span></span>
<span id="cb53-10"><a href="chapter7.html#cb53-10" aria-hidden="true" tabindex="-1"></a>} </span></code></pre></div>
<ul>
<li>MSEs for a <span class="math inline">\(j\)</span>-predictors model, <span class="math inline">\(j=1,\ldots,19\)</span>.</li>
</ul>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="chapter7.html#cb54-1" aria-hidden="true" tabindex="-1"></a>(mean.cv.mse <span class="ot">&lt;-</span> <span class="fu">apply</span>(cv.mse, <span class="dv">2</span>, mean)) <span class="co"># mean mse values, surrounding with parentheses prints the results</span></span></code></pre></div>
<pre><code>##        1        2        3        4        5        6        7        8 
## 149821.1 130922.0 139127.0 131028.8 131050.2 119538.6 124286.1 113580.0 
##        9       10       11       12       13       14       15       16 
## 115556.5 112216.7 113251.2 115755.9 117820.8 119481.2 120121.6 120074.3 
##       17       18       19 
## 120084.8 120085.8 120403.5</code></pre>
<ul>
<li>Models with 8, 10 and 11 predictors are close to each other, however, the 10 predictor model is slightly better than the other two as shown also by the following figure, so a model with 10 predictors would be our choice.</li>
</ul>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="chapter7.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb56-2"><a href="chapter7.html#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">19</span>, <span class="at">y =</span> mean.cv.mse, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;N of Predictors&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;MSE&quot;</span>,</span>
<span id="cb56-3"><a href="chapter7.html#cb56-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Best 10-fold Cross-Validation MSEs</span><span class="sc">\n</span><span class="st">for Different Number of Predictors&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/cvplot-1.png" width="672" /></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="chapter7.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="do">## dev.print(pdf, &quot;../lectures/figures/ex51b.pdf&quot;)</span></span>
<span id="cb57-2"><a href="chapter7.html#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">regsubsets</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">nvmax =</span> <span class="dv">19</span>), <span class="at">id =</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##  (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns 
##  162.5354420   -2.1686501    6.9180175    5.7732246   -0.1300798    1.4082490 
##         CRBI       CWalks    DivisionW      PutOuts      Assists 
##    0.7743122   -0.8308264 -112.3800575    0.2973726    0.2831680</code></pre>
<p><br></p>
</div>
</div>
</div>
<div id="shrinkage-methods" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Shrinkage Methods<a href="chapter7.html#shrinkage-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Ridge regression and Lasso
<ul>
<li>The subset selection methods use least squares to fit a linear model that contains a subset of the predictors.</li>
<li>As an alternative, we can fit a model containing all <span class="math inline">\(p\)</span> predictors using a technique that constrains or regularized the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.</li>
<li>It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance.</li>
</ul></li>
</ul>
<div id="ridge-regression" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Ridge Regression<a href="chapter7.html#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Recall that the least squares fitting procedure estimates <span class="math inline">\(\beta_0,\beta_1,\ldots, \beta_p\)</span> using the values that minimize</li>
</ul>
<p><span class="math display">\[
RSS=\sum_{i=1}^n (y_i -\beta_0 - \sum_{j=1}^p \beta_j x_{ij} )^2
\]</span></p>
<ul>
<li>In contrast, the ridge regression coefficient estimates <span class="math inline">\(\hat{\beta}^R\)</span> are the values that minimize</li>
</ul>
<p><span class="math display">\[
\sum_{i=1}^n (y_i -\beta_0 - \sum_{j=1}^p \beta_j x_{ij} )^2+\lambda \sum_{j=1}^p \beta_j^2 = RSS +\lambda\sum_{j=1}^p \beta_j^2
\]</span></p>
<p>      where <span class="math inline">\(\lambda\ge 0\)</span> is a tuning parameter, to be determined separately.</p>
<ul>
<li><p>As with least squares, eidge regression seeks coefficient estimates that fit the data well, by making the RSS small.</p></li>
<li><p>However, the second term, <span class="math inline">\(\lambda \sum_j\beta_j^2\)</span>, called a <em>shrinkage penalty</em>, is small when <span class="math inline">\(\beta_1, \ldots, \beta_p\)</span> are close to zero, and so it has the effect of <em>shrinking</em> the estimats of <span class="math inline">\(\beta_j\)</span> towards zero.</p></li>
<li><p>The tuning parameter <span class="math inline">\(\lambda\)</span> serves to control the relative impact of these two terms on the regression coefficient estimates.</p></li>
<li><p>Selecting a good value for <span class="math inline">\(\lambda\)</span> is critical; cross-validation is used for this.</p></li>
</ul>
<p><img src="fig7/f5.png" /></p>
<ul>
<li><p>In the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of <span class="math inline">\(\lambda\)</span></p></li>
<li><p>The right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying <span class="math inline">\(\lambda\)</span> on the <span class="math inline">\(x\)</span>-axis, we now display <span class="math inline">\(||\hat{\beta}_{\lambda}^R||_2/||\hat{\beta}||_2\)</span>, where <span class="math inline">\(\hat{\beta}\)</span> denotes the vector of least squares coefficient estimates.</p></li>
<li><p>The notation <span class="math inline">\(||\beta||_2\)</span> denotes the <span class="math inline">\(l_2\)</span> norm (pronounced “ell 2”) of a vector, and is defined as <span class="math inline">\(||\beta||_2=\sqrt{\sum_{j=1}^p \beta_j^2}\)</span>.</p></li>
</ul>
<div id="scaling-of-predictors" class="section level4 unnumbered hasAnchor">
<h4>Scaling of predictors<a href="chapter7.html#scaling-of-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The standard least squares coefficient estimates are <em>scale equivariant</em>: multiplying <span class="math inline">\(X_j\)</span> by a constant <span class="math inline">\(c\)</span> simply leads to a scaling of the least squares coefficient estimates by a factor of <span class="math inline">\(1/c\)</span>.</p>
<ul>
<li>In other words, regardless of how the <span class="math inline">\(j\)</span>th predictor is scaled, <span class="math inline">\(X_j\hat{\beta}_j\)</span> will remain the same.</li>
</ul></li>
<li><p>In contrast, the ridge regression coefficient estimates can change <em>substantially</em> when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.</p></li>
<li><p>Therefore, it is best to apply ridge regression after <em>standardizing the predictors</em>, using the formula</p></li>
</ul>
<p><span class="math display">\[
\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n (x_{ij}-\bar{x}_j)^2}}
\]</span></p>
</div>
<div id="the-bias-variance-tradeoff" class="section level4 unnumbered hasAnchor">
<h4>The Bias-variance tradeoff<a href="chapter7.html#the-bias-variance-tradeoff" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig7/f6.png" /></p>
<ul>
<li>Simulated data with <span class="math inline">\(n=50\)</span> observations, <span class="math inline">\(p=45\)</span> predictors, all having nonzero coefficients.
<ul>
<li>Squared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(||\hat{\beta}_{\lambda}^R||_2/||\hat{\beta}||_2\)</span>.</li>
<li>The horizontal dashed lines indicate the minimum possible MSE.</li>
<li>The purple crosses indicate the ridge regression models for which the MSE is smallest.</li>
</ul></li>
</ul>
</div>
</div>
<div id="rasso-regression" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> RASSO Regression<a href="chapter7.html#rasso-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Ridge regression does have on obvious disadvantage:
<ul>
<li>Unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all <span class="math inline">\(p\)</span> prediction in the final model</li>
</ul></li>
<li>The LASSO (least absolute shrinkage and selection operator) is a relatively recent alternative to ridge regression that overcomes this disadvantage.
<ul>
<li>The lasso coefficients, <span class="math inline">\(\hat{\beta}_{\lambda}^L\)</span>, minimize the quantity</li>
</ul></li>
</ul>
<p><span class="math display">\[
\sum_{i=1}^n (y_i -\beta_0 - \sum_{j=1}^p \beta_j x_{ij} )^2+\lambda \sum_{j=1}^p |\beta_j| = RSS +\lambda\sum_{j=1}^p |\beta_j|
\]</span></p>
<ul>
<li><p>In statistical parlance, the lasso uses an <span class="math inline">\(l_1\)</span> (pronounced “ell 1”) penalty instead of an <span class="math inline">\(l_2\)</span> penalty.</p>
<ul>
<li>The <span class="math inline">\(l_1\)</span> norm of a coefficient vector <span class="math inline">\(\beta\)</span> is given by <span class="math inline">\(||\beta||_1=\sum_j|\beta_j|\)</span></li>
</ul></li>
<li><p>As with ridge regression, the lasso shrinks the coefficient estimates towards zero.</p></li>
<li><p>However, in the case of the lasso, the <span class="math inline">\(l_1\)</span> penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter <span class="math inline">\(\lambda\)</span> is sufficiently large.</p></li>
<li><p>Hence, much like best subset selection, the lasso performs <em>variable selection</em>.</p></li>
<li><p>We say that the lasso yields <em>sparse</em> models - that is, models that involve only a subset of the variables.</p></li>
<li><p>As in ridge regression, selecting a good value of <span class="math inline">\(\lambda\)</span> for the lasso is critical; cross-validation is again the method of choice.</p></li>
</ul>
<p><img src="fig7/f7.png" /></p>
<ul>
<li><p>Why is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero?</p></li>
<li><p>One can shwo that the lasso and ridge regression oefficient estimates solve the problems</p></li>
</ul>
<p><span class="math display">\[
minimize_{\beta}\sum_{i=1}^n (y_i -\beta_0 - \sum_{j=1}^p \beta_j x_{ij} )^2 \,\,\, subject \,\,\, to \,\,\, \sum_{j=1}^p |\beta_j|\le s
\]</span></p>
<p>      and</p>
<p><span class="math display">\[
minimize_{\beta}\sum_{i=1}^n (y_i -\beta_0 - \sum_{j=1}^p \beta_j x_{ij} )^2 \,\,\, subject \,\,\, to \,\,\, \sum_{j=1}^p \beta_j^2\le s
\]</span></p>
<p>      respectively.</p>
<p><img src="fig7/f8.png" /></p>
<div id="comparing-the-lasso-and-ridge-regression" class="section level4 unnumbered hasAnchor">
<h4>Comparing the Lasso and Ridge regression<a href="chapter7.html#comparing-the-lasso-and-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig7/f9.png" /></p>
<ul>
<li>Left: plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set.</li>
<li>Right: Comparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed).
<ul>
<li>Both are plotted against their <span class="math inline">\(R^2\)</span> on the training data, as a common form of indexing.</li>
<li>The crosses in both plots indicate the lasso model for which the MSE is smallest.</li>
</ul></li>
</ul>
<p><img src="fig7/f10.png" /></p>
<ul>
<li>Left: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso.
<ul>
<li>The simulated data is similar to that prior, except that now only tow predictors are related to the response.</li>
</ul></li>
<li>Right: Comparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed).
<ul>
<li>Both are plotted against their <span class="math inline">\(R^2\)</span> on the training data, as a common form of indexing.</li>
<li>The crosses in both plots indicate the lasso model for which the MSE is smallest</li>
</ul></li>
</ul>
</div>
<div id="conclusions" class="section level4 unnumbered hasAnchor">
<h4>Conclusions<a href="chapter7.html#conclusions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>These two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.</p></li>
<li><p>In general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.</p></li>
<li><p>However, the number of predictors that is related to the response is never known a <em>priori</em> for real data sets.</p></li>
<li><p>A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.</p></li>
</ul>
</div>
<div id="selecting-the-tuning-parameter-for-ridge-regression-and-lasso" class="section level4 unnumbered hasAnchor">
<h4>Selecting the tuning parameter for ridge regression and lasso<a href="chapter7.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>As for subset selection, for ridge regression and lasso we require a method to determine which of the models under consideration is best.</p></li>
<li><p>That is, we require a method selecting a value for the tuning parameter <span class="math inline">\(\lambda\)</span> or equivalently, the value of the constraint <span class="math inline">\(s\)</span>.</p></li>
<li><p><em>Cross-validation</em> provides a simple way to tackle this problem.</p>
<ul>
<li>We choose a grid of <span class="math inline">\(\lambda\)</span> values, and compute the cross-validation error rate for each value of <span class="math inline">\(\lambda\)</span>.</li>
</ul></li>
<li><p>We then select the tuning parameter value for which the cross-validation error is smallest.</p></li>
<li><p>Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.</p></li>
</ul>
<p><img src="fig7/f11.png" /></p>
<ul>
<li>Left: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of <span class="math inline">\(\lambda\)</span>.
-Right: The coefficient estimates as a function of <span class="math inline">\(\lambda\)</span>.
<ul>
<li>The vertical dashed lines indicates the value of <span class="math inline">\(\lambda\)</span> selected by cross-validation.</li>
</ul></li>
</ul>
<p><img src="fig7/f12.png" /></p>
<ul>
<li><p>Left: Ten-fold cross-validation MSE for the lasso, applied to the sparse simulated data set from prior.</p></li>
<li><p>Right: The corresponding lasso coefficient estimates are displayed.</p>
<ul>
<li>The vertical dashed lines indicate the lasso fit for which the cross-validation error is smallest.</li>
</ul></li>
</ul>
</div>
<div id="remark-7" class="section level4 unnumbered hasAnchor">
<h4>Remark<a href="chapter7.html#remark-7" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>A combination of the ridge regression and the lasso, called elastic-net regularization, estimates the regressions coefficients <span class="math inline">\(\beta_0, \beta_1, \beta_2, \ldots, \beta_p\)</span> by minimizing</li>
</ul>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - \beta_0-x_i^T\beta)^2 + \lambda P_{\alpha}(\beta)
\]</span></p>
<p>      where <span class="math inline">\(x_i=(x_{i1},\ldots,x_{ip})^T\)</span>, <span class="math inline">\(\beta=(\beta_1,\ldots,\beta_p)^T\)</span>, and</p>
<p><span class="math display">\[
P_{\alpha}(\beta)=\sum_{j=1}^p (\frac{1}{2}(1-\alpha)\beta_j^2+\alpha|\beta_j|)
\]</span></p>
<ul>
<li>Thus, <span class="math inline">\(\alpha=0\)</span> implies the ridge regression and <span class="math inline">\(\alpha=1\)</span> the lasso.</li>
</ul>
</div>
<div id="another-formulation-for-ridge-regressio-and-lasso" class="section level4 unnumbered hasAnchor">
<h4>Another formulation for ridge regressio and lasso<a href="chapter7.html#another-formulation-for-ridge-regressio-and-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>For example the lasso restriction can be thought as a budget constraint that defines how large <span class="math inline">\(\sum_{j=1}^p |\beta_j|\)</span> can be.</p></li>
<li><p>Formulating the constraints of lasso and ridge as</p></li>
</ul>
<p><span class="math display">\[
\sum_{j=1}^p I(\beta_j \ne 0) \le s
\]</span></p>
<p>      where <span class="math inline">\(I(\beta_j \ne 0)\)</span> is an indicator function equaling 1 if <span class="math inline">\(\beta_j \ne 0\)</span> and zero otherwise, then RSS is minimized under the constraint that no more than <span class="math inline">\(s\)</span> coefficients can be nonzero.</p>
<ul>
<li>Thus, the problem becomes a best subset selection problem.</li>
</ul>
</div>
</div>
<div id="example-14" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Example<a href="chapter7.html#example-14" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>We utilize again the Hitters data set to demonstrate lasso.</p></li>
<li><p>R has package glmnet in which the main function to perform lasso, ridge regression, and generally elastic-net regularization estimatin is glmnet().</p></li>
<li><p>The glmnet() function does not allow missing values and R factor variables must be first transformed to 0/1 dummy variables.</p></li>
</ul>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="chapter7.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb59-2"><a href="chapter7.html#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet) <span class="co"># see library(help = glmnet)</span></span>
<span id="cb59-3"><a href="chapter7.html#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="at">help =</span> glmnet) <span class="co"># basic information about the glmnet package</span></span>
<span id="cb59-4"><a href="chapter7.html#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Hitters) <span class="co"># a few first lines</span></span></code></pre></div>
<pre><code>##                   AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun
## -Alan Ashby         315   81     7   24  38    39    14   3449   835     69
## -Alvin Davis        479  130    18   66  72    76     3   1624   457     63
## -Andre Dawson       496  141    20   65  78    37    11   5628  1575    225
## -Andres Galarraga   321   87    10   39  42    30     2    396   101     12
## -Alfredo Griffin    594  169     4   74  51    35    11   4408  1133     19
## -Al Newman          185   37     1   23   8    21     2    214    42      1
##                   CRuns CRBI CWalks League Division PutOuts Assists Errors
## -Alan Ashby         321  414    375      N        W     632      43     10
## -Alvin Davis        224  266    263      A        W     880      82     14
## -Andre Dawson       828  838    354      N        E     200      11      3
## -Andres Galarraga    48   46     33      N        E     805      40      4
## -Alfredo Griffin    501  336    194      A        W     282     421     25
## -Al Newman           30    9     24      N        E      76     127      7
##                   Salary NewLeague
## -Alan Ashby        475.0         N
## -Alvin Davis       480.0         A
## -Andre Dawson      500.0         N
## -Andres Galarraga   91.5         N
## -Alfredo Griffin   750.0         A
## -Al Newman          70.0         A</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="chapter7.html#cb61-1" aria-hidden="true" tabindex="-1"></a>Hitters <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(Hitters) <span class="co"># remove missing values</span></span></code></pre></div>
<ul>
<li><p>The glmnet() function requires its input in an <span class="math inline">\(X\)</span> matrix (predictors) and a <span class="math inline">\(Y\)</span> vector (dependent variable), and syntax <span class="math inline">\(y~x\)</span> does not work.</p></li>
<li><p>The model.matrix() function generates the required <span class="math inline">\(x\)</span>-matrix and transforms factor variables to dummy variables.</p></li>
<li><p>glmnet() performs lasso by selecting alpha=1 (which is also the default) and automatically selects a range for <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>Below we use this automatically generated grid for <span class="math inline">\(\lambda\)</span>.</p></li>
</ul>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="chapter7.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="do">## inputs matrix x and vector y for glmnet()</span></span>
<span id="cb62-2"><a href="chapter7.html#cb62-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., Hitters)[, <span class="sc">-</span><span class="dv">1</span>] <span class="co"># x-variables, drop the constant term vector</span></span>
<span id="cb62-3"><a href="chapter7.html#cb62-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> Hitters<span class="sc">$</span>Salary </span>
<span id="cb62-4"><a href="chapter7.html#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(x)</span></code></pre></div>
<pre><code>##                   AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun
## -Alan Ashby         315   81     7   24  38    39    14   3449   835     69
## -Alvin Davis        479  130    18   66  72    76     3   1624   457     63
## -Andre Dawson       496  141    20   65  78    37    11   5628  1575    225
## -Andres Galarraga   321   87    10   39  42    30     2    396   101     12
## -Alfredo Griffin    594  169     4   74  51    35    11   4408  1133     19
## -Al Newman          185   37     1   23   8    21     2    214    42      1
##                   CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists Errors
## -Alan Ashby         321  414    375       1         1     632      43     10
## -Alvin Davis        224  266    263       0         1     880      82     14
## -Andre Dawson       828  838    354       1         0     200      11      3
## -Andres Galarraga    48   46     33       1         0     805      40      4
## -Alfredo Griffin    501  336    194       0         1     282     421     25
## -Al Newman           30    9     24       1         0      76     127      7
##                   NewLeagueN
## -Alan Ashby                1
## -Alvin Davis               0
## -Andre Dawson              1
## -Andres Galarraga          1
## -Alfredo Griffin           0
## -Al Newman                 0</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="chapter7.html#cb64-1" aria-hidden="true" tabindex="-1"></a>grid.lambda <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>) <span class="co"># grid of length 100 from 10^10 down to 1/100</span></span>
<span id="cb64-2"><a href="chapter7.html#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(grid.lambda)</span></code></pre></div>
<pre><code>## [1] 10000000000  7564633276  5722367659  4328761281  3274549163  2477076356</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="chapter7.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(grid.lambda)</span></code></pre></div>
<pre><code>## [1] 0.04037017 0.03053856 0.02310130 0.01747528 0.01321941 0.01000000</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="chapter7.html#cb68-1" aria-hidden="true" tabindex="-1"></a>lasso.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid.lambda) <span class="co"># alpha = 1 peforms lasso</span></span>
<span id="cb68-2"><a href="chapter7.html#cb68-2" aria-hidden="true" tabindex="-1"></a>                                          <span class="do">## for a range of lambda values</span></span>
<span id="cb68-3"><a href="chapter7.html#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(lasso.mod) <span class="co"># structure of lasso.mod object produced by glmnet()</span></span></code></pre></div>
<pre><code>## List of 12
##  $ a0       : Named num [1:100] 536 536 536 536 536 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:100] &quot;s0&quot; &quot;s1&quot; &quot;s2&quot; &quot;s3&quot; ...
##  $ beta     :Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   .. ..@ i       : int [1:491] 11 1 10 11 1 5 10 11 1 5 ...
##   .. ..@ p       : int [1:101] 0 0 0 0 0 0 0 0 0 0 ...
##   .. ..@ Dim     : int [1:2] 19 100
##   .. ..@ Dimnames:List of 2
##   .. .. ..$ : chr [1:19] &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; &quot;Runs&quot; ...
##   .. .. ..$ : chr [1:100] &quot;s0&quot; &quot;s1&quot; &quot;s2&quot; &quot;s3&quot; ...
##   .. ..@ x       : num [1:491] 0.0752 0.1023 0.0683 0.1802 0.7216 ...
##   .. ..@ factors : list()
##  $ df       : int [1:100] 0 0 0 0 0 0 0 0 0 0 ...
##  $ dim      : int [1:2] 19 100
##  $ lambda   : num [1:100] 1.00e+10 7.56e+09 5.72e+09 4.33e+09 3.27e+09 ...
##  $ dev.ratio: num [1:100] 0 0 0 0 0 0 0 0 0 0 ...
##  $ nulldev  : num 53319113
##  $ npasses  : int 2283
##  $ jerr     : int 0
##  $ offset   : logi FALSE
##  $ call     : language glmnet(x = x, y = y, alpha = 1, lambda = grid.lambda)
##  $ nobs     : int 263
##  - attr(*, &quot;class&quot;)= chr [1:2] &quot;elnet&quot; &quot;glmnet&quot;</code></pre>
<ul>
<li>Below are shown the number of values in the automatically generated <span class="math inline">\(\lambda\)</span>-grid and some of the values.</li>
</ul>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="chapter7.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(lasso.mod<span class="sc">$</span>lambda) <span class="co"># number of values in the lambda range</span></span></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="chapter7.html#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="at">min =</span> <span class="fu">min</span>(lasso.mod<span class="sc">$</span>lambda), <span class="at">max =</span> <span class="fu">max</span>(lasso.mod<span class="sc">$</span>lambda)) <span class="co"># range of lambdas</span></span></code></pre></div>
<pre><code>##   min   max 
## 1e-02 1e+10</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="chapter7.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(lasso.mod<span class="sc">$</span>lambda) <span class="co"># a few first lamdas</span></span></code></pre></div>
<pre><code>## [1] 10000000000  7564633276  5722367659  4328761281  3274549163  2477076356</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="chapter7.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(lasso.mod<span class="sc">$</span>lambda) <span class="co"># a few last lambdas</span></span></code></pre></div>
<pre><code>## [1] 0.04037017 0.03053856 0.02310130 0.01747528 0.01321941 0.01000000</code></pre>
<ul>
<li><p>Thus, the regression coefficients are computed for 80 values of <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>These results are stored into the <span class="math inline">\(20\times 80\)</span> beta matrix in the lasso.mod object and can be extracted directly or using coef() function.</p></li>
</ul>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="chapter7.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(<span class="fu">coef</span>(lasso.mod)) <span class="co"># dimension of the coefficient matrix (beta)</span></span></code></pre></div>
<pre><code>## [1]  20 100</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="chapter7.html#cb80-1" aria-hidden="true" tabindex="-1"></a>lasso.mod<span class="sc">$</span>lambda[<span class="dv">70</span>] <span class="co"># the 50th value of lambda</span></span></code></pre></div>
<pre><code>## [1] 43.28761</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="chapter7.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lasso.mod)[, <span class="dv">70</span>] <span class="co"># the corresponding beta estimates</span></span></code></pre></div>
<pre><code>## (Intercept)       AtBat        Hits       HmRun        Runs         RBI 
##  74.7044627   0.0000000   1.6432866   0.0000000   0.0000000   0.0000000 
##       Walks       Years      CAtBat       CHits      CHmRun       CRuns 
##   1.8986354   0.0000000   0.0000000   0.0000000   0.0000000   0.1837056 
##        CRBI      CWalks     LeagueN   DivisionW     PutOuts     Assists 
##   0.3743196   0.0000000   0.0000000 -55.4369609   0.1519453   0.0000000 
##      Errors  NewLeagueN 
##   0.0000000   0.0000000</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="chapter7.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">abs</span>(<span class="fu">coef</span>(lasso.mod)[<span class="sc">-</span><span class="dv">1</span>, <span class="dv">70</span>])) <span class="co"># the corresponing L1-norm</span></span></code></pre></div>
<pre><code>## [1] 59.68885</code></pre>
<ul>
<li>The coef() function can be used to produce lasso estimates for any values of <span class="math inline">\(\lambda\)</span> (the same can be done by the predict() function)</li>
</ul>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="chapter7.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co">#help(coef.glmnet) # help for coef and predict</span></span>
<span id="cb86-2"><a href="chapter7.html#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="fu">drop</span>(<span class="fu">coef</span>(lasso.mod, <span class="at">s =</span> <span class="dv">1</span>)) <span class="co"># lasso estimates for any value of lambda (arg s)</span></span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs 
##  152.68965461   -1.92453638    6.77405213    1.25572117   -0.99184280 
##           RBI         Walks         Years        CAtBat         CHits 
##    0.00000000    5.60436806   -7.67134685   -0.06548857    0.00000000 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##    0.23105395    1.12172648    0.56793753   -0.72711075   46.55909441 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
## -116.69918432    0.28193985    0.28808499   -2.85400785  -10.14452831</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="chapter7.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="do">## the same and more can be done with predict() </span></span>
<span id="cb88-2"><a href="chapter7.html#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="fu">drop</span>(<span class="fu">predict</span>(lasso.mod, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="at">s =</span> <span class="dv">1</span>)) <span class="co"># see help(predict.glmnet)</span></span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs 
##  152.68965461   -1.92453638    6.77405213    1.25572117   -0.99184280 
##           RBI         Walks         Years        CAtBat         CHits 
##    0.00000000    5.60436806   -7.67134685   -0.06548857    0.00000000 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##    0.23105395    1.12172648    0.56793753   -0.72711075   46.55909441 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
## -116.69918432    0.28193985    0.28808499   -2.85400785  -10.14452831</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="chapter7.html#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="do">## coefficient plots for different lambda values</span></span>
<span id="cb90-2"><a href="chapter7.html#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb90-3"><a href="chapter7.html#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso.mod, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(<span class="fu">log</span>(lambda)))</span>
<span id="cb90-4"><a href="chapter7.html#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso.mod, <span class="at">xvar =</span> <span class="st">&quot;norm&quot;</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(<span class="fu">log</span>(<span class="fu">sum</span>(<span class="fu">abs</span>(beta[j])))))</span></code></pre></div>
<pre><code>## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm): 고유한
## &#39;x&#39;값들로 범위를 축소합니다</code></pre>
<p><img src="ILR_files/figure-html/cva-1.png" width="672" /></p>
<ul>
<li><p>The coefficients are plotted against <span class="math inline">\(log(\lambda)\)</span> in the left panel and logarithm of the <span class="math inline">\(l_1\)</span>-norm of the coefficients in the right panel.</p></li>
<li><p>Numbers on the top of the figure indicate the number of non-zero coefficients.</p></li>
<li><p>The figure shows that depending of the choice of <span class="math inline">\(\lambda\)</span>, some of the coefficients will be exactly equal to zero.</p></li>
<li><p>Next we use cross-validation to identify the best <span class="math inline">\(\lambda\)</span> in terms of the MSE.</p></li>
<li><p>The glmnet has the function cv.glmnet() for the purpose.</p></li>
</ul>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="chapter7.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># for replication purposes</span></span>
<span id="cb92-2"><a href="chapter7.html#cb92-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(x), <span class="fu">nrow</span>(x)<span class="sc">/</span><span class="dv">2</span>) <span class="co"># random selectio of about half of the observations for a train set</span></span>
<span id="cb92-3"><a href="chapter7.html#cb92-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="sc">-</span>train <span class="co"># test set consist observations not in the train set</span></span>
<span id="cb92-4"><a href="chapter7.html#cb92-4" aria-hidden="true" tabindex="-1"></a>y.test <span class="ot">&lt;-</span> y[test] <span class="co"># test set y-vlues</span></span>
<span id="cb92-5"><a href="chapter7.html#cb92-5" aria-hidden="true" tabindex="-1"></a>lasso.train <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> x[train, ], <span class="at">y =</span> y[train], <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid.lambda)</span>
<span id="cb92-6"><a href="chapter7.html#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="do">## similat plots as for the above tentative whole data set</span></span>
<span id="cb92-7"><a href="chapter7.html#cb92-7" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb92-8"><a href="chapter7.html#cb92-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso.train, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(<span class="fu">log</span>(lambda)))</span>
<span id="cb92-9"><a href="chapter7.html#cb92-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso.train, <span class="at">xvar =</span> <span class="st">&quot;norm&quot;</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(<span class="fu">log</span>(<span class="fu">sum</span>(<span class="fu">abs</span>(beta[j])))))</span></code></pre></div>
<p><img src="ILR_files/figure-html/cvglmnet-1.png" width="672" /></p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="chapter7.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="do">## perform cross-validation in the training set to find lambda candidates</span></span>
<span id="cb93-2"><a href="chapter7.html#cb93-2" aria-hidden="true" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid.lambda) <span class="co"># by default performs 10-fold CV</span></span>
<span id="cb93-3"><a href="chapter7.html#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)) <span class="co"># reset full plotting window</span></span>
<span id="cb93-4"><a href="chapter7.html#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.out) <span class="co"># plots MSEs against lambdas</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/cvglmnet-2.png" width="672" /></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="chapter7.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="do">## dev.print(pdf, file = &quot;ex52b.pdf&quot;) # pdf copy of the graph</span></span>
<span id="cb94-2"><a href="chapter7.html#cb94-2" aria-hidden="true" tabindex="-1"></a>(best.lam <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>lambda.min) <span class="co"># lambda for which CV MSE is at minimum</span></span></code></pre></div>
<pre><code>## [1] 8.111308</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="chapter7.html#cb96-1" aria-hidden="true" tabindex="-1"></a>lasso.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso.train, <span class="at">s =</span> best.lam, <span class="at">newx =</span> x[test, ]) <span class="co"># predicted values with coefficients corresponding best.lam</span></span>
<span id="cb96-2"><a href="chapter7.html#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((lasso.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>) <span class="co"># test MSE</span></span></code></pre></div>
<pre><code>## [1] 143890.9</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="chapter7.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="do">## for comparison compute test MSE for OLS estimated model from the test set</span></span>
<span id="cb98-2"><a href="chapter7.html#cb98-2" aria-hidden="true" tabindex="-1"></a>ols.train <span class="ot">&lt;-</span> <span class="fu">lm</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">subset =</span> train) <span class="co"># OLS train data estimats</span></span>
<span id="cb98-3"><a href="chapter7.html#cb98-3" aria-hidden="true" tabindex="-1"></a>ols.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ols.train, <span class="at">newdata =</span> Hitters[test, ]) <span class="co"># OLS prediction</span></span>
<span id="cb98-4"><a href="chapter7.html#cb98-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((ols.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>) <span class="co"># OLS test MSE</span></span></code></pre></div>
<pre><code>## [1] 168593.3</code></pre>
<ul>
<li><p>Here lasso outperforms OLS in terms of test MSE.</p></li>
<li><p>Below is a scatter plot of test set realized and predicted salaries.</p></li>
</ul>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="chapter7.html#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="do">## scatterplots of predicted and realized salaries</span></span>
<span id="cb100-2"><a href="chapter7.html#cb100-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))  <span class="co"># full plot window</span></span>
<span id="cb100-3"><a href="chapter7.html#cb100-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ols.pred, <span class="at">y =</span> y.test, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2500</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2500</span>), <span class="at">xlab =</span> <span class="st">&quot;Predicted&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Realized&quot;</span>)</span>
<span id="cb100-4"><a href="chapter7.html#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y.test <span class="sc">~</span> ols.pred), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb100-5"><a href="chapter7.html#cb100-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> lasso.pred, <span class="at">y =</span> y.test, <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>)</span>
<span id="cb100-6"><a href="chapter7.html#cb100-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y.test <span class="sc">~</span> lasso.pred), <span class="at">col =</span> <span class="st">&quot;steel blue&quot;</span>)</span>
<span id="cb100-7"><a href="chapter7.html#cb100-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb100-8"><a href="chapter7.html#cb100-8" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;OLS&quot;</span>, <span class="st">&quot;Lasso&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;Red&quot;</span>, <span class="st">&quot;Steel blue&quot;</span>),</span>
<span id="cb100-9"><a href="chapter7.html#cb100-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/cvplot2-1.png" width="672" /></p>
<p><br></p>
</div>
</div>
<div id="dimension-reduction-methods" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Dimension Reduction Methods<a href="chapter7.html#dimension-reduction-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>The methods that we have discussed so far in this chapter have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>.</p></li>
<li><p>We now explore a class of approaches that <em>transform</em> the predictors and then fit a least squares model using the transformed variables.</p>
<ul>
<li>We will refer to these techniques as <em>dimension reduction</em> methods.</li>
</ul></li>
<li><p>Let <span class="math inline">\(Z_1, Z_2, \ldots, Z_M\)</span> represent <span class="math inline">\(M&lt;p\)</span> <em>linear combinations</em> of our original <span class="math inline">\(p\)</span> predictors.</p></li>
</ul>
<p><span class="math display">\[
Z_m = \sum_{j=1}^p \phi_{mj}X_j
\]</span></p>
<p>      for some constants <span class="math inline">\(\phi_{m1},\ldots,\phi_{mp}\)</span>.</p>
<ul>
<li>We can then fit the linear regression model,</li>
</ul>
<p><span class="math display">\[
y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im}+\epsilon_i, \,\,\, i=1,\ldots,n
\]</span></p>
<p>      using ordinary least squares.</p>
<ul>
<li>Note that in above model, the regression coefficients are given by <span class="math inline">\(\theta_0, \theta_1, \ldots, \theta_M\)</span>.
<ul>
<li>If the constants <span class="math inline">\(\phi_{m1},\ldots, \phi_{mp}\)</span> are chosen wisely, then such dimension reduction approaches can often outperform OLS regression.</li>
</ul></li>
<li>Notice that from definition above,</li>
</ul>
<p><span class="math display">\[
\sum_{m=1}^M \theta_m z_{im} = \sum_{m=1}^M \theta_m \sum_{j=1}^p \phi_{mj}x_{ij}=\sum_{m=1}^M \sum_{j=1}^p \theta_m \phi_{mj}x_{ij}=\sum_{j=1}^p \beta_j x_{ij}
\]</span></p>
<p>      where</p>
<p><span class="math display">\[
\beta_j=\sum_{m=1}^M \theta_m \phi_{mj}
\]</span></p>
<ul>
<li><p>Hence model above can be thought of as a special case of the original linear regression model.</p></li>
<li><p>Dimension reduction serves to constrain the estimated <span class="math inline">\(\beta_j\)</span> coefficients, since now they must take the form later.</p></li>
<li><p>Can win in the bias-variance trade-off.</p></li>
</ul>
<div id="principal-components-regression" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Principal Components Regression<a href="chapter7.html#principal-components-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Here we apply principal components analysis (PCA) to define the linear combinations of the predictors, for use in our regression.</p></li>
<li><p>The first principal component is that (normalized) linear combination of the variables with the largest variance.</p></li>
<li><p>The second principal component has largest variance, subject to being uncorrelated with the first.</p></li>
<li><p>And so on.</p></li>
<li><p>Hence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation.</p></li>
</ul>
<p><img src="fig7/f13.png" /></p>
<ul>
<li>The population size (pop) and ad spending (ad) for 100 different cities are shown as purple circles.
<ul>
<li>The green solid line indicates the first principal component, and the blue dashed line indicates the second principal component.</li>
</ul></li>
</ul>
<p><img src="fig7/f14.png" /></p>
<ul>
<li><p>A subset of the advertising data.</p></li>
<li><p>Left: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green.</p>
<ul>
<li>These distances are represented using the black dashed line segments.</li>
</ul></li>
<li><p>Right: The left-hand panel has been rotated so that the first principal component lies on the <span class="math inline">\(x\)</span>-axis.</p></li>
</ul>
<p><img src="fig7/f15.png" /></p>
<ul>
<li>Plots of the first principal component scores <span class="math inline">\(z_{i1}\)</span> versus pop and ad.
<ul>
<li>The relationships are strong.</li>
</ul></li>
</ul>
<p><img src="fig7/f16.png" /></p>
<ul>
<li>Plots of the second principal component scores <span class="math inline">\(z_{i2}\)</span> versus pop and ad.
<ul>
<li>The relationships are weak.</li>
</ul></li>
</ul>
<div id="application-to-principal-components-regression" class="section level4 unnumbered hasAnchor">
<h4>Application to principal components regression<a href="chapter7.html#application-to-principal-components-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig7/f17.png" /></p>
<ul>
<li>PCR (principal components regression) was applied to two simulated data set.
<ul>
<li>The black, green, and purple lines correspond to squared bias, variance, and test mean squared error, respectively.</li>
<li>Left and right are from simulated data.</li>
</ul></li>
</ul>
<p><img src="fig7/f18.png" /></p>
<ul>
<li><p>Left: PCR standardized coefficient estimates on the Credit data set for different values of M.</p></li>
<li><p>Right: The 10-fold cross validation MSE obtained using PCR, as a function of M.</p></li>
</ul>
</div>
</div>
<div id="example-15" class="section level3 hasAnchor" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Example<a href="chapter7.html#example-15" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Consider again the Hitters data set</li>
</ul>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="chapter7.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb101-2"><a href="chapter7.html#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pls) <span class="co"># includes pricipal component regression and more</span></span>
<span id="cb101-3"><a href="chapter7.html#cb101-3" aria-hidden="true" tabindex="-1"></a>Hitters <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(Hitters) <span class="co"># remove missing values</span></span>
<span id="cb101-4"><a href="chapter7.html#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="co">#help(pcr)</span></span>
<span id="cb101-5"><a href="chapter7.html#cb101-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>) <span class="co"># for exact replication of the results</span></span>
<span id="cb101-6"><a href="chapter7.html#cb101-6" aria-hidden="true" tabindex="-1"></a>pcr.fit <span class="ot">&lt;-</span> <span class="fu">pcr</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">scale =</span> <span class="cn">TRUE</span>, <span class="co"># standardize each predictor</span></span>
<span id="cb101-7"><a href="chapter7.html#cb101-7" aria-hidden="true" tabindex="-1"></a>               <span class="at">validation =</span> <span class="st">&quot;CV&quot;</span>) <span class="co"># use CV to select M PCs default 10 fold</span></span>
<span id="cb101-8"><a href="chapter7.html#cb101-8" aria-hidden="true" tabindex="-1"></a><span class="fu">help</span>(mvrCv) <span class="co"># info about internal CV used by pcr</span></span>
<span id="cb101-9"><a href="chapter7.html#cb101-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pcr.fit) <span class="co"># summary of the CV results RMSEP is the square root of CV MSE</span></span></code></pre></div>
<pre><code>## Data:    X dimension: 263 19 
##  Y dimension: 263 1
## Fit method: svdpc
## Number of components considered: 19
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV             452    351.9    353.2    355.0    352.8    348.4    343.6
## adjCV          452    351.6    352.7    354.4    352.1    347.6    342.7
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV       345.5    347.7    349.6     351.4     352.1     353.5     358.2
## adjCV    344.7    346.7    348.5     350.1     350.7     352.0     356.5
##        14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
## CV        349.7     349.4     339.9     341.6     339.2     339.6
## adjCV     348.0     347.7     338.2     339.7     337.2     337.6
## 
## TRAINING: % variance explained
##         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps
## X         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96
## Salary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75
##         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps
## X         96.28     97.26     97.98     98.65     99.15     99.47     99.75
## Salary    46.86     47.76     47.82     47.85     48.10     50.40     50.55
##         16 comps  17 comps  18 comps  19 comps
## X          99.89     99.97     99.99    100.00
## Salary     53.01     53.85     54.61     54.61</code></pre>
<ul>
<li><p>The RMSEP is square root of CV MSE, and the smallest value is reached with six PCs, which is also shown by the plot of RMSEPs.</p></li>
<li><p>Six components explain 88.63% of the total variation among the predictors.</p></li>
</ul>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="chapter7.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="fu">validationplot</span>(pcr.fit, <span class="at">val.type =</span> <span class="st">&quot;RMSEP&quot;</span>) <span class="co"># Plot RMSEPs</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/pca1-1.png" width="672" /></p>
<ul>
<li><p>Next we will demonstrate in terms MSE how the PCR regression performs by using a test set.</p></li>
<li><p>First define the best number of components from the training set, estimate the corresponding regression, and compute test MSE.</p></li>
</ul>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="chapter7.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># for exact replication</span></span>
<span id="cb104-2"><a href="chapter7.html#cb104-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(Hitters), <span class="at">size =</span> <span class="fu">nrow</span>(Hitters) <span class="sc">/</span> <span class="dv">2</span>) <span class="co"># training set</span></span>
<span id="cb104-3"><a href="chapter7.html#cb104-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="sc">-</span>train <span class="co"># test set resulted by not including those in the train set</span></span>
<span id="cb104-4"><a href="chapter7.html#cb104-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(train) <span class="co"># examples of observations in the training set</span></span></code></pre></div>
<pre><code>## [1] 167 129 187  85  79 213</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="chapter7.html#cb106-1" aria-hidden="true" tabindex="-1"></a>pcr.train <span class="ot">&lt;-</span> <span class="fu">pcr</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">subset =</span> train, <span class="at">scale =</span> <span class="cn">TRUE</span>,</span>
<span id="cb106-2"><a href="chapter7.html#cb106-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">validation =</span> <span class="st">&quot;CV&quot;</span>) <span class="co"># training set</span></span>
<span id="cb106-3"><a href="chapter7.html#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pcr.train) <span class="co"># training results summary</span></span></code></pre></div>
<pre><code>## Data:    X dimension: 131 19 
##  Y dimension: 131 1
## Fit method: svdpc
## Number of components considered: 19
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV           428.3    331.5    335.2    329.2    332.1    326.4    330.0
## adjCV        428.3    330.9    334.4    327.5    330.9    325.1    328.6
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV       334.6    335.3    335.8     337.5     337.7     342.3     340.3
## adjCV    333.0    333.4    333.3     335.0     335.2     339.6     337.6
##        14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
## CV        343.2     346.5     344.2     355.3     361.2     352.5
## adjCV     340.3     343.4     340.9     351.4     356.9     348.3
## 
## TRAINING: % variance explained
##         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps
## X         39.32    61.57    71.96    80.83    85.95    89.99    93.25    95.34
## Salary    43.87    43.93    47.36    47.37    49.52    49.55    49.63    50.98
##         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps
## X         96.55     97.61     98.28     98.85     99.22     99.53     99.79
## Salary    53.00     53.00     53.02     53.05     53.80     53.85     54.03
##         16 comps  17 comps  18 comps  19 comps
## X          99.91     99.97     99.99    100.00
## Salary     55.85     55.89     56.21     58.62</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="chapter7.html#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="fu">validationplot</span>(pcr.train, <span class="at">val.type =</span> <span class="st">&quot;RMSEP&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<ul>
<li>On the basis of the training data CV RMSEP results <span class="math inline">\(M=7\)</span> principal components yields the best results.</li>
</ul>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="chapter7.html#cb109-1" aria-hidden="true" tabindex="-1"></a>pcr.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(pcr.train, Hitters[test, ], <span class="at">ncomp =</span> <span class="dv">7</span>)</span>
<span id="cb109-2"><a href="chapter7.html#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(pcr.pred) <span class="co"># a few first predictions</span></span></code></pre></div>
<pre><code>## , , 7 comps
## 
##                      Salary
## -Alvin Davis       595.5249
## -Andre Dawson     1104.4628
## -Andres Galarraga  454.3308
## -Alfredo Griffin   564.6986
## -Al Newman         160.0433
## -Argenis Salazar    10.8481</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="chapter7.html#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((pcr.pred  <span class="sc">-</span> Hitters<span class="sc">$</span>Salary[test])<span class="sc">^</span><span class="dv">2</span>) <span class="co"># test MSE</span></span></code></pre></div>
<pre><code>## [1] 140751.3</code></pre>
<ul>
<li><p>This test set MSE is slightly smaller that of lasso.</p></li>
<li><p>PCR is useful in prediction purposes.</p></li>
<li><p>If interpretation of the model is needed PCR results may be difficult to interpret.</p></li>
<li><p>Finally, estimating the <span class="math inline">\(M=7\)</span> components from the full data set yields the following results.</p></li>
</ul>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="chapter7.html#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pcr.fit <span class="ot">&lt;-</span> <span class="fu">pcr</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">scale =</span> <span class="cn">TRUE</span>, <span class="at">ncomp =</span> <span class="dv">7</span>)) <span class="co"># estimate and summarize</span></span></code></pre></div>
<pre><code>## Data:    X dimension: 263 19 
##  Y dimension: 263 1
## Fit method: svdpc
## Number of components considered: 7
## TRAINING: % variance explained
##         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X         38.31    60.16    70.84    79.03    84.29    88.63    92.26
## Salary    40.63    41.58    42.17    43.22    44.90    46.48    46.69</code></pre>
</div>
<div id="partial-least-squares" class="section level3 hasAnchor" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Partial Least Squares<a href="chapter7.html#partial-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>PCR identifies linear combinations, or <em>directions</em>, that best represent the predictors <span class="math inline">\(X_1, \ldots, X_p\)</span>.</p></li>
<li><p>These directions are identified in an <em>unsupervised</em> way, since the response <span class="math inline">\(Y\)</span> is not used to help determine the principal component directions.</p></li>
<li><p>That is, the response does not <em>supervised</em> the identification of the principal components.</p></li>
<li><p>Consequently, PCR suffers from a potentially serious drawback.</p>
<ul>
<li>There is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.</li>
</ul></li>
<li><p>Like PCR, PLS is a dimension reduction method, which first identifies a new set of features <span class="math inline">\(Z_1, \ldots, Z_M\)</span> that are linear combinations of the original features, and then fits a linear model via OLS using these M new features.</p></li>
<li><p>But unlike PCR, PLS identifies these new features in a supervised way - that is, it makes use of the response <span class="math inline">\(Y\)</span> in order to identify new features that not only approximate the old features well, but also that <em>are related to the response</em>.</p></li>
<li><p>Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.</p></li>
<li><p>After standardizing the <span class="math inline">\(p\)</span> predictors, PLS computes the first direction <span class="math inline">\(Z_1\)</span> by setting each <span class="math inline">\(\phi_{1j}\)</span> equal to the coefficient from the simple linear regression of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(X_j\)</span>.</p></li>
<li><p>One can show that this coefficient is proportional to the correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_j\)</span>.</p></li>
<li><p>Hence, in computing <span class="math inline">\(Z_1=\sum_{j=1}^p \phi_{1j}X_j\)</span>, PLS places the highest weight on the variables that are most strongly related to the response.</p></li>
<li><p>Subsequent directions are found by taking residuals and then repeating the above prescription.</p></li>
</ul>
<div id="summary" class="section level4 unnumbered hasAnchor">
<h4>Summary<a href="chapter7.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Model selection methods are an essential tool for data analysis, especially for big datasets involving many predictors.</p></li>
<li><p>Research into methods that give sparsity, such as the lasso is an especially hot area.</p></li>
</ul>
</div>
</div>
<div id="example-16" class="section level3 hasAnchor" number="7.3.4">
<h3><span class="header-section-number">7.3.4</span> Example<a href="chapter7.html#example-16" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Continuing the previous examples, we use plsr() function of the pls package.</li>
</ul>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="chapter7.html#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb115-2"><a href="chapter7.html#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pls) <span class="co"># includes pricipal component regression and more</span></span>
<span id="cb115-3"><a href="chapter7.html#cb115-3" aria-hidden="true" tabindex="-1"></a>Hitters <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(Hitters) <span class="co"># remove missing values</span></span>
<span id="cb115-4"><a href="chapter7.html#cb115-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-5"><a href="chapter7.html#cb115-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># initialize again the seed</span></span>
<span id="cb115-6"><a href="chapter7.html#cb115-6" aria-hidden="true" tabindex="-1"></a>pls.train <span class="ot">&lt;-</span> <span class="fu">plsr</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">subset =</span> train, <span class="at">scale =</span> <span class="cn">TRUE</span>,</span>
<span id="cb115-7"><a href="chapter7.html#cb115-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">validation =</span> <span class="st">&quot;CV&quot;</span>)</span>
<span id="cb115-8"><a href="chapter7.html#cb115-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pls.train)</span></code></pre></div>
<pre><code>## Data:    X dimension: 131 19 
##  Y dimension: 131 1
## Fit method: kernelpls
## Number of components considered: 19
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV           428.3    325.5    329.9    328.8    339.0    338.9    340.1
## adjCV        428.3    325.0    328.2    327.2    336.6    336.1    336.6
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV       339.0    347.1    346.4     343.4     341.5     345.4     356.4
## adjCV    336.2    343.4    342.8     340.2     338.3     341.8     351.1
##        14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
## CV        348.4     349.1     350.0     344.2     344.5     345.0
## adjCV     344.2     345.0     345.9     340.4     340.6     341.1
## 
## TRAINING: % variance explained
##         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps
## X         39.13    48.80    60.09    75.07    78.58    81.12    88.21    90.71
## Salary    46.36    50.72    52.23    53.03    54.07    54.77    55.05    55.66
##         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps
## X         93.17     96.05     97.08     97.61     97.97     98.70     99.12
## Salary    55.95     56.12     56.47     56.68     57.37     57.76     58.08
##         16 comps  17 comps  18 comps  19 comps
## X          99.61     99.70     99.95    100.00
## Salary     58.17     58.49     58.56     58.62</code></pre>
<ul>
<li><span class="math inline">\(M=2\)</span> produces the smallest CV RMSEP.</li>
</ul>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="chapter7.html#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="fu">validationplot</span>(pls.train, <span class="at">val.type =</span> <span class="st">&quot;RMSEP&quot;</span>) <span class="co"># plot of CV RMSEP</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/plsplot-1.png" width="672" /></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="chapter7.html#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="do">##dev.print(pdf, file = &quot;ex54.pdf&quot;) # pdf copy</span></span>
<span id="cb118-2"><a href="chapter7.html#cb118-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-3"><a href="chapter7.html#cb118-3" aria-hidden="true" tabindex="-1"></a>pls.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(pls.train, <span class="at">newdata =</span> Hitters[test, ], <span class="at">ncomp =</span> <span class="dv">2</span>)</span>
<span id="cb118-4"><a href="chapter7.html#cb118-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((Hitters<span class="sc">$</span>Salary[test] <span class="sc">-</span> pls.pred)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 145367.7</code></pre>
<ul>
<li>MSE if comparable but slightly higher than that of PCR.</li>
</ul>
<p><br></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter6.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ILR.pdf", "ILR.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
