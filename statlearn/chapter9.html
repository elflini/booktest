<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Tree-based Methods | Statistical Learning</title>
  <meta name="description" content="This is a Statistical Learning" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Tree-based Methods | Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a Statistical Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Tree-based Methods | Statistical Learning" />
  
  <meta name="twitter:description" content="This is a Statistical Learning" />
  

<meta name="author" content="Jin Hyun Nam" />


<meta name="date" content="2023-05-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter8.html"/>
<link rel="next" href="chapter10.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>머리말</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#understanding-data"><i class="fa fa-check"></i><b>1.1</b> Understanding Data</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="chapter1.html"><a href="chapter1.html#example-1-supervised-learning-continuous-output"><i class="fa fa-check"></i><b>1.1.1</b> Example 1 (Supervised learning: Continuous output)</a></li>
<li class="chapter" data-level="1.1.2" data-path="chapter1.html"><a href="chapter1.html#example-2-supervised-leraning-categorical-output"><i class="fa fa-check"></i><b>1.1.2</b> Example 2 (Supervised leraning: Categorical output)</a></li>
<li class="chapter" data-level="1.1.3" data-path="chapter1.html"><a href="chapter1.html#example-3-unsupervised-learning-clustering-observations"><i class="fa fa-check"></i><b>1.1.3</b> Example 3 (Unsupervised learning: Clustering observations)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#brief-history-of-statistical-learning"><i class="fa fa-check"></i><b>1.2</b> Brief History of Statistical Learning</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#notation-and-simple-matrix-algebra"><i class="fa fa-check"></i><b>1.3</b> Notation and Simple Matrix Algebra</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#what-is-statitical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statitical Learning</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="chapter2.html"><a href="chapter2.html#prediction"><i class="fa fa-check"></i><b>2.1.1</b> Prediction</a></li>
<li class="chapter" data-level="2.1.2" data-path="chapter2.html"><a href="chapter2.html#inference"><i class="fa fa-check"></i><b>2.1.2</b> Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="chapter2.html"><a href="chapter2.html#estimating-f"><i class="fa fa-check"></i><b>2.1.3</b> Estimating <span class="math inline">\(f\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="chapter2.html"><a href="chapter2.html#prediction-accuaracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.4</b> Prediction Accuaracy and Model Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter2.html"><a href="chapter2.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="chapter2.html"><a href="chapter2.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="2.2.3" data-path="chapter2.html"><a href="chapter2.html#classification-problems"><i class="fa fa-check"></i><b>2.2.3</b> Classification Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#basic-commands"><i class="fa fa-check"></i><b>3.1</b> Basic Commands</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#graphics"><i class="fa fa-check"></i><b>3.2</b> Graphics</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#indexing-data"><i class="fa fa-check"></i><b>3.3</b> Indexing Data</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#loading-data"><i class="fa fa-check"></i><b>3.4</b> Loading Data</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#additional-graphical-and-numerical-summaries"><i class="fa fa-check"></i><b>3.5</b> Additional Graphical and Numerical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#simple-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Regression</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#multiple-regression"><i class="fa fa-check"></i><b>4.2</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="chapter4.html"><a href="chapter4.html#importance-of-predictors-statistical-significance-of-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Importance of Predictors: Statistical Significance of Coefficients</a></li>
<li class="chapter" data-level="4.2.2" data-path="chapter4.html"><a href="chapter4.html#selecting-important-variables"><i class="fa fa-check"></i><b>4.2.2</b> Selecting Important Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="chapter4.html"><a href="chapter4.html#model-fit"><i class="fa fa-check"></i><b>4.2.3</b> Model Fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="chapter4.html"><a href="chapter4.html#prediction-1"><i class="fa fa-check"></i><b>4.2.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#other-consideration"><i class="fa fa-check"></i><b>4.3</b> Other Consideration</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chapter4.html"><a href="chapter4.html#qualitative-predictors"><i class="fa fa-check"></i><b>4.3.1</b> Qualitative Predictors</a></li>
<li class="chapter" data-level="" data-path="chapter4.html"><a href="chapter4.html#example-8"><i class="fa fa-check"></i>Example 8</a></li>
<li class="chapter" data-level="4.3.2" data-path="chapter4.html"><a href="chapter4.html#potential-problems"><i class="fa fa-check"></i><b>4.3.2</b> Potential Problems</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#non-parametric-regressions"><i class="fa fa-check"></i><b>4.4</b> Non-parametric Regressions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#logit-and-probit-models"><i class="fa fa-check"></i><b>5.1</b> Logit and Probit Models</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#confounding"><i class="fa fa-check"></i><b>5.2</b> Confounding</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#logit-model-for-multiple-classes"><i class="fa fa-check"></i><b>5.3</b> Logit Model for Multiple Classes</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#discriminant-analysis"><i class="fa fa-check"></i><b>5.4</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="chapter5.html"><a href="chapter5.html#bayes-theorem-for-classification"><i class="fa fa-check"></i><b>5.4.1</b> Bayes Theorem for Classification</a></li>
<li class="chapter" data-level="5.4.2" data-path="chapter5.html"><a href="chapter5.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="5.4.3" data-path="chapter5.html"><a href="chapter5.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>5.4.3</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#naive-bayes"><i class="fa fa-check"></i><b>5.5</b> Naive Bayes</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#knn-classification"><i class="fa fa-check"></i><b>5.6</b> KNN Classification</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#example-10"><i class="fa fa-check"></i><b>5.7</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Resampling Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#validation-set-approach"><i class="fa fa-check"></i><b>6.1</b> Validation Set Approach</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.2</b> K-fold Cross-validation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="chapter6.html"><a href="chapter6.html#example-11"><i class="fa fa-check"></i><b>6.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#bootstrap"><i class="fa fa-check"></i><b>6.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Model Selection</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#variable-selection"><i class="fa fa-check"></i><b>7.1</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="chapter7.html"><a href="chapter7.html#best-subset-selection"><i class="fa fa-check"></i><b>7.1.1</b> Best Subset Selection</a></li>
<li class="chapter" data-level="7.1.2" data-path="chapter7.html"><a href="chapter7.html#forward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.2</b> Forward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.3" data-path="chapter7.html"><a href="chapter7.html#backward-step-wise-selection"><i class="fa fa-check"></i><b>7.1.3</b> Backward Step Wise Selection</a></li>
<li class="chapter" data-level="7.1.4" data-path="chapter7.html"><a href="chapter7.html#choosing-the-optimal-model"><i class="fa fa-check"></i><b>7.1.4</b> Choosing the Optimal Model</a></li>
<li class="chapter" data-level="7.1.5" data-path="chapter7.html"><a href="chapter7.html#example-13"><i class="fa fa-check"></i><b>7.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#shrinkage-methods"><i class="fa fa-check"></i><b>7.2</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chapter7.html"><a href="chapter7.html#ridge-regression"><i class="fa fa-check"></i><b>7.2.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.2.2" data-path="chapter7.html"><a href="chapter7.html#rasso-regression"><i class="fa fa-check"></i><b>7.2.2</b> RASSO Regression</a></li>
<li class="chapter" data-level="7.2.3" data-path="chapter7.html"><a href="chapter7.html#example-14"><i class="fa fa-check"></i><b>7.2.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#dimension-reduction-methods"><i class="fa fa-check"></i><b>7.3</b> Dimension Reduction Methods</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="chapter7.html"><a href="chapter7.html#principal-components-regression"><i class="fa fa-check"></i><b>7.3.1</b> Principal Components Regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="chapter7.html"><a href="chapter7.html#example-15"><i class="fa fa-check"></i><b>7.3.2</b> Example</a></li>
<li class="chapter" data-level="7.3.3" data-path="chapter7.html"><a href="chapter7.html#partial-least-squares"><i class="fa fa-check"></i><b>7.3.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="7.3.4" data-path="chapter7.html"><a href="chapter7.html#example-16"><i class="fa fa-check"></i><b>7.3.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#polynomial-regression"><i class="fa fa-check"></i><b>8.1</b> Polynomial Regression</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#step-functions"><i class="fa fa-check"></i><b>8.2</b> Step Functions</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#piecewise-polynomials---splines"><i class="fa fa-check"></i><b>8.3</b> Piecewise Polynomials - Splines</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#local-regression"><i class="fa fa-check"></i><b>8.4</b> Local Regression</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#generalized-additive-models"><i class="fa fa-check"></i><b>8.5</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Tree-based Methods</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#the-basiscs-of-decision-trees"><i class="fa fa-check"></i><b>9.1</b> The Basiscs of Decision Trees</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#classification-trees"><i class="fa fa-check"></i><b>9.2</b> Classification Trees</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#bagging"><i class="fa fa-check"></i><b>9.3</b> Bagging</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#random-forests"><i class="fa fa-check"></i><b>9.4</b> Random Forests</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#boosting"><i class="fa fa-check"></i><b>9.5</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chapter10.html"><a href="chapter10.html"><i class="fa fa-check"></i><b>10</b> Support Vector Machines</a></li>
<li class="chapter" data-level="11" data-path="chapter11.html"><a href="chapter11.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chapter11.html"><a href="chapter11.html#principal-components-anlsysis"><i class="fa fa-check"></i><b>11.1</b> Principal Components Anlsysis</a></li>
<li class="chapter" data-level="11.2" data-path="chapter11.html"><a href="chapter11.html#clustering"><i class="fa fa-check"></i><b>11.2</b> Clustering</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chapter11.html"><a href="chapter11.html#k-means-clustering"><i class="fa fa-check"></i><b>11.2.1</b> <span class="math inline">\(K\)</span>-means clustering</a></li>
<li class="chapter" data-level="11.2.2" data-path="chapter11.html"><a href="chapter11.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.2.2</b> Hierarchical Clustering</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter9" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Tree-based Methods<a href="chapter9.html#chapter9" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li><p>Here we describe <em>tree-based</em> methods of regression and classificaiton.</p></li>
<li><p>These involve <em>stratifying</em> or <em>segmenting</em> the predictor space into a number of simple regions.</p></li>
<li><p>Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches ar known as <em>decision-tree</em> methods.</p></li>
</ul>
<div id="pros-and-cons" class="section level4 unnumbered hasAnchor">
<h4>Pros and Cons<a href="chapter9.html#pros-and-cons" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Tree-based methods are simple and useful for interpretation.</p></li>
<li><p>However they typically are not competitive with the best supervised learning approaches in terms of prediction accuracy.</p></li>
<li><p>Hence we also discuss <em>bagging</em>, <em>random forests</em>, and <em>boosting</em>.</p>
<ul>
<li>These methods grow multiple trees which are then combined to yield a single consensus prediction.</li>
</ul></li>
<li><p>Combining a large numnber of trees can often result in dramatic improvements in prediction accuracy, at the expense of some lose interpretation.</p></li>
</ul>
</div>
<div id="the-basiscs-of-decision-trees" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> The Basiscs of Decision Trees<a href="chapter9.html#the-basiscs-of-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Decision trees can be applied to both regression and classification problems.</p></li>
<li><p>We first consider regression problems, and then move on to classification.</p></li>
</ul>
<div id="baseball-salary-data-how-would-you-stratify-it" class="section level4 unnumbered hasAnchor">
<h4>Baseball salary data: How would you stratify it?<a href="chapter9.html#baseball-salary-data-how-would-you-stratify-it" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Salary is color-coded from low (blue, green) to high (yellow, red)</li>
</ul>
<p><img src="fig9/f1.png" /></p>
<ul>
<li>Decision tree for these data</li>
</ul>
<p><img src="fig9/f2.png" />
- For the Hitters data, a regression tree for predicting the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year.</p>
<ul>
<li>At a given internal node, the label (of the form <span class="math inline">\(X-j&lt;t_k\)</span>) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to <span class="math inline">\(X_j\ge t_k\)</span>.
<ul>
<li>For instance, the split at the top of the tree results in two large branches.</li>
<li>The left-hand branch corresponds to <span class="math inline">\(Years&lt;4.5\)</span>, and the right-hand branch corresponds to <span class="math inline">\(Years\ge 4.5\)</span>.</li>
</ul></li>
<li>The tree has two internal nodes and three terminal nodes, or leaves.
<ul>
<li>The number in each leaf is the mean of the response for the observations that fall there.</li>
</ul></li>
<li>Overall, the tree stratifies or segments the players into three regions of predictor space: <span class="math inline">\(R_1=\{X|Years&lt;4.5\}\)</span>, <span class="math inline">\(R_2 =\{X|Years\ge 4.5, Hits&lt;117.5\}\)</span>, and <span class="math inline">\(R_3=\{X|Years\ge 4.5, Hits\ge 117.5\}\)</span></li>
</ul>
<p><img src="fig9/f3.png" /></p>
</div>
<div id="terminology-for-trees" class="section level4 unnumbered hasAnchor">
<h4>Terminology for trees<a href="chapter9.html#terminology-for-trees" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>In keeping with the <em>tree</em> analogy, the regions <span class="math inline">\(R_1\)</span>, <span class="math inline">\(R_2\)</span>, and <span class="math inline">\(R_3\)</span> are known as <em>terminal nodes</em>.</p></li>
<li><p>Decision trees are typically drawn
8, in the sense that the leaves are at the bottom of the tree.</p></li>
<li><p>The points along the tree where the predictor space is split are referred to as <em>internal nodes</em>.</p></li>
<li><p>In the hitters tree, the two internal nodes are indicated by the test <span class="math inline">\(Years&lt;4.5\)</span> and <span class="math inline">\(Hits&lt;117.5\)</span>.</p></li>
</ul>
</div>
<div id="interpretation-of-results" class="section level4 unnumbered hasAnchor">
<h4>Interpretation of results<a href="chapter9.html#interpretation-of-results" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Years is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players.</p></li>
<li><p>Given that a player is less experienced, the number of Hits that he made in the previous year seems to play little role in his Salary.</p></li>
<li><p>But among players who have been in the major leagues for five or more years, the number of Hits made in the previous year does affect Salary, and Players who made more Hits last year tend to have higher salaries.</p></li>
<li><p>Surely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain.</p></li>
</ul>
</div>
<div id="tree-building-process" class="section level4 unnumbered hasAnchor">
<h4>Tree-building process<a href="chapter9.html#tree-building-process" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p>We divide the predictor space - that is, the set of possible values for <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span> - into <span class="math inline">\(J\)</span> distinct and non-overlapping regions, <span class="math inline">\(R_1,R_2,\ldots, R_J\)</span>.</p></li>
<li><p>For every observation that falls into the region <span class="math inline">\(R_j\)</span>, we make the same prediction, which is simply the mean of the response values for the training observations in <span class="math inline">\(R_j\)</span>.</p></li>
</ol>
<ul>
<li>In theory, the regions could have any shape.
<ul>
<li>However, we choose to divide the predictor space into high-dimensional rectangles, or <em>boxes</em>, for simplicity and for case of interpretation of the resulting predictive model.</li>
</ul></li>
<li>The goal is to find boxes <span class="math inline">\(R_1, \ldots, R_J\)</span> that minimize the RSS, given by</li>
</ul>
<p><span class="math display">\[
\sum_{j=1}^J\sum_{i\in R_j}(y_i-\hat{y}_{R_j})^2
\]</span></p>
<p>      where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean response for the training observations within the <span class="math inline">\(j\)</span>th box.</p>
<ul>
<li><p>Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into <span class="math inline">\(J\)</span> boxes.</p></li>
<li><p>For this reason, we take a <em>top-down</em>, <em>greedy</em> approach that is known as recursive binary splitting.</p></li>
<li><p>The approach is <em>top-down</em> because it begins at the top of the tree and then successively splits the predictor space; each split is indicated via two new branches further down on the tree.</p></li>
<li><p>It is <em>greedy</em> because at each step of the tree-building process, the <em>best</em> split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</p></li>
<li><p>We first select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into the regions <span class="math inline">\(\{X|X_j&lt;s\}\)</span> and <span class="math inline">\(\{X|X_j\ge s\}\)</span> leads to the greatest possible reduction in RSS.</p></li>
<li><p>Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.</p></li>
<li><p>However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions.</p>
<ul>
<li>We now have three regions.</li>
</ul></li>
<li><p>Again, we look to split one of these three regions further, so as to minimize the RSS.</p>
<ul>
<li>The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.</li>
</ul></li>
</ul>
</div>
<div id="predictions" class="section level4 unnumbered hasAnchor">
<h4>Predictions<a href="chapter9.html#predictions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>We predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.</li>
</ul>
<p><img src="fig9/f4.png" /></p>
<ul>
<li><p>Top left: A partition of two-dimensional feature space that could not result from recursive binary splitting.</p></li>
<li><p>Top right: The output of recursive binary splitting on a two-dimensional example.</p></li>
<li><p>Bottom left: A tree corresponding to the partition in the top right panel.</p></li>
<li><p>Bottom right: A perspective plot of the prediction survace corresponding to that tree.</p></li>
</ul>
</div>
<div id="pruning-a-tree" class="section level4 unnumbered hasAnchor">
<h4>Pruning a tree<a href="chapter9.html#pruning-a-tree" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The process described above may produce good predictions on the training set, but is likely to <em>overfit</em> the data, leading to poor test set performance. Why?</p></li>
<li><p>A smaller tree with fewer splits (that is, fewer regions <span class="math inline">\(R_1, \ldots, R_J\)</span>) might lead to lower variance and better interpretation at the cost of a little bias.</p></li>
<li><p>One possible alternative to the process described above is to grow the gree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.</p></li>
<li><p>This strategy will result in smaller trees, but is too <em>short-sighted</em>: a seemingly worthless split early on in the tree might be followed by a very good split - that is, a split that leads to a large reduction in RSS later on.</p></li>
<li><p>A better strategy is to grow a very large tree <span class="math inline">\(T_0\)</span>, and then <em>prune</em> it back in order to obtain a <em>subtree</em>.</p></li>
<li><p><em>Cost complexity pruning</em> - also known as <em>weakest link pruning</em> - is used to do this.</p></li>
<li><p>We consider a sequence of trees indexed by a nonnegative tuning parameter <span class="math inline">\(\alpha\)</span>.</p>
<ul>
<li>For each value of <span class="math inline">\(\alpha\)</span> there corresponds a subtree <span class="math inline">\(T\subset T_0\)</span> such that</li>
</ul></li>
</ul>
<p><span class="math display">\[
\sum_{m=1}^{|T|}\sum_{i:x_i\in R_m} (y_i-\hat{y}_{R_m})^2+\alpha|T|
\]</span></p>
<p>      is as small as possible.</p>
<ul>
<li>Here <span class="math inline">\(|T|\)</span> indicates the number of terminal nodes of the tree <span class="math inline">\(T\)</span>, <span class="math inline">\(R_m\)</span> is the rectanble (i.e. the subset of predictor space) corresponding to the <span class="math inline">\(m\)</span>th terminal node, and <span class="math inline">\(\hat{y}_{R_m}\)</span> is the mean of the training observations in <span class="math inline">\(R_m\)</span>.</li>
</ul>
</div>
<div id="choosing-the-best-subtree" class="section level4 unnumbered hasAnchor">
<h4>Choosing the best subtree<a href="chapter9.html#choosing-the-best-subtree" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The tuning parameter <span class="math inline">\(\alpha\)</span> controls a trade-off between the subtree’s complexity and its fit to the training data.</p></li>
<li><p>We select an optimal value <span class="math inline">\(\hat{\alpha}\)</span> using corss-validation.</p></li>
<li><p>We then return to the full data set and obtain the subtree corresponding to <span class="math inline">\(\hat{\alpha}\)</span></p></li>
</ul>
</div>
<div id="summary-tree-algorithm" class="section level4 unnumbered hasAnchor">
<h4>Summary: tree algorithm<a href="chapter9.html#summary-tree-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p>Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has vewer than some minimum number of observations.</p></li>
<li><p>Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>Use <span class="math inline">\(K\)</span>-fold cross-validation to choose <span class="math inline">\(\alpha\)</span>. For each <span class="math inline">\(k=1,\ldots, K\)</span>:</p></li>
</ol>
<p>      3.1 Repeat steps 1 and 2 on the <span class="math inline">\(\frac{K-1}{K}\)</span>th fraction of the training data, excluding the <span class="math inline">\(k\)</span>th fold.</p>
<p>      3.2 Evaluate the mean squared prediction error on the data in the left-out <span class="math inline">\(k\)</span>th fold, as a function of <span class="math inline">\(\alpha\)</span>.</p>
<p>      Average the results, and pick <span class="math inline">\(\alpha\)</span> to minimize the average error.</p>
<ol start="4" style="list-style-type: decimal">
<li>Return the subtree from Step 2 that corresponds to the chosen value of <span class="math inline">\(\alpha\)</span>.</li>
</ol>
</div>
<div id="bseball-example" class="section level4 unnumbered hasAnchor">
<h4>Bseball example<a href="chapter9.html#bseball-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>First, we randomly divided the data set in half, yielding 132 observations in the training set and 131 observations in the test set.</p></li>
<li><p>We then built a large regression tree on the training data and varied <span class="math inline">\(\alpha\)</span> in order to create subtrees with different numbers of terminal nodes.</p></li>
<li><p>Finally, we performed six-fold cross-validation in order to estimate the cross-validated MSE of the trees as a function of <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
<p><img src="fig9/f5.png" /></p>
<p><img src="fig9/f6.png" /></p>
</div>
<div id="example-17" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter9.html#example-17" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The initial tree for regression tree of log(Salary) on nine background features.</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chapter9.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages(&quot;tree&quot;)</span></span>
<span id="cb1-2"><a href="chapter9.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2) <span class="co"># load ISLR package</span></span>
<span id="cb1-3"><a href="chapter9.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tree) <span class="co"># load tree package</span></span>
<span id="cb1-4"><a href="chapter9.html#cb1-4" aria-hidden="true" tabindex="-1"></a>Hitters <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(Hitters) <span class="co"># remove missing values</span></span>
<span id="cb1-5"><a href="chapter9.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10</span>) <span class="co"># for replication</span></span>
<span id="cb1-6"><a href="chapter9.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(Hitters) <span class="co"># n of observations is 263</span></span></code></pre></div>
<pre><code>## [1] 263</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="chapter9.html#cb3-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(Hitters), <span class="dv">132</span>) <span class="co"># training sample</span></span>
<span id="cb3-2"><a href="chapter9.html#cb3-2" aria-hidden="true" tabindex="-1"></a>tree_salary <span class="ot">&lt;-</span> <span class="fu">tree</span>(<span class="fu">log</span>(Salary) <span class="sc">~</span> AtBat <span class="sc">+</span> Hits <span class="sc">+</span> HmRun <span class="sc">+</span> Runs</span>
<span id="cb3-3"><a href="chapter9.html#cb3-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="sc">+</span> RBI <span class="sc">+</span> Walks <span class="sc">+</span> Years <span class="sc">+</span> PutOuts <span class="sc">+</span> Assists,</span>
<span id="cb3-4"><a href="chapter9.html#cb3-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> Hitters, <span class="at">subset =</span> train) <span class="co"># use nine features for the intial tree</span></span>
<span id="cb3-5"><a href="chapter9.html#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tree_salary) <span class="co"># summary of the results</span></span></code></pre></div>
<pre><code>## 
## Regression tree:
## tree(formula = log(Salary) ~ AtBat + Hits + HmRun + Runs + RBI + 
##     Walks + Years + PutOuts + Assists, data = Hitters, subset = train)
## Variables actually used in tree construction:
## [1] &quot;Years&quot;   &quot;Hits&quot;    &quot;Assists&quot; &quot;RBI&quot;    
## Number of terminal nodes:  8 
## Residual mean deviance:  0.1884 = 23.36 / 124 
## Distribution of residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -1.91600 -0.24250 -0.02596  0.00000  0.29310  1.15500</code></pre>
<ul>
<li>Only 4 out of the initial 9 variables are used in the resulting 8 nodes
initial tree, <span class="math inline">\(T_0\)</span></li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="chapter9.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)) <span class="co"># full plot window</span></span>
<span id="cb5-2"><a href="chapter9.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tree_salary) <span class="co"># plot the initial tree</span></span>
<span id="cb5-3"><a href="chapter9.html#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="at">main =</span> <span class="st">&quot;Initial Regression Tree</span><span class="sc">\n</span><span class="st">of log(Salary) on Selected Features&quot;</span>, <span class="at">cex.main =</span> .<span class="dv">8</span>)</span>
<span id="cb5-4"><a href="chapter9.html#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(tree_salary, <span class="at">cex =</span> .<span class="dv">8</span>, <span class="at">digits =</span> <span class="dv">3</span>) <span class="co"># add values and splitting points</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/ex922-1.png" width="672" /></p>
<ul>
<li>Cross-validation based results:</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="chapter9.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20</span>) <span class="co"># for replication</span></span>
<span id="cb6-2"><a href="chapter9.html#cb6-2" aria-hidden="true" tabindex="-1"></a>cv_salary <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(tree_salary, <span class="at">K =</span> <span class="dv">6</span>) <span class="co"># 6 fold cross-validation, 6 divides 132 exactly</span></span>
<span id="cb6-3"><a href="chapter9.html#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="chapter9.html#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="co"># two regions</span></span>
<span id="cb6-5"><a href="chapter9.html#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv_salary<span class="sc">$</span>size, cv_salary<span class="sc">$</span>dev, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Tree size&quot;</span>, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>,</span>
<span id="cb6-6"><a href="chapter9.html#cb6-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residaul Sum of Squares&quot;</span>,</span>
<span id="cb6-7"><a href="chapter9.html#cb6-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Cross-Validation RSS for the Test Sample&quot;</span>,</span>
<span id="cb6-8"><a href="chapter9.html#cb6-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.main =</span> .<span class="dv">8</span>) <span class="co"># size of the model agains RSS</span></span>
<span id="cb6-9"><a href="chapter9.html#cb6-9" aria-hidden="true" tabindex="-1"></a>min_pos <span class="ot">&lt;-</span> <span class="fu">which.min</span>(cv_salary<span class="sc">$</span>dev) <span class="co"># position of the smallest CV RSS</span></span>
<span id="cb6-10"><a href="chapter9.html#cb6-10" aria-hidden="true" tabindex="-1"></a>cv_salary<span class="sc">$</span>size[min_pos] <span class="co"># tree size of the smallest RSS</span></span></code></pre></div>
<pre><code>## [1] 6</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="chapter9.html#cb8-1" aria-hidden="true" tabindex="-1"></a>cv_salary<span class="sc">$</span>dev[min_pos] <span class="co"># smallest CV RSS</span></span></code></pre></div>
<pre><code>## [1] 42.77326</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="chapter9.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(cv_salary<span class="sc">$</span>size[min_pos], cv_salary<span class="sc">$</span>dev[min_pos],</span>
<span id="cb10-2"><a href="chapter9.html#cb10-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">20</span>) <span class="co"># mark in the plot</span></span>
<span id="cb10-3"><a href="chapter9.html#cb10-3" aria-hidden="true" tabindex="-1"></a>prune_salary <span class="ot">&lt;-</span> <span class="fu">prune.tree</span>(tree_salary, <span class="at">best =</span> <span class="dv">7</span>) <span class="co"># prune the full tree to the best CV result</span></span>
<span id="cb10-4"><a href="chapter9.html#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(prune_salary)</span></code></pre></div>
<pre><code>## 
## Regression tree:
## snip.tree(tree = tree_salary, nodes = 6L)
## Variables actually used in tree construction:
## [1] &quot;Years&quot;   &quot;Hits&quot;    &quot;Assists&quot; &quot;RBI&quot;    
## Number of terminal nodes:  7 
## Residual mean deviance:  0.2005 = 25.07 / 125 
## Distribution of residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -1.91600 -0.20700 -0.01646  0.00000  0.24870  1.15500</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="chapter9.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(prune_salary,  <span class="at">cex.main =</span> .<span class="dv">8</span>)</span>
<span id="cb12-2"><a href="chapter9.html#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(prune_salary, <span class="at">cex =</span> .<span class="dv">8</span>, <span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb12-3"><a href="chapter9.html#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="at">main =</span> <span class="st">&quot;Training Set</span><span class="sc">\n</span><span class="st">Best Cross-Validation Tree&quot;</span>, <span class="at">cex.main =</span> .<span class="dv">8</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex923-1.png" width="672" /></p>
<ul>
<li><p>The tree with six nodes has the smallest CV RSS (or deviance).</p></li>
<li><p>It is interesting to note that the model predicts higher salary for those with lower</p></li>
<li><p>Below are presented test MSEs, cross-validation MSEs, and train sample MSEs for different tree sizes.</p></li>
<li><p>We observe that the minimum test MSE is 4 or 5, while as found above the minimum CV MSE is for tree size 6 (factually all are well within the tolerance limits for tree size 3 [or even 2]).</p></li>
<li><p>As a results it would be wise to check the performance of tree size 3 also.</p></li>
</ul>
<p><img src="ILR_files/figure-html/ex9232-1.png" width="672" /></p>
<ul>
<li><p>Finally it may be interesting to see how the prediction results compare to linear regression.</p></li>
<li><p>Using the training data, we select the best model in terms of the Cp
criterion.</p></li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="chapter9.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps) <span class="co"># regression subsets</span></span>
<span id="cb13-2"><a href="chapter9.html#cb13-2" aria-hidden="true" tabindex="-1"></a>subsets_salary_sum <span class="ot">&lt;-</span> <span class="fu">summary</span>(subsets_salary <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(<span class="fu">log</span>(Salary) <span class="sc">~</span> AtBat <span class="sc">+</span> Hits <span class="sc">+</span> HmRun</span>
<span id="cb13-3"><a href="chapter9.html#cb13-3" aria-hidden="true" tabindex="-1"></a>                                                           <span class="sc">+</span> Runs <span class="sc">+</span> RBI <span class="sc">+</span> Walks <span class="sc">+</span> Years</span>
<span id="cb13-4"><a href="chapter9.html#cb13-4" aria-hidden="true" tabindex="-1"></a>                                                           <span class="sc">+</span> PutOuts <span class="sc">+</span> Assists,</span>
<span id="cb13-5"><a href="chapter9.html#cb13-5" aria-hidden="true" tabindex="-1"></a>                                                           <span class="at">data =</span> Hitters, <span class="at">subset =</span> train))</span>
<span id="cb13-6"><a href="chapter9.html#cb13-6" aria-hidden="true" tabindex="-1"></a>best_cp <span class="ot">&lt;-</span> <span class="fu">which.min</span>(subsets_salary_sum<span class="sc">$</span>cp) <span class="co"># best model by the CP criterion</span></span>
<span id="cb13-7"><a href="chapter9.html#cb13-7" aria-hidden="true" tabindex="-1"></a>(fml <span class="ot">&lt;-</span> <span class="fu">formula</span>(<span class="fu">paste</span>(<span class="st">&quot;log(Salary) ~ &quot;</span>, <span class="fu">paste</span>(<span class="fu">names</span>(<span class="fu">coef</span>(subsets_salary, best_cp))[<span class="sc">-</span><span class="dv">1</span>],</span>
<span id="cb13-8"><a href="chapter9.html#cb13-8" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">collapse =</span> <span class="st">&quot;+&quot;</span>)))) <span class="co"># formula for lm()</span></span></code></pre></div>
<pre><code>## log(Salary) ~ Hits + Walks + Years</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="chapter9.html#cb15-1" aria-hidden="true" tabindex="-1"></a>lm_train <span class="ot">&lt;-</span> <span class="fu">lm</span>(fml, <span class="at">data =</span> Hitters, <span class="at">subset =</span> train) <span class="co"># train data regression fit of the best model</span></span>
<span id="cb15-2"><a href="chapter9.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm_train) <span class="co"># results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fml, data = Hitters, subset = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.12308 -0.39870  0.01469  0.42299  1.29067 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 3.982820   0.149021  26.727  &lt; 2e-16 ***
## Hits        0.007128   0.001333   5.348 3.95e-07 ***
## Walks       0.007926   0.003033   2.613   0.0101 *  
## Years       0.112590   0.011859   9.494  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5853 on 128 degrees of freedom
## Multiple R-squared:  0.5968, Adjusted R-squared:  0.5874 
## F-statistic: 63.16 on 3 and 128 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="chapter9.html#cb17-1" aria-hidden="true" tabindex="-1"></a>lm_test <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm_train, <span class="at">newdata =</span> Hitters[<span class="sc">-</span>train, ]) <span class="co"># test set predictions</span></span>
<span id="cb17-2"><a href="chapter9.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((lm_test <span class="sc">-</span> <span class="fu">log</span>(Hitters<span class="sc">$</span>Salary[<span class="sc">-</span>train]))<span class="sc">^</span><span class="dv">2</span>) <span class="co"># test MSE</span></span></code></pre></div>
<pre><code>## [1] 0.4792482</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="chapter9.html#cb19-1" aria-hidden="true" tabindex="-1"></a>reg_tree_test <span class="ot">&lt;-</span> <span class="fu">predict</span>(prune_salary, <span class="at">newdata =</span> Hitters[<span class="sc">-</span>train, ]) <span class="co"># predictions</span></span>
<span id="cb19-2"><a href="chapter9.html#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((reg_tree_test <span class="sc">-</span> <span class="fu">log</span>(Hitters<span class="sc">$</span>Salary[<span class="sc">-</span>train]))<span class="sc">^</span><span class="dv">2</span>) <span class="co"># reg tree test MSE</span></span></code></pre></div>
<pre><code>## [1] 0.4257519</code></pre>
<ul>
<li>Here the regression tree produces better results in terms of MSE.</li>
</ul>
</div>
</div>
<div id="classification-trees" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Classification Trees<a href="chapter9.html#classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.</p></li>
<li><p>For a classification tree, we predict that each observation belongs to the <em>most commonly occurring class</em> of training observations in the region to which it belongs.</p></li>
<li><p>Just as in the regression setting, we use recursive binary splitting to grow a classification tree.</p></li>
<li><p>In the classification setting, RSS cannot be used as criterion for making the binary splits.</p></li>
<li><p>A natural alternative to RSS is the <em>classification error rate</em>. This is simply the fraction of the training observations in that region that do not belong to the most common class:</p></li>
</ul>
<p><span class="math display">\[
E=1-max_k (\hat{p}_{mk})
\]</span></p>
<ul>
<li><p>Here <span class="math inline">\(\hat{p}_{mk}\)</span> represents the proportion of training observations in the <span class="math inline">\(m\)</span>th region that are from the <span class="math inline">\(k\)</span>th class.</p></li>
<li><p>However classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable.</p></li>
</ul>
<div id="gini-index-and-deviance" class="section level4 unnumbered hasAnchor">
<h4>Gini index and Deviance<a href="chapter9.html#gini-index-and-deviance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The <em>Gini index</em> is defined by</li>
</ul>
<p><span class="math display">\[
G=\sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})
\]</span></p>
<p>      a measure of total variance across the <span class="math inline">\(K\)</span> classes.</p>
<ul>
<li><p>The Gini index takes on a small value if all of the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are close to zero or one.</p></li>
<li><p>For this reason the Gini index is referred to as a measure of node <em>purity</em> - a small value indicates that a node contains predominantly observations from a single class.</p></li>
<li><p>An alternative to the Gini index is <em>cross-entropy</em>, given by</p></li>
</ul>
<p><span class="math display">\[
D=-\sum_{k=1}^K\hat{p}_{mk}log\hat{p}_{mk}
\]</span></p>
<ul>
<li>It turns out that the Gini index and the cross-entropy are very similar numerically.</li>
</ul>
</div>
<div id="example-heart-data" class="section level4 unnumbered hasAnchor">
<h4>Example: heart data<a href="chapter9.html#example-heart-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>These data contain a binary outcome HD for 303 patients who presented with chest pain.</p></li>
<li><p>An outcome value of Yes indicates the presence of heart disease based on an angiographic test, while No means no heart disease.</p></li>
<li><p>There are 13 predictors including Age, Sex, Chol ( a cholesterol measurement), and other heart and lung function measurements.</p></li>
<li><p>Cross-validation yields a tree with six terminal nodes.</p></li>
</ul>
<p><img src="fig9/f7.png" /></p>
</div>
<div id="trees-versus-linear-models" class="section level4 unnumbered hasAnchor">
<h4>Trees Versus linear models<a href="chapter9.html#trees-versus-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig9/f8.png" /></p>
<ul>
<li>Top row: true linear boundary</li>
<li>Bottom row: true non-linear boundary</li>
<li>Left column: linear model</li>
<li>Right column: tree-based model</li>
</ul>
</div>
<div id="advantages-and-disadvantages-of-trees" class="section level4 unnumbered hasAnchor">
<h4>Advantages and Disadvantages of trees<a href="chapter9.html#advantages-and-disadvantages-of-trees" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Trees are very easy to explain to people.</p>
<ul>
<li>In fact, they are even easier to explain than linear regression!</li>
</ul></li>
<li><p>Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches.</p></li>
<li><p>Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).</p></li>
<li><p>Trees can easily handle qualitative predictors without the need to create dummy variables.</p></li>
<li><p>Unfortunately, trees generally do not haave the same level of predictive accuracy as some of the other regression and classification approaches.</p></li>
<li><p>However, by aggregating many decision trees, the predictive performance of trees can be substantially improved.</p></li>
</ul>
</div>
</div>
<div id="bagging" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Bagging<a href="chapter9.html#bagging" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><em>Bootstrap aggregation</em>, or <em>bagging</em>, is a general-pirpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.</p></li>
<li><p>Recall that given a set of <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(Z_1, \ldots, Z_n\)</span>, each with variance <span class="math inline">\(\sigma^2\)</span>, the variance of the mean <span class="math inline">\(\bar{Z}\)</span> of the observations is given by <span class="math inline">\(\sigma^2/n\)</span>.</p></li>
<li><p>In other words, <em>averaging a set of observations reduces variance</em>.</p>
<ul>
<li>Of course, this is not practical because we generally do not have access to multiple training sets.</li>
</ul></li>
<li><p>Instead, we can bootstrap, by taking repeated samples from the (single) training data set.</p></li>
<li><p>In this approach we generate <span class="math inline">\(B\)</span> different bootstrapped training data sets.</p>
<ul>
<li>We then train our method on the <span class="math inline">\(b\)</span>th bootstrapped training set in order to get <span class="math inline">\(\hat{f}^{*b}(x)\)</span>, the prediction at a point <span class="math inline">\(x\)</span>.</li>
<li>We then average all the predictions to obtain</li>
</ul></li>
</ul>
<p><span class="math display">\[
\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B \hat{f}^{*b}(x)
\]</span></p>
<ul>
<li>This is called <em>bagging</em>.</li>
</ul>
<div id="bagging-classification-trees" class="section level4 unnumbered hasAnchor">
<h4>Bagging classification trees<a href="chapter9.html#bagging-classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The above prescription applied to regression trees.</p></li>
<li><p>For classification trees: for each test observation, we record the class predicted by each of the <span class="math inline">\(B\)</span> trees, and take a <em>majority vote</em>: the overall prediction is the most commonly occurring class among the <span class="math inline">\(B\)</span> predictions.</p></li>
</ul>
</div>
<div id="bagging-the-heart-data" class="section level4 unnumbered hasAnchor">
<h4>Bagging the heart data<a href="chapter9.html#bagging-the-heart-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><img src="fig9/f9.png" /></p>
<ul>
<li>Bagging and random forest results for the Heart data.
<ul>
<li>The test error (black and orange) is shown as a function of <span class="math inline">\(B\)</span>, the number of bootstrapped training sets used.</li>
<li>Random forests were applied with <span class="math inline">\(m=\sqrt{p}\)</span>.</li>
<li>The dashed line indicates the test error resulting from a single classification tree.</li>
<li>The green and blue traces show the OOB error, which in this case is considerably lower.</li>
</ul></li>
</ul>
</div>
<div id="out-of-bag-error-estimation" class="section level4 unnumbered hasAnchor">
<h4>Out-of-bag error estimation<a href="chapter9.html#out-of-bag-error-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>It turns out that there is a very straightforward way to estimate the test error of a bagged model.</p></li>
<li><p>Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations.</p>
<ul>
<li>One can show that on average, each bagged tree makes use of around two-thirds of the observations.</li>
</ul></li>
<li><p>The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations.</p></li>
<li><p>We can predict the response for the <span class="math inline">\(i\)</span>th observation using each of the trees in which that observation was OOB.</p>
<ul>
<li>This will yield around <span class="math inline">\(B/3\)</span> predictions for the <span class="math inline">\(i\)</span>th observation, which we average.</li>
</ul></li>
<li><p>This estimate is essentially the LOO cross-validation error for bagging, if <span class="math inline">\(B\)</span> is large.</p></li>
</ul>
</div>
</div>
<div id="random-forests" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Random Forests<a href="chapter9.html#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><em>Random forests</em> provide an improvement over bagged trees by way of a small tweak that decorrelates the trees.</p>
<ul>
<li>This reduces the variance when we average the trees.</li>
</ul></li>
<li><p>As in bagging, we build a number of decision trees on bootstrapped training samples.</p></li>
<li><p>But when building these decision trees, each time a split in a tree is considered, <em>a random selection of <span class="math inline">\(m\)</span> predictors</em> is chosen as split candidates from then full set of <span class="math inline">\(p\)</span> predictors.</p>
<ul>
<li>The split is allowed to use only one of those <span class="math inline">\(m\)</span> predictors.</li>
</ul></li>
<li><p>A fresh selection of <span class="math inline">\(m\)</span> predictors is taken at each split, and typically we choose <span class="math inline">\(m\approx \sqrt{p}\)</span> - that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors (4 out of the 13 for the Heart data)</p></li>
</ul>
<div id="example-gene-expression-data" class="section level4 unnumbered hasAnchor">
<h4>Example: gene expression data<a href="chapter9.html#example-gene-expression-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>We applied random forests to a high-dimensional biological data set consisting of expression measurements of 4,718 genes measured on tissue samples from 349 patients.</p></li>
<li><p>There are around 20,000 genes in humans, and individual genes have different levels of activity, or expression, in particular cells, tissues, and biological conditions.</p></li>
<li><p>Each of the patient samples has a qualitative label with 15 different levels: either normal or one of 14 different types of cancer.</p></li>
<li><p>We use random forests to predict cancer type based on the 500 genes that have the largest variance in the training set.</p></li>
<li><p>We randomly divided the observations into a training and a test set, and applied random forests to the training set ofr three different values of the number of splitting variables <span class="math inline">\(m\)</span>.</p></li>
<li><p>Result</p></li>
</ul>
<p><img src="fig9/f10.png" /></p>
<ul>
<li><p>Results from random forests for the fifteen-class gene expression data set with <span class="math inline">\(p=500\)</span> predictors.</p></li>
<li><p>The test error is displayed as a function of the number of trees.</p>
<ul>
<li>Each colored line corresponds to a different value of <span class="math inline">\(m\)</span>, the number of predictors available for splitting at each interior tree node.</li>
</ul></li>
<li><p>Random forests (<span class="math inline">\(m&lt;p\)</span>) lead to a slight improvement over bagging (<span class="math inline">\(m=p\)</span>).</p>
<ul>
<li>A single classification tree has an error rate of <span class="math inline">\(45.7\%\)</span>.</li>
</ul></li>
</ul>
</div>
</div>
<div id="boosting" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Boosting<a href="chapter9.html#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification.</p>
<ul>
<li>We only discuss boosting for decision trees.</li>
</ul></li>
<li><p>Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.</p></li>
<li><p>Notably, each tree is built on a bootstrap data set, independent of the other trees.</p></li>
<li><p>Boosting works in a similar way, except that the trees are grown <em>sequentially</em>: each tree is grown using information from previously grown trees.</p></li>
</ul>
<div id="boosting-algorithm-for-regression-trees" class="section level4 unnumbered hasAnchor">
<h4>Boosting algorithm for regression trees<a href="chapter9.html#boosting-algorithm-for-regression-trees" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p>Set <span class="math inline">\(\hat{f}(x)=0\)</span> and <span class="math inline">\(r_i=y_i\)</span> for all <span class="math inline">\(i\)</span> in the training set.</p></li>
<li><p>For <span class="math inline">\(b=1,2,\ldots, B\)</span>, repeat:</p></li>
</ol>
<p>      2.1 Fit a tree <span class="math inline">\(\hat{f}^b\)</span> with <span class="math inline">\(d\)</span> splits (<span class="math inline">\(d+1\)</span> terminal nodes) to the training data <span class="math inline">\((X,r)\)</span>.</p>
<p>      2.2 Update <span class="math inline">\(\hat{f}\)</span> by adding in a shrunken version of the new tree:</p>
<p><span class="math display">\[
\hat{f}(x) \leftarrow \hat{f}(x) +\lambda \hat{f}^b (x)
\]</span></p>
<p>      2.3 Update the residuals,</p>
<p><span class="math display">\[
r_i \leftarrow r_i-\lambda \hat{f}^b (x)
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Output the boosted model,</li>
</ol>
<p><span class="math display">\[
\hat{f}(x)=\sum_{b=1}^B \lambda\hat{f}^b(x)
\]</span></p>
<ul>
<li><p>Unlike fitting a single large decision tree to the data, which amounts to <em>fitting the data hard</em> and potentially overfitting, the boosting approach instead <em>learns slowly</em>.</p></li>
<li><p>Given the current model, we fit a decision tree to the residuals from the model.</p>
<ul>
<li>We then add this new decision tree into the fitted functio nin order to update the residuals.</li>
</ul></li>
<li><p>Each of thses trees can be rather small, with just a few terminal nodes, determined by the parameter <span class="math inline">\(d\)</span> in the algorithm.</p></li>
<li><p>By fitting small trees to the residuals, we slowly improve <span class="math inline">\(\hat{f}\)</span> in areas where it does not perform well.</p>
<ul>
<li>The shrinkage parameter <span class="math inline">\(\lambda\)</span> slows the process down even further, allowing more and different shaped treees to attack the residuals.</li>
</ul></li>
</ul>
</div>
<div id="boosting-for-classification" class="section level4 unnumbered hasAnchor">
<h4>Boosting for classification<a href="chapter9.html#boosting-for-classification" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Boosting for classification is similar in spirit to boosting for regression, but is a bit more complex.</p></li>
<li><p>The R package gbm (gradient boosted models) handles a variety of regression and classification problems.</p></li>
</ul>
<p><img src="fig9/f11.png" /></p>
<ul>
<li><p>Results from performing boosting and random forests on the fifteen-class gene expression data set in order to predict cancer versus normal.</p></li>
<li><p>The test error is displayed as a function of the number of trees.</p>
<ul>
<li>For the two boosted models, <span class="math inline">\(\lambda=0.01\)</span>.</li>
<li>Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around <span class="math inline">\(0.02\)</span>, making none of these differences significant.</li>
</ul></li>
<li><p>The test error rate for a single tree is <span class="math inline">\(24\%\)</span>.</p></li>
</ul>
</div>
<div id="tuning-parameters-for-boosting" class="section level4 unnumbered hasAnchor">
<h4>Tuning parameters for boosting<a href="chapter9.html#tuning-parameters-for-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p>The <em>number of trees</em> <span class="math inline">\(B\)</span>. Unlike bagging and random forests, boosting can overfit if <span class="math inline">\(B\)</span> is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select <span class="math inline">\(B\)</span>.</p></li>
<li><p>The <em>shrinkage parameter</em> <span class="math inline">\(\lambda\)</span>, a small positive number. This controls the rate at which boosting learns. Typical values are <span class="math inline">\(0.01\)</span> or <span class="math inline">\(0.001\)</span>, and the right choice can depend on the problem. Very small <span class="math inline">\(\lambda\)</span> can require using a very large value of <span class="math inline">\(B\)</span> in order to achieve good performance.</p></li>
<li><p>The <em>number of splits</em> <span class="math inline">\(d\)</span> in each tree, which controls the complexity of the boosted ensemble. Often <span class="math inline">\(d=1\)</span> works well, in which case each tree is a <em>stump</em>, consisting of a single split and resulting in an additive model. More generally <span class="math inline">\(d\)</span> is the <em>interaction depth</em>, and controls the interaction order of the boosted model, since <span class="math inline">\(d\)</span> splits can involve at most <span class="math inline">\(d\)</span> variables.</p></li>
</ol>
<ul>
<li>Another regression example</li>
</ul>
<p><img src="fig9/f12.png" /></p>
<p><img src="fig9/f13.png" /></p>
</div>
<div id="variable-importance-measure" class="section level4 unnumbered hasAnchor">
<h4>Variable importance measure<a href="chapter9.html#variable-importance-measure" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>For bagged/RF regression trees, we record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all <span class="math inline">\(B\)</span> trees.
<ul>
<li>A large value indicates an important predictor.</li>
</ul></li>
<li>Similarly, for bagged/RF classification trees, we add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all <span class="math inline">\(B\)</span> trees.</li>
</ul>
<p><img src="fig9/f14.png" /></p>
</div>
<div id="summary-1" class="section level4 unnumbered hasAnchor">
<h4>Summary<a href="chapter9.html#summary-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>Decision trees are simple and interpretable models for regression and classification.</p></li>
<li><p>However they are often not competitive with other methods in terms of prediction accuracy.</p></li>
<li><p>Bagging, random forests and boosting are good methods for improving the prediction accuracy of trees.</p>
<ul>
<li>They work by growing many trees on the training data and then combining the predictions of the resulting ensemble of trees.</li>
</ul></li>
<li><p>The latter two methods - random forests and boosting - are among the state-of-the-art methods for supervised learning.</p>
<ul>
<li>However their results can be difficult to interpret.</li>
</ul></li>
</ul>
</div>
<div id="example-18" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="chapter9.html#example-18" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p>The R package randomForest can be used for bagging and random forest
(random forest is bagging when <span class="math inline">\(m=p\)</span>).</p></li>
<li><p>Housing prices in Boston (data set in MASS library).</p></li>
</ul>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="chapter9.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS) <span class="co"># MASS library include Boston data</span></span></code></pre></div>
<pre><code>## 
## 다음의 패키지를 부착합니다: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ISLR2&#39;:
## 
##     Boston</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="chapter9.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span></code></pre></div>
<pre><code>## randomForest 4.7-1.1</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="chapter9.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">#library(help = randomForest)</span></span>
<span id="cb27-2"><a href="chapter9.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co">#for replication</span></span>
<span id="cb27-3"><a href="chapter9.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(Boston) <span class="co"># variables in Boston</span></span></code></pre></div>
<pre><code>##  [1] &quot;crim&quot;    &quot;zn&quot;      &quot;indus&quot;   &quot;chas&quot;    &quot;nox&quot;     &quot;rm&quot;      &quot;age&quot;    
##  [8] &quot;dis&quot;     &quot;rad&quot;     &quot;tax&quot;     &quot;ptratio&quot; &quot;black&quot;   &quot;lstat&quot;   &quot;medv&quot;</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="chapter9.html#cb29-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(Boston), <span class="fu">nrow</span>(Boston) <span class="sc">/</span> <span class="dv">2</span>) <span class="co"># training sample</span></span>
<span id="cb29-2"><a href="chapter9.html#cb29-2" aria-hidden="true" tabindex="-1"></a>bag.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston, <span class="at">subset =</span> train,</span>
<span id="cb29-3"><a href="chapter9.html#cb29-3" aria-hidden="true" tabindex="-1"></a>                           <span class="at">mtry =</span> <span class="dv">13</span>, <span class="co"># force all variables, which implie bagging</span></span>
<span id="cb29-4"><a href="chapter9.html#cb29-4" aria-hidden="true" tabindex="-1"></a>                           <span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-5"><a href="chapter9.html#cb29-5" aria-hidden="true" tabindex="-1"></a>bag.boston <span class="co"># print results</span></span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = medv ~ ., data = Boston, mtry = 13, importance = TRUE,      subset = train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 13
## 
##           Mean of squared residuals: 11.33119
##                     % Var explained: 85.26</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="chapter9.html#cb31-1" aria-hidden="true" tabindex="-1"></a>yhat.bag <span class="ot">&lt;-</span> <span class="fu">predict</span>(bag.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ]) <span class="co"># predictions</span></span>
<span id="cb31-2"><a href="chapter9.html#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(yhat.bag, Boston<span class="sc">$</span>medv[<span class="sc">-</span>train], <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>),</span>
<span id="cb31-3"><a href="chapter9.html#cb31-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Predicted Housing Prices&quot;</span>,</span>
<span id="cb31-4"><a href="chapter9.html#cb31-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Realized Housing Prices&quot;</span>, <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>) <span class="co"># scatter plot </span></span>
<span id="cb31-5"><a href="chapter9.html#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex9312-1.png" width="672" /></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="chapter9.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>((yhat.bag <span class="sc">-</span> Boston<span class="sc">$</span>medv[<span class="sc">-</span>train])<span class="sc">^</span><span class="dv">2</span>), <span class="dv">2</span>) <span class="co"># MSE</span></span></code></pre></div>
<pre><code>## [1] 23.46</code></pre>
<ul>
<li>Fit next random forest with <span class="math inline">\(m=6\)</span> (R default is <span class="math inline">\(p/3\)</span> for regression trees and <span class="math inline">\(\sqrt{p}\)</span> for classification trees).</li>
</ul>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="chapter9.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb34-2"><a href="chapter9.html#cb34-2" aria-hidden="true" tabindex="-1"></a>rf.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston, <span class="at">subset =</span> train,</span>
<span id="cb34-3"><a href="chapter9.html#cb34-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">6</span>, <span class="at">imprtance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb34-4"><a href="chapter9.html#cb34-4" aria-hidden="true" tabindex="-1"></a>yhat.rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ])</span>
<span id="cb34-5"><a href="chapter9.html#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>((yhat.rf <span class="sc">-</span> Boston<span class="sc">$</span>medv[<span class="sc">-</span>train])<span class="sc">^</span><span class="dv">2</span>), <span class="dv">2</span>) <span class="co"># MSE</span></span></code></pre></div>
<pre><code>## [1] 19.48</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="chapter9.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">importance</span>(rf.boston), <span class="dv">2</span>) <span class="co"># importance of predictors</span></span></code></pre></div>
<pre><code>##         IncNodePurity
## crim          1000.86
## zn             114.67
## indus          651.89
## chas            57.05
## nox            708.44
## rm            7836.65
## age            649.43
## dis            723.22
## rad             80.62
## tax            358.12
## ptratio        835.19
## black          267.00
## lstat         5997.96</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="chapter9.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf.boston, <span class="at">main =</span> <span class="st">&quot;Variable Importance&quot;</span>) <span class="co"># plor of importance measure</span></span></code></pre></div>
<p><img src="ILR_files/figure-html/ex9321-1.png" width="672" /></p>
<ul>
<li>Boosted regression trees can be fitted by gbm package using gbm() function.</li>
</ul>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="chapter9.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boost.boston)</span></code></pre></div>
<p><img src="ILR_files/figure-html/ex9332-1.png" width="672" /></p>
<pre><code>##             var    rel.inf
## rm           rm 43.9919329
## lstat     lstat 33.1216941
## crim       crim  4.2604167
## dis         dis  4.0111090
## nox         nox  3.4353017
## black     black  2.8267554
## age         age  2.6113938
## ptratio ptratio  2.5403035
## tax         tax  1.4565654
## indus     indus  0.8008740
## rad         rad  0.6546400
## zn           zn  0.1446149
## chas       chas  0.1443986</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="chapter9.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="do">## prediction performance</span></span>
<span id="cb41-2"><a href="chapter9.html#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="chapter9.html#cb41-3" aria-hidden="true" tabindex="-1"></a>yhat.boost <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ], <span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb41-4"><a href="chapter9.html#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((yhat.boost <span class="sc">-</span> Boston<span class="sc">$</span>medv[<span class="sc">-</span>train])<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 18.84709</code></pre>
<ul>
<li>Thus, here boosting with test <span class="math inline">\(MSE = 11.19\)</span> outperforms both bagging (test <span class="math inline">\(MSE = 22.74\)</span>) and random forest (test <span class="math inline">\(MSE = 21.89\)</span>).</li>
</ul>
<p><br></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter8.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter10.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ILR.pdf", "ILR.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
